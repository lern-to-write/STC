#!/usr/bin/env python3
"""
è§†é¢‘é—®ç­”(Video QA)è¯„æµ‹è„šæœ¬
ç”¨äºåˆ†ææ¨¡å‹è¾“å‡ºç»“æœå¹¶ç”Ÿæˆè¯¦ç»†çš„è¯„æµ‹æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•ï¼š
    python evaluate_results.py --result_file results.csv
    python evaluate_results.py --result_dir results/batch_20251010_155403
    
"""

import os
import pandas as pd
import numpy as np
import argparse
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional
import matplotlib.pyplot as plt
import seaborn as sns


class VideoQAEvaluator:
    """è§†é¢‘é—®ç­”è¯„æµ‹å™¨"""
    
    # ç­”æ¡ˆç´¢å¼•åˆ°é€‰é¡¹å­—æ¯çš„æ˜ å°„
    INDEX_TO_CHOICE = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E'}
    CHOICE_TO_INDEX = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}
    
    def __init__(self, result_file: str):
        """
        åˆå§‹åŒ–è¯„æµ‹å™¨
        
        Args:
            result_file: ç»“æœCSVæ–‡ä»¶è·¯å¾„
        """
        self.result_file = result_file
        self.df = None
        self.metrics = {}
        
        # åŠ è½½æ•°æ®
        self._load_data()
    
    def _load_data(self):
        """åŠ è½½ç»“æœæ•°æ®"""
        print(f"ğŸ“‚ åŠ è½½ç»“æœæ–‡ä»¶: {self.result_file}")
        
        if not os.path.exists(self.result_file):
            raise FileNotFoundError(f"ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {self.result_file}")
        
        self.df = pd.read_csv(self.result_file)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(self.df)} æ¡æ•°æ®")
        
        # æ£€æŸ¥å¿…éœ€çš„åˆ—
        required_columns = ['video_id', 'question', 'answer', 'pred_choice', 'qa_acc']
        missing_columns = [col for col in required_columns if col not in self.df.columns]
        
        if missing_columns:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_columns}")
        
        print(f"ğŸ“Š æ•°æ®åˆ—: {list(self.df.columns)}")
    
    def calculate_metrics(self) -> Dict:
        """
        è®¡ç®—å„ç§è¯„æµ‹æŒ‡æ ‡
        
        Returns:
            åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å­—å…¸
        """
        print("\n" + "="*60)
        print("ğŸ” è®¡ç®—è¯„æµ‹æŒ‡æ ‡...")
        print("="*60)
        
        # åŸºç¡€ç»Ÿè®¡
        total_samples = len(self.df)
        correct_samples = (self.df['qa_acc'] == 1.0).sum()
        accuracy = self.df['qa_acc'].mean() * 100
        
        self.metrics['basic'] = {
            'total_samples': total_samples,
            'correct_samples': int(correct_samples),
            'wrong_samples': int(total_samples - correct_samples),
            'accuracy': accuracy,
            'error_rate': 100 - accuracy
        }
        
        # æŒ‰è§†é¢‘ç»Ÿè®¡ï¼ˆå¦‚æœæœ‰å¤šä¸ªè§†é¢‘ï¼‰
        video_stats = self.df.groupby('video_id').agg({
            'qa_acc': ['count', 'sum', 'mean']
        }).round(4)
        
        self.metrics['per_video'] = video_stats
        
        # ç­”æ¡ˆåˆ†å¸ƒåˆ†æ - è½¬æ¢ä¸ºå­—æ¯
        if 'answer' in self.df.columns:
            # å°†ç´¢å¼•è½¬æ¢ä¸ºå­—æ¯
            answer_letters = self.df['answer'].map(self.INDEX_TO_CHOICE)
            answer_dist = answer_letters.value_counts().sort_index()
            self.metrics['answer_distribution'] = answer_dist.to_dict()
        
        # é¢„æµ‹ç­”æ¡ˆåˆ†å¸ƒåˆ†æ
        if 'pred_choice' in self.df.columns:
            pred_dist = self.df['pred_choice'].value_counts().sort_index()
            self.metrics['pred_distribution'] = pred_dist.to_dict()
        
        # æ··æ·†çŸ©é˜µ - æ­£ç¡®ç­”æ¡ˆ vs é¢„æµ‹ç­”æ¡ˆ
        if 'answer' in self.df.columns and 'pred_choice' in self.df.columns:
            confusion = pd.crosstab(
                self.df['answer'].map(self.INDEX_TO_CHOICE),
                self.df['pred_choice'],
                rownames=['Ground Truth'],
                colnames=['Predicted']
            )
            self.metrics['confusion_matrix'] = confusion.to_dict()
        
        # é…ç½®å‚æ•°ç»Ÿè®¡
        if 'retrieve_size' in self.df.columns:
            self.metrics['config'] = {
                'retrieve_size': self.df['retrieve_size'].iloc[0] if len(self.df) > 0 else None,
                'chunk_size': self.df['chunk_size'].iloc[0] if 'chunk_size' in self.df.columns and len(self.df) > 0 else None
            }
        
        return self.metrics
    
    def print_summary(self):
        """æ‰“å°è¯„æµ‹æ‘˜è¦"""
        if not self.metrics:
            self.calculate_metrics()
        
        basic = self.metrics['basic']
        
        print("\n" + "="*60)
        print("ğŸ“Š è¯„æµ‹ç»“æœæ‘˜è¦")
        print("="*60)
        print(f"æ€»æ ·æœ¬æ•°:        {basic['total_samples']}")
        print(f"æ­£ç¡®æ•°é‡:        {basic['correct_samples']} âœ…")
        print(f"é”™è¯¯æ•°é‡:        {basic['wrong_samples']} âŒ")
        print(f"å‡†ç¡®ç‡:          {basic['accuracy']:.2f}%")
        print(f"é”™è¯¯ç‡:          {basic['error_rate']:.2f}%")
        print("="*60)
        
        # é…ç½®ä¿¡æ¯
        if 'config' in self.metrics and self.metrics['config']['retrieve_size']:
            print(f"\nğŸ“ é…ç½®å‚æ•°:")
            print(f"æ£€ç´¢å¤§å° (retrieve_size): {self.metrics['config']['retrieve_size']}")
            if self.metrics['config']['chunk_size']:
                print(f"å—å¤§å° (chunk_size): {self.metrics['config']['chunk_size']}")
        
        # ç­”æ¡ˆåˆ†å¸ƒ
        if 'answer_distribution' in self.metrics:
            print(f"\nğŸ“ˆ æ­£ç¡®ç­”æ¡ˆåˆ†å¸ƒ:")
            for ans, count in sorted(self.metrics['answer_distribution'].items()):
                percentage = (count / basic['total_samples']) * 100
                print(f"  é€‰é¡¹ {ans}: {count} æ¬¡ ({percentage:.1f}%)")
        
        # é¢„æµ‹åˆ†å¸ƒ
        if 'pred_distribution' in self.metrics:
            print(f"\nğŸ¯ æ¨¡å‹é¢„æµ‹åˆ†å¸ƒ:")
            for choice, count in sorted(self.metrics['pred_distribution'].items()):
                percentage = (count / basic['total_samples']) * 100
                print(f"  é€‰é¡¹ {choice}: {count} æ¬¡ ({percentage:.1f}%)")
        
        # æ··æ·†çŸ©é˜µ
        if 'confusion_matrix' in self.metrics:
            print(f"\nğŸ”€ æ··æ·†çŸ©é˜µ (Ground Truth vs Predicted):")
            confusion_df = pd.DataFrame(self.metrics['confusion_matrix']).fillna(0).astype(int)
            print(confusion_df.to_string())
    
    def analyze_errors(self, top_n: int = 10) -> pd.DataFrame:
        """
        åˆ†æé”™è¯¯æ ·æœ¬
        
        Args:
            top_n: æ˜¾ç¤ºå‰Nä¸ªé”™è¯¯æ ·æœ¬
            
        Returns:
            é”™è¯¯æ ·æœ¬çš„DataFrame
        """
        print(f"\nğŸ” åˆ†æé”™è¯¯æ ·æœ¬ (æ˜¾ç¤ºå‰{top_n}ä¸ª)...")
        print("="*60)
        
        # è·å–é”™è¯¯æ ·æœ¬
        error_df = self.df[self.df['qa_acc'] == 0.0].copy()
        
        if len(error_df) == 0:
            print("ğŸ‰ æ²¡æœ‰é”™è¯¯æ ·æœ¬ï¼æ‰€æœ‰é¢„æµ‹éƒ½æ­£ç¡®ï¼")
            return error_df
        
        print(f"æ€»é”™è¯¯æ•°: {len(error_df)}\n")
        
        # æ˜¾ç¤ºå‰Nä¸ªé”™è¯¯
        for i, (idx, row) in enumerate(error_df.head(top_n).iterrows(), 1):
            print(f"é”™è¯¯æ ·æœ¬ #{i}")
            print(f"  è§†é¢‘ID: {row['video_id']}")
            print(f"  é—®é¢˜: {row['question'][:100]}...")  # åªæ˜¾ç¤ºå‰100ä¸ªå­—ç¬¦
            
            # è½¬æ¢ç´¢å¼•ä¸ºå­—æ¯
            correct_letter = self.INDEX_TO_CHOICE.get(row['answer'], '?')
            correct_text = str(row.get('correct_choice', 'N/A'))
            
            print(f"  æ­£ç¡®ç­”æ¡ˆ: {correct_letter}) {correct_text[:80]}...")
            
            # é¢„æµ‹ç­”æ¡ˆ
            pred_letter = str(row.get('pred_choice', '?'))
            pred_text = str(row.get('pred_answer', 'N/A'))
            print(f"  æ¨¡å‹é¢„æµ‹: {pred_letter}) {pred_text[:80]}...")
            print()
        
        # é”™è¯¯åˆ†æç»Ÿè®¡
        print("\nğŸ“Š é”™è¯¯åˆ†æç»Ÿè®¡:")
        
        # ç»Ÿè®¡æ¯ä¸ªæ­£ç¡®ç­”æ¡ˆçš„é”™è¯¯ç‡
        if 'answer' in error_df.columns:
            error_by_answer = error_df['answer'].map(self.INDEX_TO_CHOICE).value_counts().sort_index()
            total_by_answer = self.df['answer'].map(self.INDEX_TO_CHOICE).value_counts().sort_index()
            
            print("\nå„é€‰é¡¹çš„é”™è¯¯åˆ†å¸ƒ:")
            for choice in sorted(set(list(error_by_answer.index) + list(total_by_answer.index))):
                errors = error_by_answer.get(choice, 0)
                total = total_by_answer.get(choice, 0)
                error_rate = (errors / total * 100) if total > 0 else 0
                print(f"  æ­£ç¡®ç­”æ¡ˆä¸º{choice}: {errors}/{total} é”™è¯¯ ({error_rate:.1f}%)")
        
        # ç»Ÿè®¡æœ€å¸¸è§çš„é”™è¯¯é¢„æµ‹
        if 'pred_choice' in error_df.columns:
            print("\né”™è¯¯æ ·æœ¬ä¸­æœ€å¸¸è§çš„é¢„æµ‹:")
            pred_counts = error_df['pred_choice'].value_counts().head(5)
            for pred, count in pred_counts.items():
                percentage = (count / len(error_df)) * 100
                print(f"  é¢„æµ‹{pred}: {count} æ¬¡ ({percentage:.1f}%)")
        
        return error_df
    
    def save_detailed_report(self, output_dir: Optional[str] = None):
        """
        ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        
        Args:
            output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸ç»“æœæ–‡ä»¶åŒç›®å½•
        """
        if output_dir is None:
            output_dir = os.path.dirname(self.result_file)
        
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # 1. ä¿å­˜JSONæ ¼å¼çš„æŒ‡æ ‡
        metrics_file = os.path.join(output_dir, f'metrics_{timestamp}.json')
        
        # è½¬æ¢ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
        json_metrics = {}
        for key, value in self.metrics.items():
            if key == 'per_video':
                # å°†å¤šçº§ç´¢å¼•çš„ DataFrame è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
                per_video_dict = {}
                for video_id, row in value.iterrows():
                    per_video_dict[str(video_id)] = {
                        'count': int(row[('qa_acc', 'count')]),
                        'correct': int(row[('qa_acc', 'sum')]),
                        'accuracy': float(row[('qa_acc', 'mean')])
                    }
                json_metrics[key] = per_video_dict
            elif key == 'confusion_matrix':
                # ç¡®ä¿æ··æ·†çŸ©é˜µçš„é”®éƒ½æ˜¯å­—ç¬¦ä¸²
                if isinstance(value, dict):
                    json_metrics[key] = {str(k): v for k, v in value.items()}
                else:
                    json_metrics[key] = value
            else:
                json_metrics[key] = value
        
        with open(metrics_file, 'w', encoding='utf-8') as f:
            json.dump(json_metrics, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ æŒ‡æ ‡å·²ä¿å­˜: {metrics_file}")
        
        # 2. ä¿å­˜Markdownæ ¼å¼çš„æŠ¥å‘Š
        report_file = os.path.join(output_dir, f'evaluation_report_{timestamp}.md')
        self._generate_markdown_report(report_file)
        print(f"ğŸ“ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # 3. ä¿å­˜é”™è¯¯æ ·æœ¬
        error_df = self.df[self.df['qa_acc'] == 0.0]
        if len(error_df) > 0:
            error_file = os.path.join(output_dir, f'error_samples_{timestamp}.csv')
            error_df.to_csv(error_file, index=False)
            print(f"âŒ é”™è¯¯æ ·æœ¬å·²ä¿å­˜: {error_file}")
    def _generate_markdown_report(self, output_file: str):
        """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        basic = self.metrics['basic']
        
        md_content = f"""# ğŸ“Š è§†é¢‘é—®ç­”è¯„æµ‹æŠ¥å‘Š

    **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
    **ç»“æœæ–‡ä»¶**: `{self.result_file}`

    ---

    ## 1. æ€»ä½“è¯„æµ‹ç»“æœ

    | æŒ‡æ ‡ | æ•°å€¼ |
    |------|------|
    | æ€»æ ·æœ¬æ•° | {basic['total_samples']} |
    | æ­£ç¡®æ•°é‡ | {basic['correct_samples']} âœ… |
    | é”™è¯¯æ•°é‡ | {basic['wrong_samples']} âŒ |
    | **å‡†ç¡®ç‡** | **{basic['accuracy']:.2f}%** |
    | é”™è¯¯ç‡ | {basic['error_rate']:.2f}% |

    ---

    ## 2. é…ç½®å‚æ•°

    """
        
        if 'config' in self.metrics:
            config = self.metrics['config']
            md_content += f"- **æ£€ç´¢å¤§å° (retrieve_size)**: {config.get('retrieve_size', 'N/A')}\n"
            md_content += f"- **å—å¤§å° (chunk_size)**: {config.get('chunk_size', 'N/A')}\n"
        
        md_content += "\n---\n\n## 3. ç­”æ¡ˆåˆ†å¸ƒåˆ†æ\n\n"
        
        # æ­£ç¡®ç­”æ¡ˆåˆ†å¸ƒ
        if 'answer_distribution' in self.metrics:
            md_content += "### 3.1 æ­£ç¡®ç­”æ¡ˆåˆ†å¸ƒ\n\n"
            md_content += "| é€‰é¡¹ | å‡ºç°æ¬¡æ•° | å æ¯” |\n"
            md_content += "|------|---------|------|\n"
            
            for ans, count in sorted(self.metrics['answer_distribution'].items()):
                percentage = (count / basic['total_samples']) * 100
                md_content += f"| {ans} | {count} | {percentage:.1f}% |\n"
        
        # é¢„æµ‹åˆ†å¸ƒ
        if 'pred_distribution' in self.metrics:
            md_content += "\n### 3.2 æ¨¡å‹é¢„æµ‹åˆ†å¸ƒ\n\n"
            md_content += "| é¢„æµ‹é€‰é¡¹ | æ¬¡æ•° | å æ¯” |\n"
            md_content += "|---------|------|------|\n"
            
            for choice, count in sorted(self.metrics['pred_distribution'].items()):
                percentage = (count / basic['total_samples']) * 100
                md_content += f"| {choice} | {count} | {percentage:.1f}% |\n"
        
        # æ··æ·†çŸ©é˜µ
        if 'confusion_matrix' in self.metrics:
            md_content += "\n### 3.3 æ··æ·†çŸ©é˜µ (Ground Truth vs Predicted)\n\n"
            try:
                confusion_df = pd.DataFrame(self.metrics['confusion_matrix']).fillna(0).astype(int)
                # ç¡®ä¿è¡Œå’Œåˆ—éƒ½æŒ‰å­—æ¯é¡ºåºæ’åˆ—
                all_choices = sorted(set(list(confusion_df.index) + list(confusion_df.columns)))
                confusion_df = confusion_df.reindex(index=all_choices, columns=all_choices, fill_value=0)
                md_content += confusion_df.to_markdown() + "\n"
            except Exception as e:
                md_content += f"æ— æ³•ç”Ÿæˆæ··æ·†çŸ©é˜µ: {e}\n"
        
        # æ¯ä¸ªè§†é¢‘çš„ç»Ÿè®¡
        if 'per_video' in self.metrics and len(self.metrics['per_video']) > 1:
            md_content += "\n---\n\n## 4. æŒ‰è§†é¢‘ç»Ÿè®¡\n\n"
            md_content += "| è§†é¢‘ID | æ ·æœ¬æ•° | æ­£ç¡®æ•° | å‡†ç¡®ç‡ |\n"
            md_content += "|--------|--------|--------|--------|\n"
            
            for video_id, row in self.metrics['per_video'].iterrows():
                count = int(row[('qa_acc', 'count')])
                correct = int(row[('qa_acc', 'sum')])
                acc = row[('qa_acc', 'mean')] * 100
                video_id_short = str(video_id)[:30] + "..." if len(str(video_id)) > 30 else str(video_id)
                md_content += f"| {video_id_short} | {count} | {correct} | {acc:.2f}% |\n"
        
        md_content += "\n---\n\n## 5. æ€§èƒ½åˆ†æ\n\n"
        
        if basic['accuracy'] >= 80:
            md_content += "âœ… **ä¼˜ç§€**: æ¨¡å‹è¡¨ç°å‡ºè‰²ï¼Œå‡†ç¡®ç‡è¶…è¿‡80%\n"
        elif basic['accuracy'] >= 60:
            md_content += "âš ï¸ **è‰¯å¥½**: æ¨¡å‹è¡¨ç°å°šå¯ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´\n"
        elif basic['accuracy'] >= 40:
            md_content += "âš ï¸ **ä¸€èˆ¬**: æ¨¡å‹è¡¨ç°ä¸­ç­‰ï¼Œéœ€è¦ä¼˜åŒ–\n"
        elif basic['accuracy'] > 0:
            md_content += "âŒ **è¾ƒå·®**: æ¨¡å‹è¡¨ç°è¾ƒå·®ï¼Œéœ€è¦é‡å¤§æ”¹è¿›\n"
        else:
            md_content += "ğŸš¨ **å®Œå…¨å¤±è´¥**: æ‰€æœ‰é¢„æµ‹éƒ½é”™è¯¯ï¼è¯·æ£€æŸ¥:\n"
            md_content += "   - æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®\n"
            md_content += "   - ç­”æ¡ˆç´¢å¼•æ˜¯å¦å¯¹é½\n"
            md_content += "   - æ¨¡å‹è¾“å‡ºæ˜¯å¦æœ‰æ•ˆ\n"
        
        md_content += f"\n### æ”¹è¿›å»ºè®®\n\n"
        
        if basic['accuracy'] == 0:
            md_content += "ğŸš¨ **ç´§æ€¥**: æ¨¡å‹å®Œå…¨æ²¡æœ‰é¢„æµ‹æ­£ç¡®ï¼Œè¯·ç«‹å³æ£€æŸ¥:\n"
            md_content += "1. æ£€æŸ¥ç­”æ¡ˆæ ¼å¼å’Œç´¢å¼•æ˜¯å¦æ­£ç¡®å¯¹é½\n"
            md_content += "2. éªŒè¯æ¨¡å‹è¾“å‡ºæ ¼å¼æ˜¯å¦ç¬¦åˆé¢„æœŸ\n"
            md_content += "3. æ£€æŸ¥æ•°æ®é¢„å¤„ç†æµç¨‹æ˜¯å¦æœ‰è¯¯\n"
            md_content += "4. ç¡®è®¤è¯„æµ‹è„šæœ¬çš„é€»è¾‘æ˜¯å¦æ­£ç¡®\n"
        elif basic['error_rate'] > 50:
            md_content += "1. æ£€æŸ¥æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæ•°æ®è´¨é‡\n"
            md_content += "2. è€ƒè™‘å¢åŠ è®­ç»ƒæ•°æ®æˆ–æ”¹è¿›æ•°æ®å¢å¼ºç­–ç•¥\n"
            md_content += "3. è°ƒæ•´è¶…å‚æ•°æˆ–è®­ç»ƒç­–ç•¥\n"
        elif basic['error_rate'] > 20:
            md_content += "1. åˆ†æé”™è¯¯æ ·æœ¬ï¼Œæ‰¾å‡ºæ¨¡å‹çš„è–„å¼±ç¯èŠ‚\n"
            md_content += "2. è€ƒè™‘é’ˆå¯¹æ€§åœ°æ”¹è¿›æ¨¡å‹æˆ–æ•°æ®\n"
            md_content += "3. å¯ä»¥å°è¯•é›†æˆå­¦ä¹ æˆ–æ¨¡å‹èåˆ\n"
        else:
            md_content += "1. ç»§ç»­ä¿æŒå½“å‰ç­–ç•¥\n"
            md_content += "2. å¯ä»¥å°è¯•æ›´å¤æ‚çš„åœºæ™¯æˆ–æ•°æ®é›†\n"
            md_content += "3. è€ƒè™‘æ¨¡å‹å‹ç¼©å’Œæ•ˆç‡ä¼˜åŒ–\n"
        
        # ä¿å­˜æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(md_content)
    
    def visualize_results(self, output_dir: Optional[str] = None, show: bool = False):
        """
        å¯è§†åŒ–è¯„æµ‹ç»“æœ
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            show: æ˜¯å¦æ˜¾ç¤ºå›¾è¡¨
        """
        if output_dir is None:
            output_dir = os.path.dirname(self.result_file)
        
        os.makedirs(output_dir, exist_ok=True)
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle('Video QA Evaluation Results', fontsize=16, fontweight='bold')
        
        basic = self.metrics['basic']
        
        # 1. å‡†ç¡®ç‡é¥¼å›¾
        ax1 = axes[0, 0]
        sizes = [basic['correct_samples'], basic['wrong_samples']]
        labels = ['Correct', 'Wrong']
        colors = ['#4CAF50', '#F44336']
        explode = (0.1, 0)
        
        ax1.pie(sizes, explode=explode, labels=labels, colors=colors,
                autopct='%1.1f%%', shadow=True, startangle=90)
        ax1.set_title(f"Accuracy: {basic['accuracy']:.2f}%")
        
        # 2. ç­”æ¡ˆåˆ†å¸ƒæŸ±çŠ¶å›¾
        ax2 = axes[0, 1]
        if 'answer_distribution' in self.metrics:
            ans_dist = self.metrics['answer_distribution']
            choices = sorted(ans_dist.keys())
            counts = [ans_dist[c] for c in choices]
            ax2.bar(choices, counts, color='#2196F3')
            ax2.set_xlabel('Answer Choice')
            ax2.set_ylabel('Count')
            ax2.set_title('Ground Truth Answer Distribution')
            ax2.grid(axis='y', alpha=0.3)
        
        # 3. é¢„æµ‹åˆ†å¸ƒæŸ±çŠ¶å›¾
        ax3 = axes[1, 0]
        if 'pred_distribution' in self.metrics:
            pred_dist = self.metrics['pred_distribution']
            choices = sorted(pred_dist.keys())
            counts = [pred_dist[c] for c in choices]
            ax3.bar(choices, counts, color='#FF9800')
            ax3.set_xlabel('Predicted Choice')
            ax3.set_ylabel('Count')
            ax3.set_title('Model Prediction Distribution')
            ax3.grid(axis='y', alpha=0.3)
        
        # 4. ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬
        ax4 = axes[1, 1]
        ax4.axis('off')
        
        stats_text = f"""
Total Samples: {basic['total_samples']}
Correct: {basic['correct_samples']}
Wrong: {basic['wrong_samples']}
Accuracy: {basic['accuracy']:.2f}%
Error Rate: {basic['error_rate']:.2f}%
        """
        
        if 'config' in self.metrics:
            config = self.metrics['config']
            stats_text += f"""
Retrieve Size: {config.get('retrieve_size', 'N/A')}
Chunk Size: {config.get('chunk_size', 'N/A')}
            """
        
        ax4.text(0.1, 0.5, stats_text, fontsize=12, verticalalignment='center',
                fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        plot_file = os.path.join(output_dir, f'evaluation_plot_{timestamp}.png')
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        print(f"ğŸ“ˆ å›¾è¡¨å·²ä¿å­˜: {plot_file}")
        
        if show:
            plt.show()
        else:
            plt.close()


def compare_experiments(result_files: List[str], output_dir: str):
    """
    æ¯”è¾ƒå¤šä¸ªå®éªŒç»“æœ
    
    Args:
        result_files: ç»“æœæ–‡ä»¶åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
    """
    print("\n" + "="*60)
    print("ğŸ”„ æ¯”è¾ƒå¤šä¸ªå®éªŒç»“æœ...")
    print("="*60)
    
    results = []
    
    for result_file in result_files:
        try:
            evaluator = VideoQAEvaluator(result_file)
            evaluator.calculate_metrics()
            
            exp_name = Path(result_file).parent.name
            results.append({
                'experiment': exp_name,
                'file': result_file,
                'metrics': evaluator.metrics['basic']
            })
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½ {result_file}: {e}")
    
    if not results:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„ç»“æœæ–‡ä»¶")
        return
    
    # åˆ›å»ºå¯¹æ¯”è¡¨
    comparison_df = pd.DataFrame([
        {
            'Experiment': r['experiment'],
            'Total': r['metrics']['total_samples'],
            'Correct': r['metrics']['correct_samples'],
            'Wrong': r['metrics']['wrong_samples'],
            'Accuracy (%)': f"{r['metrics']['accuracy']:.2f}",
            'Error Rate (%)': f"{r['metrics']['error_rate']:.2f}"
        }
        for r in results
    ])
    
    print("\nğŸ“Š å®éªŒå¯¹æ¯”:")
    print(comparison_df.to_string(index=False))
    
    # ä¿å­˜å¯¹æ¯”æŠ¥å‘Š
    os.makedirs(output_dir, exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    comparison_file = os.path.join(output_dir, f'comparison_{timestamp}.csv')
    comparison_df.to_csv(comparison_file, index=False)
    print(f"\nğŸ’¾ å¯¹æ¯”ç»“æœå·²ä¿å­˜: {comparison_file}")
    
    # ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
    if len(results) > 1:
        fig, ax = plt.subplots(figsize=(12, 6))
        
        experiments = [r['experiment'] for r in results]
        accuracies = [r['metrics']['accuracy'] for r in results]
        
        bars = ax.bar(experiments, accuracies, color='#4CAF50')
        ax.set_ylabel('Accuracy (%)', fontsize=12)
        ax.set_title('Experiment Comparison', fontsize=14, fontweight='bold')
        ax.set_ylim(0, 100)
        ax.grid(axis='y', alpha=0.3)
        
        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.1f}%', ha='center', va='bottom', fontsize=10)
        
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        
        plot_file = os.path.join(output_dir, f'comparison_plot_{timestamp}.png')
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        print(f"ğŸ“ˆ å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {plot_file}")
        plt.close()


def evaluate_batch_results(batch_dir: str):
    """
    è¯„æµ‹æ‰¹é‡å®éªŒç»“æœ
    
    Args:
        batch_dir: æ‰¹é‡å®éªŒç»“æœç›®å½•ï¼ˆåŒ…å«å¤šä¸ªå­ç›®å½•ï¼‰
    """
    print(f"\nğŸ” æ‰«ææ‰¹é‡å®éªŒç›®å½•: {batch_dir}")
    
    result_files = []
    
    # é€’å½’æŸ¥æ‰¾æ‰€æœ‰results.csvæ–‡ä»¶
    for root, dirs, files in os.walk(batch_dir):
        for file in files:
            if file == 'results.csv':
                result_file = os.path.join(root, file)
                result_files.append(result_file)
    
    if not result_files:
        print(f"âŒ åœ¨ {batch_dir} ä¸­æœªæ‰¾åˆ°results.csvæ–‡ä»¶")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(result_files)} ä¸ªç»“æœæ–‡ä»¶\n")
    
    # è¯„æµ‹æ¯ä¸ªå®éªŒ
    for result_file in result_files:
        print("\n" + "="*80)
        exp_dir = os.path.dirname(result_file)
        exp_name = os.path.basename(exp_dir)
        print(f"ğŸ“Š è¯„æµ‹å®éªŒ: {exp_name}")
        print("="*80)
        
        try:
            evaluator = VideoQAEvaluator(result_file)
            evaluator.calculate_metrics()
            evaluator.print_summary()
            evaluator.analyze_errors(top_n=3)
            evaluator.save_detailed_report(output_dir=exp_dir)
            # evaluator.visualize_results(output_dir=exp_dir)  # å¯é€‰ï¼šç”Ÿæˆå›¾è¡¨
        except Exception as e:
            print(f"âŒ è¯„æµ‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # å¯¹æ¯”æ‰€æœ‰å®éªŒ
    if len(result_files) > 1:
        compare_experiments(result_files, batch_dir)


def main():
    parser = argparse.ArgumentParser(description='è§†é¢‘é—®ç­”è¯„æµ‹è„šæœ¬')
    parser.add_argument('--result_file', type=str, help='å•ä¸ªç»“æœCSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--result_dir', type=str, help='æ‰¹é‡ç»“æœç›®å½•è·¯å¾„')
    parser.add_argument('--output_dir', type=str, help='è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--visualize', action='store_true', help='ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨')
    parser.add_argument('--show_plot', action='store_true', help='æ˜¾ç¤ºå›¾è¡¨')
    parser.add_argument('--top_errors', type=int, default=10, help='æ˜¾ç¤ºçš„é”™è¯¯æ ·æœ¬æ•°é‡')
    
    args = parser.parse_args()
    
    if args.result_dir:
        # æ‰¹é‡è¯„æµ‹æ¨¡å¼
        evaluate_batch_results(args.result_dir)
    
    elif args.result_file:
        # å•æ–‡ä»¶è¯„æµ‹æ¨¡å¼
        print("="*60)
        print("ğŸš€ å¼€å§‹è¯„æµ‹...")
        print("="*60)
        
        evaluator = VideoQAEvaluator(args.result_file)
        evaluator.calculate_metrics()
        evaluator.print_summary()
        evaluator.analyze_errors(top_n=args.top_errors)
        
        output_dir = args.output_dir if args.output_dir else os.path.dirname(args.result_file)
        evaluator.save_detailed_report(output_dir=output_dir)
        
        if args.visualize:
            evaluator.visualize_results(output_dir=output_dir, show=args.show_plot)
        
        print("\n" + "="*60)
        print("âœ… è¯„æµ‹å®Œæˆ!")
        print("="*60)
    
    else:
        print("âŒ è¯·æŒ‡å®š --result_file æˆ– --result_dir å‚æ•°")
        parser.print_help()


if __name__ == "__main__":
    main()