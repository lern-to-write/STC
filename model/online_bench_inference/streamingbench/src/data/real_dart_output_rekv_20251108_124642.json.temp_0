[
    {
        "time": "[0:00:00 - 0:00:10]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions of the individual taken just now?",
                "time_stamp": "00:00:31",
                "answer": "C",
                "options": [
                    "A. The individual organized serving trays and sanitized the preparation area.",
                    "B. The individual retrieved produce from the refrigerator and began chopping vegetables.",
                    "C. The individual replenished bread inventory by placing new buns onto the shelves.",
                    "D. The individual checked inventory levels and noted items lacking in stock."
                ],
                "required_ability": "episodic memory",
                "rekv": " C"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_348_real.mp4"
    },
    {
        "time": "[0:03:17 - 0:03:27]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions of the individual taken just now?",
                "time_stamp": "00:03:26",
                "answer": "A",
                "options": [
                    "A. The individual prepared a burger by retrieving buns, and placing them on the package.",
                    "B. The individual organized utensils by sanitizing them and placing them back in their designated areas.",
                    "C. The individual cleaned the preparation area and refilled condiment dispensers.",
                    "D. The individual baked a batch of fresh buns and arranged them in an orderly manner."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_348_real.mp4"
    },
    {
        "time": "[0:06:34 - 0:06:44]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions of the individual taken just now?",
                "time_stamp": "00:06:43",
                "answer": "B",
                "options": [
                    "A. The individual arranged fresh vegetables on plates and prepared salads.",
                    "B. The individual topped burger buns with ketchup, onions.",
                    "C. The individual arranged freshly cut fruits into serving containers.",
                    "D. The individual prepared sandwiches by arranging cheese and lettuce on bread slices."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_348_real.mp4"
    },
    {
        "time": "[0:09:51 - 0:10:01]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions of the individual taken just now?",
                "time_stamp": "00:10:07",
                "answer": "B",
                "options": [
                    "A. The individual toasted the buns and added ketchup and mustard.",
                    "B. The individual placed two buns in boxes, added cheese slices, and added condiments.",
                    "C. The individual grilled burger patties and placed them on the buns with lettuce and pickles.",
                    "D. The individual wrapped sandwiches with paper and handed them to the customer."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_348_real.mp4"
    },
    {
        "time": "[0:13:08 - 0:13:18]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions of the individual taken just now?",
                "time_stamp": "00:13:22",
                "answer": "C",
                "options": [
                    "A. The individual selected burger buns, added patties, and placed them on a griddle.",
                    "B. The individual retrieved burger buns, added lettuce and sauces, and placed them on a tray.",
                    "C. The individual took burger buns, added lettuce and condiments, and arranged them on a preparation area.",
                    "D. The individual cleaned the preparation area, organized serving trays, and put away ingredients."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_348_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:20]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What can be seen in the cockpit's center screen right now?",
                "time_stamp": "00:00:06",
                "answer": "B",
                "options": [
                    "A. A compass.",
                    "B. A map.",
                    "C. An altitude meter.",
                    "D. A weather radar."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_416_real.mp4"
    },
    {
        "time": "[0:02:23 - 0:02:43]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What colors are the pilot's handheld controls right now?",
                "time_stamp": "00:02:37",
                "answer": "A",
                "options": [
                    "A. Black.",
                    "B. Red and green.",
                    "C. Yellow and blue.",
                    "D. Black and orange."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_416_real.mp4"
    },
    {
        "time": "[0:04:46 - 0:05:06]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What are the visible weather conditions right now?",
                "time_stamp": "00:05:01",
                "answer": "A",
                "options": [
                    "A. Clear sky with scattered clouds.",
                    "B. Overcast sky with rain.",
                    "C. Foggy with low visibility.",
                    "D. Thunderstorms with lightning."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_416_real.mp4"
    },
    {
        "time": "[0:07:09 - 0:07:29]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What are colors of the curved flight path right now?",
                "time_stamp": "00:07:15",
                "answer": "D",
                "options": [
                    "A. Yellow and blue.",
                    "B. Blue and white.",
                    "C. Green and ograne.",
                    "D. Red and blue."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_416_real.mp4"
    },
    {
        "time": "[0:09:32 - 0:09:52]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is located to the bottom-left of the cockpit's central control panel right now?",
                "time_stamp": "00:09:33",
                "answer": "A",
                "options": [
                    "A. A blue handle.",
                    "B. A red button.",
                    "C. An altitude meter.",
                    "D. A compass."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_416_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:30]",
        "questions": [
            {
                "task_type": "Prospective Reasoning",
                "question": "What might the speaker explain next?",
                "time_stamp": "00:00:11",
                "answer": "D",
                "options": [
                    "A. The area of different shapes.",
                    "B. How to measure the diameter.",
                    "C. The concept of length in 2-dimensional shapes.",
                    "D. How to calculate perimeter."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "math tutorials",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_221_real.mp4"
    },
    {
        "time": "[0:01:32 - 0:02:02]",
        "questions": [
            {
                "task_type": "Prospective Reasoning",
                "question": "What will the speaker most likely explain next?",
                "time_stamp": "00:02:13",
                "answer": "A",
                "options": [
                    "A. How to calculate the perimeter of the shape.",
                    "B. How to convert meters into centimeters.",
                    "C. The differences between perimeter and area.",
                    "D. How to measure the perimeter accurately."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "math tutorials",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_221_real.mp4"
    },
    {
        "time": "[0:03:04 - 0:03:34]",
        "questions": [
            {
                "task_type": "Prospective Reasoning",
                "question": "What might the speaker explain next?",
                "time_stamp": "00:03:33",
                "answer": "B",
                "options": [
                    "A. How to calculate the area of the rectangle.",
                    "B. How to find the perimeter of a rectangle.",
                    "C. How to label the units.",
                    "D. How to measure with a ruler."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "math tutorials",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_221_real.mp4"
    },
    {
        "time": "[0:04:36 - 0:05:06]",
        "questions": [
            {
                "task_type": "Prospective Reasoning",
                "question": "What might the speaker discuss next?",
                "time_stamp": "00:05:06",
                "answer": "A",
                "options": [
                    "A. How to calculate the perimeter of any regular polygon.",
                    "B. The significance of repeated addition.",
                    "C. The properties of regular polygons.",
                    "D. The importance of perimeter in geometry."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "math tutorials",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_221_real.mp4"
    },
    {
        "time": "[0:06:08 - 0:06:38]",
        "questions": [
            {
                "task_type": "Prospective Reasoning",
                "question": "What might the speaker ask the students to do next?",
                "time_stamp": "00:06:32",
                "answer": "B",
                "options": [
                    "A. Calculate the area of the shape.",
                    "B. Determine the perimeter of the shape.",
                    "C. Measure each side length.",
                    "D. Convert the measurements to centimeters."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "math tutorials",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_221_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:20]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the primary color of the control panel shown right now?",
                "time_stamp": "00:00:09",
                "answer": "A",
                "options": [
                    "A. Black.",
                    "B. White.",
                    "C. Blue.",
                    "D. Grey."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_417_real.mp4"
    },
    {
        "time": "[0:02:36 - 0:02:56]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the primary color of the mountains visible right now?",
                "time_stamp": "00:02:53",
                "answer": "A",
                "options": [
                    "A. White.",
                    "B. Green.",
                    "C. Brown.",
                    "D. Red."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_417_real.mp4"
    },
    {
        "time": "[0:05:12 - 0:05:32]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the main object visible outside the aircraft right now?",
                "time_stamp": "00:05:27",
                "answer": "A",
                "options": [
                    "A. A mountain.",
                    "B. An ocean.",
                    "C. A forest.",
                    "D. A desert."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_417_real.mp4"
    },
    {
        "time": "[0:07:48 - 0:08:08]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the predominant weather condition visible right now?",
                "time_stamp": "00:07:28",
                "answer": "A",
                "options": [
                    "A. Sunny.",
                    "B. Cloudy.",
                    "C. Foggy.",
                    "D. Rainy."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_417_real.mp4"
    },
    {
        "time": "[0:10:24 - 0:10:44]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color is the body of water visible right now?",
                "time_stamp": "00:10:44",
                "answer": "A",
                "options": [
                    "A. Green.",
                    "B. Blue.",
                    "C. Brown.",
                    "D. Red."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_417_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:09]: The video begins with a close-up view of a sketched portrait of a female face on a canvas. The drawing is in thin, light pencil lines, showing detailed facial features such as eyes, nose, lips, and hair. A paintbrush appears in the frame, held by a hand, starting to add details to the drawing. The text \"SQUARESPACE\" with the Squarespace logo appears prominently over the canvas. [0:00:10 - 0:00:12]: The paintbrush continues to add details to the sketched face, filling in the eyes with black paint to create the outline and pupils. The brush focuses on one eye and gradually moves to the other. [0:00:13 - 0:00:17]: The brush begins to add color to the eyes, starting with a blue hue on the left eye. The brush strokes are precise, filling the iris with a vibrant blue color while making sure not to paint over the details of the pupil. [0:00:18 - 0:00:20]: The paintbrush continues adding blue color to the right eye, completing the eye coloring process. Both eyes now have a blue hue, and the facial features of the sketched portrait are more pronounced with the added paint. The detailed work of the brush is evident as the eyes appear more lifelike.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What colo was added to the eyes of the sketched portrait?",
                "time_stamp": "00:00:20",
                "answer": "C",
                "options": [
                    "A. Green.",
                    "B. Brown.",
                    "C. Blue.",
                    "D. Hazel."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_127_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "captions": "[0:03:40 - 0:04:00] [0:03:40 - 0:03:43]: The video starts with a close-up view of a portrait painting in progress, focusing on the subject's face. The face is incomplete, with only the eyes, nose, and mouth fully detailed. The eyes have blue irises and heavy, dramatic makeup with green and dark shading around them. The lips are painted red. The surrounding skin areas are sketched but not filled in yet. [0:03:43 - 0:03:49]: A paintbrush enters the frame from the right and begins adding details to the area around the nose. The brush moves with precision, adding subtle changes to the shading and definition of the nose area. The background remains consistent, with the rest of the face's outline visible but uncolored. [0:03:50 - 0:03:53]: The video continues with the paintbrush carefully working on the portrait. Text saying \"2. RED DOT MANIA\" briefly appears across the screen, seemingly indicating a segment of a series or a step in the painting process. [0:03:54 - 0:03:56]: The paintbrush now focuses on adding detail to the skin around the nose and eye area. The surrounding outlines of the hair and other facial features remain unpainted and white. [0:03:57 - 0:03:59]: The final part of the video shows the artist continuing to refine the details around the eye and nose with the paintbrush. The brush adds layers and texture, enhancing the portrait's realism. The background and unfinished parts of the painting remain the same, with the focus firmly on the nose and eye area.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What action is the paintbrush performing right now?",
                "time_stamp": "00:03:49",
                "answer": "C",
                "options": [
                    "A. Adding details to the hair.",
                    "B. Coloring the lips.",
                    "C. Adding details to the nose.",
                    "D. Shading the background."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Spatial Understanding",
                "question": "Which parts of the face remain unpainted right now?",
                "time_stamp": "00:04:00",
                "answer": "B",
                "options": [
                    "A. Eyes and lips.",
                    "B. Hair and surrounding skin areas.",
                    "C. Nose and lips.",
                    "D. Eyes and nose."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_127_real.mp4"
    },
    {
        "time": "[0:07:00 - 0:08:00]",
        "captions": "[0:07:20 - 0:07:40] [0:07:20 - 0:07:40]: The video starts with a close-up view of a detailed painting of a woman's face. The artwork features vivid red hair, intense blue eyeshadow, and red lips. Throughout the video, a paintbrush is visible, moving across different areas of the painting, indicating ongoing adjustments or additions. At different moments, the paintbrush is seen near the eyes, the forehead, and the nose areas, providing subtle refinements to the portrait. The background remains neutral, ensuring the focus stays on the painting and the artist's actions. The woman's face in the painting exhibits a serious and slightly intense expression, with strong brushstroke details highlighting the contours and shadows of the face.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What is the primary focus of the video?",
                "time_stamp": "0:07:40",
                "answer": "C",
                "options": [
                    "A. The artist mixing paints.",
                    "B. The artist adjusting the background.",
                    "C. The detailed painting of a woman's face.",
                    "D. The artist framing the artwork."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_127_real.mp4"
    },
    {
        "time": "[0:11:00 - 0:12:00]",
        "captions": "[0:11:00 - 0:11:20] [0:11:00 - 0:11:05]: The video presents a close-up view of a canvas on an easel displaying a painting of a woman's face with long red hair and blue eyes. The background of the canvas is yellow. A green plant can be seen in the background on the left side of the frame. The painting shows vivid brush strokes that detail the woman's facial features, especially her striking blue eyes and red lips. [0:11:05 - 0:11:09]: The painter uses a fine brush to add details to the painting, focusing on different areas. The brush is actively moved across the woman's face in several strokes, enhancing the eyes, hair, and facial contours. [0:11:09 - 0:11:12]: The painter continues refining the areas around the eyes and cheeks, using swift and deliberate strokes. The brush points and dabs gently around the areas, indicating detailing and blending of colors. [0:11:12 - 0:11:15]: More attention is paid to the lower face, including the lips and chin. The painter adds subtle touches to enhance the dimensionality of the painting. A steady hand guides the brush to ensure precise and controlled application of paint. [0:11:15 - 0:11:17]: The painter revisits the upper part of the painting, working on the forehead and hair. The brush strokes are carefully applied to blend colors and add texture to the red hair, highlighting strands and adding depth. [0:11:17 - 0:11:20]: The final touches focus slightly on the lower face and shoulder area. The painter's hand holds the brush with care, adding the last strokes that bring out the clarity and final details of the portrait. The changes are subtle but provide a polished finish.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What does the painter focus on during the last strokes?",
                "time_stamp": "0:11:20",
                "answer": "B",
                "options": [
                    "A. The eyes and hair.",
                    "B. The lower face.",
                    "C. The background and plant.",
                    "D. The canvas texture."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_127_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:01]: In a kitchen, a person wearing a grey shirt stands in front of a counter with cooking elements. The person concentrates on cooking, with a frying pan on the stove containing ingredients.  [0:02:01 - 0:02:02]: The person adjusts items on the counter, appears to be preparing something beside a frying pan. [0:02:02 - 0:02:03]: He handles a cut of meat, preparing to place it in the frying pan. [0:02:03 - 0:02:04]: He lowers the cut of meat into the frying pan, initiating the cooking process. [0:02:04 - 0:02:05]: He peers into the frying pan, monitoring the cooking meat. [0:02:05 - 0:02:06]: His hand hovers over the pan, possibly adjusting the seasoning or the heat. [0:02:06 - 0:02:07]: He makes broader gestures, likely indicating the next steps or explaining a process. [0:02:07 - 0:02:08]: His hands lower closer to the pan, focusing again on cooking. [0:02:08 - 0:02:10]: A close-up shot of the meat sizzling in the frying pan, starting to cook. [0:02:10 - 0:02:11]: The person turns back towards the stove, potentially preparing another ingredient. [0:02:11 - 0:02:12]: He organizes items on the counter, setting the scene for further cooking steps. [0:02:12 - 0:02:13]: He moves toward the stove with a chopping board containing additional ingredients. [0:02:13 - 0:02:14]: The view shifts to an overhead shot, displaying the addition of more meat portions into the frying pan. [0:02:14 - 0:02:15]: He scrapes the chopping board, ensuring no remnants are left behind as he adds the meat to the pan. [0:02:15 - 0:02:16]: He turns to place the chopping board down, organizing the workspace. [0:02:16 - 0:02:17]: The person gestures towards the frying pan with emphasis, potentially explaining the cooking process. [0:02:17 - 0:02:18]: He appears to highlight important points or tips related to cooking. [0:02:18 - 0:02:19]: The person reaches for an ingredient on the counter, likely to enhance the dish. [0:02:19]: A close-up displays the person adding seasoning to the meat in the frying pan, focusing on enhancing the flavor.\n[0:02:20 - 0:02:40] \n[0:02:40 - 0:03:00] ",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What action is the person taking with the meat?",
                "time_stamp": "0:02:04",
                "answer": "B",
                "options": [
                    "A. He seasons the meat.",
                    "B. He lowers the meat into the frying pan.",
                    "C. He cuts the meat into pieces.",
                    "D. He removes the meat from the frying pan."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_17_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:03]: The video begins with a close-up view of a pair of hands working on a wooden cutting board. Several halved fruits, including red and yellow plums, are positioned on the board. A person is seen squeezing a yellow fruit, and juice is dripping onto the cutting board. The board sits atop a kitchen counter with a gas stove and other kitchen tools nearby. [0:04:04 - 0:04:09]: The scene transitions to a wider view, revealing a kitchen with white brick walls and shelves filled with kitchenware like plates, bowls, and jars. A person in a grey shirt is seen on the right side of the screen, arranging the halved fruits on the cutting board. They move around the kitchen island, adjusting pans and utensils in preparation for cooking. [0:04:10 - 0:04:12]: The person picks up a dark brown small bowl containing some seasoning and moves toward a stove, where two pieces of meat are searing in a frying pan. The camera changes to a top-down view showing two pieces of meat sizzling in a pan next to another pan with something brown being stirred. [0:04:13 - 0:04:19]: The camera angle changes back to a wider kitchen view, showing the person sprinkling the seasoning onto the meat in the pan. They continue to cook and stir the contents, causing some steam to rise from the pan. The video captures the robust kitchen environment, with various kitchen tools and ingredients visible on the countertop, evoking a bustling cooking session.\n[0:04:20 - 0:04:40] \n[0:04:40 - 0:05:00] ",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What type of fruit is being squeezed right now?",
                "time_stamp": "00:04:03",
                "answer": "C",
                "options": [
                    "A. Blueberry.",
                    "B. Orange.",
                    "C. Yellow Plum.",
                    "D. Apple."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_17_real.mp4"
    },
    {
        "time": "[0:06:00 - 0:07:00]",
        "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:01]: A man, seen in profile, is wearing a grey shirt and is talking, his right hand raised with the index finger extended. Behind him, a blue wall with shelves containing jars and other kitchen items is visible. [0:06:01 - 0:06:02]: The man continues speaking, his facial expression showing engagement. The background remains the same with kitchen shelves and a blue wall. [0:06:02 - 0:06:03]: The man is now using a pair of red kitchen tongs to cook food in a pan on the stove. There are two visible pans on the stove, and the counter has various utensils and a plate. [0:06:03 - 0:06:04]: He continues cooking, now gesturing with his left hand while holding the red tongs in the right hand. Kitchen shelves with jars are still in the background. [0:06:04 - 0:06:05]: The man stands more upright, speaking and gesturing with his left hand. The counter and kitchen utensils remain the same. [0:06:05 - 0:06:06]: His left hand gestures while he speaks; the right hand rests on his hip. Behind him, the shelves with jars and kitchen items are still visible. [0:06:06 - 0:06:07]: He turns slightly to his right, pointing at something off-camera. The kitchen countertop and shelves in the background remain unchanged. [0:06:07 - 0:06:08]: A close-up shot shows his hand moving food in the pan with a metal spatula. On the stove, two pans are visible, one with pieces of food being cooked, and the other with whole vegetables. [0:06:08 - 0:06:09]: The focus remains on the cooking process; he continues to stir food in the second pan. Various kitchen utensils and spices are displayed on the counter. [0:06:09 - 0:06:10]: From an overhead view, the contents of both pans are clearly visible. The left pan contains two pieces of food, while the right pan has multiple small round vegetables. [0:06:10 - 0:06:11]: The man bends slightly forward, continuing his tasks with an expression of engagement while holding a kitchen cloth. The background shelves filled with dishes are visible. [0:06:11 - 0:06:12]: Standing upright again, the man looks to the right, holding the kitchen cloth at his side. Various kitchen items on shelves remain visible in the background. [0:06:12 - 0:06:13]: The man moves towards the stove, appearing to prepare something. Kitchen utensils and a plate are on the counter in the foreground. [0:06:13 - 0:06:14]: The man faces the stove, focused on cooking. The background shows a blue wall with kitchen shelves and a window with plants outside. [0:06:14 - 0:06:15]: The man uses a spoon to stir food in the pan while talking. The countertop near the stove has various kitchen utensils. [0:06:15 - 0:06:16]: A close-up of the man\u2019s hands stirring food in a pan with a spoon, two yellow and red vegetables visible in the pan. Various kitchen items are on the counter. [0:06:16 - 0:06:17]: He continues to stir the food in the pan, adjusting the items with the spoon. The kitchen counter and utensils remain in the background. [0:06:17 - 0:06:18]: The overhead view shows the contents of both pans again, with the man\u2019s hand adjusting the vegetables in the right pan. Items on the counter are still visible. [0:06:18 - 0:06:19]: The overhead shot shows him stirring the food in the pan on the right, ensuring even cooking. The kitchen layout remains consistent, with items neatly arranged. [0:06:19 - 0:06:20]: He continues to adjust the food in the pan, ensuring they are well-cooked. The general setup of the kitchen, with utensils and kitchen items organized, remains in view.\n[0:06:20 - 0:06:40] \n[0:06:40 - 0:07:00] ",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the man using to cook food in the pan?",
                "time_stamp": "0:06:03",
                "answer": "C",
                "options": [
                    "A. A wooden spoon.",
                    "B. A metal spatula.",
                    "C. Red kitchen tongs.",
                    "D. A fork."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_17_real.mp4"
    },
    {
        "time": "[0:08:00 - 0:09:00]",
        "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: A kitchen counter is equipped with various cooking items. A white plate with cooked food rests on the top left. A bowl of vegetables sits nearby, alongside a few small dishes, each containing various spices. An empty frying pan is positioned on the stove in the center. On the right, sausage slices simmer in a pan. A hand appears on the left, placing a bunch of leafy greens onto a wooden cutting board.    [0:08:01 - 0:08:03]: The scene primarily focuses on the kitchen counter from an aerial perspective. Apart from the earlier-actionated items, a person\u2019s arm is seen moving a fresh piece of leafy greens onto the cutting board. [0:08:03 - 0:08:04]: The countertop scene remains consistent, though more attention is directed towards the leafy greens on the wooden cutting board. The small bowl of butter and spices remain intact. [0:08:04 - 0:08:05]: The leafy greens stay centralized on the cutting board while the surrounding bowls of spices remain undisturbed. [0:08:05 - 0:08:06]: The camera shifts focus to a man standing before a row of kitchen utensils and shelves packed with plates, bottles, and kitchenware. He is wearing a grey T-shirt and has a short haircut. The man is talking and gesturing, holding a utensil in his right hand, facing slightly to the side. [0:08:06 - 0:08:07]: Still focused on the person, the man redirects his attention to the cookware around him. [0:08:07 - 0:08:09]: The downward camera angle shows the leafy green on the cutting board as hands wrap around the greens, preparing for cutting among the assortment of small bowls holding ingredients. [0:08:09 - 0:08:10]: The camera zooms in on the vibrant green leafy vegetable on the cutting board as a hand holds it firmly. [0:08:10 - 0:08:11]: A closer angle shows a leafy green vegetable being sliced. [0:08:11 - 0:08:12]: The hand holds a knife mid-cut through the leafy green, clearly displayed in the center of the board. [0:08:12]: A close-up depicts hands slicing through leafy greens on the cutting board. The knife is freshly chopping the greens into smaller pieces. [0:08:12 - 0:08:13]: The man is standing by the kitchen counter, focused on an activity before him. Kitchen shelves laden with items form the background while he chops vegetables. [0:08:13 - 0:08:14]: The man is continuing to chop vegetables on the board. [0:08:14 - 0:08:15]: A focused expression on the man's face as he looks down at the vegetables he's cutting. [0:08:15 - 0:08:16]: The man continues to slice the vegetables attentively, standing at the counter. [0:08:16 - 0:08:17]: The vegetables on the cutting board are now in smaller pieces, with the man's hand still holding the knife and slicing. [0:08:17 - 0:08:18]: The man remains focused on preparing the vegetables on the counter, with his hand chopping more finely. [0:08:18 - 0:08:19]: He picks up a stainless steel utensil to stir the contents of the pot on the stove in front of him. The cutting board is now cluttered with finely chopped greens.\n[0:08:20 - 0:08:40] \n[0:08:40 - 0:09:00] ",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is simmering in a pan on the right side of the kitchen counter right now?",
                "time_stamp": "00:08:01",
                "answer": "C",
                "options": [
                    "A. Cooked food.",
                    "B. Vegetables.",
                    "C. Sausage slices.",
                    "D. Leafy greens."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_17_real.mp4"
    },
    {
        "time": "[0:09:00 - 0:10:00]",
        "captions": "[0:09:00 - 0:09:20] \n[0:09:20 - 0:09:40] \n[0:09:40 - 0:10:00] ",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is this person doing now right now?",
                "time_stamp": "0:09:12",
                "answer": "B",
                "options": [
                    "A. Dice the green onions finely.",
                    "B. Chop the cilantro finely.",
                    "C. Climbing ropes and carabiners.",
                    "D. First aid kits and maps."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_17_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:05]: The video begins with a first-person perspective of a city street. The foreground shows a row of parked cars on the left side of the road and a bright red metal fence to the right. The pavement is made of light, neatly arranged cobblestones. The building on the right features a large mural with geometric patterns in different shades of blue, red, and brown, extending from the top to the sidewalk. In the background, multiple multi-story buildings line both sides of the street, including some with ornate architectural details. A tall street lamp is positioned near the center of the frame, providing additional vertical interest. The sky is clear and blue, enhancing the overall brightness. [0:00:06 - 0:00:10]: As the video progresses, the view continues down the street, with the perspective moving slightly forward. Signposts indicating parking information appear more prominently on the left side, and various parked cars are visible, including a white car in the forefront. The street remains relatively empty of pedestrians. The geometric mural on the right building becomes slightly more detailed as the camera moves closer, and a shadow of the streetlamp falls onto the sidewalk. [0:00:11 - 0:00:15]: Moving further down the street, a blue parking meter becomes visible on the left side of the frame, adjacent to the parked cars. The architectural details of the buildings on both sides of the street become more defined, with visible balconies, windows, and facades. The mural on the right wall continues to dominate the visual scene on that side, and the bright red fence runs parallel to the building's facade, complementing the sophisticated patterns of the mural. [0:00:16 - 0:00:20]: In the final stretch of the video, the perspective continues forward, revealing more of the street ahead. The shadow of the streetlamp elongates across the cobblestone pavement. At the end of the red metal fence, the sidewalk extends, leading to more cars parked on both sides of the street. A few more architectural details of the distant buildings become discernible, including additional street signs and a white van parked near the curb further down the road. The street remains calm and orderly, reflecting a tranquil urban environment under a clear blue sky.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What object is visible on the left side of the frame next to the parked cars?",
                "time_stamp": "0:00:15",
                "answer": "B",
                "options": [
                    "A. A red mailbox.",
                    "B. A blue parking meter.",
                    "C. A green trash can.",
                    "D. A yellow bike rack."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_330_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:43]: The video begins with a view of a plaza featuring two buildings with distinct architectural styles. One building has red doors and red-framed windows with small balconies, while the other is decorated with blue tiles and intricate stone carvings. The sky is clear and blue, and pedestrians are walking in the plaza. [0:02:44 - 0:02:47]: A woman in a yellow top and blue skirt continues walking past the camera on the left, moving towards the center of the plaza. The ornate building's stone carvings become more prominent as the camera shifts slightly. [0:02:48 - 0:02:50]: The woman exits the frame, and the focus shifts to the right side where an outdoor caf\u00e9 setup with wooden tables and chairs appears. Further ahead, more pedestrians can be seen walking down the street. [0:02:51 - 0:02:55]: The ornate stone building with large arched windows and intricate carvings becomes the main focus. The caf\u00e9 area remains visible on the right, and the street continues to extend forward, showing more buildings with colorful facades. [0:02:56 - 0:02:59]: The camera tilts upward, revealing more of the detailed carvings on the stone building, including statues and decorative elements. The sky remains clear, and the buildings on the left feature balconies with black iron railings and red-framed windows. The video ends focusing on this architectural detail.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What distinct feature is highlighted on the ornate stone building right now?",
                "time_stamp": "00:03:00",
                "answer": "B",
                "options": [
                    "A. Blue doors.",
                    "B. Stone carvings.",
                    "C. Red-framed windows.",
                    "D. Iron railings."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_330_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:32]: The video depicts a scene on a bustling pedestrian street on a sunny day. Both sides of the street are lined with multi-story buildings featuring various architectural details. The facades exhibit a mix of neutral tones and occasional colorful accents. Some buildings have balconies with ornate railings and shuttered windows at upper levels. On the ground level, there are shops with large display windows and awnings, some of which are red. Several pedestrians walk along the evenly-paved street, some wearing casual clothing such as t-shirts and jeans, while others wear more formal or summertime outfits. In the foreground, a group of four people, two men and two women, walk in the same direction, their backs to the camera. Of the group, one man in a bright pink shirt and blue jeans is slightly ahead, and another, wearing a blue backpack, is to his right. The women, one with a black skirt and another with black pants, walk to the left. Further down the street, more people can be seen, some walking individually and others in small groups. A few people are standing and conversing by shop entrances. The street is well-lit with sunlight, casting crisp shadows, and the sky is clear and blue above. [0:05:33 - 0:05:39]: As the video progresses, more pedestrians enter the frame, including a woman in black pants walking closer to the position of the camera. A woman carrying a bag emerges from one of the shops on the right, while another woman in shorts and sunglasses appears from the left. Three people, conversing animatedly, walk together on the right side of the street. The general flow of people heading in both directions continues, maintaining the lively atmosphere of the area. Overhead, a bird soars through the clear sky, adding a dynamic element to the scene. Tables and chairs outside a caf\u00e9 on the right come into view, suggesting places to sit and relax. The overall impression is one of a vibrant, active street in a city or town with a mix of local and tourists enjoying the day.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What color is the shirt of the man who is slightly ahead in the group of four people?",
                "time_stamp": "00:05:32",
                "answer": "C",
                "options": [
                    "A. Green.",
                    "B. Blue.",
                    "C. White.",
                    "D. Red."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_330_real.mp4"
    },
    {
        "time": "[0:08:00 - 0:09:00]",
        "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:09]: In a lively urban setting with bustling activity, the scene unfolds on a cobblestone pedestrian street. The right side is dominated by a historic, grand building with intricate architectural details such as arched windows and a clock tower. Adjacent to this building, colorful graffiti and street art adorn construction barriers, adding a contrasting modern touch. On the left side, a row of old buildings with balconies line the street. Several outdoor caf\u00e9s with red and brown umbrellas provide seating for customers, many of whom are engaged in conversations. People walk along the street, some wearing masks, others enjoying the sunny weather. Among the pedestrians, a group of tourists is seen walking and taking in the surroundings. One man in a black shirt and another in pink are noticeable, along with a couple strolling hand in hand. A street lamp is visible in the background near the central architectural landmark. [0:08:09 - 0:08:12]: The camera continues to move forward, showing more details of the lively atmosphere. More caf\u00e9 tables are occupied, with people facing the street or each other, engaging in casual conversation. The background reveals another historical building further up the street, and the top of a tower is visible in the distance. Pedestrians continue to walk in both directions, passing by the caf\u00e9s and the construction barriers. [0:08:12 - 0:08:16]: As the motion persists, the camera captures additional details of the street and its occupants. There is a slight increase in the number of people walking and sitting at the caf\u00e9s. The scene's rich architectural details become more apparent, showcasing the city's historic charm. The right side reveals more of the construction barrier, filled with graffiti and street art, standing in contrast to the old buildings. [0:08:16 - 0:08:19]: The scene involves a brief glance at a glass structure, possibly a modern bus stop or an information kiosk. People pass by it, and some are seen waiting inside or nearby. The street continues to be lively and crowded with locals and tourists alike, all enjoying the pleasant weather and the vibrant urban atmosphere. The surrounding historic architecture and the various activities on the street paint a vivid picture of daily life in this city. [0:08:19 - 0:08:20]: The camera momentarily focuses on a specific individual wearing a light-colored dress, walking past the modern glass structure. This quick moment highlights the mix of traditional and contemporary elements coexisting in the urban landscape. The historic buildings with intricate designs create a picturesque backdrop against the modern street life bustling with pedestrians, caf\u00e9s, and street art.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What type of umbrellas are seen at the outdoor caf\u00e9s right now?",
                "time_stamp": "0:08:05",
                "answer": "B",
                "options": [
                    "A. Blue and white.",
                    "B. Red and brown.",
                    "C. Green and yellow.",
                    "D. Black and white."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Event Understanding",
                "question": "What does the the glass structure likely depict right now?",
                "time_stamp": "0:08:19",
                "answer": "B",
                "options": [
                    "A. A historic monument.",
                    "B. A modern bus stop.",
                    "C. A construction site.",
                    "D. A marketplace."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_330_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:10]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions just now?",
                "time_stamp": "00:00:10",
                "answer": "D",
                "options": [
                    "A. The individual cleans the espresso machine and places the portafilter in position.",
                    "B. The individual grinds coffee beans, tamps the ground coffee, and brews an espresso shot.",
                    "C. The individual serves a coffee beverage, cleans up the work area, and restocks coffee supplies.",
                    "D. The individual steams the milk using the steam wand, and prepares it for a coffee beverage."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_369_real.mp4"
    },
    {
        "time": "[0:01:50 - 0:02:00]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions just now?",
                "time_stamp": "00:02:00",
                "answer": "D",
                "options": [
                    "A. The individual prepares a hot beverage by steaming milk and selecting a paper cup.",
                    "B. The individual clears the espresso machine area by wiping it clean, rearranging cups, and replacing a filter.",
                    "C. The individual operates the deep fryer to prepare a food item, setting the timer and monitoring the cooking process.",
                    "D. The individual brews an espresso shot by grinding beans, tamping them, and inserting the portafilter into the machine."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_369_real.mp4"
    },
    {
        "time": "[0:03:40 - 0:03:50]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions just now?",
                "time_stamp": "00:03:50",
                "answer": "D",
                "options": [
                    "A. The individual brews an espresso shot, mixes it with hot water to create an Americano, and places it on the counter.",
                    "B. The individual dismantles the espresso machine, cleans each part thoroughly, and reassembles it.",
                    "C. The individual heats milk on the stovetop, pours it into a blender, and prepares a smoothie.",
                    "D. The individual operates the espresso machine, froths milk using the steam wand."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_369_real.mp4"
    },
    {
        "time": "[0:05:30 - 0:05:40]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions just now?",
                "time_stamp": "00:05:40",
                "answer": "D",
                "options": [
                    "A. The individual inserts the portafilter into the espresso machine and begins making an espresso shot.",
                    "B. The individual cleans the coffee grinder and refills it with new coffee beans.",
                    "C. The individual inspects the coffee machine, runs a cleaning cycle, and restocks the supply tray.",
                    "D. The individual measures ground coffee, fills the portafilter, and use spoon to adjust it."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_369_real.mp4"
    },
    {
        "time": "[0:07:20 - 0:07:30]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions just now?",
                "time_stamp": "00:07:30",
                "answer": "D",
                "options": [
                    "A. The individual rinses out a used cup and places it in the dishwasher.",
                    "B. The individual refills the coffee machine with water and prepares it for brewing.",
                    "C. The individual measures out coffee grounds and places them into the portafilter.",
                    "D. The individual fills a glass with ice cubes from a container."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_369_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: In a brightly lit storage or stockroom, a gloved hand is seen reaching towards a stack of yogurt containers on a metal rack. The containers are in white trays with blue and white packaging. In the background, there is a glimpse of a steel door and other stacked items. [0:00:01 - 0:00:03]: The hand starts to grab a tray of yogurt from the stack. The background shows more shelves and products. [0:00:03 - 0:00:06]: The tray is now being positioned in front of a refrigerator shelf stocked with various milk products. The gloved hand adjusts the placement of the tray. [0:00:06 - 0:00:09]: The person is placing the tray on the shelf and positioning the yogurt containers. The shelf has many similar containers already stacked. [0:00:09 - 0:00:10]: The person\u2019s hands are seen moving the containers around, ensuring they are properly aligned and organized on the shelf. [0:00:10 - 0:00:12]: The empty tray is now visible as the containers have been shelved. The hands continue to make final adjustments. [0:00:12]: A blur indicates a quick movement, possibly the person turning or moving away from the shelf. The background shows more shelves with various products. [0:00:13]: The person is carrying another tray of blue-lidded yogurt containers. The background shows another person and more stock items. [0:00:13 - 0:00:14]: The person approaches the shelf again with the new tray of yogurt. Nearby milk cartons are visible. [0:00:14 - 0:00:17]: The hand places the new tray onto the shelf and starts distributing the containers. The shelf gets more crowded with the additional products. [0:00:17 - 0:00:19]: The containers are being adjusted to fit properly. Some containers are taken from the tray and placed on the shelf. [0:00:19 - 0:00:20]: The final adjustments are made, and the shelf looks organized with yogurt containers neatly placed. The person then retracts their hands, completing the stocking process.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the gloved hand reaching towards at the beginning of the video?",
                "time_stamp": "0:00:01",
                "answer": "A",
                "options": [
                    "A. Yogurt containers.",
                    "B. Milk cartons.",
                    "C. Juice bottles.",
                    "D. Cheese blocks."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Attribute Recognition",
                "question": "What color is the packaging of the yogurt containers?",
                "time_stamp": "0:00:10",
                "answer": "B",
                "options": [
                    "A. Red and white.",
                    "B. Blue and white.",
                    "C. Green and white.",
                    "D. Yellow and white."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "storage_manage",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_443_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:25]: The scene opens in a store, focusing on a refrigerated section stocked with multiples of the same product, a blue and white container labeled \"cr\u00e8me fra\u00eeche.\" A hand wearing a black glove picks up one of the containers from the upper shelf and places it into a cardboard tray with circular cutouts designed to hold them in place. The hand continues to pick up and arrange the containers systematically. [0:02:26 - 0:02:27]: The person holding the cardboard tray moves away from the refrigerated section, walking through the aisle, which is stocked with various other products on the metal shelves.  [0:02:28 - 0:02:32]: The scene transitions to a back room or storage area where the hand stacks several empty cardboard trays onto a larger stack. Nearby, there are boxes labeled \"BRAVO,\" and the surroundings suggest a stocking or inventory area with carts and additional supplies. [0:02:33 - 0:02:38]: The individual picks up a case of new cr\u00e8me fra\u00eeche containers from a shelf and places it on a cart with other boxes. The cart is densely packed and sits in an area filled with shelves carrying various inventory items. The hand moves carefully to ensure the items are securely placed on the cart. [0:02:39]: The video briefly shows a final shot of the cr\u00e8me fra\u00eeche containers, indicating the task's repetitive nature and the prominence of this particular product in the scene.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What does the individual do after picking up the yogurt container?",
                "time_stamp": "0:02:25",
                "answer": "D",
                "options": [
                    "A. Opens it.",
                    "B. Discards it.",
                    "C. Hands it to someone else.",
                    "D. pick it up on the shelf."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "storage_manage",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_443_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:42]: The video begins with a person wearing black gloves picking up a yellow carton box labeled \"Bravo\" from a shelf in a warehouse-like storage room. The box has orange text reading \"Apelsin Juice.\" [0:04:42 - 0:04:44]: The camera then turns to show a wider view of the room with shelves filled with boxes and products on the left side and directly ahead. The floor is gray, and there are industrial carts loaded with items at the far end of the room. [0:04:44 - 0:04:45]: The perspective shifts back towards the yellow \"Bravo\" boxes, with the person holding two boxes and placing them on a cart filled with additional \"Bravo\" boxes. [0:04:45 - 0:04:47]: The individual moves another box labeled \"Bravo\" from the shelf and places it on the cart, which is stacked with more boxes. [0:04:47 - 0:04:48]: The view pans to show another part of the storage area, revealing more shelves filled with various items. The perspective focuses on a box being held open. [0:04:48 - 0:04:50]: The camera shows more details of the storage area, including stacked green boxes and additional items on metal carts. The person then picks up a large cardboard box. [0:04:50 - 0:04:51]: The individual closely examines and holds the large carton box. [0:04:51 - 0:04:53]: They continue handling the cardboard box, tearing it open to reveal more boxes with \"Bravo\" written on them. [0:04:53 - 0:04:55]: The person then lifts one of the opened boxes, showing several \"Bravo\" juice cartons inside. The view focuses on the yellow cartons stacked neatly in the box. [0:04:55 - 0:04:57]: The perspective shifts to show the person organizing the contents, ensuring the \"Bravo\" juice cartons are correctly arranged within the box. [0:04:57 - 0:04:59]: The view then shows the individual picking up a single \"Bravo\" carton from the box. The hand and gloves are clearly visible as they lift the carton. [0:04:59 - 0:05:00]: To end, the person is seen moving towards the original shelf with boxes still being held, and more product boxes can be seen on the shelves in the background.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What does the individual do after picking up the large cardboard box?",
                "time_stamp": "0:05:00",
                "answer": "D",
                "options": [
                    "A. Places it back on the shelf.",
                    "B. Moves it to another room.",
                    "C. Examines it and sets it aside.",
                    "D. Take out the orange juice inside and put it on the shelf."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "storage_manage",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_443_real.mp4"
    },
    {
        "time": "[0:07:00 - 0:08:00]",
        "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:01]: The video begins with a view of several shelves in a store. The shelves display various boxed items, likely juices, in different colors - predominantly green and red packaging. The boxes are neatly arranged in rows. [0:07:02 - 0:07:03]: The camera moves closer, and a gloved hand appears, reaching for one of the red boxes. The hand grabs a box, partially obscuring the view. [0:07:04 - 0:07:05]: The hand places the box back on the shelf, but slightly further to the left. The camera angle shifts slightly to the left as well. [0:07:06]: The gloved hand reappears, pointing towards another red box on the right. The surrounding shelves remain stocked with green and red boxes. [0:07:07 - 0:07:11]: The camera briefly focuses back from the shelves, showing more of the gloved hand and black sleeve. The hand reaches for another box, placing it back on the shelf. The camera starts to move away from the shelf. [0:07:12 - 0:07:15]: The scene changes to another part of the store with a focus on a large cardboard box. The person appears to be breaking down the box or inspecting it. The background includes shelves and some carts with various products. [0:07:16 - 0:07:17]: The camera's focus moves downward, showing the person continuing to work on the cardboard boxes, which are now on the floor, possibly flattening them. The gloved hands are visible, holding and manipulating the boxes. [0:07:18]: The view shifts to a close-up of a dark, plain surface. It's not immediately clear what the surface is, but it obscures most of the frame. [0:07:19 - 0:07:20]: The final frames show the camera focusing on a different set of shelves stocked with various beverages, including some green and yellow bottled drinks. The shelves are organized, and there are many product options visible.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is the scene focusing on in the store right now?",
                "time_stamp": "0:07:19",
                "answer": "B",
                "options": [
                    "A. The checkout counter.",
                    "B. A different set of shelves with beverages.",
                    "C. The entrance of the store.",
                    "D. A display of snacks."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "storage_manage",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_443_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:10]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions of the barista taken just now?",
                "time_stamp": "00:00:10",
                "answer": "B",
                "options": [
                    "A. The barista prepared a cappuccino, stirred it, and served it to a customer.",
                    "B. The barista steamed milk, prepared two lattes, and began making a new latte.",
                    "C. The barista brewed coffee, added sugar and cream, and handed it to a customer.",
                    "D. The barista made a latte, added cocoa powder, and served it to a customer."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_362_real.mp4"
    },
    {
        "time": "[0:01:56 - 0:02:06]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions taken just now?",
                "time_stamp": "00:02:18",
                "answer": "A",
                "options": [
                    "A. The worker prepared an espresso by measuring coffee grounds, tamping them, and placing them in the machine.",
                    "B. The worker prepared a cup of tea by measuring loose leaves, placing them in a pot, and adding hot water.",
                    "C. The worker cleaned the coffee machine and sorted cups for the next order.",
                    "D. The worker brewed a fresh pot of coffee by measuring water and coffee grounds, then starting the machine."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_362_real.mp4"
    },
    {
        "time": "[0:03:52 - 0:04:02]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions taken just now?",
                "time_stamp": "00:03:58",
                "answer": "B",
                "options": [
                    "A. The individual cleaned the coffee machine and placed a cup on the scale.",
                    "B. The individual measured cocoa powder into a pitcher.",
                    "C. The individual selected a cup, poured coffee grounds into it, and added hot water.",
                    "D. The individual brewed fresh coffee by placing a new filter and coffee grounds into the machine."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_362_real.mp4"
    },
    {
        "time": "[0:05:48 - 0:05:58]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions taken just now?",
                "time_stamp": "00:06:02",
                "answer": "C",
                "options": [
                    "A. The individual placed the milk jug under the steamer, wiped the machine, and steamed the milk.",
                    "B. The individual brewed a fresh cup of coffee, cleaned the counter, and served the drink.",
                    "C. The individual cleaned the work area, steamed milk, and prepared a milk-based beverage.",
                    "D. The individual restocked supplies, brewed coffee, and served it to a customer."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_362_real.mp4"
    },
    {
        "time": "[0:07:44 - 0:07:54]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions taken just now?",
                "time_stamp": "00:07:47",
                "answer": "B",
                "options": [
                    "A. The individual brewed a fresh cup of tea, added sugar, and served it to a customer.",
                    "B. The individual steamed milk, prepared a latte, and added a lid to the cup.",
                    "C. The individual prepared an espresso shot, cleaned the coffee machine, and discarded used grounds.",
                    "D. The individual restocked the cups, cleaned the counter, and replaced the lids."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_362_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:00:16",
                "answer": "D",
                "options": [
                    "A. 2.",
                    "B. 3.",
                    "C. 4.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            },
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:00:37",
                "answer": "D",
                "options": [
                    "A. 2.",
                    "B. 3.",
                    "C. 4.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_91_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:03:16",
                "answer": "B",
                "options": [
                    "A. 3.",
                    "B. 5.",
                    "C. 6.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_91_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:04:14",
                "answer": "D",
                "options": [
                    "A. 2.",
                    "B. 4.",
                    "C. 3.",
                    "D. 6."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_91_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:05:13",
                "answer": "B",
                "options": [
                    "A. 2.",
                    "B. 9.",
                    "C. 8.",
                    "D. 6."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_91_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video begins with a close-up of a stylus arm of a turntable, showcasing a dark metal tone and a small screw on its surface. It is oriented horizontally, casting shadows on a dark vinyl record spinning beneath it. The background displays an out-of-focus setting which seems to include additional devices or components. [0:00:04 - 0:00:05]: The scene transitions to a wooden desk with various small objects on it like rings and a red book. The drawer of the desk is partly open, revealing a colorful assortment of fabrics or clothing inside. [0:00:06 - 0:00:10]: The hand is seen opening the drawer further and taking out a light pink fabric item with light blue accents. It is then unfolded, revealing it to be a piece of clothing with straps, possibly a garment or accessory. [0:00:11 - 0:00:12]: The hand places the light pink item back onto the desk, adjusting position to reach for something else from the drawer. The desk's top remains the same with rings, a red book, and other decorative objects. [0:00:13 - 0:00:16]: A new fabric piece, colorful with white and floral patterns, is retrieved from the drawer. The item is inspected by hands as it spreads out to reveal its design, once again likely a piece of clothing. [0:00:17 - 0:00:19]: The focus shifts to a woman with light hair sitting in front of a mirror, seen from a shoulder-height perspective. She adjusts her hair and facial expression subtly changes as she looks into the mirror, then glances sideways. The background depicts a warmly lit room and blurred details of furniture or decorative items.\n[0:00:40 - 0:01:00] [0:00:40 - 0:00:44]: The video shows a well-lit bedroom with posters on the wall, a bed with patterned bedsheets, a wooden desk with drawers, and a chair. A person, dressed in a light blue bra and partially covered in a floral skirt, stands next to the bed and is putting on a black dress with red and white vertical stripes running down the middle. Her hair is tied back in a ponytail, and she is in the action of lifting the dress up over her head. [0:00:45 - 0:00:52]: The person is now standing upright, adjusting the dress across her torso and smoothing it out. She continues to look down, checking the fit of the dress, and straightens it around her waist. The dress is now fully on, and she appears to be ensuring everything is in place. [0:00:53 - 0:00:55]: The focus shifts briefly to a close-up of the bedside table, showing some items placed on it, including an orange object resembling a book or a planner. [0:00:56 - 0:00:59]: The person moves away from the bedside table and walks towards a small wooden stool. She bends down, appearing to pick something up from the floor, then rises slightly while still looking downwards towards the stool, continuing with her task.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What are the patterns on the second fabric piece retrieved from the drawer?",
                "time_stamp": "0:00:16",
                "answer": "C",
                "options": [
                    "A. Polka dots.",
                    "B. Stripes.",
                    "C. Floral patterns.",
                    "D. Geometric shapes."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "getting_dressed",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_158_real.mp4"
    },
    {
        "time": "[0:01:00 - 0:02:00]",
        "captions": "[0:01:20 - 0:01:40] [0:01:20 - 0:01:24]: A person wearing checkered tights is standing on a colorful patterned rug, next to a pair of red shoes. The shoes have a low heel and a strap. The person lifts one foot and begins to position it into one of the shoes. They then proceed to guide their foot into the shoe with their hands, ensuring it fits properly. [0:01:25 - 0:01:30]: Continuing the process, the person secures the strap of the shoe around their foot. The person then reaches for the second shoe, and repeats the process, carefully fitting their other foot into the red shoe and adjusting the strap to secure it. [0:01:31 - 0:01:36]: Both shoes are now on the person's feet. They adjust the straps, making sure the fit is snug and secure. The patterned rug below features horizontal stripes in a variety of colors, adding a vibrant background to the scene. [0:01:37 - 0:01:39]: The camera then cuts to a close-up of the person\u2019s face. The person has light-colored hair tied back and is wearing makeup. They appear focused, perhaps adjusting their appearance or ensuring everything is in place. In the background, an interior space with various objects is visible.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the person putting on their feet?",
                "time_stamp": "0:01:30",
                "answer": "B",
                "options": [
                    "A. Red boots.",
                    "B. Red shoes with a low heel and a strap.",
                    "C. Black sneakers.",
                    "D. Sandals."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Attribute Recognition",
                "question": "What pattern is on the stocking the person is wearing?",
                "time_stamp": "0:01:24",
                "answer": "C",
                "options": [
                    "A. Stripes.",
                    "B. Polka dots.",
                    "C. Checkered.",
                    "D. Floral."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Spatial Understanding",
                "question": "What is featured in the background of the scene right now?",
                "time_stamp": "0:01:36",
                "answer": "B",
                "options": [
                    "A. A plain white wall.",
                    "B. A patterned rug with horizontal stripes in various colors.",
                    "C. A window with curtains.",
                    "D. A bookshelf."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Attribute Recognition",
                "question": "What is the focus of the person when the camera cuts to a close-up of their face?",
                "time_stamp": "0:01:40",
                "answer": "C",
                "options": [
                    "A. They are smiling and talking.",
                    "B. They are eating.",
                    "C. They appear focused, possibly adjusting their appearance.",
                    "D. They are looking at the camera and waving."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "getting_dressed",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_158_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:00:16",
                "answer": "D",
                "options": [
                    "A. 2.",
                    "B. 3.",
                    "C. 4.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_98_real.mp4"
    },
    {
        "time": "[0:01:00 - 0:02:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:01:26",
                "answer": "D",
                "options": [
                    "A. 2.",
                    "B. 3.",
                    "C. 4.",
                    "D. 1."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_98_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:03:18",
                "answer": "C",
                "options": [
                    "A. 3.",
                    "B. 4.",
                    "C. 6.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_98_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:04:41",
                "answer": "B",
                "options": [
                    "A. 2.",
                    "B. 8.",
                    "C. 3.",
                    "D. 7."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_98_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:05:24",
                "answer": "B",
                "options": [
                    "A. 2.",
                    "B. 8.",
                    "C. 3.",
                    "D. 7."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_98_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:00:58",
                "answer": "A",
                "options": [
                    "A. 1.",
                    "B. 2.",
                    "C. 3.",
                    "D. 4."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_83_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:03:07",
                "answer": "A",
                "options": [
                    "A. 6.",
                    "B. 5.",
                    "C. 4.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_83_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:04:02",
                "answer": "A",
                "options": [
                    "A. 7.",
                    "B. 2.",
                    "C. 3.",
                    "D. 6."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_83_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:05:01",
                "answer": "A",
                "options": [
                    "A. 8.",
                    "B. 2.",
                    "C. 7.",
                    "D. 6."
                ],
                "required_ability": "episodic memory"
            },
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:05:33",
                "answer": "A",
                "options": [
                    "A. 9.",
                    "B. 5.",
                    "C. 8.",
                    "D. 6."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_83_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:05]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What are the green cones on the road being used for right now?",
                "time_stamp": "00:00:03",
                "answer": "A",
                "options": [
                    "A. Signaling road construction.",
                    "B. Indicating a lane merge.",
                    "C. Marking a pedestrian walkway.",
                    "D. Reserving parking spaces."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_394_real.mp4"
    },
    {
        "time": "[0:01:59 - 0:02:04]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the predominant color of the buildings' facades right now?",
                "time_stamp": "00:02:02",
                "answer": "A",
                "options": [
                    "A. Red and brown.",
                    "B. Blue and gray.",
                    "C. Green and yellow.",
                    "D. Pink and orange."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_394_real.mp4"
    },
    {
        "time": "[0:03:58 - 0:04:03]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the main color of the awnings above the shop windows right now?",
                "time_stamp": "00:04:00",
                "answer": "B",
                "options": [
                    "A. Blue.",
                    "B. Red.",
                    "C. Green.",
                    "D. Yellow."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_394_real.mp4"
    },
    {
        "time": "[0:05:57 - 0:06:02]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the food cart on the right side of the road selling right now?",
                "time_stamp": "00:06:00",
                "answer": "A",
                "options": [
                    "A. Hot dogs and burgers.",
                    "B. Ice cream.",
                    "C. coffe.",
                    "D. marshmallow."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_394_real.mp4"
    },
    {
        "time": "[0:07:56 - 0:08:01]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color are the buses parked on the street right now?",
                "time_stamp": "00:08:00",
                "answer": "D",
                "options": [
                    "A. Red and white.",
                    "B. Yellow and grey.",
                    "C. Green and grey.",
                    "D. Blue and white."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_394_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video begins from a first-person perspective in an urban environment on a wet, rainy day. The viewer is positioned on a sidewalk composed of both dark and light tiles, running alongside a street. The sidewalk tiles vary in size and shape, with the larger, darker ones adjacent to the road and smaller, lighter ones closer to the walls on the left. A manhole cover embedded in the street and a series of concrete blocks are visible. [0:00:03 - 0:00:06]: The scene progresses forward, revealing more of the street and sidewalk. On the left, greenery and smaller plants line the sidewalk, adding a touch of color in contrast to the drab, rainy surroundings. The street itself has a clear, straight white line indicating traffic direction. [0:00:06 - 0:00:10]: Moving further, buildings with various facades appear on both sides of the street. The sidewalks are lined with bushes, and some low concrete barriers are visible on the left, which are plants of assorted green bushes that punctuate the mostly grey scene. The manhole cover appears more prominent in the middle of the street, emphasizing the wet, shiny road surface. [0:00:10 - 0:00:14]: The urban setting has buildings of different heights and styles, including some with metallic roller shutters. The viewer continuously moves down the street, nearing residential buildings. There's a noticeable incline in pavement closer to the buildings' edges, redirecting the water flow from the rain. [0:00:14 - 0:00:18]: The path showcases more residential buildings with windows, doors, and small front gardens. Some of the buildings have overhangs and balconies, which partially shield parts of the sidewalk from the rain. A series of parked vehicles, covered with white and grey tarpaulins, punctuate the scene. [0:00:18 - 0:00:20]: The video continues to advance down the wet street, capturing a more detailed view of the surrounding environment. Electrical poles and cables intersect overhead, and more details of the residential and commercial buildings become apparent. The continued presence of greenery and covered vehicles adds to the overall feel of a quiet, rainy day in the city.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What movement is depicted from the first-person perspective in the video?",
                "time_stamp": "0:00:18",
                "answer": "C",
                "options": [
                    "A. Moving backward on the street.",
                    "B. Moving sideways on the sidewalk.",
                    "C. Progressing forward down the street.",
                    "D. Standing still."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_307_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:20 - 0:04:40] [0:04:20 - 0:04:27]: The video depicts a narrow urban street on a rainy day, with wet asphalt reflecting buildings and surroundings. On the left side of the scene, there is a sidewalk bordered by greenery and various plants. The sidewalk is adjacent to a tall grey building that gradually becomes beige with a visible air conditioner unit. Moving towards the background, a smaller white car is parked straight ahead under a green and yellow building structure, while colorful posters adorn a pole near the sidewalk. On the right side, a reddish-brown brick building is visible with small plants and bushes along the edge.  [0:04:28 - 0:04:33]: As the video progresses, the viewer moves forward along the street, approaching the parked car. A wooden fence appears on the left side, adjacent to the sidewalk. The street slightly curves to the right, leading toward the parked white car under a metallic awning against a yellow wall. The road remains wet, with clear reflections of the surrounding structures. [0:04:34 - 0:04:37]: The viewer draws nearer to the parked white car, which now takes up a larger portion of the frame. The scene shows more fence detail, and a metal barrier with gates to a driveway becomes visible. On the right, large windows of a modern building reflect the damp street.  [0:04:38 - 0:04:39]: The video concludes with the viewer closely approaching the white car and the adjacent car beside it. The reflection on the road continues to be prominent, highlighting the rainy conditions. The building on the right side has large glass windows, and the overall scene depicts a calm, wet, urban environment.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What was on the left side of the street now?",
                "time_stamp": "00:04:32",
                "answer": "C",
                "options": [
                    "A. A series of tall trees.",
                    "B. A brick building with large windows.",
                    "C. A wooden fence.",
                    "D. A row of parked cars."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_307_real.mp4"
    },
    {
        "time": "[0:08:00 - 0:09:00]",
        "captions": "[0:08:40 - 0:09:00] [0:08:40 - 0:08:50]: The scene takes place on a city sidewalk during a rainy day, where the wet ground reflects light. Two people holding umbrellas walk side by side on the red brick sidewalk; the person on the right wears a dark jacket and white shoes, and the person on the left wears a green jacket and dark pants. To their left, a building with signage and a red vending machine is visible, while ahead of them, cars move along the wet street, and a green P parking sign is lit on the building across the street. Various buildings line the street, including multi-story residential and commercial structures. [0:08:51 - 0:08:59]: As the video progresses, they continue walking down the sidewalk with the same orientation. A green garbage truck appears on the street, and the pair approaches a crosswalk at an intersection. The crosswalk's white stripes contrast with the wet black asphalt. They navigate around a pole with street signs, pass some parked bicycles and scooters near the buildings, and proceed to the other side along the crosswalk, crossing the path of a small white truck. The environment remains consistently urban with buildings and parked vehicles on both sides.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color is the jacket worn by the person on the left?",
                "time_stamp": "00:09:04",
                "answer": "C",
                "options": [
                    "A. Blue.",
                    "B. Red.",
                    "C. Green.",
                    "D. Yellow."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Spatial Understanding",
                "question": "What is to the left of the two people walking on the sidewalk now?",
                "time_stamp": "00:08:43",
                "answer": "B",
                "options": [
                    "A. A green P parking sign.",
                    "B. A red vending machine and building signage.",
                    "C. Parked bicycles and scooters.",
                    "D. A garbage truck."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_307_real.mp4"
    },
    {
        "time": "[0:13:00 - 0:14:00]",
        "captions": "[0:13:00 - 0:13:20] [0:13:00 - 0:13:03]: The video begins on a rainy street, captured from a first-person perspective. A clear umbrella is visible, displaying raindrops and the umbrella's ribs. On the left-hand side, there is a restaurant with a brightly lit signboard and a display showing various food dishes. A few people, some with umbrellas, can be seen walking along the sidewalk. [0:13:04 - 0:13:06]: The perspective shifts slightly to the right, revealing more of the sidewalk as it moves forward. The restaurant signage continues to be visible, while additional pedestrians, some holding black umbrellas, walk towards the walker. Further ahead, a man with a light jacket walks in the same direction as the camera. [0:13:07 - 0:13:09]: The walker continues down the sidewalk, which has a clear red awning stretching over it, providing cover from the rain. The awning stretches to the end of the frame with various shops lining the left side. The sidewalk has several trash bins positioned on the right edge, closer to the street where cars pass by amidst the rain. [0:13:10 - 0:13:13]: As the walker progresses, more shops are seen on the left, selling books and magazines. Stacks of books are neatly organized on tables, with some stands displaying open pages. The path ahead appears clear, with a few pedestrians further in the distance. [0:13:14 - 0:13:19]: The walker moves closer to a shop displaying more books and possibly art. The sidewalk, still covered by the red awning, is wet from the rain. The street on the right shows a steady stream of traffic in the rain. The person holding the umbrella continues to walk straight ahead, moving past additional stores and their displays.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What was visible on the left-hand side of the street just now?",
                "time_stamp": "00:13:06",
                "answer": "B",
                "options": [
                    "A. A coffee shop.",
                    "B. A brightly lit signboard of a restaurant.",
                    "C. A bookstore.",
                    "D. A clothing store."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_307_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:20]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What device is the person currently using to capture the video?",
                "time_stamp": "00:00:01",
                "answer": "A",
                "options": [
                    "A. Head-mounted camera.",
                    "B. Smartphone.",
                    "C. DSLR camera.",
                    "D. Camcorder."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_421_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:02:20]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What type of vehicle is visible in the parking lot right now?",
                "time_stamp": "00:02:03",
                "answer": "A",
                "options": [
                    "A. Car.",
                    "B. Heavy truck.",
                    "C. Bus.",
                    "D. Helicopter."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_421_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:04:20]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What type of aircraft is visible right now?",
                "time_stamp": "00:04:10",
                "answer": "A",
                "options": [
                    "A. Helicopter.",
                    "B. Airplane.",
                    "C. Drone.",
                    "D. Glider."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_421_real.mp4"
    },
    {
        "time": "[0:06:00 - 0:06:20]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the person grabbing from the food basket right now?",
                "time_stamp": "00:06:00",
                "answer": "A",
                "options": [
                    "A. Chicken strips.",
                    "B. French fries.",
                    "C. Onion rings.",
                    "D. Mozzarella sticks."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_421_real.mp4"
    },
    {
        "time": "[0:08:00 - 0:08:20]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What navigational tool is visible on the screen right now?",
                "time_stamp": "00:08:09",
                "answer": "A",
                "options": [
                    "A. GPS device.",
                    "B. Compass.",
                    "C. Altimeter.",
                    "D. Airspeed indicator."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_421_real.mp4"
    },
    {
        "time": "0:00:00 - 0:00:20",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:09]: Two hands are holding a decorative paper wreath. The wreath is composed of alternating dark red and cream-colored flowers with green leaves. They are symmetrically placed in a circular pattern. Each flower has a golden bead at its center. The background features a grey grid pattern, giving a precise and organized appearance. The hands slowly rotate the wreath to showcase its entirety. [0:00:10]: The screen turns black. [0:00:11 - 0:00:13]: The text \u201cDIY Paper Christmas Wreath\u201d appears centered on a black background. [0:00:14]: The screen turns black. [0:00:15 - 0:00:19]: The text \u201cCut files available in: - SVG / DXF format for die cutting machines - PDF format for print and hand cut\u201d is displayed on a black screen.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What are the hands doing right now?",
                "time_stamp": "00:00:08",
                "answer": "B",
                "options": [
                    "A. Arranging flowers on a table.",
                    "B. Rotating a decorative paper wreath.",
                    "C. Painting a wreath.",
                    "D. Holding a black screen."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_58_real.mp4"
    },
    {
        "time": "0:02:00 - 0:02:20",
        "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:07]: The video begins with a top-down view of a Cricut cutting machine in action. The machine has a light gray and mint green body with a control knob on the right side. The cutting mechanism, labeled \"Cricut Cut Smart 2,\" is situated at the center of the machine. In the cutting area, a green cutting mat holds a piece of light beige paper in place. The machine's blade is positioned over the paper, as it starts cutting intricate lines that appear to form a flower pattern. The cutting head moves horizontally from left to right, and then vertically, making precise incisions on the paper. The work area beneath the machine shows a gridded pattern in dark gray and white, providing a textured background. [0:02:08 - 0:02:15]: As the cutting machine continues its operation, the flower pattern becomes more visible with each cut. The cutting head moves primarily from the left side to the middle and slightly toward the right side of the machine, maintaining precision and steadiness. The blade makes fine incisions, adding details to the flower petals on the beige paper. The background with the gridded pattern consistently remains visible, providing context to the workspace setup. Towards the end of this segment, some parts of the flower pattern are fully cut out. [0:02:16 - 0:02:19]: The view now shifts to a new segment where a different material, a brown cardboard-like paper, is positioned on the green cutting mat inside the Cricut machine. The cutting machine's head starts at the center again, and the blade is poised to begin a new cut. The material is flat and evenly laid out beneath the blade. The control knob on the right side of the machine and other mechanical parts remain unchanged in this view.  [0:02:20]: The cutting head initiates a new cutting sequence on the brown cardboard. A text overlay at the bottom of the video appears, stating, \"I had to split this card because the '12x12' size was out of stock.\" The cutting machine\u2019s blade continues its precise movement, working on the brown material with the same accuracy as it did with the beige paper earlier.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the machine doing right now?",
                "time_stamp": "0:02:18",
                "answer": "B",
                "options": [
                    "A. Pausing the cutting process.",
                    "B. Initiating a new cutting sequence.",
                    "C. Moving the cutting mat.",
                    "D. Changing the blade."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_58_real.mp4"
    },
    {
        "time": "0:04:00 - 0:04:20",
        "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:01]: Hands, clothed in a maroon-colored knitted sweater, are manipulating paper petals to form a flower. On the checkered cutting mat in the background, multiple completed paper flowers are lying around; cream-colored flowers on the left and red-colored ones on the right.  [0:04:01 - 0:04:02]: One hand holds the cream-colored paper petals together while the other hand adds another petal to the assembly. The petals are arranged precisely and fit together to form a flower. [0:04:02 - 0:04:03]: Now, both hands are seen carefully bringing more petals together to complete the flower. The cream-colored flower in focus has six petals. [0:04:03]: The hands take a brief pause as if inspecting the flower, ensuring the petals are aligned correctly. The background remains the same, with a grid mat and completed flowers at the top left and right corners. [0:04:04 - 0:04:06]: The hands resume the process of assembling paper petals to form more flowers. Checking occasionally to ensure everything is coming together nicely. [0:04:06 - 0:04:09]: The hands continue their work, now moving to what seems to be a finished part of the flower assembly and adjusting the petals to ensure symmetry and alignment. [0:04:09 - 0:04:11]: A finished cream-colored flower is placed on the mat beside other completed flowers. The hands begin gathering another set of petals for a new flower assembly. [0:04:11 - 0:04:13]: With careful and precise movements, the hands continue the process of flower assembly. The repetitive motion of joining petals is observed. [0:04:13 - 0:04:16]: Once the cream-colored flower is complete, the hands move aside and pick up a red-colored set of petals next. The attention is shifted towards the assembly of a red flower. [0:04:16 - 0:04:19]: Using a tool, likely a pen, the hands begin outlining or scoring the red paper petals to give them a lifelike curve or shape. More red petals are carefully laid out on the mat, prepared for the same treatment. [0:04:19 - 0:04:20]: The tool is used to make precise lines on the petals. The background remains consistent with the earlier setup of flowers and cutting mat. [0:04:20 - 0:04:21]: Each petal is meticulously scored or outlined to add dimension. The hands demonstrate a repetitive, skilled motion in the flower-making process. [0:04:21 - 0:04:24]: The process of scoring or outlining continues; the red petals' edges become more pronounced. The grid background and scattered flowers remain unchanged, providing a consistent workspace. [0:04:24 - 0:04:25]: The transition from one petal to another is smooth; every petal receives the same detailed attention with the tool. The hands are very steady, ensuring each flower petal is perfectly shaped. [0:04:25 - 0:04:28]: The petals are organized in groups for a more systematic flower formation. The hands appear to follow a methodical approach, ensuring each petal is scored or outlined perfectly before proceeding to assemble the flower. [0:04:28 - 0:04:29]: Despite the repetitiveness, each petal looks unique yet uniform. The hands skillfully manage multiple tasks - handling petals, using the tool, and organizing the workspace. [0:04:29 - 0:04:31]: The work is delicate and seems very involved, with hands constantly engaged in positioning, outlining, or pondering over the next steps in this crafting process.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What are the hands doing right now?",
                "time_stamp": "0:04:20",
                "answer": "B",
                "options": [
                    "A. Assembling cream-colored paper petals together.",
                    "B. Adding dimension by scoring the red petals.",
                    "C. Sorting finished cream-colored flowers.",
                    "D. Disposing of unwanted petals."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_58_real.mp4"
    },
    {
        "time": "0:06:00 - 0:06:20",
        "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:02]: The scene shows a person working on a craft project involving paper flowers. The background is a checkered grey cutting mat, and on it lies four assembled red paper flowers in two rows at the top part. The person\u2019s hands are seen in the lower part of the frame, arranging a red and green paper flower that has already been partially assembled. The person appears to be placing the red petals onto the green base, adding depth to the flower. They are wearing a purple long-sleeve sweater. [0:06:02 - 0:06:06]: The person continues to press the red petals onto the green base carefully. The focus remains on the flower being assembled, which is taking shape as layers are added. The right hand is slightly above the flower, with fingers positioned to adjust the petals, while the left hand holds the flower in place. [0:06:06 - 0:06:07]: The right hand reaches for one of the fully assembled red paper flowers from the top row. The remaining completed flowers stay in place. [0:06:07 - 0:06:08]: The right hand brings the taken red flower closer to the crafting area. Meanwhile, the left hand continues to stay near the red and green flower. [0:06:08 - 0:06:10]: Both hands move the red paper flower closer to the partially assembled red and green flower. The left hand appears to be positioning the flower, while the right hand adjusts the red petals. [0:06:10 - 0:06:12]: The person\u2019s hands position the red flower on top of the partially assembled one, layering it on the green base. The right hand ensures that the petals align correctly. [0:06:12 - 0:06:14]: Adjustment of the petals continues with both hands ensuring the newly place flower is secure and aligned as desired. [0:06:14 - 0:06:16]: The right hand completes the adjustments, moving slightly away from the finished work to give a final look at the positioning of all petals. [0:06:16 - 0:06:18]: The person\u2019s hands move the newly completed red and green flower slightly to present it properly and ensure all petals are well adjusted. Three fully assembled red flowers remain visible at the top. [0:06:18 - 0:06:20]: After completing adjustments, the person\u2019s hands move the finished flower into a line with the other red flowers at the top of the frame. The background remains the same with its checkered grey pattern. The hands retrieve another green base to start the next assembly.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What are the person's hands doing right now?",
                "time_stamp": "00:06:02",
                "answer": "B",
                "options": [
                    "A. Cutting paper into flower shapes.",
                    "B. Placing a finished flower with others.",
                    "C. Assembling a new set of flower petals.",
                    "D. Drawing a design on the cutting mat."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_58_real.mp4"
    },
    {
        "time": "0:08:00 - 0:08:20",
        "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:02]: The video begins with a first-person view of a hand moving towards a partially completed wreath placed on a checkered grid background. The wreath is constructed from a green ring base and decorated with a combination of five red and white artificial poinsettia flowers with green leaves. The red flowers are positioned at the top left and bottom right, and the white flowers are at the top center and bottom left. The hand is positioned on the left side of the wreath, preparing to adjust it. A logo \u201cDIY CRAFT TUTORIALS\u201d is visible in the bottom right corner;  [0:08:02 - 0:08:08]: The person\u2019s hand continues to adjust the flowers on the wreath, adding a red poinsettia at the lower right position, slightly above the already existing red flower at the bottom right. Another hand enters the frame from below, holding another red poinsettia;  [0:08:09 - 0:08:13]: With both hands, the person positions the new red poinsettia at the lower left part of the wreath, ensuring equal spacing. Simultaneously, the person continues to adjust the previously positioned white poinsettia to achieve a symmetrical and aesthetically arranged pattern;  [0:08:14 - 0:08:17]: The hands rearrange the flowers along the bottom of the wreath to perfect their positions, with slight adjustments made to the surrounding green leaves. Small movements ensure the wreath looks balanced and the colors are evenly distributed;  [0:08:18 - 0:08:19]: With the final adjustments made, the person's hands move away slightly as the video shows a fully decorated wreath featuring a symmetrical alternation of red and white poinsettias around the entire green ring base. The hands are then positioned at the bottom corners of the frame.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What are the person's hands doing right now?",
                "time_stamp": "00:08:22",
                "answer": "B",
                "options": [
                    "A. Spraying water on the flowers.",
                    "B. Rearranging flowers and leaves on the wreath.",
                    "C. Cutting the flowers with scissors.",
                    "D. Painting the flowers on the wreath."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_58_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:00:16",
                "answer": "D",
                "options": [
                    "A. 2.",
                    "B. 3.",
                    "C. 4.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            },
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:00:56",
                "answer": "D",
                "options": [
                    "A. 1.",
                    "B. 3.",
                    "C. 4.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_102_real.mp4"
    },
    {
        "time": "[0:01:00 - 0:02:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:01:26",
                "answer": "A",
                "options": [
                    "A. 0.",
                    "B. 3.",
                    "C. 4.",
                    "D. 1."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_102_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:03:26",
                "answer": "D",
                "options": [
                    "A. 3.",
                    "B. 4.",
                    "C. 6.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_102_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:04:41",
                "answer": "D",
                "options": [
                    "A. 2.",
                    "B. 8.",
                    "C. 3.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_102_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:05:04",
                "answer": "B",
                "options": [
                    "A. 2.",
                    "B. 0.",
                    "C. 3.",
                    "D. 7."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_102_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:05]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is mounted on the lamp post right now?",
                "time_stamp": "00:00:05",
                "answer": "C",
                "options": [
                    "A. A camera.",
                    "B. A streetlight.",
                    "C. An American flag.",
                    "D. A traffic sign."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_380_real.mp4"
    },
    {
        "time": "[0:02:07 - 0:02:12]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the predominant color of the banners on the street right now?",
                "time_stamp": "00:02:08",
                "answer": "A",
                "options": [
                    "A. Purple.",
                    "B. Red.",
                    "C. Blue.",
                    "D. Green."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_380_real.mp4"
    },
    {
        "time": "[0:04:14 - 0:04:19]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Which street is indicated on the green sign right now?",
                "time_stamp": "00:04:16",
                "answer": "A",
                "options": [
                    "A. E 40 St.",
                    "B. E 34 St.",
                    "C. W 42 St.",
                    "D. Broadway."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_380_real.mp4"
    },
    {
        "time": "[0:06:21 - 0:06:26]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is positioned next to the road right now?",
                "time_stamp": "00:06:24",
                "answer": "C",
                "options": [
                    "A. Parking meters.",
                    "B. Trash bins.",
                    "C. Orange traffic barrels.",
                    "D. Benches."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_380_real.mp4"
    },
    {
        "time": "[0:08:28 - 0:08:33]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What sign is visible on the left side of the street right now?",
                "time_stamp": "00:08:28",
                "answer": "A",
                "options": [
                    "A. One Way.",
                    "B. Speed Limit.",
                    "C. No Parking.",
                    "D. Yield."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_380_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:20]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What object is visible on the left side of the cockpit?",
                "time_stamp": "00:00:09",
                "answer": "A",
                "options": [
                    "A. A blue handler.",
                    "B. A red first aid kit.",
                    "C. A yellow flashlight.",
                    "D. A green map."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_419_real.mp4"
    },
    {
        "time": "[0:02:25 - 0:02:45]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the dominant color of the pants the person is wearing right now?",
                "time_stamp": "00:02:38",
                "answer": "A",
                "options": [
                    "A. Beige.",
                    "B. Blue.",
                    "C. Black.",
                    "D. Gray."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_419_real.mp4"
    },
    {
        "time": "[0:04:50 - 0:05:10]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Which way is the plane turning now?",
                "time_stamp": "00:05:12",
                "answer": "C",
                "options": [
                    "A. Left.",
                    "B. Right.",
                    "C. Up.",
                    "D. Down."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_419_real.mp4"
    },
    {
        "time": "[0:07:15 - 0:07:35]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What object is visible in the lower-left corner of the cockpit right now?",
                "time_stamp": "00:07:33",
                "answer": "A",
                "options": [
                    "A. A blue handler.",
                    "B. A red first aid kit.",
                    "C. A yellow flashlight.",
                    "D. A green map."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_419_real.mp4"
    },
    {
        "time": "[0:09:40 - 0:10:00]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the current lighting condition as seen right now?",
                "time_stamp": "00:09:54",
                "answer": "A",
                "options": [
                    "A. Bright and sunny.",
                    "B. Overcast.",
                    "C. Dusk.",
                    "D. Night."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_419_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:00:16",
                "answer": "D",
                "options": [
                    "A. 2.",
                    "B. 3.",
                    "C. 4.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_87_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:03:04",
                "answer": "A",
                "options": [
                    "A. 4.",
                    "B. 5.",
                    "C. 6.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_87_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:04:11",
                "answer": "B",
                "options": [
                    "A. 2.",
                    "B. 4.",
                    "C. 3.",
                    "D. 6."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_87_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:05:01",
                "answer": "C",
                "options": [
                    "A. 2.",
                    "B. 5.",
                    "C. 4.",
                    "D. 6."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_87_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:06:19",
                "answer": "D",
                "options": [
                    "A. 4.",
                    "B. 8.",
                    "C. 6.",
                    "D. 5."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_87_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:30]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Where does this letter on the ground come from?",
                "time_stamp": "00:00:39",
                "answer": "D",
                "options": [
                    "A. It was delivered by the postman earlier.",
                    "B. It fell out of someone's pocket as they walked by.",
                    "C. It was dropped by a courier who was in a hurry.",
                    "D. Mr. Bean just stuffed it in."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_240_real.mp4"
    },
    {
        "time": "[0:02:09 - 0:02:39]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why does Mr. Bean's sitting figure react with confusion or surprise?",
                "time_stamp": "00:02:29",
                "answer": "A",
                "options": [
                    "A. Because Mr. Bean interacts with the TV remote.",
                    "B. Because Mr. Bean gets distracted by a phone call.",
                    "C. Because the TV suddenly turns off.",
                    "D. Because Mr. Bean falls asleep while watching TV."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_240_real.mp4"
    },
    {
        "time": "[0:04:18 - 0:04:48]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why does the wall have green patterns painted on it?",
                "time_stamp": "00:04:37",
                "answer": "C",
                "options": [
                    "A. Because the wall was decorated with a stencil and green spray paint.",
                    "B. Because the man accidentally spilled green paint while working on a project.",
                    "C. Because the man applied green paint using a teddy bear attached to a stick.",
                    "D. Because the wall was originally green, and someone tried to cover it up but failed."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_240_real.mp4"
    },
    {
        "time": "[0:06:27 - 0:06:57]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why does the person with multiple cameras leave the room?",
                "time_stamp": "00:06:34",
                "answer": "C",
                "options": [
                    "A. Because the person with cameras needed to recharge their equipment.",
                    "B. Because the person with cameras had an urgent phone call.",
                    "C. Because he has finished taking the photos.",
                    "D. Because the person with cameras spotted something interesting in the other room."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_240_real.mp4"
    },
    {
        "time": "[0:08:36 - 0:09:06]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why is the queenly figure angry right now?",
                "time_stamp": "00:08:35",
                "answer": "C",
                "options": [
                    "A. Because someone accidentally spilled tea on her dress.",
                    "B. Because her crown was knocked off by a gust of wind.",
                    "C. Because the Mr.bean pushing the broom past her face.",
                    "D. Because Mr. Bean interrupted her speech with loud noises."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_240_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:13]: The video begins with a close-up of a painting. The painting features the portrait of a young woman with blonde hair against a dark blue background with scattered white dots resembling stars. Pink clouds are visible on the right side of the painting. The portrait appears well-lit, emphasizing the woman's soft facial features and the gentle blending of colors in her hair, which ranges from light blonde to hints of darker shades. There is a bright white circle in the background, which seems to represent the moon, situated to the left of the woman's head. The video is filmed from a steady, first-person perspective, focusing on the upper part of the painting. [0:00:14]: The scene shifts to a different view showing a blank white canvas or paper. The surface is oriented horizontally, and the edges of the paper are visible. The background now appears to be a light-toned surface, perhaps a table or desk. The lighting is even, without casting heavy shadows, indicating a well-lit area. [0:00:15 - 0:00:16]: An object, which looks like a piece of gray fabric or paper, enters the frame from the right side and is placed over the white canvas. The first-person perspective is maintained, and the object is moved into position quickly, covering a portion of the canvas. [0:00:17 - 0:00:18]: A black-and-white printed photograph of the same woman depicted in the painting appears, placed on top of the gray fabric or paper. The portrait in the photograph shows the woman in a similar pose, with her head angled slightly upward. The fabric or paper underneath is partially visible around the edges of the photograph. [0:00:19 - 0:00:20]: A hand holding a blue pen starts to trace or draw over the photograph, focusing on the contours and details of the woman's face. The other hand holds the photograph steady, indicating that the person is using the printed image as a reference or template for their artwork. The first-person perspective captures the entire process from above.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What happens right after the scene shifts to a different view showing a blank white canvas?",
                "time_stamp": "0:00:16",
                "answer": "A",
                "options": [
                    "A. A piece of gray fabric is placed over the canvas.",
                    "B. The canvas is painted with light colors.",
                    "C. A new portrait appears on the canvas.",
                    "D. The video zooms out to show the entire table."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_125_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:06]: A hand is actively painting a portrait of a woman with light-colored hair on a white canvas. The brush adds depth and detail to the hair on the left side of the woman\u2019s head. In the background, a yellow circle is situated in the upper left, resembling a sun, while cloud outlines stretch across the canvas. [0:03:07 - 0:03:12]: The focus remains on the left side of the woman\u2019s hair and face. The hand continues to add layers of paint. The brush moves carefully around the ear and cheek areas, enhancing the texture and shading. [0:03:13 - 0:03:15]: The brush moves towards the lower part of the woman\u2019s face, concentrating on the chin and neck regions. The hair and facial features are becoming more defined, adding to the portrait\u2019s realism. [0:03:16 - 0:03:19]: The artist's hand shifts attention to the top of the head, making sure to add highlights and shadows to give the hair volume and a sense of light source. The yellow circle and cloud patterns in the background remain constant throughout the video.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color is the circle in the background of the painting?",
                "time_stamp": "00:03:19",
                "answer": "D",
                "options": [
                    "A. Red.",
                    "B. Blue.",
                    "C. Green.",
                    "D. Yellow."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_125_real.mp4"
    },
    {
        "time": "[0:06:00 - 0:07:00]",
        "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:06]: A hand holding a paintbrush is seen painting a detailed portrait of a woman with blonde hair. The background is a mixture of dark blue at the top and pink hues at the bottom. The paintbrush is applying light, intricate strokes to the subject's hair, blending in various shades of white and blonde to create texture and depth in the hair. [0:06:07 - 0:06:14]: The focus shifts to the lower part of the painting, where the hand continues to paint pink and purple hues below the subject's hair. The brush adds layers of color, suggesting a background or additional elements within the painting. The strokes are broader, and the colors are more blended, creating a soft and vibrant transition. [0:06:15 - 0:06:19]: The brush returns to work on an area near the woman's hair where the dark blue background meets the pink hues. The brush adjusts and refines the transition between these sections, adding subtle details and blending to smooth the contrast between the colors. The hand appears steady, providing detailed and delicate brushwork to enhance the overall composition.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the hand doing just now?",
                "time_stamp": "00:06:06",
                "answer": "D",
                "options": [
                    "A. Painting a landscape.",
                    "B. Blending the background colors.",
                    "C. Painting the subject's face.",
                    "D. Adding details to the hair."
                ],
                "required_ability": "episodic memory"
            },
            {
                "task_type": "Event Understanding",
                "question": "What is the hand doing when it shifts focus to the lower part of the painting?",
                "time_stamp": "00:06:27",
                "answer": "B",
                "options": [
                    "A. Adding texture to the hair.",
                    "B. Painting broad strokes with pink and purple hues.",
                    "C. Detailing the woman's face.",
                    "D. Refining the transition between background colors."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_125_real.mp4"
    },
    {
        "time": "[0:09:00 - 0:10:00]",
        "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:05]: A hand is holding a small metallic object, possibly a screw, and pointing it towards a piece of dark blue paper or canvas. There is a white circular shape on the canvas, surrounded by small white dots, likely representing stars. The hand moves slightly, positioning the object closer to the canvas. [0:09:06 - 0:09:19]: The perspective shifts to reveal a detailed painting of a woman with blonde hair on the same dark blue background. The painting includes a full moon and clouds in the background. The woman is depicted in a three-quarter profile, looking towards the right side of the frame. As the video progresses, the painting stays mostly static, highlighting the intricate details and colors of the artwork, including the realistic textures of the woman's hair and the soft gradients in the background. [0:09:20]: The static view of the painting continues, emphasizing the vivid colors and precise details, particularly in the shading and highlights on the woman's face and hair, as well as the blending of colors in the clouds.",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following best summarizes the painting shown in the video?",
                "time_stamp": "00:09:20",
                "answer": "A",
                "options": [
                    "A. A detailed painting of a woman with blonde hair on a dark blue background with a full moon and clouds.",
                    "B. A landscape painting with mountains and rivers.",
                    "C. An abstract painting with vibrant colors and shapes.",
                    "D. A close-up portrait of an elderly man."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_125_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video begins with a view of an indoor track where multiple remote-controlled (RC) cars are racing. The background includes a banner on the left with the word \"FALKEN,\" and there is a spectator kneeling on the floor. A tool bag is visible in the background. The cars, featuring various designs and colors, navigate a curved section of the track, surrounded by barriers, with the primary focus on a red car and a white car. [0:00:04 - 0:00:07]: The camera angle shifts slightly to follow the RC cars as they continue racing. The red car leads, followed closely by other cars with different paint jobs. The track is outlined with small barriers and some scattered leaves or debris. There is an additional green RC car visible in the background near a small white and yellow structure that might be a pit area or control station. [0:00:08 - 0:00:11]: The cars navigate another bend with some gaining speed. A variety of obstacles and barriers, including red and white striped barriers, segment the track. A person is seen standing near the track's edge, and several more spectators are visible in the background. The area around the track includes a setup of tables, tools, and equipment likely used by participants. [0:00:12 - 0:00:17]: Two of the RC cars, one black and one red, are seen with considerable speed and maneuverability as they continue through the course. The video shows more detail of the surroundings, including stacked barriers and a detailed view of the indoor setting. A person is walking along the track, actively involving in either monitoring or possibly retrieving a car. [0:00:18 - 0:00:20]: The final frames show an overview of the track with multiple RC cars racing and several participants and spectators engaged in the background. The indoor space appears spacious with various setups, indicative of a competition or showcase for RC car enthusiasts.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "Which RC car leads the race initially?",
                "time_stamp": "0:00:02",
                "answer": "C",
                "options": [
                    "A. White car.",
                    "B. Green car.",
                    "C. Red car.",
                    "D. Black car."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_482_real.mp4"
    },
    {
        "time": "[0:01:00 - 0:02:00]",
        "captions": "[0:01:20 - 0:01:40] [0:01:20 - 0:01:23]: A white remote-controlled car with black accents and a spoiler is navigating a marked indoor track. There are red and white striped barriers on the track. The background shows structures such as an orange chair and a metal ladder; [0:01:24 - 0:01:27]: The RC car continues to move around the track, approaching some scattered red obstacles that resemble bricks or another type of small debris. The car maintains steady speed and follows the curve of the track; [0:01:28 - 0:01:32]: The white RC car drives through the red obstacles, pushing them aside as it passes. The vehicle appears to be designed for off-road or complex tracks, giving it the ability to traverse the uneven surface; [0:01:33 - 0:01:38]: After passing through the debris, the car continues on a clear path that is bordered by more red and white barriers. The track loops around multiple obstacles, showcasing the agility and control of the RC car; [0:01:39 - 0:01:40]: As the car navigates through the track, other RC cars and various objects like ramps and barriers are visible. The background includes a large banner advertising tires and other racing-related items, indicating the setting is likely a controlled environment such as a competition or exhibition.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What abnormal event just happened?",
                "time_stamp": "0:01:32",
                "answer": "A",
                "options": [
                    "A. A white racing car crashed into the obstacle beside the track.",
                    "B. A blue racing car lost control and crashed into the barrier at the curve of the track.",
                    "C. A red racing car skidded on the wet surface and collided with the guardrail on the straightaway.",
                    "D. A yellow racing car spun out and hit the tire wall at the chicane."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_482_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:43]: The video begins with a view of a red and black RC car on a race track. The car is positioned diagonally on the track with red and white striped barriers around it. The car is decorated with various sponsor stickers and has a detailed design. [0:02:43 - 0:02:46]: The camera then cuts to a different point on the track where an orange and white RC car is driving. People and various pieces of RC equipment are visible in the background. The car appears to be maneuvering around the track. [0:02:46 - 0:02:48]: The orange and white RC car continues to drive, passing by more red and white barriers. The surroundings include a blue banner with text and other items related to the event. [0:02:48 - 0:02:50]: The car makes a turn, showing a better view of its details and the track. There is some sort of foliage in the background along with additional barriers. [0:02:50 - 0:02:53]: The car continues to drift along the track, showing its movement and the tire marks on the ground. It creates a playful and thrilling scene as it turns and speeds. [0:02:53 - 0:02:56]: The RC car approaches several large block-like barriers. The car maneuvers close to these barriers, giving a sense of the driving skill and precision involved. [0:02:56 - 0:02:58]: The car continues to drive around these obstacles. The driver's control over the car is evident as it navigates close proximity to the barriers without hitting them. [0:02:58 - 0:03:00]: The video ends with the RC car driving past more red barriers and what appears to be some landscaping or decorative elements on the track. It showcases the complexity and enjoyable aspects of this RC car racing event.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What unusual event just happened?",
                "time_stamp": "00:02:14",
                "answer": "D",
                "options": [
                    "A. A red and white car skidded and slammed into the blue barrier at the edge of the circuit.",
                    "B. A green and silver car lost control and collided with the yellow obstacle beside the straight track.",
                    "C. A blue and orange car spun out and crashed into the red barrier near the corner.",
                    "D. A black and gold car crashed into the orange obstacle on the side of the track after drifting."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_482_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:02]: A small, first-person view remote-controlled police car, with blue and red flashing lights, is moving on a gray concrete road. The car is mainly black with white doors featuring a badge. There are yellow and black caution stripes along the edge of the track. [0:04:03 - 0:04:06]: The police car moves along the track, passing by a green and yellow bulldozer model. It is near a barrier with yellow and black caution stripes, and some people and other models can be seen in the background. [0:04:07 - 0:04:08]: Several small, remote-controlled racing cars appear on a wider stretch of the track. There are various models in different colors, including a green car, a red car, white cars, and another silver car. The area is surrounded by a barrier on one side and has some decorative, red objects scattered along the track. [0:04:09 - 0:04:11]: The remote-controlled cars continue racing on the track. They are maintaining tight but controlled movements, staying within the white lines that mark the lanes on the gray asphalt surface. There is a blue sponsor banner visible in the background. [0:04:12 - 0:04:13]: The racing continues with the cars moving curve right, maintaining consistent speeds. The red car and silver car are side-by-side as they navigate the turn. [0:04:14]: The silver car moves ahead of the red car coming out of the curve into a short straightaway. The track has some scattered obstacles in the form of red items along the inside lane. [0:04:15]: Both cars continue racing, entering another turn. The silver car is ahead of the red car, while more cars appear on the other side of the course. The background includes barriers with advertising banners and a mock race setup. [0:04:16]: The green and white cars continue along the course in front of a large blue sponsor banner, while other cars navigate different parts of the track. The racing action maintains a high pace with tight control. [0:04:17 - 0:04:18]: Different perspectives show cars navigating the track, including a red truck and other vehicles moving through curves and straight sections. Colored barriers and scattered debris add elements of course complexity. [0:04:19 - 0:04:20]: The green car moves quickly along a straight section of the track, passing background elements, including other cars and a small building structure that seems part of the course\u2019s environment. There is also a purple truck visible in the background.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the main color of the remote-controlled police car?",
                "time_stamp": "0:04:02",
                "answer": "A",
                "options": [
                    "A. Black.",
                    "B. White.",
                    "C. Blue.",
                    "D. Red."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Event Understanding",
                "question": "What happens just after the silver car moves ahead of the red car?",
                "time_stamp": "0:04:15",
                "answer": "D",
                "options": [
                    "A. The red car overtakes the silver car again.",
                    "B. Both cars stop racing.",
                    "C. The green car crashes into the silver car.",
                    "D. The red car knocked the silver car off the track."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_482_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:20]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color is the hat the man is wearing right now?",
                "time_stamp": "00:00:02",
                "answer": "C",
                "options": [
                    "A. White.",
                    "B. Blue.",
                    "C. Gray.",
                    "D. Black."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_418_real.mp4"
    },
    {
        "time": "[0:02:19 - 0:02:39]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the primary color of the navigation panel right now?",
                "time_stamp": "00:02:27",
                "answer": "B",
                "options": [
                    "A. Yellow.",
                    "B. Black.",
                    "C. Blue.",
                    "D. White."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_418_real.mp4"
    },
    {
        "time": "[0:04:38 - 0:04:58]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the general shape of the fields visible right now?",
                "time_stamp": "00:04:42",
                "answer": "C",
                "options": [
                    "A. Circular.",
                    "B. Triangular.",
                    "C. Rectangular.",
                    "D. Irregular."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_418_real.mp4"
    },
    {
        "time": "[0:06:57 - 0:07:17]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What type of vehicles are visible right now?",
                "time_stamp": "00:07:06",
                "answer": "C",
                "options": [
                    "A. Cars.",
                    "B. Bicycles.",
                    "C. Gliders.",
                    "D. Trains."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_418_real.mp4"
    },
    {
        "time": "[0:09:16 - 0:09:36]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the predominant color of the mountains visible right now?",
                "time_stamp": "00:09:20",
                "answer": "B",
                "options": [
                    "A. Brown.",
                    "B. Green.",
                    "C. Gray.",
                    "D. White."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_418_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins with a black screen and then transitions to a scenic view of a Minecraft landscape. The scene showcases a mountainous area with lush greenery, trees scattered across the hills, and a body of water extending towards the right. A cave opening can be seen on the left side of the mountain, adding depth to the landscape. The sky is clear with a few clouds scattered across. [0:00:03]: A title card appears, indicating \"MINECRAFT 1.19\" in bold, block letters, superimposed on the same landscape view as the previous frame. The image is slightly blurred to make the text more prominent. [0:00:04 - 0:00:06]: The screen transitions to black with white text appearing gradually. The text initially reads \"AND I HAVE SOME\" and then \"SOME BIVERY,\" likely a typo or an incomplete word. [0:00:07 - 0:00:09]: The scene changes to a first-person view standing on a wooden dock in Minecraft. The player character, wearing a full set of iron armor, is visible on the dock. The dock is connected to a small fishing hut, with wooden barrels placed on the sides. The background consists of green hills and trees, providing a serene setting. The player's health and hunger bars are shown at the bottom of the screen.  [0:00:10 - 0:00:11]: The character continues to stand on the wooden dock in a similar position as the previous frame. The background includes a small house constructed from wooden planks and birch wood, accessible via a wooden staircase. The surrounding area is lush with greenery, and trees dot the landscape. [0:00:12 - 0:00:13]: The character moves slightly, providing a clearer view of the wooden fishing hut. Additional details of the hut become visible, such as a hanging lantern near the entrance and a small garden plot to the right of the hut. The dock extends into the water, suggesting a peaceful fishing spot. [0:00:14 - 0:00:15]: The camera shifts focus towards the fishing hut, emphasizing its construction details. The hut has a pitched roof with a small chimney on top, releasing smoke particles. Wooden barrels and crates are arranged around the entrance, adding to the rustic feel. The terrain includes tiered grassy platforms leading up the hill. [0:00:16 - 0:00:17]: The camera angle changes to a closer view of the fishing hut's front door. The architecture of the hut showcases the detailed use of different wood textures. Inside, a lantern hangs, giving a warm, inviting ambiance. There are no signs of other players or mobs. [0:00:18 - 0:00:19]: The scene transitions inside the fishing hut. The interior is compact but cozy, with wooden walls and floors. A brick chimney is prominent on the left side, adding a touch of homeliness to the hut. The inside lighting appears sufficient, casting soft shadows. The video concludes with this view of the interior.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What can be seen on the left side of the wooden house at the beginning of the video?",
                "time_stamp": "0:00:20",
                "answer": "D",
                "options": [
                    "A. A waterfall.",
                    "B. A village.",
                    "C. A tower.",
                    "D. A cave opening."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "minecraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_200_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "captions": "[0:03:20 - 0:03:40] [0:03:21 - 0:03:23]: The video begins at the corner of a wooden house. The walls are made of oak logs and planks, with a small area of brick visible near the doorway. There is a large open area to the right, with a view of a river flowing gently and land rising in the background.  [0:03:24]: Turning inside, the video moves down a narrow wooden hallway. There are two furnaces side by side with a bright yellow glow indicating they are active.  [0:03:25]: Exiting the house, the scene shifts to an open field with long grass and scattered trees. A body of water is visible in the distance. [0:03:26]: The video focuses on a cow standing in a grassy area. The cow is black and white with a wide stance. [0:03:27]: The first-person perspective approaches and hits the cow with an iron sword. The result is a puff of white smoke indicating the cow has been defeated.  [0:03:28]: Moving across rolling grassy hills, the perspective reveals more of the open landscape. In the background, clusters of trees are standing. [0:03:29]: Continuing to traverse the green hills, the video maintains this pace, passing a lone tree standing at the top of a small incline. [0:03:30]: A scene with a village in the background, several houses with wooden walls and thatched or wooden roofs can be seen. Another cow is struck, causing red particles to fly. [0:03:31]: Near a small pond surrounded by crops, the video shows another interaction with a cow. The animal is hit by the iron sword, sending more red particles flying. [0:03:32]: Moving closer to the water, the perspective switches to gather tall reeds growing near the shore. Houses from the village are visible in the background. [0:03:33]: Passing through the village, a range of houses, additional crops, and wooden fencing creates a picturesque agrarian scene. There is a hint of other beings in the area, but no clear view. [0:03:34-0:03:35]: The video concludes with a view inside a house, looking at an open chest. The chest contains black helmets, pieces of obsidian, and several saplings. The inventory below shows a variety of items obtained during the journey including beef, leather, a sword, and a bed.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What action does the player take towards the cow standing in the grassy area?",
                "time_stamp": "0:03:27",
                "answer": "D",
                "options": [
                    "A. Feeds the cow.",
                    "B. Leads the cow to water.",
                    "C. Ties the cow to a tree.",
                    "D. Hits the cow with a sword."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "minecraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_200_real.mp4"
    },
    {
        "time": "[0:06:00 - 0:07:00]",
        "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:42]: The video begins in a cavernous space characterized by rugged terrain. There are patches of amethyst crystals on the right-hand side and a light source casting shadows on the walls. The view shows a sword and a shield icon at the bottom of the screen, indicating the first-person perspective of a character. The ground is littered with various types of blocks, and torches are placed sporadically to illuminate the area. [0:06:42 - 0:06:45]: The camera moves forward, approaching a stone block with two torches on either side. Ahead, there are various types of stone blocks creating a small mound. The right side of the scene remains dominated by the amethyst cluster, while the area to the left is more open and spacious. [0:06:45 - 0:06:47]: As the camera continues moving forward, it passes the stone block and approaches a rectangular wooden structure on stilts. This structure supports multiple chests. The perspective shifts slightly upward to capture the height of the supporting beams and the chests. [0:06:47 - 0:06:50]: The camera looks up at the wooden beam that supports the chest and captures more details of the upper area of the cavern. The view includes a mix of stone and dirt blocks overhead, enhancing the complexity and depth of the cavern's design. [0:06:50 - 0:06:53]: Moving closer to the wooden support beam, the camera orientation changes to provide a close-up view of the part supporting structure. The texture of the wooden blocks and the attached granite blocks becomes more evident, showcasing their pixelated appearance. [0:06:53 - 0:06:56]: The camera moves around the wooden beam, providing alternate angles and perspectives of the structure in the cavern. The surrounding area shows some additional detail, including the texture of the ground blocks and more torches placed strategically around. [0:06:56 - 0:06:58]: The camera moves back to the cavern space, further exploring the surroundings. It captures more of the varied block patterns on the ground and walls, and additional light sources continue to highlight certain areas within the cavern. [0:06:58 - 0:07:00]: As the camera turns to a different section of the cavern, more of the space becomes visible. Another section with a rock formation is revealed, partially illuminated by torches. The texture of the wall shows a mix of different stone blocks, contributing to a varied texture and appearance. [0:07:00 - 0:07:03]: The camera shifts to face another part of the cavern, which appears more expansive and less cluttered. The area includes several larger stone blocks stacked against the wall, likely as part of the layout and structure within the game environment. [0:07:03 - 0:07:05]: This section of the cavern includes a large wall covered in dark red blocks, partially lit by torches. The ground level appears slightly lower as the camera captures more of the surrounding textures and construction. [0:07:05 - 0:07:07]: The camera continues to move, capturing more of the extensive cavern space. A tall wooden column rises from the ground, and details of the far walls and various structures are further visible. [0:07:07 - 0:07:10]: The video shows the character moving forward across the extensive wooden floor. Torches keep the space well-lit, with varied block patterns on the walls enhancing the aesthetic diversity of the scene. [0:07:10 - 0:07:12]: Moving towards the far wall, the camera passes several dark stone blocks positioned against the wall. The layout and spacing suggest some structured design choices within the game's environment. [0:07:12 - 0:07:15]: The camera captures the far wall with greater detail, showing a large section overlaid with red and brown blocks, partly lit by torches placed evenly around the room's perimeter. [0:07:15 - 0:07:17]: As the character moves back, focus shifts to a large open floor space once more. The illuminated areas reveal more of the architectural elements and consistent texture patterns in the wooden floor and stone walls. [0:07:17 - 0:07:20]: The camera pans across the large open space, continuously revealing different textures and shades of the block elements. The upper parts of the cavern show intricate details that define the overarching design of the room. [0:07:20 - 0:07:23]: The final frames capture a broader view of the cavern's interior. The ceiling shows a complex array of blocks, while the ground level appears methodically structured with wooden planks and strategic torch placements, illuminating the vast space.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is the box supported by right now?",
                "time_stamp": "00:06:56",
                "answer": "D",
                "options": [
                    "A. Wood floor.",
                    "B. soil block.",
                    "C. stone.",
                    "D. Nothing."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "minecraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_200_real.mp4"
    },
    {
        "time": "[0:10:00 - 0:11:00]",
        "captions": "[0:10:00 - 0:10:20] [0:10:00 - 0:10:02]: The video begins with a first-person perspective showing a view downward into a dark crimson landscape with blocky, pixelated textures. The player character, indicated by the heads-up display, holds a golden object in the right hand and a shield in the left, with various tools and items detailed in a hotbar at the bottom of the screen.  [0:10:03]: The camera angle shifts slightly to the right, providing another view of the blocky terrain.  [0:10:04]: The scene moves forward slightly, offering further perspective into the crimson environment from a higher ledge. [0:10:05 - 0:10:06]: The camera shifts again, glancing around the environment, revealing a lava pool in the distance. The deep red blocks dominate the view, with the character holding the same gold item and shield combination. [0:10:07]: Another shift in perspective shows the left screen edge with more of the peculiar red environment, including jagged block formations. [0:10:08 - 0:10:09]: The camera switches views, climbing upwards, and reveals some glowing white ore embedded in the red blocks. [0:10:10]: The scene now captures a broader expanse of the cavernous crimson world ahead, returning to a similar vantage as earlier but higher up. [0:10:11]: The player\u2019s point of view shifts slightly forward, and to the right, offering a more centered view of the network of red blocks. [0:10:12]: The camera perspective lowers, focusing closer on the ground-level blocks where the character stands. [0:10:13 - 0:10:14]: The character moves through a narrow passageway, with the camera facing forward. [0:10:15]: The player rounds a corner, with a forward view of the red textured wall directly ahead, continuing through the narrow path. [0:10:16]: The character progresses further down the corridor, the red-tinged path continues, seemingly carved out by the character. [0:10:17]: The camera shifts downward, revealing a small creature wearing purple armor at the bottom of the passageway. [0:10:18]: The player\u2019s focus remains on the armored creature, which appears to interact or acknowledge the player character. [0:10:19]: Another angle shows the character\u2019s inventory screen briefly, indicating the various items and equipment in possession. The focus then returns to the creature, now being offered an item. [0:10:20]: The creature continues to acknowledge the player character while holding the item. The scene ends with the character continuing to move through the passage.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What happens when the player encounters the small creature wearing purple armor?",
                "time_stamp": "00:10:25",
                "answer": "D",
                "options": [
                    "A. The creature attacks the player.",
                    "B. The player hit the creature.",
                    "C. The player kill the creature.",
                    "D. The player left without doing anything."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "minecraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_200_real.mp4"
    },
    {
        "time": "[0:12:00 - 0:12:42]",
        "captions": "[0:12:40 - 0:12:42] [0:12:40 - 0:12:41]: A player character dressed in iron armor is standing inside a large wooden structure with a vaulted ceiling. The structure is well-lit with torches placed along the walls and on the ground. The player is positioned behind a crafting table, holding a pink item in their hand. The crafting table is centrally located within the frame, with a chest and a stone cutter to the left of the player. The background includes wooden stairs leading to an upper level, with bookshelves and wooden beams visible. There are also multiple stone pillars and wooden railings visible, enhancing the interior architecture of the structure.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the player character holding in his left hand right now?",
                "time_stamp": "00:12:35",
                "answer": "D",
                "options": [
                    "A. A beef steak.",
                    "B. A sword.",
                    "C. A red item.",
                    "D. A shield."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "minecraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_200_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:07]: A woman with dark hair is standing in a room with a floral-patterned dress. She is wearing a white slip dress with lace detailing at the top.  She wears a floral robe with red and pink flowers. The background shows a floral wallpaper, a wooden chest of drawers with a mirror and various items on it, a bed with a salmon-colored bedspread, and a white nightstand with a lamp. The woman begins to take off the floral robe and sets it down on the bed. [0:00:08 - 0:00:12]: The woman continues to tidy up, setting down the robe and some clothing items on the bed. She walks towards a nightstand on the left side of the room and picks up another item of clothing from there. The room's decor, including the floral wallpaper and the furniture arrangement, remains consistent. [0:00:13 - 0:00:19]: The woman sits on a chair next to the nightstand and starts putting on what appears to be white stockings or leggings. She is focused on adjusting the stockings, looking down at them as she works. The lighting is bright as daylight streams in through the sheer curtains behind her, illuminating the room. The bed in the foreground has neatly arranged articles of clothing laid out on top of it.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What pattern is on the woman's robe?",
                "time_stamp": "0:00:02",
                "answer": "C",
                "options": [
                    "A. Stripes.",
                    "B. Polka dots.",
                    "C. Floral.",
                    "D. Geometric."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "getting_dressed",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_161_real.mp4"
    },
    {
        "time": "[0:01:00 - 0:02:00]",
        "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:20]: The video takes place in a bedroom with floral wallpaper and a bright atmosphere, created by a large window with sheer white curtains allowing sunlight to stream through.  A woman with dark hair styled in an updo is in the room, initially wearing a light-colored camisole and standing near the bed. The bed has a peach-colored cover with two piles of clothing. A small table with a lamp is positioned next to the bed, while a wooden dresser with a mirror is situated against the wallpapered wall. Various objects such as bottles, jewelry boxes, and a brush, are neatly arranged on the dresser, indicating it is used for grooming or dressing. The woman picks up a green, yellow, and floral-patterned skirt and begins to step into it, pulling it up to her waist and adjusting the waistband. She then takes the shoulder straps of the dress and begins putting them on, ensuring the dress fits correctly by adjusting the straps and smoothing the fabric. She carefully places the floral pattern of the dress to align it properly on her chest, checking the fit and making minor adjustments. After ensuring the dress is properly worn, she moves to grab a pair of beige shoes. She sits down, gracefully lifting her dress slightly to put on the shoes, revealing a glimpse of her legs as she adjusts her attire to ensure comfort and proper fit. The video is characterized by smooth, deliberate motions as the woman dresses, with a calm and orderly background complementing her actions.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color is the cover on the bed?",
                "time_stamp": "0:01:20",
                "answer": "C",
                "options": [
                    "A. Blue.",
                    "B. Green.",
                    "C. Peach.",
                    "D. White."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Action Recognition",
                "question": "What does the woman do after putting on the dress?",
                "time_stamp": "0:01:29",
                "answer": "B",
                "options": [
                    "A. Brushes her hair.",
                    "B. Put on her high heels.",
                    "C. Adjusts the mirror.",
                    "D. Picks up a jewelry box."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Event Understanding",
                "question": "What is the woman primarily doing in the video?",
                "time_stamp": "0:01:20",
                "answer": "C",
                "options": [
                    "A. Cleaning the room.",
                    "B. Reading a book.",
                    "C. Dressing herself.",
                    "D. Writing a letter."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "getting_dressed",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_161_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:13]: A woman is standing in front of a mirrored vanity in a room with floral wallpaper and soft lighting. She wears a green, gold-patterned dress and a beaded necklace. She is seen putting on a yellow scarf or shawl, adjusting it around her shoulders and arms. The bed with brass railings and a green chair are visible beside her. The vanity holds various small items, including a mirrored jewelry box and decorative items. [0:02:14 - 0:02:17]: The scene transitions to a close-up of two elegant glasses filled with a clear beverage, one with a lemon twist garnish. This part focuses on the delicate details of the glasses, their reflections, and the liquid they contain. [0:02:18 - 0:02:19]: The video shows a woman wearing a gold dress with intricate beadwork. She has short hair styled with a decorative headband, and she is smiling. The background appears to be an elegantly decorated indoor space, possibly with warm lighting.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the woman doing right now?",
                "time_stamp": "00:02:42",
                "answer": "B",
                "options": [
                    "A. Singing.",
                    "B. Dancing.",
                    "C. Smiling.",
                    "D. Reading."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "getting_dressed",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_161_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:20]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video begins with a black screen. [0:00:01 - 0:00:04]: A close-up view of an RTX 4090 graphics card is shown. The card is placed on a cushioned surface against a gradient background with purple and blue hues. The graphics card has a large fan on the right and a smaller one on the left, with the label \"RTX 4090\" visible on its side. [0:00:05 - 0:00:08]: The scene transitions to a top view of two RTX graphics cards placed on a white table. Both cards are identical in design, featuring black and silver coloring, large fans, and prominently displayed \"RTX 4090\" labels. The cards lie parallel to each other with a slight overlap. The background is mostly composed of a white surface and some out-of-focus elements. [0:00:09 - 0:00:13]: A split-screen view shows a comparison between two gameplay footages. The left side displays \"RTX 3090,\" while the right side displays \"RTX 4090.\" Both frames show a sports car driving down a rural road with fields on either side. Performance indicators are shown on the screen, with the RTX 4090 side indicating higher frame rates (measured in FPS) compared to the RTX 3090. [0:00:14 - 0:00:17]: A chart is displayed comparing the average frame rates of the RTX 4090 versus the RTX 3090 across ten different video games at 4K, high/ultra settings. Games listed include \"DOOM Eternal,\" \"F1 22,\" \"Assetto Corsa Competizione,\" \"Forza Horizon 5,\" and others. The RTX 4090 shows higher performance across all games with noticeable differences in FPS. [0:00:18 - 0:00:19]: The final scene features a person seated at a table, presenting the RTX 4090 graphics card. The person wears a green shirt and is situated in a dark room with a subtle light source to the left. They hold the graphics card and speak to the camera.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Which graphics card performs better in this game?",
                "time_stamp": "00:00:13",
                "answer": "A",
                "options": [
                    "A. RTX 4090.",
                    "B. RTX 3080.",
                    "C. RTX 3090.",
                    "D. RTX 4080."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_115_real.mp4"
    },
    {
        "time": "[0:02:40 - 0:03:00]",
        "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:42]: Two graphics cards are positioned side by side on a wooden surface with a black corrugated background. The card on the left is black with two fans, while the card on the right is silver with a single large fan and a distinctive X-shaped design. [0:02:43 - 0:02:46]: The screen showcases an advertisement for the GeForce RTX 4060 Family, featuring a sleek black and silver graphics card with an X-shaped design. The background has green and black streaks, and there is text highlighting specifications and pricing information starting from \"AUD $545.\" [0:02:47 - 0:02:51]: A person is holding a Radeon graphics card with both hands. The card is mostly black with some red accents. Initially, the person holds it horizontally, showing the top side with the Radeon branding, and then slightly tilts it, revealing the side with the ports and a large heatsink. [0:02:52 - 0:02:55]: A close-up side view of the Radeon graphics card is displayed, focusing on the heatsink and the power connector at the end of the card. [0:02:56]: A dark screen displays various specifications for two models, RX 6600 and RX 6700, but the text is dark and hard to read. [0:02:57 - 0:02:59]: The same specifications screen becomes clearer, highlighting details such as \"Silicon,\" \"Compute Units,\" \"Boost,\" \"Memory\" and \"Power\" for both RX 6600 and RX 6700, alongside their respective prices \"$329\" for RX 6600 and \"$269\" for RX 6700 in distinct columns.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What price is shown for the GeForce RTX 4060 Family graphics card right now?",
                "time_stamp": "00:02:43",
                "answer": "A",
                "options": [
                    "A. AUD $545.",
                    "B. AUD $499.",
                    "C. USD $545.",
                    "D. USD $499."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_115_real.mp4"
    },
    {
        "time": "[0:05:20 - 0:05:40]",
        "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:26]: The video features a first-person perspective sequence set in a vibrant, futuristic city filled with neon lights and towering buildings adorned with digital signage. A car with bright red rear lights is seen traveling on an elevated road, featuring large billboards on either side, displaying colorful advertisements. The screen is split into two views, each displaying the car's lane on the road under slightly different graphical settings, indicated by on-screen text. The left view shows \"RTX 4090\" with \"RT Psycho + DLSS 3 Perf,\" while the right view adds \"Frame Generation\" to the same settings. The city's architecture is detailed with a blend of modern and futuristic elements, including holographic billboards and bright lighting. [0:05:27 - 0:05:28]: The perspective shifts to a more open area of the city with the car passing through a section flanked by tall buildings. The road is clearly defined with yellow lines and curves gently. The screen still shows both views, comparing different graphical settings. A subtle change is noted in lighting and reflections on the road surface and buildings. [0:05:29 - 0:05:34]: The video then transitions to a scene featuring a person seated at a table in a quieter indoor environment. This person is wearing a green t-shirt and is discussing two graphics cards placed in front of him on the table. The surrounding area appears to be a controlled setting with dark walls and soft lighting. He gestures with his hands, explaining or comparing something about the graphics cards. There are two additional boxes on either side of the table, possibly related to the graphics cards. [0:05:35]: The same person continues to speak, gesturing with their hands to emphasize their points. The focus remains on him and the graphics cards on the table. [0:05:36 - 0:05:39]: The scene changes again to show a gameplay footage on a computer monitor. The game appears to be a fast-paced shooter, with the player using a futuristic weapon to navigate a complex environment filled with obstacles and other characters. The graphics are vivid and colorful, featuring a blend of blue, red, and orange hues in a dynamic in-game setting. Another screen is visible to the left, likely part of a multi-monitor setup.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What graphical setting is added to the right view that is not included in the left view right now?",
                "time_stamp": "00:05:26",
                "answer": "A",
                "options": [
                    "A. Frame Generation.",
                    "B. DLSS 3 Perf.",
                    "C. RT Psycho.",
                    "D. RTX 4090."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_115_real.mp4"
    },
    {
        "time": "[0:08:00 - 0:08:20]",
        "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: A person sits at a table with various computer graphics cards in front of him, including several placed on the table. He is wearing a green t-shirt and is interacting with the objects on the table with both hands. The background is dark with vertical ridges on the left side. [0:08:02 - 0:08:04]: Close-up of an NVIDIA GeForce RTX 4070 graphics card placed on a white surface, showing the cooling fan and part of the card's body. [0:08:05 - 0:08:06]: A side view of the GeForce RTX graphics card, with the card's body slightly tilted to reveal the fan and part of the card's internal structure. The background is out of focus with green and black hues. [0:08:07 - 0:08:11]: A screen displays a comparison chart of the performance of RTX 4070 Ti vs. 4070, displaying average gains in various games at 1440p, high/ultra settings. Game titles and performance metrics are listed, with green bars representing the data visually. [0:08:12 - 0:08:15]: The view returns to the person sitting at the table. He is again interacting with the graphics cards, explaining something. His hands are moving over the cards while he speaks, showing them and pointing towards them. [0:08:16 - 0:08:19]: Close-up of multiple NVIDIA GeForce RTX graphics cards on the table, viewed from above. The person holds and adjusts one card, showing its design features and cooling fans. The cards are arranged to display their fans and top labels.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What comparison is displayed on the screen right now?",
                "time_stamp": "00:08:08",
                "answer": "A",
                "options": [
                    "A. 4070Ti vs 4070 in various games.",
                    "B. Cooling efficiency of RTX 4070 against previous models.",
                    "C. 4070 in different regions.",
                    "D. 4070."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_115_real.mp4"
    },
    {
        "time": "[0:09:40 - 0:09:46]",
        "captions": "[0:09:40 - 0:09:46] [0:09:40 - 0:09:42]: A hand is shown placing a black GeForce RTX graphics card on a box. The graphics card is seen from the side, with the GeForce RTX branding visible. The box underneath the card bears the text \"INSPIRED BY GAMERS, BUILT BY NVIDIA.\" The background is dark and blurred, hinting at a focused shot on the product. [0:09:43 - 0:09:45]: The scene shifts to a person sitting at a table with multiple Nvidia products in front of them. The person, wearing a green t-shirt, is interacting with the products, expressing something with a calm and friendly demeanor. Various models of Nvidia products are spread across the table: some types of graphics cards and a possibly related accessory. The background remains dark, maintaining focus on the person and the products on the table.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is the message printed on the box underneath the GeForce RTX graphics card right now?",
                "time_stamp": "00:09:40",
                "answer": "A",
                "options": [
                    "A. INSPIRED BY GAMERS, BUILT BY NVIDIA.",
                    "B. DESIGNED FOR GAMERS, MADE BY NVIDIA.",
                    "C. CREATED FOR GAMERS, MADE BY NVIDIA.",
                    "D. DESIGNED FOR GAMERS, BUILT BY NVIDIA."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_115_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:05]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color is the man's sweatshirt right now?",
                "time_stamp": "00:00:04",
                "answer": "A",
                "options": [
                    "A. Beige.",
                    "B. Blue.",
                    "C. Red.",
                    "D. Green."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_433_real.mp4"
    },
    {
        "time": "[0:02:17 - 0:02:22]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What type of vehicle appears near the open parking lot right now?",
                "time_stamp": "00:02:06",
                "answer": "D",
                "options": [
                    "A. Motorcycle.",
                    "B. Helicopter.",
                    "C. Bicycle.",
                    "D. Car."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_433_real.mp4"
    },
    {
        "time": "[0:04:34 - 0:04:39]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the vehicle being operated right now?",
                "time_stamp": "00:04:35",
                "answer": "D",
                "options": [
                    "A. Airplane.",
                    "B. Boat.",
                    "C. Car.",
                    "D. Helicopter."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_433_real.mp4"
    },
    {
        "time": "[0:06:51 - 0:06:56]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "What type of headset is the man wearing right now?",
                "time_stamp": "00:06:53",
                "answer": "D",
                "options": [
                    "A. HiFi.",
                    "B. Reverberant.",
                    "C. No function.",
                    "D. Aviation."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_433_real.mp4"
    },
    {
        "time": "[0:09:08 - 0:09:13]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What shape is the island in the water right now?",
                "time_stamp": "00:09:17",
                "answer": "A",
                "options": [
                    "A. Rectangular.",
                    "B. Circular.",
                    "C. Triangular.",
                    "D. Oval."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_433_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:00:16",
                "answer": "D",
                "options": [
                    "A. 2.",
                    "B. 3.",
                    "C. 4.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_89_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:03:34",
                "answer": "A",
                "options": [
                    "A. 3.",
                    "B. 5.",
                    "C. 6.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_89_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:04:21",
                "answer": "B",
                "options": [
                    "A. 2.",
                    "B. 4.",
                    "C. 3.",
                    "D. 6."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_89_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:05:19",
                "answer": "B",
                "options": [
                    "A. 2.",
                    "B. 5.",
                    "C. 4.",
                    "D. 6."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_89_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:06:19",
                "answer": "C",
                "options": [
                    "A. 4.",
                    "B. 8.",
                    "C. 6.",
                    "D. 7."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_89_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:00:16",
                "answer": "D",
                "options": [
                    "A. 2.",
                    "B. 3.",
                    "C. 4.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            },
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:00:50",
                "answer": "D",
                "options": [
                    "A. 2.",
                    "B. 3.",
                    "C. 4.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_97_real.mp4"
    },
    {
        "time": "[0:01:00 - 0:02:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:01:00",
                "answer": "D",
                "options": [
                    "A. 2.",
                    "B. 3.",
                    "C. 4.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_97_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:03:29",
                "answer": "A",
                "options": [
                    "A. 3.",
                    "B. 4.",
                    "C. 6.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_97_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:04:14",
                "answer": "B",
                "options": [
                    "A. 2.",
                    "B. 5.",
                    "C. 3.",
                    "D. 7."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_97_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:30]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why did the elderly woman step off the bus with a tray of tea?",
                "time_stamp": "00:00:21",
                "answer": "C",
                "options": [
                    "A. To serve tea to the bus driver during his break.",
                    "B. To offer refreshments to passengers waiting at the bus stop.",
                    "C. To deliver food to the elderly lady in a pink dress who has a foot injury.",
                    "D. To serve tea to passengers waiting at the bus stop."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_245_real.mp4"
    },
    {
        "time": "[0:02:12 - 0:02:42]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why is Mr. Bean in the hospital ward?",
                "time_stamp": "00:02:29",
                "answer": "B",
                "options": [
                    "A. Because he is visiting a friend who is a patient.",
                    "B. Because this is his fantasy.",
                    "C. Because he mistakenly walked into the wrong building.",
                    "D. Because he is participating in a medical study."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_245_real.mp4"
    },
    {
        "time": "[0:04:24 - 0:04:54]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why does the character try to manipulate the thermometer reading?",
                "time_stamp": "0:04:47",
                "answer": "A",
                "options": [
                    "A. To pretend to be sick.",
                    "B. To make the nurse angry.",
                    "C. To make the nurse laugh.",
                    "D. To test the accuracy of the thermometer."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_245_real.mp4"
    },
    {
        "time": "[0:06:36 - 0:07:06]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why is the patient prepped for surgery?",
                "time_stamp": "00:07:03",
                "answer": "A",
                "options": [
                    "A. Because Mr. Bean tampered with the check, making himself appear very unhealthy.",
                    "B. Because the patient requested the surgery as a precaution.",
                    "C. Because the doctor misread the patient's medical chart.",
                    "D. Because the patient was mistakenly identified as someone else."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_245_real.mp4"
    },
    {
        "time": "[0:08:48 - 0:09:18]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why do the babies start crying?",
                "time_stamp": "0:09:16",
                "answer": "C",
                "options": [
                    "A. Because they are hungry.",
                    "B. Because they need a diaper change.",
                    "C. Because the person in the wheelchair enters the room.",
                    "D. Because they are tired."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_245_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:05]: The video begins with a black notebook lying flat on a tabletop. Surrounding it, there is a red pencil on the left side, an eraser, and a tube of red paint towards the top right. The camera is set in a first-person perspective, capturing two hands as they reach towards the notebook and start to open it. [0:00:06 - 0:00:13]: As the notebook is opened, blank pages are revealed initially. The individual flips through the pages, revealing detailed paintings. The first painting depicts a creature with a long beak, and the subsequent page shows a feline face with expressive eyes, followed by a realistic painting of a fish. [0:00:14 - 0:00:20]: After flipping through a page with the fish painting, the person continues turning blank pages, seemingly towards the end of the notebook. The video concludes with the hands resting on a blank page with the notebook still open.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What sequence of paintings was revealed in the notebook?",
                "time_stamp": "0:00:17",
                "answer": "B",
                "options": [
                    "A. A fish, a tree, and a landscape.",
                    "B. A creature with a long beak, a feline face, and a realistic fish.",
                    "C. An abstract design, a human portrait, and a cityscape.",
                    "D. A bird, a flower, and a mountain."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_131_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:03]: The video shows an artist painting the head of a rooster on a piece of paper. The rooster's head is painted with vibrant colors, including a pink comb, a brown and white face, and an orange eye. The artist uses a paintbrush to add details to the rooster's head, particularly focusing on the eye and the area around it. [0:03:04 - 0:03:06]: The artist continues to work on the rooster's head, applying more paint to bring out the details and colors. The brush moves smoothly, adding various shades of color to enhance the painting's texture and depth. [0:03:07 - 0:03:12]: The artist shifts the focus to different parts of the rooster's head, applying paint to the comb and face. The brushstrokes are meticulous, showing the artist's careful attention to detail. The background remains a plain, light beige color, keeping the focus on the painted rooster. [0:03:13 - 0:03:16]: The artist begins to fill in more details on the rooster's neck and body area, adding layers of brown and white paint. The brush moves in smooth, deliberate strokes, highlighting the intricate features of the rooster. The overall composition of the painting becomes more defined and detailed. [0:03:17 - 0:03:19]: The artist continues to refine the painting, focusing on the lower part of the rooster's body. The brushstrokes create a textured effect, adding depth to the image. The colors blend seamlessly, showcasing the artist's skillful technique. The rooster's head and body appear more lifelike and detailed as the artist adds the finishing touches.",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following best summarizes the content shown in the video just now?",
                "time_stamp": "00:03:19",
                "answer": "D",
                "options": [
                    "A. The artist is painting a landscape with various animals.",
                    "B. The artist is sketching a detailed rooster using a pencil.",
                    "C. The artist is painting abstract shapes and patterns.",
                    "D. The artist is painting a rooster's head and body with vibrant colors and detailed brushstrokes."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_131_real.mp4"
    },
    {
        "time": "[0:06:00 - 0:07:00]",
        "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:19]: A hand is painting a realistic chicken image in a sketchbook using a paintbrush. The chicken's feathers are a mix of brown, black, and white colors, with a vibrantly colored red comb and wattle. The painter\u2019s hand holds a wooden paintbrush with white metal ferrule. The painting is being meticulously detailed, specifically around the neck and feather areas, with varying brush strokes to add texture and depth. The background is a plain off-white surface, and the sketchbook is open flat on a table. Nearby are a few art supplies including a red pencil and a tube of paint. The brush's movement is consistent, adding layers to the painting to enhance realism. The overall scene remains static except for the painter\u2019s hand and brush. [0:06:19 - 0:06:20]: The painting of the chicken remains the focus, with the painter\u2019s hand continuing to refine the image using precise strokes.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What is the painter focusing right now?",
                "time_stamp": "0:06:12",
                "answer": "B",
                "options": [
                    "A. Adding a background landscape.",
                    "B. Detailing the chicken's neck and feathers.",
                    "C. Painting the chicken's feet.",
                    "D. Sketching a second chicken."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_131_real.mp4"
    },
    {
        "time": "[0:09:00 - 0:10:00]",
        "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:04]: The video depicts a close-up scene of someone painting a rooster on a piece of paper or canvas. The background is filled with dark blue brush strokes. The rooster features a red comb and wattles, with a brightly colored head comprising shades of pink and red. The lower part of the rooster's neck and body is painted in shades of brown with lighter highlights. A hand holding a paintbrush can be seen working diligently on the painting, applying more blue paint to the background on the left side of the rooster's head. [0:09:05 - 0:09:09]: The hand continues to move quickly, applying more blue paint to the upper left corner of the painting. The rooster's details, such as the eye, comb, and wattles, become more defined as the painter works around these features, adding depth and contrast to the artwork. The use of contrasting colors accentuates the vibrant look of the rooster against the dark blue background. [0:09:10 - 0:09:14]: The painter continues to add details and refine the painting. They focus on enhancing the background and making sure the rooster's head stands out prominently. The strokes are purposeful and controlled, displaying the skill and technique of the painter. The brush moves from the top of the rooster's comb towards the upper part of the canvas, adding more depth to the background. [0:09:15 - 0:09:19]: The final stages of the painting process showcase the painter making final touches and adjustments. The brush moves steadily to fill in any remaining gaps and to create a well-blended background. The painter's hand appears steady and deliberate, ensuring the painting has a polished and finished look. By the end of the sequence, the rooster's head is vividly highlighted against the rich blue background, giving the painting a striking appearance.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What action is the painter performing at the upper left corner of the painting?",
                "time_stamp": "0:09:09",
                "answer": "C",
                "options": [
                    "A. Adding red paint.",
                    "B. Adding green paint.",
                    "C. Adding blue paint.",
                    "D. Adding yellow paint."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Event Understanding",
                "question": "What is the painter focusing right now?",
                "time_stamp": "0:09:19",
                "answer": "C",
                "options": [
                    "A. The painter begins a new painting.",
                    "B. The painter adds blue paint to the rooster's body.",
                    "C. The painter makes touches and adjustments to the background.",
                    "D. The painter signs the painting."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_131_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:02]: A person is shown chopping a red onion on a wooden cutting board. The person uses a large knife to make precise cuts; the red onion is halved with the flat side facing down. On the counter surrounding the cutting board, there are ingredients such as olive oil, a block of butter, and some leafy greens in a bowl. [0:02:03 - 0:02:07]: The camera perspective widens, showing more of the kitchen setting. The individual, wearing a blue shirt, continues to finely chop the onion. The kitchen has white brick walls, shelves with plates and glasses, and stainless-steel knives hanging on the wall. On the kitchen counter, a frying pan is on the stovetop, and other ingredients, including yellow corn and cherry tomatoes, are visible on the counter. [0:02:08 - 0:02:09]: The view zooms back in on the chopping board as the person continues to slice the onion into thin pieces. The precision of the cuts reveals the layers of the onion, and more of the surrounding ingredients are visible, such as a glass bottle of olive oil and a small plate of butter chunks. [0:02:10 - 0:02:12]: The camera perspective shifts again, showing the person moving the chopped onion pieces to one side of the cutting board. The individual wipes the knife clean with their hand, and in the background, the kitchen shelves are stocked with various colorful plates and bowls, adding a vibrant touch to the setting. [0:02:13 - 0:02:13]: The person appears to be speaking or giving instructions; however, no audio details are available. They emphasize their actions with hand movements, making it clear that they are engaged in demonstrating the cooking process. [0:02:14 - 0:02:17]: The focus shifts to the stovetop where the person begins placing the chopped onion into a heated frying pan. The onions sizzle as they come into contact with the hot surface. In the pan, there are already pieces of yellow corn. The person's hands are busy distributing the sliced onions evenly across the pan. [0:02:18 - 0:02:19]: The view briefly switches to an overhead angle, showing the contents of the pan from above. The yellow corn and finely sliced onions mix together as the person continues to stir and ensure even cooking. Two other pans are seen on the stovetop, although they are not in use at this moment.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the person using to chop the onion?",
                "time_stamp": "0:02:02",
                "answer": "B",
                "options": [
                    "A. A small wooden knife.",
                    "B. A sliver knife.",
                    "C. A pair of scissors.",
                    "D. A fork."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Spatial Understanding",
                "question": "How are the yellow corn and onions distributed in the frying pan?",
                "time_stamp": "0:02:19",
                "answer": "C",
                "options": [
                    "A. Randomly scattered.",
                    "B. Piled in the center.",
                    "C. Evenly distributed.",
                    "D. Grouped separately."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_32_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:01]: A stainless steel frying pan containing cooking ingredients is on the stove's burner, with a slice of what appears to be raw meat about to be added. Several small jars are placed in the background near the stovetop, one containing wooden items like a spoon and spatula, and another with cooking tongs. [0:04:01 - 0:04:02]: A person wearing a teal shirt is beside the stove, which has multiple burners. The background has white subway tiles and shelves with kitchenware and dishes. The person appears to be adjusting the heat on one of the stove burners. [0:04:02 - 0:04:03]: The individual in the teal shirt holds a pan in one hand and seems to be discussing or explaining something, pointing to emphasize. [0:04:03 - 0:04:04]: The person places the pan back onto the stove's burner, ready for the next step in the cooking process. They have a casual yet attentive manner. [0:04:04 - 0:04:05]: The individual gestures with both hands, likely explaining the next cooking step or ingredient in focus. [0:04:05 - 0:04:06]: The person stands to the right of the stove, with an extended hand toward a bowl of ingredients lying on the right-hand side. [0:04:06 - 0:04:07]: The top view reveals a chopping board with halved cherry tomatoes varying in color. The board is surrounded by several bowls of ingredients, such as fresh greens, butter, and salt. [0:04:07 - 0:04:08]: The person's hand coats several pieces of raw meat with flour or a similar substance, while the prepared ingredients remain positioned around the chopping board. [0:04:08 - 0:04:09]: The hand of the individual continues to work the raw meat into the flour mixture in the bowl, focusing on making sure they are well-coated. [0:04:09 - 0:04:10]: The person\u2019s hand completes dusting the pieces of raw meat with flour, with the other ingredients awaiting further steps. [0:04:10 - 0:04:11]: The individual adds some seasoning to the raw meat placed in a bowl on the counter, preparing for the next cooking step. The surroundings include various bowls, bottles, and utensils neatly arranged. [0:04:11 - 0:04:12]: The person sprinkles seasoning over the raw meat, now further prepared for cooking, while the stove in the background has a pan frying other ingredients. [0:04:12 - 0:04:13]: The individual continues to season the raw meat in the bowl. Different colorful ingredients and cooking utensils are positioned around the kitchen counter. [0:04:13 - 0:04:14]: With one hand, the person holds the bowl of seasoned meat and uses the other to lift the pan off the stove, showing readiness to rotate between the preparations. [0:04:14 - 0:04:15]: The person hovers between the stove and counter, holding a bowl of seasoned meat in one hand and a frying pan in the other, aligning them for the upcoming steps. [0:04:15 - 0:04:16]: Over the stove, the person readies the pan for the seasoned meat while other kitchen tools and ingredients are visible on the counter. [0:04:16 - 0:04:17]: A view from above shows the frying pan containing colorful cooking ingredients such as corn and onions, along with an empty frying pan beside it. [0:04:17 - 0:04:18]: The person appears to ready another pan, likely in the preparation stage for the cooking process. [0:04:18 - 0:04:19]: Holding the bowl close, the person places the first piece of seasoned meat into an empty frying pan, beginning the cooking process. [0:04:19]: Additional pieces of seasoned meat are added to the empty frying pan as other ingredients continue to cook in the adjacent pan.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the person holding in one hand at the stove?",
                "time_stamp": "0:04:02",
                "answer": "B",
                "options": [
                    "A. A wooden spoon.",
                    "B. A pan.",
                    "C. A bowl.",
                    "D. A knife."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_32_real.mp4"
    },
    {
        "time": "[0:06:00 - 0:07:00]",
        "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:01]: In a modern kitchen setup with a white brick wall and blue cabinets, my hands are shown working over two stainless steel frying pans on a black stovetop. The left pan contains food that is being prodded with a fork, while the right pan holds a saut\u00e9 mix of diced vegetables, predominately yellow and red. [0:06:01 - 0:06:02]: Using a spoon and fork, I turn over a piece of food in the left pan. The action is precise, and my fingers are close to the pan, indicating careful handling. The vibrant colors of the vegetables in the right pan remain visible.  [0:06:02]: My right hand uses a spoon to flip the piece of food in the pan again. My left hand is steadying the pan by holding its handle. [0:06:03 - 0:06:04]: Now, I use a spoon in my right hand to continue adjusting the food in the left pan. The focus is on ensuring even cooking, as the steam rises lightly from both pans. Background items, such as kitchen utensils and condiments, are neatly arranged. [0:06:04 - 0:06:05]: Pulling back slightly, I step away from the stove but keep my attention on the food. The kitchen's decor becomes more visible, including white shelves holding jars and a cutting board with small items. [0:06:05 - 0:06:06]: I continue managing the food with the spoon, leaning toward the stove. The blue overhead cabinets and various jars on the shelves are more prominent, enhancing the kitchen's modern aesthetic. [0:06:06 - 0:06:07]: I turn away from the stove and reach for a white towel to clean my hands. The expression indicates attentiveness, ensuring that everything is prepared correctly. The background includes a kettle and additional cooking tools. [0:06:07 - 0:06:08]: Holding the towel in both hands, I look at the kitchen counter, likely checking for any necessary adjustments or cleanups needed. [0:06:08 - 0:06:09]: Using a black-handled frying pan in my right hand, I transfer the food to another area in the kitchen. The view of the white brick wall and wooden shelves add context to the spacious cooking area. [0:06:09 - 0:06:10]: I hold the frying pan and walk towards a wooden countertop on the right. Plates and bowls on the counter await the food, creating a warm and organized cooking environment. [0:06:10 - 0:06:11]: Pouring the contents from the pan onto a white plate, I ensure the pieces are carefully placed in a specific arrangement. Bottles and bowls nearby suggest a setup for further preparation or garnishing. [0:06:11 - 0:06:12]: The pan is tilted further, allowing more food to slide onto the plate, filling it gradually. Limes and other ingredients on the counter prepare for final food presentation steps. [0:06:12 - 0:06:13]: Continuing the pour, the plate now shows neatly placed food pieces, while I adjust the pan to guide any remaining items. The organized kitchen space provides ample room for efficient cooking. [0:06:13 - 0:06:14]: The last of the food is guided from the pan to the plate with precision. My focus remains on ensuring all pieces are transferred without any spills. [0:06:14 - 0:06:15]: Any remaining food bits are being carefully poured onto the plate. The kitchen counter includes multiple cooking instruments and ingredients, highlighting a well-stocked cooking environment. [0:06:15 - 0:06:16]: I shake the pan slightly to nudge the last piece of food onto the plate. The pristine white plate contrasts with the rich colors of the food, making it visually appealing. [0:06:16 - 0:06:17]: All food now appears on the plate, neatly arranged, while I lift the pan away. Adjacent are bowls, a cutting board, and ingredients ready for the final touches. [0:06:17 - 0:06:18]: Placing the pan back on the stove, steam rises from the fully cooked food. The countertop remains organized with several different kitchen tools and some green vegetables. [0:06:18 - 0:06:19]: I finish the cooking process with a final look over the stove and kitchen. The kitchen wall's \"COOK\" sign is prominent, blending well within the white brick and wooden shelf decor.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "Where does the person transfer the food after cooking?",
                "time_stamp": "00:06:24",
                "answer": "B",
                "options": [
                    "A. Onto a white plate on the left countertop.",
                    "B. Onto a white plate on the right countertop.",
                    "C. Into a bowl on the left countertop.",
                    "D. Into a bowl on the right countertop."
                ],
                "required_ability": "episodic memory"
            },
            {
                "task_type": "Attribute Recognition",
                "question": "What is the color of the towel used to clean the hands?",
                "time_stamp": "00:06:07",
                "answer": "B",
                "options": [
                    "A. Blue.",
                    "B. White.",
                    "C. Red.",
                    "D. Yellow."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_32_real.mp4"
    },
    {
        "time": "0:00:00 - 0:00:20",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins inside a dealership featuring a modern, first-person perspective. The interior has checkerboard-patterned flooring and dark walls.  There is a counter on the left side and a waiting area with couches against the walls.  [0:00:03 - 0:00:06]: The character runs toward the center of the dealership, passing through a lobby area. Displayed cars and additional seating areas are visible throughout. [0:00:07 - 0:00:07]: The character makes a brief stop in the showroom area where vehicles are displayed, with cars parked on the checkered floor.  [0:00:08 - 0:00:11]: The character continues moving through the showroom area, approaching the counter where various car accessories are displayed. [0:00:12 - 0:00:12]: The character stops at the counter which is cluttered with various items, and moves towards the computer terminal. [0:00:13 - 0:00:15]: The character interacts with the computer terminal at the counter, initiating a car selection menu. [0:00:16 - 0:00:17]: The video shifts focus to a car selection interface showing different car models, each with its own price and name overlaying a grid. [0:00:18 - 0:00:20]: The character scrolls through various car models in the selection menu, examining different vehicles visually.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the character doing right now?",
                "time_stamp": "00:00:17",
                "answer": "D",
                "options": [
                    "A. Running towards the center of the dealership.",
                    "B. Stopping in the showroom area.",
                    "C. Interacting with the computer terminal.",
                    "D. Viewing the car selection interface."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_275_real.mp4"
    },
    {
        "time": "0:02:20 - 0:02:40",
        "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:40]: The video appears to be a first-person perspective gameplay or streaming session, as indicated by the screen layout. On the left side corner, there is a webcam feed of the player, who is looking at the screen and interacting with the game. The player's room is visible in the background, featuring a lit gaming setup with multiple monitors. The main scene is centered on a character in the game, standing against a two-toned wall background. The character wears a green hoodie with \"AIR FORCE\" written on it and looks directly ahead, showing varied facial expressions. On the right side of the screen, a chat window displays messages from viewers, along with notifications or alerts. Also, there is a vertical menu indicating an \"Available Jobs\" list in the game, associated with different roles like Sanitation Worker, Lumberjack, Trucking, etc. The player scrolls through this job list as the video progresses. The overall context involves selecting or browsing job options within the game interface.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the player doing right now?",
                "time_stamp": "00:02:24",
                "answer": "C",
                "options": [
                    "A. Interacting with the chat window.",
                    "B. Selecting a game character.",
                    "C. Browsing the job options list.",
                    "D. Customizing the game character's appearance."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_275_real.mp4"
    },
    {
        "time": "0:04:40 - 0:05:00",
        "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:43]: The video starts with a scene showing a detailed digital map of a city area in a software application, complete with streets, buildings, and various icons. A small window in the top left corner displays a person with various expressions and hand movements, presumably narrating or interacting with the map. On the right side of the screen, there is a chat section with colorful text messages from viewers, indicating a live-streaming environment. [0:04:44 - 0:04:46]: The map view remains, but now the webcam frame shows the person more intently focusing on the map. The interactions in the chat are still actively updating as the person continues their detailed explanations or actions related to the map content. [0:04:47 - 0:04:49]: Suddenly, the video transitions to a first-person perspective in a digital game environment. The scene depicts a field with high grass and flowers under a green night-vision filter. Trees and bushes are visible in the background. The lower left corner of the screen displays various icons and small text boxes, indicative of a game's HUD. The person in the webcam window reacts to this new environment. [0:04:50 - 0:04:55]: The first-person perspective remains in the grassy field, with subtle movements and slight changes in view angles. The interaction in the chat reflects reactions to the game scenario, and the person in the webcam continues to display focused engagement. [0:04:56 - 0:04:59]: The scene shifts to a different area, showing a tram station with urban surroundings, still under the night-vision filter. A tram evolves into view and comes to a stop. The HUD icons persist in the lower left corner, and the chat reactions vary with excitement and comments about the scene change. The person in the webcam seems to be providing commentary on the new scene. [0:05:00]: The scene advances to the tram interior, showcasing seats and windows from a first-person view. The night-vision filter persists, as does the interaction from the chat and the on-screen person\u2019s reactions to the transition inside the tram.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the person in the webcam window doing right now?",
                "time_stamp": "00:05:00",
                "answer": "C",
                "options": [
                    "A. Narrating and interacting with a digital map.",
                    "B. Reacting to a tram station scene.",
                    "C. Waiting for the game to finish loading..",
                    "D. Switching between different screens."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_275_real.mp4"
    },
    {
        "time": "0:07:00 - 0:07:20",
        "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:03]: The video starts with a first-person perspective, showing a city intersection from inside a moving blue car. The car is situated on the left lane near the crosswalk, waiting for the traffic light to change. There are tall white buildings on either side of the street, and traffic lights are visible in the distance. To the left of the screen is a live feed of a streamer in the top corner. Some chat messages are displayed on the right side of the screen; [0:07:04 - 0:07:13]: The car begins to move forward through the intersection. The surroundings remain consistent with tall buildings on both sides of the street. The sky is overcast, indicating it might be early morning or late afternoon. The driver's view shows the straight path ahead, with fewer cars visible on the road; [0:07:14 - 0:07:19]: The car drives straight under an overpass, and the area becomes darker due to the shadows from the structure overhead. The street ahead continues straight, and minimal traffic is visible. The streamer remains engaged with the viewers, as shown in the live feed; [0:07:20]: The car veers slightly to the right and moves into a large vehicle parking area or garage. Inside, the light is dim and there are various vehicles, including white and green trucks, parked along the sides. A person is also visible standing near one of the trucks, and the car moves towards this person before coming to a stop. The live feed shows the streamer continuing to interact with the viewers. The chat messages persist on the right side of the screen.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the car doing right now?",
                "time_stamp": "00:07:13",
                "answer": "D",
                "options": [
                    "A. Turning left at the intersection.",
                    "B. Moving forward through the crosswalk.",
                    "C. Driving straight under an overpass.",
                    "D. Veering slightly to the right into a parking area."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_275_real.mp4"
    },
    {
        "time": "0:09:20 - 0:09:23",
        "captions": "[0:09:20 - 0:09:23] [0:09:20 - 0:09:23]: In a nighttime urban environment, a blue car is seen driving on a dark street with yellow lane markings. The vehicle stops and a person, clad in white clothing, appears near the driver's side door, seemingly after having been hit by the car. The individual on the ground is positioned sideways and motionless. In the top-left inset, a small window shows a person with headphones speaking animatedly into a microphone, indicating that this is a livestream. The chat window on the right contains multiple user comments reacting to the event. The in-game map and information show that this event is part of a video game.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the car doing right now?",
                "time_stamp": "0:09:21",
                "answer": "B",
                "options": [
                    "A. Speeding down the street.",
                    "B. Hit an electric scooter and stopping on the dark street.",
                    "C. Turning at an intersection.",
                    "D. Parking in a garage."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_275_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:05]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What brand's advertisement is visible on the back of the bus right now?",
                "time_stamp": "00:00:04",
                "answer": "D",
                "options": [
                    "A. Toyota.",
                    "B. Chevrolet.",
                    "C. Nissan.",
                    "D. Ford."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_451_real.mp4"
    },
    {
        "time": "[0:01:55 - 0:02:00]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What company's logo is visible on the left side of the building right now?",
                "time_stamp": "00:01:55",
                "answer": "D",
                "options": [
                    "A. Intel.",
                    "B. Tesla.",
                    "C. AT&T.",
                    "D. Optus."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_451_real.mp4"
    },
    {
        "time": "[0:03:50 - 0:03:55]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the current status of the traffic light ahead?",
                "time_stamp": "00:03:52",
                "answer": "D",
                "options": [
                    "A. Red.",
                    "B. Yellow.",
                    "C. Flashing.",
                    "D. Green."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_451_real.mp4"
    },
    {
        "time": "[0:05:45 - 0:05:50]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What brand is visible on the building right now?",
                "time_stamp": "00:05:46",
                "answer": "D",
                "options": [
                    "A. Citibank.",
                    "B. Westpac.",
                    "C. ANZ.",
                    "D. Coleslocal."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_451_real.mp4"
    },
    {
        "time": "[0:07:40 - 0:07:45]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "Which brand's logo is visible on the right side of the road right now?",
                "time_stamp": "00:07:44",
                "answer": "D",
                "options": [
                    "A. McDonald's.",
                    "B. KFC.",
                    "C. Subway.",
                    "D. Starbucks."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_451_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:05]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "Which country's flag is hanging on the right side of the road now?",
                "time_stamp": "00:00:03",
                "answer": "C",
                "options": [
                    "A. Australia.",
                    "B. Italy.",
                    "C. United States.",
                    "D. UK."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_383_real.mp4"
    },
    {
        "time": "[0:02:04 - 0:02:09]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Which store sign is currently visible on the right side of the road right now?",
                "time_stamp": "00:02:08",
                "answer": "C",
                "options": [
                    "A. Victoria's Secret.",
                    "B. Lululemon.",
                    "C. Ann Taylor.",
                    "D. Forever 21."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_383_real.mp4"
    },
    {
        "time": "[0:04:08 - 0:04:13]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "Which basketball-related sign is currently visible in the video right now?",
                "time_stamp": "00:04:11",
                "answer": "A",
                "options": [
                    "A. NBA Store.",
                    "B. Nike Store.",
                    "C. Adidas Shop.",
                    "D. Reebok Shop."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_383_real.mp4"
    },
    {
        "time": "[0:06:12 - 0:06:17]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Which store sign is visible on the right side of the road right now?",
                "time_stamp": "00:06:14",
                "answer": "B",
                "options": [
                    "A. Starbucks.",
                    "B. Pret A Manger.",
                    "C. Chipotle.",
                    "D. McDonald's."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_383_real.mp4"
    },
    {
        "time": "[0:08:16 - 0:08:21]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Which roads are currently visible in the video right now?",
                "time_stamp": "00:08:18",
                "answer": "B",
                "options": [
                    "A. East 40th St and 3rd Ave.",
                    "B. East 36th St and 5th Ave.",
                    "C. East 42nd St and Madison Ave.",
                    "D. East 30th St and Park Ave."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_383_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins with a view inside an art gallery. Various framed artworks are hung on white walls, prominently featuring a collection of photographs and paintings. Several pieces consist of abstract designs and dark backgrounds, while others are more detailed. People are seen moving around the space. [0:00:03 - 0:00:06]: The camera moves forward, offering a closer look at some of the displayed works. The artworks include a mix of photographic prints and a screen displaying text. One prominent frame contains green text on a black background that reads, \"WE BUY & SELL LUXURY.\" [0:00:07 - 0:00:10]: The camera focuses on an exhibit section labeled \"ARTSNITCH.\" This area displays four pieces of framed art, two of which feature digital or neon text. [0:00:11 - 0:00:12]: The camera angle shifts to capture more of the gallery interior, showing additional artwork along adjacent walls. Some of these pieces feature intricate, detailed designs. [0:00:13 - 0:00:14]: As the camera moves along the walls, more artwork comes into view. This includes detailed paintings with intricate patterns and a mix of dark and light backgrounds. [0:00:15 - 0:00:17]: The camera continues panning along the wall, showing a series of artworks that incorporate complex patterns and organic shapes. The lighting highlights the texture and detail of each piece. [0:00:18 - 0:00:20]: The video concludes with a close-up view of a large, intricate artwork that appears to be a symmetrical, detailed design in shades of yellow and white, alongside another abstract piece featuring a burst of colors.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What words are written on the painting in the lower left corner of the wall?",
                "time_stamp": "00:00:06",
                "answer": "C",
                "options": [
                    "A. \"WE SELL ART\".",
                    "B. \"BUY & SELL ART\".",
                    "C. \"WE BUY & SELL LUXURY\".",
                    "D. \"ART FOR SALE\"."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Counting",
                "question": "How many pieces of art paintings are displayed in the \"ARTSNITCH\" section?",
                "time_stamp": "00:00:10",
                "answer": "B",
                "options": [
                    "A. Two.",
                    "B. Three.",
                    "C. Four.",
                    "D. Five."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "art_show",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_464_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:22]: There are two large framed artworks on a white wall. The one on the left is a red and pink abstract piece depicting Mickey Mouse's head, while the one on the right features two white cartoonish hands pointing at each other on a blue background. Below these artworks, there are two colorful abstract canvases, one displaying vibrant flowers. [0:02:20 - 0:02:23]: To the left of the frames, there is a vertical colorful digital display, showing an underwater scene featuring a figure with blue and purple hues. Another framed canvas displays a toy-like painting. [0:02:21 - 0:02:25]: The underwater-themed digital art transitions, showing a more colorful view with a central figure having rainbow patterns on its face. Next to it, another digital portrait shows an animal with large antlers. [0:02:23 - 0:02:26]: Three digital frames are hanging on a white wall. The leftmost frame exudes gray-toned rural imagery; the center frame displays a vibrant depiction of a person with neon lighting; the rightmost frame continues to depict the antlered animal. [0:02:26 - 0:02:27]: On the far-left display, an artistic graphic portrait showing black and white text over a face is shown.  [0:02:26 - 0:02:28]: The far-right display transitions again showing more vibrant digital art pieces. [0:02:27 - 0:02:29]: As the view moves left, a fourth digital display shows a whimsical scene featuring a character holding an umbrella covered in colorful patterns. [0:02:28 - 0:02:30]: Hanging next to it is a black and white line drawing of a face. Underneath are colorful abstract squares some with prominent cartoon heads. [0:02:29 - 0:02:31]: A mix of digital frames and framed artworks adorn the wall being viewed in quick succession. The artworks on display range from monochrome portraits to vibrantly colorful digital paintings. [0:02:29 - 0:02:30]: A large painting with a black and white graphical pattern and iconic cartoon head. [0:02:31 - 0:02:34]: As the camera shifts, some attendees are seen interacting with the artwork. A person in a jacket crouches near the graphic painting, taking a picture.  [0:02:33 - 0:02:34]: The person crouching moves into a dynamic pose, and more attendees appear to be engaged. [0:02:34 - 0:02:36]: A bright dynamic scene, as more vibrant digital artworks are shown transitioning with bright light flashes ensuring the continuity to a digital screen on the right displaying pinkish tones. [0:02:35 - 0:02:36]: Attendees continue engaging; one person is seen enthusiastically explaining something to another person. The central figure moves forward, showing interaction dynamics. [0:02:36 - 0:02:39]: New artwork is visible with themes varying from minimalistic black lines on white to vivid color patterns. The digital displays continue showing different vibrant artworks, moving around another digital display. [0:02:37 - 0:02:39]: The group interaction continues, with several people involved in discussions or viewing displays. One attendee holds a drink while conversing, and another closely interacting with the art display. [0:02:38 - 0:02:40]: The viewing angle broadens revealing more parts of the exhibit with attendees mingling around different art pieces. The digital screens display dynamic images and changing artworks capture viewers\u2019 attention.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What action is being performed by a person in a jacket near the graphic painting?",
                "time_stamp": "0:02:34",
                "answer": "A",
                "options": [
                    "A. Taking a picture.",
                    "B. Painting.",
                    "C. Cleaning the artwork.",
                    "D. Pointing at the artwork."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "art_show",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_464_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:41]: The video begins with a view of an art gallery wall. A framed artwork depicting a muscular man holding a basket of lemons is on the left side. Below the artwork, the text \"Lemon Lad BRAND\" is visible. To the right of this painting, there are two alternatingly placed white shelves on the wall. The lower shelf holds a brown sculpture resembling a gourd. [0:04:41 - 0:04:45]: The scene continues to focus on the two shelves, with the brown gourd-shaped sculpture becoming more centered in the footage. Next to the sculpture, to the right, there's another framed painting of an old-fashioned car below a large, orange dome structure, and palm trees in the background. [0:04:45 - 0:04:48]: The camera pans up to reveal another blue, sculptural object positioned on the upper shelf. Next to these shelves, the framed painting showing the orange dome continues to be visible. [0:04:48 - 0:04:51]: The video captures an overall view of the blue sculpture on the shelf. Adjacent to it, the painting with the orange dome structure is clearly framed against a dark sky with searchlights.  [0:04:51 - 0:04:56]: Now, to the right side of the orange dome painting, another framed artwork depicting a variety of vibrant, abstract flowers comes into view. Both paintings are mounted on a white wall, and below them, a black folding chair is positioned.  [0:04:56 - 0:04:58]: The camera angle shifts to show both the 'orange dome' painting and the 'flower' painting fully. The black folding chair remains centered at the bottom. [0:04:58 - 0:05:00]: Finally, the video provides a broader view of the gallery wall, bringing several elements into frame: the \"Lemon Lad\" artwork on the far left, the blue sculptural piece on the top shelf, the brown gourd-like sculpture on the lower shelf, and both adjacent paintings. A second black folding chair is barely visible to the right.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What text is visible on the artwork of the muscular man holding a basket of lemons?",
                "time_stamp": "0:04:40",
                "answer": "B",
                "options": [
                    "A. \"Lemon Art BRAND\".",
                    "B. \"Lemon Lad BRAND\".",
                    "C. \"Lemon Grove BRAND\".",
                    "D. \"Lemon Orchard BRAND\"."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "art_show",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_464_real.mp4"
    },
    {
        "time": "[0:07:00 - 0:08:00]",
        "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:03]: The video begins with a view of four paintings hanging on a white wall. The top left painting depicts a stormy scene with lightning striking down over a dark road lined with trees, and it is predominantly gray and black. To its right, the top right painting shows a vivid orange and red sunset over a hilly landscape. Below these two larger paintings are two smaller ones, each showing a monochromatic, foggy landscape with twisted trees. To the right of these, a horizontal painting features abstract splashes of black, blue, yellow, and orange. [0:07:03 - 0:07:06]: The camera shifts slightly to the left, revealing more artwork on the white wall. A colorful abstract painting with swirling patterns in green, blue, and other vibrant colors is visible to the left, alongside a small bronze sculpture placed on a black pedestal.  [0:07:06 - 0:07:09]: The focus shifts further left to reveal a large painting of a vivid sunset with orange, red, and yellow hues near the top. Below it, two vibrant pieces with vivid colors and apparent abstract themes are displayed. The lower left painting has bold paint strokes of pink, yellow, white, and black, depicting an abstract scene, while the center artwork shows colorful circular shapes on a blue background. [0:07:09 - 0:07:12]: The camera continues to move left, revealing more framed paintings. Two paintings featuring complex, abstract faces in rich colors are displayed. The top painting has swirling patterns in blue, yellow, and white, while the bottom painting features a face with elaborate gold, red, and white patterns. To their left, there are more colorful, nature-inspired landscapes in predominantly green, pink, and blue hues. [0:07:12 - 0:07:15]: The view pans further left, showing a series of landscape paintings in vibrant shades of pink, green, and blue. These depict various fields of flowers and pathways, capturing the beauty of nature. There are also paintings of different scenes like markets and urban views with intricate details and a rich color palette. [0:07:15 - 0:07:18]: The camera continues along the same trajectory, adding more artworks to the visible collection. The highlighted paintings capture wider countryside scenes with fields of vibrant wildflowers, a winding path, and distant mountains under pink and blue skies. A well-detailed urban scene with structures and streets painted with intricate details is also visible. [0:07:18 - 0:07:20]: The video concludes with the camera focusing on two landscape paintings filled with vivid floral blooms, featuring similar pathways leading through colorful fields and surrounded by lush greenery. The top painting also showcases a vibrant field, with the sky being depicted in soft, blended hues of pink, blue, and purple.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is depicted in the top left painting on the white wall?",
                "time_stamp": "00:07:00",
                "answer": "B",
                "options": [
                    "A. A vivid sunset over a hilly landscape.",
                    "B. A stormy scene with lightning over a dark road.",
                    "C. A monochromatic, foggy landscape with twisted trees.",
                    "D. An abstract painting with splashes of colors."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "art_show",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_464_real.mp4"
    },
    {
        "time": "0:00:00 - 0:00:20",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:20]: The video is set in a virtual environment featuring what appears to be a character creation screen from a video game. A person with blond hair, a black belt, black pants, and white shoes stands in the center of the room. They seem to be meant as the player's character. The room is dimly lit with shelves and equipment, predominantly showcasing various items. To the right of the screen, there is a menu that shows different customization options, seemingly for editing the character's appearance. There is another character visible only in partial view on the right, dressed in a dark outfit with high-heeled boots.  At the top left, a small in-game camera display shows another person, possibly the player, seated at a desk and focused on the screen. This secondary display features changing expressions and movements of this individual, suggesting active engagement with the customization menu. Various user interface elements are scattered across the screen, including text chats and clickable buttons indicating options like \"Character Creation\" and various clothing and accessory choices. The background consists of a partially visible metal gate, adding to the industrial ambiance of the environment. Through the video, there are changes in the character's clothing, moving from an orange safety vest to a plain black polo shirt, indicating active customization of the character.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the person with blond hair doing right now?",
                "time_stamp": "00:00:20",
                "answer": "A",
                "options": [
                    "A. Changing clothing.",
                    "B. Sitting at a desk.",
                    "C. Walking around the room.",
                    "D. Exiting through the metal gate."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_281_real.mp4"
    },
    {
        "time": "0:03:00 - 0:03:20",
        "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:06]: The video begins with a first-person perspective of a person standing in a room. The character on screen is wearing a green safety vest, a headset, and goggles. The backdrop consists of a wooden panel wall, a shelf with various items on it, including a small television, and a neatly made bed. In the upper left corner, there is an inset screen showing a person in a room with multiple monitors and LED lights. The chat section on the right side of the screen shows real-time messages from viewers, interacting with the stream. [0:03:07 - 0:03:14]: Gradually, the character remains in the same spot, with minimal movement. The chat section continues to be active with similar messages. The inset screen in the left corner shows the person talking and observing the scene attentively. [0:03:15 - 0:03:16]: A new prompt pops up in the middle of the screen, asking if the player is sure about saving their clothing changes. The options include \"Cancel,\" \"Save,\" and \"Discard.\" [0:03:17 - 0:03:19]: The scene transitions to the character starting to walk towards the exit. The inset screen and chat section remain unchanged. [0:03:20]: The perspective shifts slightly, showcasing the character still walking inside the room, now seen moving past various clothing displays and shelves. Other characters, presumably NPCs, can be seen standing around in the background, indicating the setting is inside a shop or store.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the character doing right now?",
                "time_stamp": "00:03:19",
                "answer": "B",
                "options": [
                    "A. Standing still in a room.",
                    "B. Walking towards the exit.",
                    "C. Interacting with the chat section.",
                    "D. Changing clothing."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_281_real.mp4"
    },
    {
        "time": "0:06:00 - 0:06:20",
        "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:20]: The video is taken from the first-person perspective of someone sitting in a vehicle at night, driving through a city. The setting is urban with tall buildings illuminated by streetlights and colorful signage. The streets are relatively empty. The vehicle the person is driving is a large van with prominent red lights on its rear. The interior of the van is dark, and the dashboard has illuminated controls.  The video starts with the van turning left onto a street with red and blue lights illuminating a building on the left. As the van continues driving, it passes multiple buildings with various light displays. The streets have typical urban elements such as streetlights, commercial buildings, and sidewalks. The exterior view shows the van making turns and driving straight on the broad, well-lit road.  At [0:06:10-0:06:13], the perspective briefly changes for a second to a head-on view from inside the van, showing the driver's hands on the steering wheel and a brightly lit dashboard. The van later stops at a traffic light and then proceeds once the light turns green.  As the van drives, the scene shifts forward with buildings continuing to line both sides of the street. The cityscape is very active with artificial light sources illuminating the buildings and streets. The view switches back and forth between the van's rear with the red outline of lights and the dashboard views. The video ends with the van driving further down the street.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What did the driver do just now?",
                "time_stamp": "00:06:08",
                "answer": "A",
                "options": [
                    "A. Turned the van left onto a street.",
                    "B. Stopped at a traffic light.",
                    "C. Drove straight down the street.",
                    "D. Checked the dashboard."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_281_real.mp4"
    },
    {
        "time": "0:09:00 - 0:09:20",
        "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:03]: The video starts with the person standing in front of a building at night. There is a man wearing a white shirt and cap standing on the sidewalk, and the player character approaches him. The area is lit by streetlights, and there are steps to the right leading up to a doorway. [0:09:04 - 0:09:07]: The person approaches closer, engaging in a conversation with the man in the white shirt. A dialog window appears, identifying the man as Harry Miller, with options to \"Open Stock\" or \"Leave Conversation.\" [0:09:08]: The camera is focused on Harry Miller, showing a close-up of his face as the conversation continues, with the same options visible on the screen. [0:09:09]: The screen transitions to an inventory interface, displaying multiple slots containing different items. The player character's inventory is visible on the left, showing personal information and items. Harry Miller's stock is displayed on the right. [0:09:10]: The player character now has possession of a medium-sized cardboard box with blue tape on it. He is standing back on the sidewalk near the building. [0:09:11]: The character walks away from the building, holding the box. A white car is parked nearby on the street, and the area is lined with buildings and trees. [0:09:12 - 0:09:13]: The character continues to walk along the sidewalk, approaching a large dark van with \"Prime\" written on the side. The area remains well-lit by streetlights. [0:09:14 - 0:09:16]: The character reaches the back of the van and interacts with it, opening the cargo bay. Another inventory interface appears displaying both the character's inventory and the van's cargo space. [0:09:17]: The interface shows various items placed in the cargo space. The character completes the action and closes the inventory interface. [0:09:18]: The character steps back from the van and begins walking toward the cab of the vehicle, preparing to enter the driver\u2019s seat. [0:09:19]: The character approaches the driver\u2019s door, opens it, and starts climbing into the seat. [0:09:20]: The video ends with the character settled in the driver\u2019s seat, preparing to drive. The street remains visible through the windshield with another stationary vehicle in front of the van.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What did the character just do?",
                "time_stamp": "00:09:15",
                "answer": "B",
                "options": [
                    "A. Walked towards a white car.",
                    "B. Opened the door of the van and climbed into the driver's seat.",
                    "C. Stood in front of a building with Harry Miller.",
                    "D. Placed items in a cardboard box."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_281_real.mp4"
    },
    {
        "time": "0:11:40 - 0:11:46",
        "captions": "[0:11:40 - 0:11:46] [0:11:40 - 0:11:41]: A person stands by the corner of a building, wearing a green safety vest and a headset. They are holding a phone or small tablet looking down. The view is in a cityscape at night, with buildings and streetlights illuminating the scene. A large screen on the left shows another figure, possibly the same individual, seemingly engaged in streaming or a video call. [0:11:41 - 0:11:42]: The person begins to move to the right side of the frame toward a black, rectangular structure, possibly a vehicle or a storage unit. Their back is facing the camera, continuing to hold the item in their hand. [0:11:42 - 0:11:43]: The movement continues as the person approaches the black object with a firm grip on their item. The city lights and buildings remain visible in the background. [0:11:43 - 0:11:44]: The person stands directly in front of the black object, leaning slightly forward, possibly inspecting or interacting with it. [0:11:44 - 0:11:46]: Now, within an inventory interface displayed on the screen, different grids showing categories like \"Player,\" \"Backpack,\" \"Cargo,\" and \"Ground\" are visible. The individual seems to be organizing or moving items between these categories. The streaming screen on the left shows the person concentrating on the task.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the person doing right now?",
                "time_stamp": "00:11:38",
                "answer": "D",
                "options": [
                    "A. Standing by a corner of a building.",
                    "B. Moving to the right side of the frame.",
                    "C. Inspecting or interacting with a black object.",
                    "D. Organizing items within an inventory interface."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_281_real.mp4"
    },
    {
        "time": "0:00:00 - 0:00:20",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:14]: Throughout the video, the main focus is a large black vehicle with a rectangular shape and red outlines. The vehicle is being driven on a multi-lane highway that is lined with green trees, powerlines, and light poles. The highway leads through a rather wooded area, and traffic is light, with a few other vehicles visible ahead and in adjacent lanes. The overall scene appears to capture various perspectives from this ongoing drive, with the camera situated directly behind the vehicle. [0:00:02 - 0:00:12]: Side by side with the main scene, a smaller inset in the top-left corner features a person viewed from a frontal angle. The individual is wearing a headset and appears to be either focused on a screen or communicating during the drive. Below the individual, a game-like interface is superimposed over the video, displaying changing icons and text, indicating some user interactions or selections being made, especially towards the midsection of the video where the screen turns green multiple times, likely signaling menu navigation or selections by the user. [0:00:06 - 0:00:14]: Various radio or sound icons appear on the screen during these green overlays, suggesting that the driver is choosing media options while driving. This is revealed by the circular interface overlaying the driving scene, each time showing icons like music notes, microphones, or headphones.  [0:00:08], [0:00:15], and [0:00:19]: During intervals, the camera returns to the normal driving view, shifting briefly back, allowing an unobstructed focus on the vehicle progressing on the clear path ahead. [0:00:12 - 0:00:20]: The drive continues on the straight road as the video frames capture steady and uneventful highway driving, maintaining the focus on the moving vehicle and the driver's interactions through the interface.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the driver doing right now?",
                "time_stamp": "00:00:16",
                "answer": "D",
                "options": [
                    "A. Talking to a passenger.",
                    "B. Looking at the road ahead.",
                    "C. Adjusting the rearview mirror.",
                    "D. Selecting media options."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_283_real.mp4"
    },
    {
        "time": "0:02:40 - 0:03:00",
        "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:58]: In the video, two individuals are present in an urban environment. The person on the left is wearing a neon yellow safety vest over a black outfit, white goggles, and headphones. They stand near a potted plant situated in the center of a tiled sidewalk. The person on the right, who faces the first individual, is wearing a black and white striped top, denim shorts, and white sneakers. Behind them, there is a modern building with large windows, on which green emblems and signs related to healthcare are displayed. In the background, a partially obscured cityscape with tall buildings is visible, and a vehicle is parked on the street. The video primarily captures their interaction as they converse. The person on the left occasionally gestures animatedly, while the individual on the right mostly maintains a relaxed posture. [0:02:59 - 0:03:00]: Throughout the scene, both individuals continue their conversation without any significant changes in their positions or surroundings. The ambiance remains consistent with few people and objects visible in the background.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the person on the right doing right now?",
                "time_stamp": "00:03:00",
                "answer": "D",
                "options": [
                    "A. Gesturing animatedly.",
                    "B. Walking towards the building.",
                    "C. Sitting near the potted plant.",
                    "D. Standing still."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_283_real.mp4"
    },
    {
        "time": "0:05:20 - 0:05:40",
        "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:23]: As the video starts, a delivery van is driving down a narrow street with buildings on either side. The camera shows a first-person view from the cabin of the vehicle, with a view ahead towards an underground parking entrance. A palm tree grows on the sidewalk to the left. [0:05:24 - 0:05:27]: The van continues to approach the parking entrance. A low barrier partially blocks the entrance, and the road ahead slightly curves to the left. [0:05:28 - 0:05:31]: The van gets closer to the entrance, and the camera focuses more on the red structure containing the barrier. Some signage on the building says \"We aim\" with partial words visible. [0:05:32 - 0:05:35]: Upon reaching the entrance, the barrier lifts automatically, and the van begins entering the parking area marked with \u201cMAX CLEARANCE\u201d overhead. To the left, there is a small control booth. [0:05:36 - 0:05:39]: The signs on the building become clearer, revealing \"We aim not to lose.\" The van fully enters the parking area, angling toward a left turn. [0:05:40 - 0:05:43]: Inside the parking area, the van turns left. On-screen text updates to indicate different incoming communications. The ground and walls inside the parking area are concrete, and there is a delivery bay painted red straight ahead. [0:05:44 - 0:05:47]: The van completes the left turn inside the parking bay, positioning itself parallel to the left wall. The camera continues to capture this first-person driving perspective. [0:05:48 - 0:05:51]: After coming to a stop inside the parking space, the view from inside the van shows it parked snugly against the wall on the left. [0:05:52 - 0:05:55]: The driver gets out of the van, revealing a delivery person in a high-visibility vest. The perspective now includes a mix of the inside of the parking area and the vehicle's interior. [0:05:56 - 0:05:59]: Stepping away from the vehicle, the camera follows the driver as they walk towards the back of the van. The background includes various parked vehicles and the structure of the parking area. [0:06:00 - 0:06:03]: The driver prepares to unload the van, standing at the back door. The van is now stationary, and the bustling environment of the parking garage is apparent, with various background details such as pipes and wall markings becoming clearer.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What did the driver do just now?",
                "time_stamp": "00:05:41",
                "answer": "D",
                "options": [
                    "A. Drove out of the parking area.",
                    "B. Reached the parking entrance.",
                    "C. Entered the small control booth.",
                    "D. Got out of the van."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_283_real.mp4"
    },
    {
        "time": "0:08:00 - 0:08:20",
        "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:05]: The video starts with a first-person perspective of a van driving through a city street. The camera view is from behind the van. The van is heading towards an intersection where the light is red. On the left side of the screen, a small image-in-image displays a person, possibly the driver, with chat messages appearing to the right, indicating streaming activity. The street is lined with buildings and various shopfronts. [0:08:05 - 0:08:10]: The van progresses towards and stops at the intersection with a red traffic light. A dark-colored car is seen driving on the left lane crossing the intersection. [0:08:10 - 0:08:15]: The van continues moving forward after the intersection and passes by a few buildings with varying facades. The camera is fixed behind the van, showing a straight road ahead with more infrastructure in the distance. [0:08:16 - 0:08:20]: The scene shows the van driving under a series of large, elevated structures that appear to be bridges or overpasses. The background includes advertisements and an assortment of urban elements. Occasionally, the display briefly switches to a user interface showing audio options, overlaying the driving scene.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the van doing right now?",
                "time_stamp": "00:08:15",
                "answer": "D",
                "options": [
                    "A. Stopping at a red traffic light.",
                    "B. Reversing towards the intersection.",
                    "C. Turning left at the intersection.",
                    "D. Moving forward past the intersection."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_283_real.mp4"
    },
    {
        "time": "0:10:20 - 0:10:33",
        "captions": "[0:10:20 - 0:10:33] [0:10:20 - 0:10:23]: The video starts with a first-person perspective showing a screen with a \"Trucking Progression\" interface. The UI displays several missions with information like destination, reward, and status for various locations such as Los Santos Hospital and Sandy Shores Hospital. There are also multiple small icons and text columns. [0:10:23 - 0:10:25]: The interface remains on the screen, seemingly static. The viewer\u2019s perspective indicates they are observing the details of the missions listed. [0:10:26 - 0:10:27]: There is slight movement within the video, suggesting the person is scrolling or navigating through the menu, but the primary view does not change drastically. [0:10:28 - 0:10:29]: A new screen appears within the interface. This screen provides additional options or settings related to the trucking missions, including details about distance, time, deliveries, and payout. [0:10:30]: The view quickly returns to the previous \"Trucking Progression\" mission list screen, similar to the initial frames. [0:10:31 - 0:10:32]: The perspective shifts from the screen and reveals an interaction with another person. The camera now shows a person in a white shirt holding a tablet or clipboard likely in the process of discussing or handling some tasks. [0:10:33]: The video concludes with a closer view of the person in the white shirt as they appear to be checking something on the tablet or clipboard.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the person in the white shirt doing right now?",
                "time_stamp": "00:10:26",
                "answer": "D",
                "options": [
                    "A. Scrolling through a menu.",
                    "B. Navigating the \"Trucking Progression\" interface.",
                    "C. Discussing missions with another person.",
                    "D. Checking something on a clipboard."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_283_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins with a blurred, abstract background of dark purple, black, and bluish tones with some texture. As the seconds progress, the background remains abstract with slight variations in color intensity and features what appears to be static or interference lines. [0:00:02 - 0:00:04]: The display transitions to a clear image of a vibrant logo. The logo has a yellow, red, and blue triangular shape with the text \"GRAVITY THROTTLE RACING\" written in bold, white letters. [0:00:05 - 0:00:12]: The video then shows two miniature models of people standing side by side against a bright orange background with desert elements, such as cacti and a distant mountain range. The \"GRAVITY THROTTLE RACING\" logo remains in the backdrop. Both figures are holding microphones; the one on the left wears a brown jacket and gray pants, while the other, on the right, wears a white jacket and tan pants. [0:00:13 - 0:00:20]: The figures appear to be engaging in conversation, holding their microphones towards each other. There are no significant movements, but their postures suggest they are communicating, possibly interviewing each other. The backdrop remains consistent with the previously described elements, featuring the logo and desert landscape.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is the license plate number of the car currently shown in the video?",
                "time_stamp": "00:00:30",
                "answer": "D",
                "options": [
                    "A. 18.",
                    "B. 197.",
                    "C. 56.",
                    "D. 46."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_494_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:22]: The video shows a snowy race track with several toy cars positioned in rows that appear ready for a race. The background includes snow-covered hills and a blue sky. Several cars are lined up on the left side of the track, and a digital starting sign with green lights is visible on the right side. [0:03:23 - 0:03:27]: The toy cars start to accelerate, and the orange and green cars move forward from the rest. The terrain remains snowy, with the cars now beginning to navigate through the initial curves of the track. Other cars in the row follow closely. [0:03:28 - 0:03:29]: The cars continue to race onward, moving further along the snow-covered and curvy track. The background reveals more of the snowy hilly terrain with sparse, miniature trees and a blue sky. The green and orange cars maintain their lead, while additional cars become visible on the track behind them. [0:03:30 - 0:03:31]: The scene transitions to an overview of the track's layout, which shows the winding path the cars are taking. The track features multiple curves and slopes, with the main competitors still being the orange and green cars. [0:03:32 - 0:03:33]: The cars are now seen racing on a sharp incline of the track, which elevates and wraps around a hilly landscape. The orange and green cars remain in the lead, while other cars maneuver the curve behind them. The entire race environment maintains a snowy and mountainous theme. [0:03:34 - 0:03:35]: The track showcases a long, snaking path with a deep curve. The toy cars speed through, maintaining their relative positions. More of the rugged environment becomes visible, featuring rocky formations and minimal plant life, in addition to the snow. [0:03:36 - 0:03:37]: The cars continue around the elevated, curved path. The road and surrounding terrain look steep and rocky. The green and orange cars remain at the front of the pack while the blue car joins the leading group. [0:03:38 - 0:03:39]: The race persists as the cars enter a tunnel segment of the track. A mix of rocky and snowy background transitions to semi-arid terrain, indicating a change in the racing environment. The leading cars maintain their speed and navigate the challenging course.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color is the car that is in the lead along with the green car?",
                "time_stamp": "00:03:39",
                "answer": "D",
                "options": [
                    "A. Blue.",
                    "B. Red and black.",
                    "C. Yellow and red.",
                    "D. Black and blue."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Event Understanding",
                "question": "Which racing car won first place in this competition?",
                "time_stamp": "0:05:51",
                "answer": "D",
                "options": [
                    "A. The pink car.",
                    "B. The dark green car.",
                    "C. The light green car.",
                    "D. The green car."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_494_real.mp4"
    },
    {
        "time": "[0:06:00 - 0:07:00]",
        "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:41]: A white racing car is navigating a curved, uphill road next to a rocky cliff. There are two large advertising signs positioned on the rock face, one labeled \"Bluechrome Racing\" with a blue and white logo of a flame, and another advertisement with red elements slightly visible in the background and partly obscured by a tree. [0:06:42 - 0:06:45]: The scene transitions to a different perspective showing a winding mountain road covered in snow. There are several cars on this road, including a brown truck navigating a sharp incline. Below this road, viewers can see a red car parked at a roadside edge, trees dotting the landscape, and a \"Replay\" branding element at the bottom right corner. [0:06:46 - 0:06:50]: Continuing on the snowy road, the brown truck descends towards a sharp curve, with the camera perspective providing a top-down view. There are two other cars, blue and brown, visible in the distance, moving around the bend, with pine trees lining both sides of the road and snow-covered terrain. [0:06:51 - 0:06:57]: The blue and brown cars speed around the bend of the mountain road, with each still in movement. The setting emphasizes the sharpness of the curve with varying elevations, rocks, and decreasing snow as it transitions into a more asphalt-covered path. By the end of this segment, both the cars are starting to converge side by side at close quarters. [0:06:58 - 0:06:59]: The perspective switches to a closer, more detailed view of multiple cars making their way up what seems to be another snowy incline. The foreground includes cars with appearances suggesting a racing competition, amidst a backdrop of snow and a blue sky. The formations indicate a challenging climb typical of high-altitude mountain roads.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "Which racing car won first place in this competition?",
                "time_stamp": "0:06:36",
                "answer": "D",
                "options": [
                    "A. The pink car.",
                    "B. The dark green car.",
                    "C. The light green car.",
                    "D. The brown car."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_494_real.mp4"
    },
    {
        "time": "[0:08:00 - 0:10:00]",
        "captions": "[0:10:00 - 0:10:20] [0:10:00 - 0:10:02]: Two small cars, one green and one blue, race on a smooth, winding road surrounded by a dry, shrub-dotted landscape. The road is bordered by a cliffside on one side and barren terrain on the other. The green car is ahead of the blue one, both cars are oriented forward, heading towards an upward slope. [0:10:03 - 0:10:04]: A digital display board appears on the left side of the road, showing race information. The green car soon disappears from view while the blue one follows, moving past the board. The surrounding landscape remains unchanged. [0:10:05 - 0:10:11]: The scene shifts to a snowy mountainous area with multiple cars on a multi-lane track. Several cars, including the green and blue ones, race downhill on snow-covered slopes. Some cars are moving uphill, while others navigate the winding slopes. A crowd of people stand beside the track, watching the race. The background features high, snow-capped peaks and sparse trees. [0:10:12 - 0:10:13]: The racing continues in the snow-covered mountains, with cars maneuvering around the bends of the track. The green car leads, followed closely by the blue car. The track twists and turns, and more trees become visible as the landscape slopes downward. [0:10:14 - 0:10:19]: The viewpoint changes to an aerial view over a different section of the track, situated along the edge of a steep cliff with lush green terrain below. A train is seen traveling parallel to the road. The green car continues to race along the winding road, followed by other cars. The road descends and curves around the cliffside, with trees and billboards visible on the landscape.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "Which racing car won first place in this competition?",
                "time_stamp": "0:06:36",
                "answer": "A",
                "options": [
                    "A. The green car.",
                    "B. The dark green car.",
                    "C. The light green car.",
                    "D. The brown car."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_494_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:01]: The video begins with a person in a blue t-shirt, holding a small red object in his left hand. The kitchen background has white brick walls, blue cabinets, various utensils, and a mid-sized window with a view of outdoor foliage. There\u2019s an assortment of colorful vegetables and spices on the counter in the foreground. [0:02:01 - 0:02:02]: The person holds the red object closer to view, presumably inspecting it. The expression on their face seems focused as they glance towards the counter. Behind them, the word \"COOK\" is displayed on a shelf with various jars and bottles. [0:02:02 - 0:02:03]: The person turns slightly to the side, looking at several pots on the stove in front of them. The pots are of varying sizes and appear to contain different ingredients. The layout of the kitchen is designed for efficient cooking with all necessary utensils and ingredients within arm\u2019s reach. [0:02:03 - 0:02:04]: The person starts moving the red object towards the pots on the stove. There are three pots, one with boiling liquid and the other two with ingredients being cooked.  [0:02:04 - 0:02:05]: The frame shifts to an aerial view of the cooking surface. There are four burners, two of which have pots on them. One pot contains eggs in water while another pot has a darker liquid with some solid food items. On the left side of the counter, there are various chopped vegetables and other cooking ingredients. [0:02:05 - 0:02:06]: The person begins moving some ingredients from the cutting board to one of the pots. Their movements are precise, suggesting they are experienced in cooking. The overall setup of the kitchen remains consistent, with clean counters and organized utensils. [0:02:06 - 0:02:07]: The person adds more ingredients to the pots on the stove, constantly observing and adjusting the contents as they cook. The aerial view continues to show the detailed arrangement of everything on the counter. [0:02:07 - 0:02:08]: The person uses a spoon to stir the contents of one of the pots. The close-up perspective highlights the careful attention they pay to the cooking process. Simultaneously, they seem to be making sure all ingredients are cooking evenly. [0:02:08 - 0:02:09]: The view shifts again to a front perspective, where the person is adjusting the heat on the stove. The blue cabinets and white-brick wall in the background continue to add an aesthetic touch to the modern kitchen setup. [0:02:09 - 0:02:10]: The person appears to be explaining something, possibly giving instructions or tips on cooking. Their focus alternates between the stove and the camera. [0:02:10 - 0:02:11]: The focus shifts back to the stove, where the person holds two pieces of cauliflower, examining them before placing them into a pot. The arrangement of kitchen utensils remains meticulous and practical. [0:02:11 - 0:02:12]: They add the pieces of cauliflower to the pot and stir the contents again, ensuring all ingredients blend well. The attention to detail continues to be evident as they manage multiple pots simultaneously. [0:02:12 - 0:02:13]: The individual continues to cook and stir, focusing on every detail. The kitchen's overall arrangement, with plates, jars, and knives neatly organized, enhances the cooking efficiency. [0:02:13 - 0:02:14]: The person checks the contents of the other pots, making sure everything is cooking properly. The kitchen's aesthetic, with the mix of modern and rustic elements, provides a visually pleasing background. [0:02:14 - 0:02:15]: The person tastes the food, adjusting the seasoning if necessary. The focus on perfection is evident as they ensure the dish meets certain standards. [0:02:15 - 0:02:16]: The frame returns to an aerial view, highlighting the stirring action in one of the pots. The dark liquid and noodles are visible, being carefully blended with a spoon. [0:02:16 - 0:02:17]: The person continues stirring the pot, making sure the noodles are cooked evenly. The aerial perspective provides a clear view of the cooking process, showing the consistency and texture of the food. [0:02:17 - 0:02:18]: The person stirs more vigorously, perhaps to achieve a specific consistency. The organized layout of the cooking tools and ingredients remains evident in this frame. [0:02:18 - 0:02:19]: The person continues to monitor the cooking process",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the person holding in their right hand at the beginning of the video?",
                "time_stamp": "0:02:01",
                "answer": "B",
                "options": [
                    "A. A green vegetable.",
                    "B. A red bottle cap.",
                    "C. A blue utensil.",
                    "D. A white jar."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Attribute Recognition",
                "question": "What are the colors of the cabinets and the walls in the kitchen?",
                "time_stamp": "0:02:40",
                "answer": "B",
                "options": [
                    "A. Red cabinets and white walls.",
                    "B. Blue cabinets and white brick walls.",
                    "C. White cabinets and blue walls.",
                    "D. Green cabinets and white brick walls."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_42_real.mp4"
    },
    {
        "time": "[0:06:00 - 0:07:00]",
        "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:01]: In a bright kitchen with a prominent green plant by the window, a person in a blue shirt is standing at a wooden countertop. On the countertop, there are various kitchen utensils and cooking appliances, including a frying pan on the stove, two pots, a bowl with a spatula, and a set of knives in a holder. The person appears to be engaged in cooking. [0:06:01 - 0:06:02]: The person picks up a bowl filled with shredded ingredients, perhaps cheese or vegetables, from the countertop and holds it over the stove. [0:06:02 - 0:06:05]: The scene shifts to a wider view of the kitchen. A white brick wall with shelves holding plates, jars, and utensils is in the background. Decorative letters spelling \"COOK\" are prominently displayed on the top shelf. The person continues to stand in front of the stove, holding the bowl and preparing to pour its contents into a pan. On the counter in the foreground, there are more cooking utensils and ingredients. [0:06:05 - 0:06:06]: The person now has their focus on the pan while carefully adding the contents from the bowl. The scene remains in the same kitchen setup with the background unchanged. [0:06:06 - 0:06:07]: The person looks down intently at the pan while adding the ingredients from the bowl, making sure to distribute everything evenly. [0:06:07 - 0:06:10]: The view returns to the previous angle in the kitchen. The person is adjusting the pans and pots on the stove. The countertop remains cluttered with various kitchen essentials, and the green plant in the background adds a touch of nature to the setting. [0:06:10 - 0:06:11]: The person leans towards a pan on the stove and begins adding more ingredients from the bowl. The view remains focused on the cooking area with the same background elements. [0:06:11 - 0:06:13]: Close-up of the person\u2019s hand holding the bowl and actively adding the shredded ingredients into a frying pan on the stove. The surrounding countertop shows multiple kitchen tools and ingredients, indicating a busy cooking session. [0:06:13 - 0:06:15]: The hand continues to add shredded ingredients from the bowl into the frying pan. Adjacent to the pan are spatulas, ladles, and other cooking essentials within arm's reach. [0:06:15 - 0:06:17]: The person's hand dips back into the bowl to grab another handful of ingredients, then proceeds to sprinkle them into the frying pan. The stovetop shows two additional pots, one covered and one boiling. [0:06:17 - 0:06:18]: A top-down view of the stove reveals three active burners. One pot has a lid and contains boiling food items, and the frying pan is filled with the shredded ingredients that are being cooked. The person's hand blurs slightly as it moves about, indicating continuous activity. The countertop holds various additional kitchen supplies and ingredients for the meal being prepared. [0:06:18 - 0:06:19]: The perspective shifts back to a wider shot of the person in front of the stove in the kitchen. The brick wall with shelves is still visible, along with the decorative \"COOK\" letters and multiple kitchen items on the countertop. The person seems to be finishing up their task and turning away from the stove, possibly to get more ingredients or tools.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What does the person hold over after touching the handle of the pan?",
                "time_stamp": "0:06:03",
                "answer": "B",
                "options": [
                    "A. A frying pan.",
                    "B. A bowl filled with shredded ingredients.",
                    "C. A pot.",
                    "D. A set of knives."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Counting",
                "question": "How many active burners are revealed in the top-down view of the stove?",
                "time_stamp": "0:06:25",
                "answer": "C",
                "options": [
                    "A. One.",
                    "B. Two.",
                    "C. Five.",
                    "D. Four."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Text-Rich Understanding",
                "question": "What do the decorative letters on the top shelf spell?",
                "time_stamp": "0:06:05",
                "answer": "C",
                "options": [
                    "A. EAT.",
                    "B. CHEF.",
                    "C. COOK.",
                    "D. FOOD."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_42_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:05]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color is the car parked near the Diner right now?",
                "time_stamp": "00:00:04",
                "answer": "D",
                "options": [
                    "A. Blue.",
                    "B. Red.",
                    "C. Yellow.",
                    "D. Pink."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_481_real.mp4"
    },
    {
        "time": "[0:00:45 - 0:00:50]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What object is the track passing under right now?",
                "time_stamp": "00:00:48",
                "answer": "C",
                "options": [
                    "A. A chair.",
                    "B. A table.",
                    "C. A bed.",
                    "D. A couch."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_481_real.mp4"
    },
    {
        "time": "[0:01:30 - 0:01:35]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color is the car on top of the cabinet right now?",
                "time_stamp": "00:01:30",
                "answer": "B",
                "options": [
                    "A. Red.",
                    "B. Blue.",
                    "C. Green.",
                    "D. Yellow."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_481_real.mp4"
    },
    {
        "time": "[0:02:15 - 0:02:20]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What object is the track curving around right now?",
                "time_stamp": "00:02:19",
                "answer": "C",
                "options": [
                    "A. A couch.",
                    "B. A table.",
                    "C. A sofa.",
                    "D. A window."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_481_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:03:05]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the color of the walls near the track right now?",
                "time_stamp": "00:03:00",
                "answer": "B",
                "options": [
                    "A. White.",
                    "B. Beige.",
                    "C. Blue.",
                    "D. Green."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_481_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:10]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions taking place just now?",
                "time_stamp": "00:00:10",
                "answer": "C",
                "options": [
                    "A. The individual prepared fresh lettuce by washing it thoroughly in a sink.",
                    "B. The individual took lettuce from a drawer, cooked it briefly, and then placed it on a preparation surface.",
                    "C. The individual retrieved spinach from a compartment, cooked it on a flat top grill, and seasoned it.",
                    "D. The individual prepared a salad by chopping tomatoes and cucumbers and adding them to a bowl."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_413_real.mp4"
    },
    {
        "time": "[0:02:56 - 0:03:06]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions taken just now in the kitchen?",
                "time_stamp": "00:03:06",
                "answer": "B",
                "options": [
                    "A. The cook selected a frying pan, saut\u00e9ed ingredients, and served them with a sauce.",
                    "B. The cook retrieved pans, prepared a mixture, and added a creamy sauce to one pan.",
                    "C. The cook diced vegetables, fried them, and placed them on a serving tray.",
                    "D. The cook heated a pan, added oil, and mixed it with pre-prepared sauces."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_413_real.mp4"
    },
    {
        "time": "[0:05:52 - 0:06:02]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the recent activity just shown?",
                "time_stamp": "00:06:02",
                "answer": "C",
                "options": [
                    "A. The individual was plating a dish, adding final garnishes, and preparing the plate for serving.",
                    "B. The individual was cooking an omelet, flipping it multiple times, and placing it on a plate.",
                    "C. The individual was preparing a dish by stirring ingredients in a pan and adding various seasonings.",
                    "D. The individual was cleaning the kitchen station, wiping surfaces, and organizing utensils."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_413_real.mp4"
    },
    {
        "time": "[0:08:48 - 0:08:58]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions of the individual just now?",
                "time_stamp": "00:08:58",
                "answer": "B",
                "options": [
                    "A. The individual cleaned the kitchen area and organized the utensils.",
                    "B. The individual saut\u00e9ed some vegetables, operated the stove, and managed multiple pans on a busy kitchen station.",
                    "C. The individual prepared a salad by chopping and mixing fresh ingredients.",
                    "D. The individual baked a cake, frosted it, and placed it in a refrigerator to cool."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_413_real.mp4"
    },
    {
        "time": "[0:11:44 - 0:11:54]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions performed just now?",
                "time_stamp": "00:11:54",
                "answer": "B",
                "options": [
                    "A. The individual mixed a variety of raw ingredients in a bowl and started blending them.",
                    "B. The individual saut\u00e9ed green vegetables in a pan, transferred them to a serving dish, and prepared different sauce pans.",
                    "C. The individual grilled several pieces of meat and then plated them with garnish.",
                    "D. The individual baked a loaf of bread and buttered it before serving."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_413_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins with a view of a brightly colored, abstract painting on a white wall. The painting features various shapes and patches of colors including yellow, orange, purple, blue, green, and red. In the background, part of another white wall and some indistinguishable objects or people can be seen. [0:00:03 - 0:00:04]: The camera pans slightly to the right, revealing more of the white wall and another abstract painting next to the first. This second painting, by Stephen Rowe, is composed of numerous splashes and strokes of various colors including black, green, yellow, and red on a predominantly white background. [0:00:05 - 0:00:09]: The focus continues on the second painting by Stephen Rowe, fully revealing it. The painting is large, occupying most of the frame. The use of splashes and strokes creates a chaotic yet visually engaging composition.  [0:00:10 - 0:00:12]: As the video progresses, the camera gradually moves farther to the right, displaying the right side of the painting and part of another gallery wall that is white. Some visitors of the gallery are now visible in the background to the far right side. [0:00:13 - 0:00:15]: The camera further moves to the right, showing more of the gallery space. It reveals more paintings and exhibits on the walls and a few people walking around the gallery. The setting appears to be well-lit with high ceilings. [0:00:16 - 0:00:17]: The focus shifts to another artwork on the right side of the gallery. These are boxes mounted on the wall with various colorful, three-dimensional structures inside. The boxes have multilayered components with bright red, purple, and yellow pieces interwoven. [0:00:18 - 0:00:19]: The video ends with the camera capturing the upper portion of the box structures with intricate details, including some cartoonish eyes drawn on one of the higher boxes. More of the gallery space, including more artworks, is visible in the background.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Which artist's work has appeared in the video?",
                "time_stamp": "00:00:09",
                "answer": "C",
                "options": [
                    "A. Vincent van Gogh.",
                    "B. Pablo Picasso.",
                    "C. Stephen Rowe.",
                    "D. Claude Monet."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "art_show",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_462_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:44]: The video starts with a view of an art gallery wall containing two framed paintings. The upper painting depicts an underwater scene in shades of green and blue, while the lower painting illustrates a wave breaking in the ocean. A person moves from the left side toward the center. [0:02:45 - 0:02:49]: The camera moves slightly to the right, revealing more of the gallery. In addition to the lower seascape painting, an adjacent larger painting of underwater rocks and reflections becomes visible. The colors are primarily green and blue with some white highlights illustrating the reflections. [0:02:50 - 0:02:51]: The camera continues panning to the right, further revealing details of the large underwater painting. Bright, diffused light appears reflected on the large green rocks submerged underwater. [0:02:52 - 0:02:54]: The camera pans more to the right, capturing another substantial painting with a similar underwater theme, featuring clear, blue-green water and submerged rock formations. Both paintings showcase realistic reflections on the water surface. [0:02:55 - 0:02:57]: The camera focuses on the third piece of art. Above it, a black sign with white text describing \"Irwin Cumberland\" is visible. A section of the ceiling implies indoor gallery lighting. [0:02:58 - 0:02:59]: The video concludes with a close-up of the upper portion of the last blue-green water painting and the artist's name sign. Reflections continue to be a focal point in the water's surface, enhancing the realism of the artwork.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the black sign with white text describing \"Irwin Cumberland\" located in relation to the paintings?",
                "time_stamp": "00:02:57",
                "answer": "C",
                "options": [
                    "A. Below the painting.",
                    "B. To the left of the painting.",
                    "C. Above the painting.",
                    "D. To the right of the painting."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Object Recognition",
                "question": "What is depicted in the painting shown in the video right now?",
                "time_stamp": "00:02:42",
                "answer": "A",
                "options": [
                    "A. A wave breaking in the ocean.",
                    "B. A forest scene.",
                    "C. An abstract pattern.",
                    "D. A cityscape."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "art_show",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_462_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:22]: On a white wall, the video captures several black and white photos framed in black, arranged in a grid layout. The images depict various scenes and people, each with distinct themes and settings. [0:05:23 - 0:05:26]: The camera pans to the right, revealing a framed painting of a rural landscape during sunset with a house and trees in the foreground. Above the painting, a sign reads \"Michael Miller.\" [0:05:27 - 0:05:28]: The camera continues to focus on the painting, with the name \"Michael Miller\" still visible at the top. [0:05:29 - 0:05:35]: The camera moves down, showing the painting more closely and then begins to pan to the right side, revealing more framed images on the walls. The painting of the rural landscape remains the focal point. [0:05:36 - 0:05:39]: The camera finishes panning right, showing a different wall with framed photographs featuring portraits of people, prominently displaying a woman with red hair and another person in different poses and attire. The video ends with the name \"Kira Tanelle\" visible at the top.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "which painting did the camera focus on before revealing the name \"Kesja Tabaczuk\"?",
                "time_stamp": "00:05:39",
                "answer": "A",
                "options": [
                    "A. A painting of a rural landscape.",
                    "B. A mirror.",
                    "C. A bookshelf.",
                    "D. A window."
                ],
                "required_ability": "episodic memory"
            },
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is the last name shown in the video?",
                "time_stamp": "00:05:54",
                "answer": "B",
                "options": [
                    "A. Michael Miller.",
                    "B. Kesja Tabaczuk.",
                    "C. John Smith.",
                    "D. Sarah Parker."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "art_show",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_462_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video starts with a first-person view in a supermarket or similar store. The viewer's perspective shows a stack of cardboard boxes on a cart in front of refrigerated display units. Inside the units at the top, packages of dairy products are visible. The floor is made of light-colored tiles. [0:00:01 - 0:00:02]: The viewpoint shifts slightly forward, showing a clearer view of the refrigerated display with various dairy products. [0:00:02 - 0:00:03]: The scene changes to look into the refrigerated unit, filled with various cartons of plant-based milk stacked neatly. The camera angle captures a bird's-eye view of the shelves. [0:00:03 - 0:00:10]: A person wearing gloves reaches into the refrigerated display. They start organizing and adjusting the milk cartons, ensuring they are neatly placed. The hands are consistently rearranging the cartons, moving them from one spot to another to ensure they are correctly aligned on the shelf. [0:00:10 - 0:00:15]: The person continues to organize the milk cartons on the shelves, ensuring all the items are front-facing and evenly positioned. The movement of hands and cartons is smooth, indicating careful arrangement. [0:00:15 - 0:00:16]: The perspective shifts back to a wider view, showing the stack of cardboard boxes on the cart, the refrigerated display, and more of the store surroundings. [0:00:16 - 0:00:19]: The person handling the camera focuses back on the boxes on the cart. They start opening one of the boxes, revealing more cartons similar to those being arranged on the shelf. The arrangement of boxes, cans, and cartons on the cart becomes clearer as the person continues to work.",
        "questions": [
            {
                "task_type": "Prospective Reasoning",
                "question": "What might the person do next after revealing more cartons in the box?",
                "time_stamp": "0:00:19",
                "answer": "B",
                "options": [
                    "A. Leave the store.",
                    "B. Start arranging the new cartons on the shelf.",
                    "C. Throw away the cartons.",
                    "D. Close the refrigerated display."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "storage_manage",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_444_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:23]: The video starts with a first-person perspective view of a person placing cartons of milk on a refrigerated shelf in a store. The individual is wearing black gloves and seems to be organizing the cartons neatly in rows. The cartons have a white and blue design with some colorful graphics.  [0:02:24 - 0:02:31]: The camera angle shifts to reveal a wider view of the transactions, showing a cart loaded with cardboard boxes and packs of items possibly waiting to be shelved. The person then lifts some of the boxes off the cart and presumably begins to unpack them for shelving. [0:02:32 - 0:02:33]: The view returns to the refrigerated display where the individual places another carton on the shelf among the already arranged milk cartons. They continue to ensure the cartons are aligned correctly. [0:02:34 - 0:02:38]: The camera focuses back on the cart loaded with items. The person appears to continue unpacking and handling the cartons, moving them to the refrigerated section. They keep alternating between picking up new cartons and arranging them on the shelf. [0:02:39]: Finally, the view shows the individual making slight adjustments to the placement of the cartons, ensuring the neat alignment of the items on the shelf. The refrigeration unit and another set of milk cartons are visible in the background, displaying an organized and methodical stocking process.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is being placed on the refrigerated shelf right now?",
                "time_stamp": "0:02:23",
                "answer": "A",
                "options": [
                    "A. Cartons of milk.",
                    "B. Bottles of water.",
                    "C. Packs of juice.",
                    "D. Bags of vegetables."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "storage_manage",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_444_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:41]: In a supermarket, looking at a refrigerated display filled with various juices and dairy products. Predominantly yellow, purple, and green packages are on the top shelf, along with blue and yellow items on the bottom shelf. The person is removing items from a cardboard box. [0:04:42 - 0:04:43]: The camera shifts slightly to the right, revealing more of the display. The cardboard box has been placed on the bottom shelf. More yellow and green juice containers are visible on the top shelf. [0:04:44]: Close-up of the cardboard box in the person's hands. They inspect the box, positioning it over the drinks. [0:04:45]: The person lifts the box to reveal multiple cartons of purple and orange drinks on a cart beside the refrigerated display. [0:04:46]: More of the refrigerated display is shown. The cardboard box is now empty and lying flat, with two rows of purple and orange drink cartons neatly arranged next to it. [0:04:47]: The person continues to organize items inside the refrigerated display. Their gloved hands are visible, along with additional rows of drinks in purple and orange packaging. [0:04:48]: The same area of the refrigerated display with a neat arrangement of purple and orange drinks. The emptied cardboard box remains in place. [0:04:49]: A view showing more of the surroundings, such as the tiled floor and adjacent shelves. The drinks are neatly organized within the display. [0:04:50 - 0:04:51]: The camera shifts back to the refrigerated display, providing a wider view of the setup and showing more items to the left. There are bottles and cartons of various colors arranged neatly. [0:04:52]: Moving back to the cart, it is stacked with cartons of drinks, specifically orange, purple, and red ones. The person's gloved hand is visible. [0:04:53 - 0:04:54]: The person pushes the shopping cart, filled with colorful drink cartons, towards the right. [0:04:55]: The perspective shifts to the left, showing the refrigerated display full of dairy products and juices. Items are being organized and rearranged. [0:04:56]: The person is handling items, moving them between the cart and display. Their gloved hands continue arranging the colorful drink cartons. [0:04:57 - 0:04:59]: The person maneuvers the cart filled with various colored drinks on a tiled floor. Some drinks have been taken out and placed in the refrigerated display.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the cardboard box placed after the person removes items from it?",
                "time_stamp": "0:04:43",
                "answer": "A",
                "options": [
                    "A. On the top shelf.",
                    "B. On the floor.",
                    "C. On the bottom shelf.",
                    "D. On the cart."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "storage_manage",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_444_real.mp4"
    },
    {
        "time": "[0:07:00 - 0:08:00]",
        "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:02]: The video begins in a supermarket, visible from a first-person perspective. The camera faces downward at a display of various yogurt cups. The yogurt cups are brightly colored, mostly in purples, oranges, and whites, arranged in organized rows within a refrigerated section. Beside the rows of yogurt cups, a stack of flat, folded cardboard trays is positioned to the right of the camera's view. [0:07:03 - 0:07:04]: The individual in the video moves their hands into view, each wearing black gloves. They lift one of the flat cardboard trays from the stack and begin unfolding it. Behind the tray, more refrigerated items, potentially dairy products, are visible. [0:07:05 - 0:07:07]: The camera shows the person placing the now unfolded cardboard tray down with the yogurt cups. The individual starts organizing the yogurt cups within the tray. The tiling on the floor reflects some overhead lighting, revealing a clean, well-maintained supermarket setting. [0:07:08 - 0:07:11]: The person continues handling the yogurt cups, placing more into the folded trays. They are methodically working, indicating an organized and systematic approach. Meanwhile, some more sections of refrigerated goods are visible in the background, along with the tiled floor and aisles of the store. [0:07:12 - 0:07:15]: The first-person perspective remains consistent, showing the person continuing their process of moving the yogurt cups and organizing them into the cardboard trays. The background reveals more items within the refrigeration units, likely providing perishables or other dairy-related products. [0:07:16 - 0:07:19]: The person, still wearing gloves, reaches for another flat cardboard tray from the stack. They unfold it and place it down, continuing the organized placement of yogurt cups into the tray. The tidy presentation of the supermarket\u2019s aisles and clear lighting emphasizes the cleanliness and order of the environment.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What main activity was carried out just now?",
                "time_stamp": "00:07:19",
                "answer": "A",
                "options": [
                    "A. Move the drinks from the cart to the refrigerated shelf.",
                    "B. Cleaning the store aisles.",
                    "C. Shopping for groceries.",
                    "D. Checking out at the register."
                ],
                "required_ability": "episodic memory"
            },
            {
                "task_type": "Spatial Understanding",
                "question": "Where are the folded cardboard trays located in relation to the camera's view right now?",
                "time_stamp": "00:06:56",
                "answer": "B",
                "options": [
                    "A. To the left of the camera.",
                    "B. To the right of the camera.",
                    "C. Directly in front of the camera.",
                    "D. Behind the camera."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "storage_manage",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_444_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:20]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What object is pointing out the window right now?",
                "time_stamp": "00:00:10",
                "answer": "C",
                "options": [
                    "A. A pencil.",
                    "B. A pen.",
                    "C. A finger.",
                    "D. A stick."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_420_real.mp4"
    },
    {
        "time": "[0:02:03 - 0:02:23]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the weather condition right now?",
                "time_stamp": "00:02:09",
                "answer": "C",
                "options": [
                    "A. Clear skies.",
                    "B. Heavy rain.",
                    "C. Overcast with clouds.",
                    "D. Snowing."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_420_real.mp4"
    },
    {
        "time": "[0:04:06 - 0:04:26]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is the current speed of the wind?",
                "time_stamp": "00:04:09",
                "answer": "C",
                "options": [
                    "A. 20 kph.",
                    "B. 25 kph.",
                    "C. 30 kph.",
                    "D. 35 kph."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_420_real.mp4"
    },
    {
        "time": "[0:06:09 - 0:06:29]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color is the horizon right now?",
                "time_stamp": "00:06:22",
                "answer": "C",
                "options": [
                    "A. Red.",
                    "B. Yellow.",
                    "C. Orange.",
                    "D. Blue."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_420_real.mp4"
    },
    {
        "time": "[0:08:12 - 0:08:32]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color is the sky right now?",
                "time_stamp": "00:08:32",
                "answer": "B",
                "options": [
                    "A. Bright red.",
                    "B. Dark grey.",
                    "C. Clear with no clouds.",
                    "D. Yellowish-orange."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_420_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:20]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is located on the left side of the current road?",
                "time_stamp": "00:00:10",
                "answer": "B",
                "options": [
                    "A. A dense forest.",
                    "B. An open field.",
                    "C. A river.",
                    "D. A row of houses."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_174_real.mp4"
    },
    {
        "time": "[0:02:02 - 0:02:22]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the wire fence located right now?",
                "time_stamp": "0:02:20",
                "answer": "B",
                "options": [
                    "A. On the right side of the road.",
                    "B. On the left side of the road.",
                    "C. Ahead on the left.",
                    "D. Behind on the right."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_174_real.mp4"
    },
    {
        "time": "[0:04:04 - 0:04:24]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is situated on the right side of the path right now?",
                "time_stamp": "00:04:19",
                "answer": "B",
                "options": [
                    "A. A lamp post.",
                    "B. A bench.",
                    "C. A trash can.",
                    "D. A bicycle rack."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_174_real.mp4"
    },
    {
        "time": "[0:06:06 - 0:06:26]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where are the overhead power lines located right now?",
                "time_stamp": "00:06:17",
                "answer": "B",
                "options": [
                    "A. Crossing the road ahead.",
                    "B. Running parallel to the left of the road.",
                    "C. Running parallel to the right of the road.",
                    "D. Not visible at the moment."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_174_real.mp4"
    },
    {
        "time": "[0:08:08 - 0:08:28]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is visible on the left side of the path right now?",
                "time_stamp": "00:08:18",
                "answer": "A",
                "options": [
                    "A. A field with some fencing.",
                    "B. A dense forest.",
                    "C. A village with a few houses.",
                    "D. A river flowing parallel to the path."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_174_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: A pair of hands holds a green, conical object and is pressing it down onto a large rectangular piece of marble. There are some red splotches on the marble surface under the green object. Next to the marble slab is a wooden bowl containing two eggs and some dark colored small objects. [0:00:04 - 0:00:07]: The scene changes to an elderly person with a white beard and glasses, holding a quill pen. The person appears to be writing on a large, flat surface that is illuminated by a candle in the background. The setup appears like a desk with a pot nearby. [0:00:08 - 0:00:11]: The elderly person is shown in a close-up profile view, still holding the quill pen and continuing to write. This view emphasizes their focused expression as they work. [0:00:12 - 0:00:15]: The scene changes to a close-up view of an illuminated manuscript. The manuscript is decorated with gold and red colors, and has ornate text and designs, revealing that it is an old, possibly medieval, document. [0:00:16 - 0:00:19]: The final segment shows a younger person in a black robe sitting at a desk with an open book. They are holding a quill pen and writing in the book. This scene is set in a room with a large window made of small, square panes of glass. Different objects, including vessels and a candle, are present on the desk around them.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the elderly person with a white beard doing in the video?",
                "time_stamp": "0:00:10",
                "answer": "D",
                "options": [
                    "A. Reading a book.",
                    "B. Painting on a canvas.",
                    "C. Cooking with a pot.",
                    "D. Writing with a quill pen."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Counting",
                "question": "How many eggs are in the wooden bowl next to the marble slab?",
                "time_stamp": "0:00:03",
                "answer": "B",
                "options": [
                    "A. One.",
                    "B. Five.",
                    "C. Three.",
                    "D. Four."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "getting_dressed",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_155_real.mp4"
    },
    {
        "time": "[0:01:00 - 0:02:00]",
        "captions": "[0:01:20 - 0:01:40] [0:01:20 - 0:01:28]: Inside a room with wood and plaster walls, there is a man in loose-fitting, beige pants, adjusting the waistband. Various pieces of clothing and objects are scattered around, including a bed with a fur blanket and a bench; [0:01:29 - 0:01:33]: The camera focuses on the close-up of the man\u2019s hands as he adjusts a string on the waistband of his pants; [0:01:34 - 0:01:38]: The man stands up straight and begins to reach for a light-colored piece of clothing on a bench; [0:01:39]: A close-up of the man's face shows him looking down, possibly concentrating on what he is doing with the clothing.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What action does the man perform after adjusting his waistband?",
                "time_stamp": "00:01:40",
                "answer": "D",
                "options": [
                    "A. Sits down.",
                    "B. Looks out the window.",
                    "C. Lies down on the bed.",
                    "D. Reaches for a piece of clothing."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "getting_dressed",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_155_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:49]: A person is bending over while sitting on a bench or stool. The individual, wearing a light-colored shirt and blue jeans, is seen tying the laces on a pair of brown leather boots. The surrounding floor is covered in hay, suggesting a barn or rustic setting. The person's hands are engaged in adjusting and securing the boot laces.  [0:02:50 - 0:02:55]: The camera moves to a wider angle showing the full body of the person who is bent over and securing the boots. Wooden beams and a rustic interior can be seen in the background. The person, dressed in a long, loose shirt, continues to tighten and adjust the laces. [0:02:56]: The camera angle shifts to show a portion of the wall with wooden beams and some hanging items, possibly indicating a rural interior setting. [0:02:57]: The person in the video begins to stand up from a partially obscured position, leaning forward. [0:02:58 - 0:02:59]: The individual, now fully upright, is adjusting their clothing, possibly preparing to leave. The rustic wooden beams and plaster wall remain visible in the background.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What type of boots is the person wearing?",
                "time_stamp": "0:02:49",
                "answer": "A",
                "options": [
                    "A. Brown leather boots.",
                    "B. Rubber boots.",
                    "C. Sneakers.",
                    "D. Sandals."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "getting_dressed",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_155_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:04:53]",
        "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:05]: A person with short brown hair and light facial hair, wearing a light brown coat and a dark purple scarf draped around their neck, is standing facing mostly towards the left. The background consists of a grassy field with some scattered flowers and a line of dense trees in the distance under a cloudy sky. The person appears to be looking somewhere off in the distance. [0:04:06 - 0:04:11]: The person starts walking away from the camera, revealing a back view of their coat and scarf. They continue walking towards the dense tree line in the distance. The field is vast and open with clusters of small yellow flowers scattered around. [0:04:12 - 0:04:20]: The person continues to walk further away, gradually becoming smaller in size until they are no longer discernible. The frame is dominated by the lush green grass and the dark green tree line in the background.\n[0:04:40 - 0:04:53] [0:04:40 - 0:04:41]: A field with numerous small yellow flowers stretches out in the foreground, occupying most of the frame. Green grass covers the ground, and there is a hedge line along the horizon. A tall tree stands on the right side, and the sky is overcast, casting a muted light over the scene. [0:04:42 - 0:04:47]: The scene transitions to an abstract blue geometric shape on a black background. The shape appears three-dimensional and consists of numerous elongated rectangular prisms radiating from a central point, forming a symmetrical pattern. The prisms move slightly, creating an impression of motion. [0:04:48 - 0:04:51]: The blue geometric shape continues to evolve, becoming more intricate as additional prisms appear and rotate. The background remains black. [0:04:48 - 0:04:50]: Text reading \"CROW'S EYE PRODUCTIONS\" appears below the geometric shape. A thin white beam of light points from the upper right towards the center of the shape, creating a spotlight effect. [0:04:51 - 0:04:52]: The scene fades to black.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the person wearing around their neck right now?",
                "time_stamp": "00:04:05",
                "answer": "A",
                "options": [
                    "A. A dark purple scarf.",
                    "B. A dark green scarf.",
                    "C. A light brown scarf.",
                    "D. A dark blue scarf."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "getting_dressed",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_155_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:20]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the cyclist with the number 894 located relative to the main pack right now?",
                "time_stamp": "00:00:19",
                "answer": "B",
                "options": [
                    "A. Leading the main pack.",
                    "B. Directly behind the main pack.",
                    "C. In the middle of the main pack.",
                    "D. To the side of the main pack."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_175_real.mp4"
    },
    {
        "time": "[0:02:05 - 0:02:25]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What structure is the cyclist approaching right now?",
                "time_stamp": "00:02:07",
                "answer": "A",
                "options": [
                    "A. A National Championships banner.",
                    "B. A McDonald's sign.",
                    "C. A residential building.",
                    "D. A traffic light."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_175_real.mp4"
    },
    {
        "time": "[0:04:10 - 0:04:30]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is the relative position of the red brick building to the cyclist right now?",
                "time_stamp": "00:04:19",
                "answer": "C",
                "options": [
                    "A. On the left side of the road.",
                    "B. Directly ahead.",
                    "C. On the right side of the road.",
                    "D. Directly behind the cyclist."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_175_real.mp4"
    },
    {
        "time": "[0:06:15 - 0:06:35]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the \"National Championships\" banner located right now?",
                "time_stamp": "00:06:34",
                "answer": "C",
                "options": [
                    "A. On the right side of the road.",
                    "B. Above the cyclists.",
                    "C. Hanging in the middle of the road.",
                    "D. Directly behind the cyclists."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_175_real.mp4"
    },
    {
        "time": "[0:08:20 - 0:08:40]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is the relative position of the cyclist in the green shoes to the group right now?",
                "time_stamp": "00:08:32",
                "answer": "C",
                "options": [
                    "A. Leading the group.",
                    "B. In the middle of the group.",
                    "C. At the back of the group.",
                    "D. Outside the group."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_175_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:10]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following actions best summarizes what the individual just now did?",
                "time_stamp": "00:00:12",
                "answer": "C",
                "options": [
                    "A. The individual greeted the customers, prepared two glasses of wine, and served the drinks.",
                    "B. The individual chatted with the customers, filled two glasses with ice, and arranged them on the counter.",
                    "C. This person places three glass cups in front of the table.",
                    "D. The individual cleaned the counter, arranged empty glasses, and interacted with other staff members."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_404_real.mp4"
    },
    {
        "time": "[0:02:04 - 0:02:14]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions taken by the person just now?",
                "time_stamp": "00:02:18",
                "answer": "C",
                "options": [
                    "A. The person was rearranging glasses on the counter and chatting with colleagues.",
                    "B. The person was cleaning the counter and organizing different bottles.",
                    "C. The person was taking out a bottle from a refrigerator and pouring its contents into glasses with ice.",
                    "D. The person was preparing food items and arranging plates and utensils."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_404_real.mp4"
    },
    {
        "time": "[0:04:08 - 0:04:18]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "What best summarizes the actions performed by the individual just now?",
                "time_stamp": "00:04:18",
                "answer": "C",
                "options": [
                    "A. The individual cleaned the bar area, organized supplies, and inspected the beverage inventory.",
                    "B. The individual prepared a batch of food ingredients, cooked them, and plated the dishes.",
                    "C. The individual prepared garnishes, assembled multiple drinks.",
                    "D. The individual greeted guests, took their orders, and rang up the sales at the cash register."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_404_real.mp4"
    },
    {
        "time": "[0:06:12 - 0:06:22]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following actions best summarizes what the individual just now did?",
                "time_stamp": "00:06:22",
                "answer": "C",
                "options": [
                    "A. The individual greeted customers, prepared a meal, and served it.",
                    "B. The individual cleaned the bar area, organized supplies, and managed the inventory.",
                    "C. The individual prepared a cocktail, stirred it, and placed it on the counter.",
                    "D. The individual filled a glass with ice, poured a drink, and wiped the counter."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_404_real.mp4"
    },
    {
        "time": "[0:08:16 - 0:08:26]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following actions best summarizes what the individual just now did?",
                "time_stamp": "00:08:26",
                "answer": "C",
                "options": [
                    "A. The individual took drink orders from customers while others were conversing.",
                    "B. The individual cleared dishes from the table while customers waited for their drinks.",
                    "C. The individual entered order details on a tablet while talking with the customers.",
                    "D. The individual prepared cocktails using a shaker while engaging with customers."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_404_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video begins with a transition from a black screen to a lively street scene, showing a cobbled sidewalk and a modern building with glass windows reflecting the surroundings. A motorbike with a rider wearing a helmet is seen moving on the road, passing a traffic light that is red. [0:00:04 - 0:00:07]: The video provides a clearer view of the street, revealing more details of the surroundings. A large tree is visible in the center of the scene, with various buildings in the background showcasing traditional European architecture. The red traffic light remains in view, and additional vehicles, including a black car, are seen moving through the intersection. [0:00:08 - 0:00:09]: The motorbike from earlier proceeds through the intersection, while the black car moves closer to the camera. Pedestrians are seen walking on the opposite sidewalk. The video continues to depict the vibrant urban environment with shops and a broad pedestrian area. [0:00:10 - 0:00:14]: More vehicles, including a dark-colored car, are seen driving through the intersection. The motorbike from earlier continues forward. The camera angle slightly shifts, showing more details of the sidewalk and the surrounding buildings. Pedestrians and cyclists are also visible in the background. [0:00:15 - 0:00:20]: The camera continues to move along the sidewalk, providing a view of the street with a mix of traffic including cars, motorbikes, and bicycles. A red delivery scooter and a cyclist are spotted moving along the road in the same direction as the camera. The various storefronts, signs, and urban artwork add to the dynamic atmosphere of the scene.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What type of sidewalk is depicted in the video right now?",
                "time_stamp": "0:00:20",
                "answer": "C",
                "options": [
                    "A. Metal sidewalk.",
                    "B. Wooden sidewalk.",
                    "C. Stone sidewalk.",
                    "D. Concrete sidewalk."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Action Recognition",
                "question": "What did the red motorbike do just now?",
                "time_stamp": "0:00:09",
                "answer": "C",
                "options": [
                    "A. Stops at the intersection.",
                    "B. Turns left at the traffic light.",
                    "C. Proceeds through the intersection and moves along the road.",
                    "D. Parks on the sidewalk."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_325_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:59]: The video begins on a bright and sunny day with a view of a paved street lined with buildings on the left and a sidewalk on the right. The camera appears to be held by a person walking down the street. To the right, there's a small metal kiosk with various advertisements on it. A white van is parked next to the sidewalk on the right. The buildings on the left appear to be shops and are decorated with colorful signs. Some people are visible walking on the sidewalk; a few are heading towards the camera while others walk away. In the distance, more buildings can be seen, including a tall building that stands out due to its height. A line of trees is seen further down the street, adding greenery to the urban setting. As the camera moves forward, the left side shows shops with red chairs and tables outside, suggesting a small caf\u00e9 or dining area. On both sides of the street, there are lampposts and various other signs, contributing to the lively atmosphere of the area. Close to the end of the video, on the right side, bicycles are parked against a fence, and to the right of the fence is a set of stairs leading upward.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What type of vehicle is parked in the middle of the frame right now?",
                "time_stamp": "00:03:00",
                "answer": "B",
                "options": [
                    "A. A red car.",
                    "B. A white car.",
                    "C. A black motorcycle.",
                    "D. A yellow truck."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_325_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:31]: A bustling urban street stretches into the distance, lined with multi-story buildings on both sides. The buildings have large windows, some with balconies, adorned with railings. The street is paved with cobblestones, featuring tram tracks running down the center. Multiple cars are parked along the right side of the street, while a white van is parked near the center of the right lane. Pedestrians walk along the sidewalks; a woman in a black dress walks away from the camera on the right side of the screen, while a person in a green shirt walks away on the left. Shops with signs and displays line the ground floor of the buildings. In the background, the street slopes downhill, revealing more buildings further along. [0:05:32 - 0:05:39]: As the video continues, the scene changes slightly with pedestrians moving about. A group of individuals walks on the left sidewalk, gradually progressing further down. The woman with a red bag, previously near the center, moves more toward the right side. The lighting remains bright, suggesting a clear day with blue skies visible between the buildings. The street retains its cobblestone and tram tracks, and cars continue to be parked along the sides. The white van remains stationary, contributing to the scene's bustling yet orderly atmosphere.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the white van parked in relation to the rest of the cars on the street?",
                "time_stamp": "00:05:30",
                "answer": "D",
                "options": [
                    "A. Near the center of the left lane.",
                    "B. Near the center of the right lane.",
                    "C. Along the left side of the street.",
                    "D. Along the right side of the street."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_325_real.mp4"
    },
    {
        "time": "[0:08:00 - 0:09:00]",
        "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:04]: The scene opens on a large stone building with detailed and ornate architecture. The sky is clear, and the weather is sunny. A man in a light blue shirt, dark pants, and blue shoes stands at the entrance of the building. The entrance is framed by large columns and features a wooden door. Several people are seen walking on the sidewalk, some at the entrance of the building, and a person wearing a neon yellow-green safety vest is visible on the right side next to a parked car.  [0:08:05 - 0:08:06]: Inside the building, it features a grand, large hall with high ceilings and ornate detailing. There are several large arched windows along the walls, which let in ample natural light. A large clock is prominently displayed on the wall above the entrance. The floor has a checkered black and white pattern near the entrance and a more subtle beige and brown pattern in the rest of the hall. People are seen walking and standing around, including a man with glasses and a gray shirt standing with his hands on his hips and a person sitting near the entrance. [0:08:07 - 0:08:20]: As the scene progresses, the interior of the hall becomes more visible. Beautiful large blue and white tile murals depicting historical scenes adorn the walls. The hall is spacious, with people walking around or standing in groups, some taking photos. The ceiling features intricate details, including plasterwork and large chandeliers. The hall has a calm and grand ambiance, with a blend of historical architectural styles. Towards the end of the sequence, a woman in a light blue shirt and a skirt stands near the middle of the hall, and the natural light streaming through the tall arched windows illuminates the intricate details of the murals and the architecture.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is prominently displayed on the wall above the entrance inside the building?",
                "time_stamp": "0:08:09",
                "answer": "B",
                "options": [
                    "A. A painting.",
                    "B. A clock.",
                    "C. A sculpture.",
                    "D. A mirror."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_325_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:05]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What logo is visible on the gas station sign right now?",
                "time_stamp": "00:00:05",
                "answer": "A",
                "options": [
                    "A. Shell.",
                    "B. Chevron.",
                    "C. Exxon.",
                    "D. BP."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_251_real.mp4"
    },
    {
        "time": "[0:01:06 - 0:01:11]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What brand is the steering wheel right now?",
                "time_stamp": "00:01:06",
                "answer": "C",
                "options": [
                    "A. BMW.",
                    "B. Audi.",
                    "C. Mercedes-Benz.",
                    "D. Porsche."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_251_real.mp4"
    },
    {
        "time": "[0:02:12 - 0:02:17]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is the current speed of the car right now?",
                "time_stamp": "00:02:16",
                "answer": "B",
                "options": [
                    "A. 150 km/h.",
                    "B. 200 km/h.",
                    "C. 225 km/h.",
                    "D. 250 km/h."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_251_real.mp4"
    },
    {
        "time": "[0:03:18 - 0:03:23]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color are the gloves the driver is wearing right now?",
                "time_stamp": "00:03:20",
                "answer": "B",
                "options": [
                    "A. Red and black.",
                    "B. Blue and black.",
                    "C. Black and white.",
                    "D. Green and black."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_251_real.mp4"
    },
    {
        "time": "[0:04:24 - 0:04:29]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is the current speed of the car right now?",
                "time_stamp": "00:04:23",
                "answer": "D",
                "options": [
                    "A. 150 km/h.",
                    "B. 200 km/h.",
                    "C. 225 km/h.",
                    "D. 250 km/h."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_251_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:20]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is located on the left side of the cyclists right now?",
                "time_stamp": "00:00:19",
                "answer": "A",
                "options": [
                    "A. A row of shops.",
                    "B. A forest.",
                    "C. A parking lot.",
                    "D. A large park."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_168_real.mp4"
    },
    {
        "time": "[0:01:59 - 0:02:19]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the National Championships banner located right now?",
                "time_stamp": "00:02:17",
                "answer": "A",
                "options": [
                    "A. Hanging above the cyclist.",
                    "B. On the left side of the road.",
                    "C. On the right side of the road.",
                    "D. Directly behind the cyclist."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_168_real.mp4"
    },
    {
        "time": "[0:03:58 - 0:04:18]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the stop sign located right now?",
                "time_stamp": "00:04:13",
                "answer": "A",
                "options": [
                    "A. On the right side of the cyclists.",
                    "B. On the left side of the road.",
                    "C. Directly ahead of the cyclists.",
                    "D. Behind the cyclists."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_168_real.mp4"
    },
    {
        "time": "[0:05:57 - 0:06:17]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is positioned on the right side of the cyclists right now?",
                "time_stamp": "00:06:09",
                "answer": "A",
                "options": [
                    "A. A green fence.",
                    "B. A banner with an advertisement.",
                    "C. A row of trees.",
                    "D. A pedestrian sitting area."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_168_real.mp4"
    },
    {
        "time": "[0:07:56 - 0:08:16]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the cyclist with the orange socks located right now?",
                "time_stamp": "00:08:12",
                "answer": "B",
                "options": [
                    "A. To the right of the cameraman.",
                    "B. Directly in front of the cameraman.",
                    "C. To the left of the cameraman.",
                    "D. Behind the cameraman."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_168_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:20]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video starts with a completely black screen. [0:00:01 - 0:00:07]: Two computer processors with the labels \"AMD Ryzen 5 5600G\" and \"AMD Ryzen 7 5700G\" are placed side by side on a white surface. In the background, blurred images with various colors can be seen on a screen. [0:00:08]: The processors are now shown against a dark gradient background, with one side having a hint of red and the other blue. [0:00:09 - 0:00:17]: Information about the two processors is displayed on the screen. The comparison shows that both processors belong to the Zen 3 architecture for the CPU. For the GPU, the Ryzen 5600G has Vega 7 graphics, while the Ryzen 5700G has Vega 8 graphics. [0:00:18 - 0:00:19]: A person is seen sitting behind a table holding two boxes of the Ryzen processors. The setting appears to be a studio with various tech items on shelves in the blurred background. [0:00:20]: The person, still holding the Ryzen processors, is speaking directly to the camera. They seem to be gesturing as they describe or explain something.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What GPU does the AMD Ryzen 7 5700G have right now?",
                "time_stamp": "00:00:20",
                "answer": "C",
                "options": [
                    "A. Vega 6.",
                    "B. Vega 7.",
                    "C. Vega 8.",
                    "D. Vega 9."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_109_real.mp4"
    },
    {
        "time": "[0:03:20 - 0:03:40]",
        "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:27]: A man is sitting at a white table, discussing two Ryzen processor boxes on the table in front of him. He is dressed in a white shirt, wearing glasses, and has a microphone attached to his shirt. The background consists of shelves with various tech items, including trophies, gadgets, and models, against a blue gradient wall. He gestures with his hands to emphasize his points, often pointing towards the Ryzen boxes. [0:03:27 - 0:03:32]: The video transitions to a series of graphics showing benchmarking results for different processors. The graph is titled 'TimeSpy \u56fe\u5f62\u5206' (TimeSpy Graphics Score). In the first graph, two processors, Ryzen 5700G (Radeon Vega 8) and Ryzen 5600G (Radeon Vega 7), are compared, with bars indicating their performance percentages. [0:03:32 - 0:03:33]: As the graph stays on the screen, it shows numerical scores: Ryzen 5700G at 1429 and Ryzen 5600G at 1312. The scores are represented by orange bars. [0:03:33 - 0:03:36]: The graph evolves to include another processor, i9-11900K (UHD 750 Integrated Graphics), which has a significantly lower score of 862. The bar for this processor is shown in blue, indicating a different color scheme for identification. [0:03:36 - 0:03:38]: The graph expands further and now also includes the i7-1165G7 (Iris Xe Graphics), with a score of 1656, depicted by a light blue bar. The comparisons indicate that the i7-1165G7 performs the best among the listed processors.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is the score of the i7-1165G7 (Iris Xe Graphics) processor right now?",
                "time_stamp": "00:03:38",
                "answer": "B",
                "options": [
                    "A. 862.",
                    "B. 1656.",
                    "C. 1312.",
                    "D. 1429."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_109_real.mp4"
    },
    {
        "time": "[0:06:40 - 0:07:00]",
        "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:45]: The scene begins with a first-person perspective, showing a snowy, narrow pathway lined with bare trees and bushes. The path leads up to an open gate in the distance, and the viewer holds a handgun in their right hand, which is partly bandaged. [0:06:46 - 0:06:48]: The pathway continues, and the viewer moves closer to the gate, which has a tall, dark, and ominous appearance. Snow covers the ground, and some footprints and tire marks can be seen on the path. [0:06:49 - 0:06:51]: As the viewer approaches the gate, the surrounding environment remains snowy with dark, leafless trees on the sides. The gate appears closer, showing more details, including some rust and snow accumulation. [0:06:52 - 0:06:54]: The viewer walks beneath the gate's arch and heads into a darker, confined area with walls at the sides. The pathway still shows snow, but the environment becomes gloomier. [0:06:55]: The scene changes abruptly to a menu screen with system performance statistics and graphical settings. Options like \"Display,\" \"Graphics,\" and \"FidelityFX Super Resolution 1.0\" are visible. A grayscale background image with a dark, shadowy figure and a hand is partially visible. [0:06:56]: The same menu screen is shown, but there are slight variations in the system performance statistics. [0:06:57]: The menu screen remains the same, but there are further changes in the system performance statistics. [0:06:58 - 0:06:59]: The system performance statistics continue to change, showing different power measurements, temperatures, and frame rates, while the rest of the menu screen remains the same. [0:07:00]: The scene switches back to the first-person perspective. The viewer now stands on a snowy pathway with rugged terrain ahead. There is a handgun in their right hand, and snow-covered bushes are visible along the traill.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What graphical setting is mentioned on the menu screen right now?",
                "time_stamp": "00:06:45",
                "answer": "D",
                "options": [
                    "A. Display.",
                    "B. Graphics.",
                    "C. Anti-Aliasing.",
                    "D. FidelityFX Super Resolution 1.0."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_109_real.mp4"
    },
    {
        "time": "[0:10:00 - 0:10:20]",
        "captions": "[0:10:00 - 0:10:20] [0:10:00 - 0:10:01]: The video begins with a detailed shot of a computer motherboard BIOS interface. It displays settings for overclocking, CPU configuration, and GPU settings. The options shown include \"Precision Boost Overdrive,\" \"Core Control,\" \"SMT Control,\" and more technical terms, with values shown such as 2500 MHz for \"GFX Clock Frequency\" and \"GFX Voltage\" set to 1.400V. [0:10:02]: This transition then reveals a man in a white shirt is sitting at a white table. He appears to be in a studio with a blue gradient background. Behind him are shelves with various items such as computer hardware and collectibles. Two AMD Ryzen processor boxes are positioned on the table in front of him. [0:10:03]: The man begins speaking, gesturing animatedly with his hands. He is seen holding up a finger as if to make a point or explain something while continuing his conversation directly to the camera. [0:10:04]: He then changes his gesture, spreading his hands apart. His focus remains on the audience, maintaining an engaging and informative demeanor. [0:10:05]: The man continues speaking, now bringing his hands closer together as he describes something, his expression showing enthusiasm. [0:10:06 - 0:10:07]: He shifts to a more relaxed stance, folding his hands together on the table as he continues to address the audience. His demeanor is friendly and informative. [0:10:08 - 0:10:09]: The man's gestures become more dynamic again, with him making a circular motion with his hands. His facial expression suggests he is explaining a point in more detail. [0:10:10]: He continues his narrative, occasionally glancing to the side while maintaining a conversational tone. His hands rest on the table near the Ryzen boxes. [0:10:11]: He looks directly at the camera and resumes speaking, potentially returning to a main point or summarizing the previous discussion. [0:10:12]: The man continues explaining, using his right hand to emphasize his points. The boxes labeled Ryzen 7 and Ryzen 5 remain prominently displayed on the table. [0:10:13]: He articulates a new point by bringing both hands closer to each other. His expression becomes more earnest as he addresses the camera. [0:10:14]: The next segment shows a close-up of a benchmark software interface on a computer screen. The software, 3DMark, shows performance metrics, including a \"Graphics score\" of 1,869 and a \"CPU score\" of 10,813.  [0:10:15 - 0:10:16]: The camera focuses on details of the 3DMark interface, highlighting the performance scores. The interface displays additional data such as estimated game performance and monitoring chart, along with a score of 21,133 under \"Time Spy.\" [0:10:17]: The camera zooms in on the \"Graphics score\" of 1,869 and the \"CPU score\" of 10,813, emphasizing these figures. Other details such as \"Estimated game performance\" and \"Monitoring\" charts are also visible but less emphasized in this shot. [0:10:18 - 0:10:20]: The focus remains on the same software interface with the performance metrics reiterated. The camera holds steady, ensuring the viewer notices the significant scores displayed for graphical and CPU performance.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What value was set for the \"GFX Voltage\" just now?",
                "time_stamp": "00:10:01",
                "answer": "D",
                "options": [
                    "A. 1.200V.",
                    "B. 1.500V.",
                    "C. 1.250V.",
                    "D. 1.400V."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_109_real.mp4"
    },
    {
        "time": "[0:12:20 - 0:12:26]",
        "captions": "[0:12:20 - 0:12:26] [0:12:20 - 0:12:23]: A person with short black hair and glasses is seen sitting at a white table in a room with a blue background. They are wearing a plain white T-shirt. In front of them on the table is a boxed CPU labeled \"AMD Ryzen 5.\" The room has shelves behind the person with various items, including gadgets and figurines. The person appears to be speaking and making gestures with their right hand. [0:12:23 - 0:12:26]: The screen transitions to a black background with white, glowing text and graphics, including a gear symbol and some Chinese characters. The glow effect and the gear symbol suggest a tech-related context. The text and graphics appear to remain static, highlighting the transition or a logo screen.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is the label on the boxed CPU seen right now?",
                "time_stamp": "0:12:23",
                "answer": "B",
                "options": [
                    "A. Intel Core i7.",
                    "B. AMD Ryzen 5.",
                    "C. NVIDIA GeForce GTX.",
                    "D. Intel Core i5."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_109_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a first-person perspective focused on a wooden surface. In the center, there is a white cotton canvas with the dimensions 28cm by 23cm (11 inches by 9 inches) written on it. To the right of the canvas is a paint palette with visible paint smudges at the edges. [0:00:01 - 0:00:04]: A hand appears holding a tube of black acrylic paint, which is squeezed onto the palette. Next, a tube of cerulean blue paint is also squeezed onto the palette. [0:00:04 - 0:00:06]: The hand squeezes white paint onto the palette, and the labeled colors \u201cBlack,\u201d \u201cCerulean Blue,\u201d and \u201cWhite\u201d appear above their respective blobs of paint. [0:00:07 - 0:00:10]: The canvas' labeled text disappears, leaving just the blank canvas. A 25mm flat brush comes into view, and the hand holding the brush dips it into the black paint, followed by the white paint, and then the cerulean blue paint on the palette. [0:00:10 - 0:00:17]: The brush is moved around, mixing the colors on the palette. The palette now has a mixture of black, white, and cerulean blue. [0:00:18 - 0:00:20]: The hand begins painting on the blank canvas with the mixed paint, starting from the top-left corner, applying the color with horizontal strokes.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the paint palette located relative to the canvas?",
                "time_stamp": "00:00:01",
                "answer": "B",
                "options": [
                    "A. To the left of the canvas.",
                    "B. To the right of the canvas.",
                    "C. Below the canvas.",
                    "D. Above the canvas."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_142_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:09]: In a close-up view, a hand holds a fine paintbrush and applies white paint to a curved, grey surface that occupies the center and right portion of the canvas. The background is dark. The brush initially makes contact near the top of the curved surface, creating a small white patch that gradually expands with gentle strokes. The hand consistently moves the brush in a controlled manner, adding delicate brushstrokes to create a subtle texture. [0:02:10 - 0:02:19]: The hand continues applying white paint, extending the white patch downward along the curved edge. The strokes become slightly longer, filling in more of the dark background area and gradually increasing the size of the white-painted section. The texture of the white area begins to resemble a pattern, possibly clouds or another soft element, with the brush making varied, small swirling motions as more paint is applied. The contrast between the white paint and the dark background becomes more pronounced.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the hand doing in the video?",
                "time_stamp": "00:02:19",
                "answer": "A",
                "options": [
                    "A. Applying white paint to a curved surface.",
                    "B. Drawing a straight line with a pencil.",
                    "C. Cleaning the paintbrush.",
                    "D. Mixing colors on a palette."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_142_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:03]: The video begins with a close-up of an artist's hand holding a paintbrush, which moves over a circular, cratered surface resembling the moon. The moon is painted in shades of white and gray, occupying the left and lower side of the canvas, while the background is a dark gradient, giving a space-like feel. On the right, there's a palette with various colors; light and dark blue paint is visible. [0:04:04 - 0:04:09]: The brush continues to paint subtle details on the moon's surface. The canvas is centered, and the palette, labeled \"Liner Brush,\" is on the right. The palette exhibits different colors, including white and dark blue. The artist's hand picks up light blue paint and mixes it on the palette. [0:04:10 - 0:04:11]: The artist's hand holds the paintbrush and dips it into the light blue paint on the palette before continuing to paint on the canvas. [0:04:12 - 0:04:19]: The brush moves meticulously, adding fine details to the surface. The artist switches between adding fine lines and small patches of white, incorporating intricate textures that mimic the moon\u2019s craters and surface irregularities. The hand's movement shows careful attention to enhancing the moon's realistic appearance.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the artist doing right now",
                "time_stamp": "00:04:11",
                "answer": "B",
                "options": [
                    "A. Mixing colors on the palette.",
                    "B. Applying paint to the moon's surface.",
                    "C. Cleaning the paintbrush.",
                    "D. Adjusting the canvas."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_142_real.mp4"
    },
    {
        "time": "[0:06:00 - 0:07:00]",
        "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:06]: The video shows the detailed process of painting, starting with a hand holding a brush and painting black vertical lines of varying heights on a canvas. The canvas background features a detailed moon illustration, depicting craters and texture against a dark sky. The hand making the lines is steady and precise.  [0:06:07 - 0:06:12]: After finishing the lines, the video shifts to show an artist's palette on the right side of the frame. The palette holds multiple blobs of paint in different colors, including black, white, and shades of blue. The hand picks up a larger paintbrush and mixes some of the black paint.  [0:06:13 - 0:06:14]: The video then focuses on the canvas again, where the artist begins adding foliage to one of the vertical lines using the larger paintbrush dipped in black paint. [0:06:15 - 0:06:19]: The artist continues adding detail to the tree, creating a realistic texture by dabbing and dragging the brush. The tree's appearance becomes more defined, and the brushstrokes suggest individual branches. [0:06:20]: The video likely continues with the same detailed painting process, focusing on adding more details to the trees and the overall composition.",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why does the artist pick up a larger paintbrush after finishing the lines?",
                "time_stamp": "00:06:21",
                "answer": "A",
                "options": [
                    "A. To adds foliage details.",
                    "B. To clean the previous paintbrush.",
                    "C. To start a new painting.",
                    "D. To add a new color to the palette."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following best summarizes the artist's actions just shown in the video",
                "time_stamp": "00:06:20",
                "answer": "C",
                "options": [
                    "A. The artist sketches a rough outline of a landscape.",
                    "B. The artist adds fine details to a previously sketched portrait.",
                    "C. The artist paints vertical lines, mixes paint, and adds foliage details to the canvas.",
                    "D. The artist starts a new painting with a fresh palette and blank canvas."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_142_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: A blank white canvas is lying flat on a wooden surface. The wood has a light brown hue with a visible grain pattern running horizontally. [0:00:02 - 0:00:04]: The name \"CorreaArt\" appears in black handwritten text in the center of the white canvas. [0:00:05]: A translucent plastic sheet is being placed over the canvas, partially obscuring the text \"CorreaArt.\" [0:00:06 - 0:00:10]: The plastic sheet is now fully covering the canvas. The text \"CorreaArt\" remains visible through the plastic. The sheet has a few scattered paint stains of various colors. [0:00:11 - 0:00:12]: A hand holding a blue tube of paint appears from the top left and begins to dispense paint onto the plastic sheet. [0:00:13]: The hand is still dispensing the blue paint onto the top section of the plastic sheet. [0:00:14]: The hand appears again, now holding a green tube of paint, and starts dispensing green paint onto the sheet below the blue paint. [0:00:15]: The same hand now holds a red paint tube and dispenses red paint onto the sheet below the green paint. [0:00:16]: The hand holds a yellow tube of paint, dispensing yellow paint onto the sheet below the red paint. [0:00:17]: The hand, now holding a black tube of paint, is about to dispense paint onto the sheet. [0:00:18 - 0:00:19]: White paint is being dispensed onto the bottom section of the plastic sheet by the hand holding a white paint tube.",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following best describes the initial actions shown in the clip?",
                "time_stamp": "00:00:12",
                "answer": "D",
                "options": [
                    "A. A hand is painting directly on a canvas.",
                    "B. A plastic sheet is covered with paint stains, then a name appears.",
                    "C. A blank canvas is being prepared for painting.",
                    "D. Paint is being dispensed onto a plastic sheet covering a canvas."
                ],
                "required_ability": "episodic memory"
            },
            {
                "task_type": "Prospective Reasoning",
                "question": "What is likely the next step after dispensing all the paint colors onto the plastic sheet?",
                "time_stamp": "00:00:19",
                "answer": "B",
                "options": [
                    "A. The hand will spread the paint across the sheet.",
                    "B. The plastic sheet will be removed from the canvas.",
                    "C. Additional colors will be added.",
                    "D. The text \"CorreaArt\" will be erased."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_137_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:01]: The video shows an artist's hand holding a paintbrush and approaching a canvas. The canvas is already prepared with a gradient sky background transitioning from a light blue at the top to a warm orange near the horizon. The artist adds dark purple cloud shapes in the sky. Below the sky, the canvas features lightly sketched outlines indicating future landscape elements.  [0:05:01 - 0:05:08]: The artist continues painting the dark purple clouds in a sweeping motion across the top of the sky. On a wooden table to the right of the canvas, there are various squeezed paint tubes, including colors like red, yellow, green, blue, and beige mixed on a palette. The artist\u2019s movements are deliberate, filling in the outlines with the dark purple color. [0:05:09 - 0:05:15]: The artist focuses on building up the contour of the clouds, adding and spreading the dark purple paint to create more defined and fuller cloud shapes. The brush strokes are varied, suggesting a textured and dynamic sky. [0:05:15 - 0:05:20]: The lower portion of the canvas remains untouched, with only the initial sketches visible. The artist continues to enhance the sky, adding more clouds and shaping the existing ones to create a dramatic effect. The background objects stay in place, while the artist's attention remains on the upper part of the canvas.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where are the various squeezed paint tubes located in relation to the canvas?",
                "time_stamp": "0:05:08",
                "answer": "D",
                "options": [
                    "A. To the left of the canvas.",
                    "B. On a shelf above the canvas.",
                    "C. Below the canvas on the floor.",
                    "D. On a wooden table to the right of the canvas."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_137_real.mp4"
    },
    {
        "time": "[0:10:00 - 0:11:00]",
        "captions": "[0:10:00 - 0:10:20] [0:10:00 - 0:10:01]: A canvas painting of a serene beach with clear blue water and a colorful sunset sky is placed on the left side of the frame. A palette with a variety of paint colors, including red, yellow, green, blue, and several shades of mixed colors, is visible on the right side. A person's finger is seen at the bottom center, holding the palette steady. [0:10:02 - 0:10:06]: The person begins to mix different paint colors together with a palette knife, primarily working on blending the white, blue, and red paints. The mixed paint has a range of hues, transitioning from dark blue to a purplish color. The palette knife moves back and forth, creating swirls and patterns in the paint. [0:10:07 - 0:10:09]: The person continues to mix the paints, adding more blue and white to the mixture, creating a lighter, more varied shade of purplish-blue. The palette knife continues to move in circular and sweeping motions. [0:10:10 - 0:10:14]: The painter further mixes the paints, incorporating more red into the mixture, resulting in a deeper and richer purple color. The palette knife is mostly focused on the center region of the paint mix. [0:10:15 - 0:10:19]: The painter adds white to the mixture, lightening the color once again. The process involves careful blending to ensure the hues are well-integrated, resulting in a more complex and nuanced range of purples. The palette knife's movements are precise and controlled, indicating a methodical approach to achieving the desired color. The entire scene is set against a wooden texture background.",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why does the paint mixture become deeper and richer in purple color right now?",
                "time_stamp": "00:10:16",
                "answer": "B",
                "options": [
                    "A. Because more orange paint was added.",
                    "B. Because more red and blue paint was added.",
                    "C. Because purple paint was added.",
                    "D. Because the palette knife was cleaned."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_137_real.mp4"
    },
    {
        "time": "[0:15:00 - 0:16:00]",
        "captions": "[0:15:00 - 0:15:20] [0:15:00 - 0:15:04]: A painting of a beach sunset is visible to the left of the frame. It depicts a vibrant sunset sky with hues of purple, pink, and orange, a calm ocean with gentle waves, and sandy shores. To the right, there is a palette containing various colors of paint, organized in blobs. The colors range from reds and yellows at the bottom to blues, greens, and purples towards the top. A paint knife is seen close to the palette, mixing yellow paint with white paint. [0:15:05 - 0:15:09]: The paint knife picks up the mixed paint and moves towards the painting. As it reaches the canvas, it adds more paint to the waves, refining the texture and enhancing the highlights. The colors on the palette appear slightly more mixed, suggesting ongoing blending of shades. The beach painting now shows enhanced details in the wave. [0:15:10 - 0:15:15]: A brush returns to the palette, picking another color before further detailing the artwork. The paint knife continues to add and refine textures on the waves, capturing more realistic water movement and reflections. The bright colored sunset contrasts strikingly against the blue-green tones of the waves. [0:15:16 - 0:15:20]: The painter continues to apply details to the crashing waves, emphasizing white caps and the movement of water. The palette's colors now show signs of multiple mixes, and the painting gains more life with each brushstroke, bringing out the vibrancy of the sunset and the dynamic atmosphere of the sea waves.",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following best summarizes the artistic process just shown in the clip?",
                "time_stamp": "00:15:20",
                "answer": "D",
                "options": [
                    "A. A painter adding final touches to a portrait.",
                    "B. A painter cleaning their tools after finishing a painting.",
                    "C. A painter starting a new abstract piece.",
                    "D. A painter refining the details of ocean waves."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_137_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:09]: The video starts with a display of artwork on a gallery wall. There are three black-framed, rectangular pieces on the left, positioned one above the other. The top two frames feature abstract designs with silver and red elements. On the right side, there are four canvases with black brushstroke designs. Below the frames, there are two QR codes mounted on the wall. As the video progresses, the camera starts moving to the right, revealing more of the gallery space and faintly displaying other visitors and artworks in the background. [0:00:10 - 0:00:19]: The camera continues to pan right, transitioning into another section of the gallery. Here, a wide view of the gallery space is visible, showcasing various artworks on display on both sides. The perspective shifts, focusing on a wall with a collection of framed pictures arranged in different shapes and sizes, all featuring a monochromatic theme. As the camera moves forward, it captures a detailed view of each piece, ending on the highest row of artworks that include both rectangular and circular frames. The video concludes with a frontal shot of these pieces.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What happens as the camera starts moving to the left?",
                "time_stamp": "00:00:13",
                "answer": "A",
                "options": [
                    "A. It reveals more of the gallery space and visitors.",
                    "B. It zooms in on the top frame.",
                    "C. It transitions to a close-up of a single artwork.",
                    "D. It focuses on a sculpture in the middle."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "art_show",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_470_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:23]: The video begins by showcasing three artworks hanging on a white wall. Each artwork consists of cartoon figures in dynamic poses set against a gold background. The figures are simplified with white and other colors, appearing in action poses such as running and jumping. Under each artwork is a small label.   [0:02:24 - 0:02:26]: The camera slowly moves to the left, revealing another artwork on the wall. This artwork is colorful and abstract, depicting a variety of floral shapes and elements with vibrant hues such as red, purple, green, and blue. The paint appears to be applied in thick, three-dimensional layers, creating a textured effect. [0:02:27 - 0:02:30]: Attention is focused entirely on the newly revealed abstract artwork, with the camera capturing it from different angles. The painting continues to grow in complexity, featuring a mix of colors and shapes that overlap and intertwine. [0:02:31 - 0:02:33]: The camera reveals another artwork next to the previous one. This new one is on a blue background with an abstract tree consisting of multi-colored, swirling paint, resembling leaves and branches. The colors are vibrant, creating a stark contrast with the solid blue background. [0:02:34 - 0:02:36]: The camera continues to move left, revealing more of the wall. There is an artwork similar to the second abstract painting but with a different color palette and another unique, abstract tree. This piece has more serene undertones with a mix of pastel colors against a gray background. [0:02:37 - 0:02:39]: The video continues to explore the last artwork in this sequence. It remains focused on intricate details, showcasing the thick, textured paint application, and vibrant color combinations. The camera finally pulls away, giving a broader perspective of the art on display.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is depicted in the three artworks shown in the video right now?",
                "time_stamp": "0:02:23",
                "answer": "B",
                "options": [
                    "A. Abstract floral shapes.",
                    "B. Cartoon figures in dynamic poses.",
                    "C. Realistic portraits of people.",
                    "D. Landscapes with mountains."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Spatial Understanding",
                "question": "What is the background color of the artwork shown right now?",
                "time_stamp": "0:02:33",
                "answer": "C",
                "options": [
                    "A. Red.",
                    "B. Green.",
                    "C. Blue.",
                    "D. Yellow."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "art_show",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_470_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:44]: The video starts with views of an art gallery wall displaying several framed paintings. On the left side, there are two small square paintings aligned vertically next to a larger rectangular painting with an oceanic scene, showing waves and clouds, along with abstract forms in the sky. These paintings are framed in a metallic color. [0:04:45 - 0:04:48]: The movement continues, focusing closely on the large painting with the oceanic scene. The camera then slightly shifts to the right, gradually moving past the paintings. [0:04:49 - 0:04:52]: The camera moves around a corner to reveal another section of the gallery. There are people milling around and examining the art. The perspective then shifts to a new wall displaying several small framed abstract paintings, each featuring a circular form against a colorful background. [0:04:53 - 0:04:58]: The camera continues to showcase the series of abstract paintings, revealing a grid of nine similar paintings on this section of the gallery wall. The artworks are symmetrical and neatly organized. [0:04:59]: The video ends with the camera capturing a perpendicular view of the nine abstract paintings, focusing on the intricate details and color variations of the circular forms within them.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What kind of forms do the small framed abstract paintings shown right now feature?",
                "time_stamp": "00:04:58",
                "answer": "C",
                "options": [
                    "A. Rectangular forms.",
                    "B. Triangular forms.",
                    "C. Circular forms.",
                    "D. Star-shaped forms."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "art_show",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_470_real.mp4"
    },
    {
        "time": "[0:07:00 - 0:08:00]",
        "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:03]: The scene starts with a blue table. On the table, a square-shaped, colorful art piece with numerous small squares in a grid pattern is seen in the center. This grid pattern features various vibrant colors and small images inside each square. To the left, two letters 'L' and 'P' are mounted on the blue wall, and to the right, a painting depicts a city scene with cars and tall buildings under a partly cloudy sky. [0:07:04 - 0:07:11]: The camera then focuses and zooms in on a three-part portrait that appears to be a stylized or pixelated representation of a person's face, seen from different angles and emotions. The main color tones are red, black, and beige. The camera captures the three sections of the face sequentially, starting from the top middle area comprising the eyes and lips, then the left segment, capturing each part of the face in more detail until it reaches the lips. [0:07:12 - 0:07:16]: The focus continues close-up on the pixelated art, emphasizing the eyes and the red lips composed of small, circular elements. Each of these small elements has alphanumeric characters on them, indicating the intricate details used to create the portrait.  [0:07:17 - 0:07:19]: The camera then zooms out slightly, showing parts of the face near the lips and slowly moving downwards to showcase the lower portion of the portrait. The texture and details of the small elements forming the portrait are visible, showing a mix of red, black, and beige colors arranged to portray the image. [0:07:20]: The final segment extends outward to reveal the bottom of the framed art piece, where another artwork or object partially comes into view. The overall portrayal includes the varied and detailed close-ups of the pixel-art style portrait and other pieces around the blue tabletop.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What letters are mounted on the blue wall shown just now?",
                "time_stamp": "00:07:10",
                "answer": "B",
                "options": [
                    "A. L and T.",
                    "B. L and P.",
                    "C. P and T.",
                    "D. L and M."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "art_show",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_470_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: In the video, a first-person perspective reveals an art gallery space characterized by white walls and high ceilings. In the background, multiple works of art are displayed, including a sculpture of two seated, nude, mannequin-like figures wearing white headgear. Near the center and to the right of the frame, a large, glossy red gorilla sculpture is visible. This sculpture appears to be made of a shiny, reflective material and stands on a white pedestal. [0:00:03 - 0:00:08]: Next to the red gorilla is another sculpture of a blue gorilla holding a cylindrical object labelled \"USA.\" This sculpture, also on a white pedestal, features a patriotic theme with red, blue, and white colors, as well as stars and various decorative elements. As the viewer progresses further, another colorful gorilla sculpture comes into view, adorned with vibrant and eclectic patterns in shades of purple, green, pink, and yellow. [0:00:08 - 0:00:12]: The last two sculptures observed are part of the same series as the gorillas, displaying their bold and diverse colors. The colorful gorilla is positioned in front of the white wall, separating them from the rest of the exhibition. Near the end of this segment, a man in a gray jacket and a white cap appears on the right, walking towards the sculptures. [0:00:12 - 0:00:17]: The video continues to show the detailed designs on the sculptures, highlighting their polished finishes. The observer pans to the right, capturing more of the gallery space. The walls display other artworks, including a brightly painted surfboard on the left side of the frame. A more prolonged look at the gorilla sculptures stands out. [0:00:17 - 0:00:19]: As the viewer moves further into the gallery, the surrounding areas become more visible, including another room in the background where additional visitors are present. On the right side, a child in green clothing and a woman pushing a stroller can be seen approaching the gorilla sculptures. The background includes an additional artwork, likely a painting or photograph of a person's back, reinforcing the gallery's diverse exhibits.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What object is the blue gorilla holding?",
                "time_stamp": "00:00:08",
                "answer": "D",
                "options": [
                    "A. A banana.",
                    "B. A small sculpture.",
                    "C. A book.",
                    "D. A cylindrical object labeled \"USA\"."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "art_show",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_466_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:44]: The video begins with a clear view of a white wall displaying text about an exhibition titled \"RAINTYPE\" by Lorenzo Marini at the Bruce Lurie Gallery, curated by Luca Beatrice. The text provides an overview of the installation, describing it as inspired by a rainy day, with letters gaining meaning when combined with others in the immersive environment. The background shows other parts of the gallery with artworks visible on the walls. [0:02:45 - 0:02:46]: The camera begins to pan to the right, revealing more of the gallery space. A few visitors are seen, some standing and looking at exhibits. The text on the wall starts to move out of the frame's center. [0:02:47 - 0:02:50]: The view opens up to a larger section of the gallery. There is a significant installation in the center, consisting of numerous small, colorful items hanging from the ceiling, reflecting light and creating a visually complex and immersive scene. The floor is polished, reflecting the hanging installation and visitors. People are walking around, observing the artwork. [0:02:51 - 0:02:56]: The camera continues to capture the expansive installation. Visitors of various ages and appearances move around, some taking photos and others interacting with the space. The hanging objects are densely packed, creating a mosaic-like effect with reflections on the floor. [0:02:57 - 0:02:59]: The camera maintains its focus on the central installation. The space is well-lit, with bright and colorful details from the hanging pieces standing out. Visitors are engaged, some taking pictures or closely examining parts of the installation. The pattern of hanging items persists, enhancing the immersive nature of the exhibition.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is the title of the text written on this wall?",
                "time_stamp": "00:02:44",
                "answer": "A",
                "options": [
                    "A. \"RAINTYPE LORENZO MARINI\".",
                    "B. \"SUNSHINE\".",
                    "C. \"LIFE IN COLOR\".",
                    "D. \"NIGHT LIGHTS\"."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Object Recognition",
                "question": "What significant installation is revealed in the center of the gallery right now?",
                "time_stamp": "00:02:51",
                "answer": "D",
                "options": [
                    "A. A large sculpture in the center.",
                    "B. A collection of paintings on the walls.",
                    "C. A video projection.",
                    "D. Numerous small, colorful items hanging from the ceiling."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "art_show",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_466_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:22]: The video begins with a view of a wall displaying five framed artworks. The background wall is white. The artworks vary in size and are rectangular in shape, featuring abstract designs with different colors. The frames are arranged in two horizontal rows. The largest frame on the right contains a composition of green and yellow hues, while the leftmost artwork combines red, beige, and gray tones. [0:05:23 - 0:05:24]: The camera moves slightly to the left, revealing more of the exhibit. More framed artworks come into view. A group of six smaller, black-and-white photographs are visible, arranged in a 3x2 grid. Each photograph is individually framed with a black border and has a white matting. [0:05:25 - 0:05:26]: The view continues to focus on the grid of six photographs. Each photograph appears to depict architectural elements. Small labels are positioned to the right of each frame, likely providing details about the artwork. [0:05:27 - 0:05:29]: Adjacent to the grid of photographs, there are more framed pieces. To the left of the grid, there are four smaller square frames, arranged in a vertical column, and each containing abstract compositions. The frames are dark and offset by a light background. [0:05:30 - 0:05:33]: As the camera continues to pan, the focus moves to the artwork on the left. Below the four smaller frames is an empty white wall space. To the left, there is another artwork featuring a large white area with vivid splashes of color, including yellow, red, pink, black, and blue. The painting captures attention with its dynamic and vibrant appearance. [0:05:34 - 0:05:36]: The central part of the colorful painting on the left becomes more visible. It features a yellow splash that transitions into various other colors, giving it an energetic and lively feel. Drippings and brush strokes are evident, enhancing the sense of movement. [0:05:37 - 0:05:39]: The final frames focus entirely on the colorful painting. The camera settles, providing a clear and centered view of the large, vibrant artwork. The painting stands out against the white wall background, highlighting the contrast and expressive use of color.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What appears below the four smaller frames as the camera pans to the left?",
                "time_stamp": "00:05:40",
                "answer": "B",
                "options": [
                    "A. Another set of photographs.",
                    "B. An empty white wall space.",
                    "C. A sculpture.",
                    "D. A window."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "art_show",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_466_real.mp4"
    },
    {
        "time": "[0:08:00 - 0:09:00]",
        "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: The scene depicts an art gallery with white walls. On the left side, a complex sculpture is mounted on the wall. Several people, including a man in a dark blazer and blue jeans, are walking around. [0:08:01 - 0:08:04]: The camera moves forward, capturing more of the gallery space. The focus shifts to a white wall displaying artworks. A group of people, including a woman in a magenta jacket, stands in front of the exhibits. A television screen mounted on the wall displays an image of an older man with glasses, identified by the text \"LARRY KAGAN.\" [0:08:04 - 0:08:09]: The camera centers on the television screen showing sequences of a man working with metal objects, identified as Larry Kagan. Additional text above the screen reads \"THOMAS PAUL FINE ART presents LARRY KAGAN.\" [0:08:09 - 0:08:19]: The television continues to display various clips of Larry Kagan working on his sculpture. It shows close-up views of his hands manipulating metal and different stages of his art process. The camera remains focused on the TV screen, which occupies most of the frame. The gallery visitors are visible in the background, subtly illuminated by the light from the screen.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What is the man doing in the sequences shown on the television screen?",
                "time_stamp": "00:08:10",
                "answer": "C",
                "options": [
                    "A. Painting.",
                    "B. Drawing.",
                    "C. Working with metal objects.",
                    "D. Writing."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "art_show",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_466_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:05]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What type of weather condition is present right now?",
                "time_stamp": "00:01:35",
                "answer": "A",
                "options": [
                    "A. Sunny.",
                    "B. Stormy.",
                    "C. Foggy.",
                    "D. Rainy."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_460_real.mp4"
    },
    {
        "time": "[0:01:56 - 0:02:01]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the current color of the sky?",
                "time_stamp": "00:01:59",
                "answer": "A",
                "options": [
                    "A. Orange.",
                    "B. Blue.",
                    "C. Grey.",
                    "D. Pink."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_460_real.mp4"
    },
    {
        "time": "[0:03:52 - 0:03:57]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the color of the traffic light right now?",
                "time_stamp": "00:06:02",
                "answer": "A",
                "options": [
                    "A. Red.",
                    "B. Green.",
                    "C. Yellow.",
                    "D. Blue."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_460_real.mp4"
    },
    {
        "time": "[0:05:48 - 0:05:53]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is the number displayed on the electronic screen of the bus in front?",
                "time_stamp": "00:05:50",
                "answer": "A",
                "options": [
                    "A. 590.",
                    "B. 591.",
                    "C. 1314.",
                    "D. G1314."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_460_real.mp4"
    },
    {
        "time": "[0:07:44 - 0:07:49]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Which supermarket logo is visible on the right side right now?",
                "time_stamp": "00:07:46",
                "answer": "D",
                "options": [
                    "A. Aldi.",
                    "B. Woolworths.",
                    "C. IGA.",
                    "D. Coles."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_460_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:04]: In a room with a grayish-blue wall, a person's hand is adjusting the position of a gray metal easel on a wooden table. The person is wearing a gray long-sleeved shirt. To the left of the easel, there is a plant with broad green leaves in a blue and white striped pot, with a small spotlight illuminating it from below. There is also a large circular object on the table behind the easel. [0:00:05 - 0:00:09]: The person places a piece of white paper with a pencil sketch of a human face onto the easel. The sketch appears to be a portrait of a woman with long hair. The person uses both hands to align the paper straight. [0:00:10 - 0:00:12]: The paper with the portrait sketch is now centered and secured on the easel. The person makes small adjustments by touching the paper. [0:00:13 - 0:00:14]: The person runs their hand over the paper, possibly to smooth it out or check if it is firmly attached. [0:00:15 - 0:00:19]: The camera focuses on the portrait sketch on the easel. A paintbrush with a cloth wrapped around it is held next to the paper, possibly used for shading or blending previous marks. The plant and the same desk clutter are still visible in the background.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What does the person do immediately after aligning the paper on the easel?",
                "time_stamp": "0:00:19",
                "answer": "A",
                "options": [
                    "A. Makes small adjustments by touching the paper.",
                    "B. Places a new sketch on the easel.",
                    "C. Moves the easel to a different spot.",
                    "D. Changes the spotlight angle."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_132_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:19]: An artist's hand, holding a paintbrush, is actively working on a portrait of a woman drawn on a white canvas. The portrait is characterized by its realistic style, focusing on the woman\u2019s face, which has blue eyes, a slightly open mouth, and detailed features. The artist is applying light brown and peach tones to the neck area. The background is a neutral gray wall, with a potted plant on the left side and various art supplies visible on the right. The brush movements are meticulous, adding shading and depth to the neck of the female subject. The hand moves back and forth, carefully blending the colors to achieve a smooth gradient. The process emphasizes the gradual build-up of layers to enhance the three-dimensional effect of the portrait.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the potted plant located in relation to the portrait?",
                "time_stamp": "00:04:20",
                "answer": "C",
                "options": [
                    "A. On the right side.",
                    "B. Behind the canvas.",
                    "C. On the left side.",
                    "D. Under the table."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following best summarizes the artist's activity just shown in the video?",
                "time_stamp": "00:04:20",
                "answer": "B",
                "options": [
                    "A. Creating a landscape painting.",
                    "B. Working on a realistic portrait of a woman, focusing on the neck area.",
                    "C. Sketching a preliminary outline for a new artwork.",
                    "D. Mixing colors on a palette for future use."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_132_real.mp4"
    },
    {
        "time": "[0:12:00 - 0:13:00]",
        "captions": "[0:12:00 - 0:12:20] [0:12:00 - 0:12:20]: The video showcases the process of painting a portrait on a canvas. The hand of the painter, holding a brush, is actively engaged in the painting process, moving across the canvas to apply and blend the colors. The painting depicts a woman\u2019s face with blonde hair and realistic details, particularly focusing on her bluish eyes, red lips, and subtle shading to reflect depth and dimension. The artist uses a variety of brush strokes, from broad to careful detailing, to refine the image. In the background, there are several objects, including a potted plant with green leaves on the left side and bird decorations on a shelf to the right. The environment appears to be a well-organized and creative workspace, with the canvas and painting tools positioned centrally and the focus mainly on the fine details being added to the portrait. The background colors are muted, featuring gray walls, directing the attention towards the vibrant colors of the painting itself.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where are the bird decorations located in the workspace?",
                "time_stamp": "00:12:20",
                "answer": "C",
                "options": [
                    "A. On the left side.",
                    "B. In front of the canvas.",
                    "C. On a shelf to the right.",
                    "D. Next to the potted plant."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following best summarizes the painting process just shown in the video?",
                "time_stamp": "00:12450",
                "answer": "C",
                "options": [
                    "A. The artist is organizing painting tools in a vibrant studio.",
                    "B. The painter is applying broad strokes to a landscape painting.",
                    "C. The video captures the detailed process of painting background of a portrait with various brush strokes.",
                    "D. The artist is blending colors on the palette while preparing to paint."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_132_real.mp4"
    },
    {
        "time": "0:00:00 - 0:00:20",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:06]: In this video, a virtual inventory screen is displayed, showing different compartments labeled as \"Player,\" \"Cargo,\" \"Backpack,\" and \"Ground.\" Various items like documents, boxes, and pieces of paper appear in these compartments. On the left side of the screen, a character's profile with personal information is visible, featuring sections like \"ID,\" \"Age,\" and \"Skills.\" [0:00:07 - 0:00:13]: The viewpoint swaps from the virtual inventory interface to a first-person perspective scene. The person appears to be carrying a large cardboard box and stands next to a large vehicle, possibly a truck, with its back door open. [0:00:14 - 0:00:17]: As they walk while holding the box, a pathway and a building with red and white walls are visible in the background. Another person dressed in a delivery uniform becomes visible near the door. [0:00:18 - 0:00:20]: The individual carrying the box approaches the person in the delivery uniform and stops next to them. They seem to be interacting, and the delivery person holds a tablet or a similar device. The screen then switches back briefly to the virtual inventory interface. [0:00:21 - 0:00:23]: The inventory screen reveals details about the person being interacted with (\"Harry Miller - Trucking License\"). Various items are moved between different compartments like \"Trucking Cargo\" and \"Backpack,\" indicating organization or preparation for delivery.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the person doing right now?",
                "time_stamp": "00:00:18",
                "answer": "D",
                "options": [
                    "A. Organizing items in the virtual inventory.",
                    "B. Carrying a large cardboard box.",
                    "C. Walking towards the building.",
                    "D. Interacting with the delivery person and delivering the goods."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_288_real.mp4"
    },
    {
        "time": "0:02:40 - 0:03:00",
        "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:44]: The video showcases a first-person perspective of driving on a multi-lane road, with a large bridge visible in the background. Vehicles on the road include a white car on the right and another vehicle further ahead. A man is visible in an inset video at the top left corner, appearing to be the person controlling the vehicle. [0:02:45 - 0:02:51]: The vehicle continues to move forward, passing more parked and moving cars on the right side, including some utility poles and trees along the sidewalk. The bridge in the background becomes more prominent as the vehicle approaches. [0:02:52 - 0:02:54]: The road slightly curves to the right, and the driver approaches an intersection. The bridge is now closer in view, and several more vehicles can be seen on the road, including another parked car on the right side. [0:02:55 - 0:02:58]: The driver appears to slow down as they approach the intersection. Traffic lights and street signs become visible. The bridge now looms overhead, and there are several vehicles around, including a large truck on the left. [0:02:59 - 0:03:04]: The driver continues forward, passing beneath the bridge structure. Palm trees are noticeable on the sidewalk, and a large sign with advertisements is visible on a street lamp to the right. [0:03:05 - 0:03:09]: The vehicle proceeds straight toward another intersection. Traffic lights are visible, and several vehicles, including a blue car, are seen moving ahead. The road looks clear, with visible greenery and industrial buildings adjacent. [0:03:10 - 0:03:12]: The driver proceeds through the intersection under the bridge. There are more palm trees and street signs in view. The road continues to a slight incline, heading towards what seems like a shipping port area. [0:03:13 - 0:03:17]: The vehicle drives on an elevated section of the road, with a large body of water and ships visible to the left. The road ahead appears clear, with minimal traffic, and the inset of the man is still present in the top left corner. [0:03:18 - 0:03:21]: Continuing forward, the surrounding area shows more industrial buildings and infrastructure. The waterway and shipping containers are visible on the left. Vehicles, including a van, are seen traveling in the opposite direction. [0:03:22 - 0:03:24]: The vehicle drives along the elevated road, passing more industrial elements and signs indicating the direction of traffic. The road appears to be in good condition, with clear markings. [0:03:25 - 0:03:27]: The vehicle continues towards a more open stretch of the road, with palm trees lining the center divider and both sides of the street. The weather is clear, indicating daytime. [0:03:28 - 0:03:32]: The vehicle progresses further, and more commercial buildings and infrastructure elements become visible. The road remains straight and well-maintained with minimal traffic in sight. [0:03:33 - 0:03:35]: The driver continues on the straight road, passing trees and commercial structures on the right. A sense of progression is visible as the scene reveals more of the urban environment. [0:03:36 - 0:03:40]: Movement is steady forward, showcasing an expansive road with clear skies and orderly vehicle flow. The surrounding environment indicates an industrial area transitioning to commercial zones.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the driver doing right now?",
                "time_stamp": "0:02:40",
                "answer": "C",
                "options": [
                    "A. Turning left at an intersection.",
                    "B. Slowing down to stop at a traffic light.",
                    "C. Driving on a straight road.",
                    "D. Parking the vehicle on the side of the road."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_288_real.mp4"
    },
    {
        "time": "0:05:20 - 0:05:40",
        "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:26]: The video shows a large, grey delivery truck parking next to a white car in an industrial area. Barriers and other vehicles are seen nearby.  [0:05:27 - 0:05:30]: The delivery truck finishes parking and a person wearing a light green safety vest gets out from the driver's side, walking towards the back of the truck.  [0:05:31 - 0:05:34]: The individual, who appears to be a delivery person, continues walking around the truck. The background features other vehicles, some industrial equipment, and a large building.  [0:05:35 - 0:05:38]: The delivery person starts running towards a group of people and a building. There is a mix of standing and walking individuals in various outfits, indicating an active work area.  [0:05:39 - 0:05:42]: The delivery person continues running towards a man in a white shirt leaning against a brick wall, with some debris and scattered items on the ground nearby.  [0:05:43 - 0:05:46]: The person in the safety vest stops in front of the man in the white shirt, appearing to initiate a conversation.  [0:05:47 - 0:05:50]: A close-up view shows the man in the white shirt, who is wearing a blue wristwatch and holding some papers.  [0:05:51 - 0:05:55]: The man in the white shirt, identified as \"Frank Miller\" through an overlay, continues the conversation. Different options appear on the screen, indicating possible interactions.  [0:05:56 - 0:06:01]: The focus remains on Frank as the conversation progresses, with the overlay showing the player's dialogue choices.  [0:06:02 - 0:06:06]: A close-up of the character \"Frank Miller\" shows his detailed facial features and expression.  [0:06:07 - 0:06:09]: The view shifts back to the delivery person in the safety vest, turning around and walking away from Frank.  [0:06:10 - 0:06:13]: The delivery person heads towards the delivery truck called \"grime,\" which is parked in the same area as before.  [0:06:14 - 0:06:16]: The delivery person walks towards the back of the truck as some individuals stand around talking near the truck.  [0:06:17 - 0:06:20]: The delivery person approaches the rear door of the truck, triggering an interaction option. The characters and other environmental details remain visible in the background.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the delivery person doing right now?",
                "time_stamp": "00:06:20",
                "answer": "B",
                "options": [
                    "A. Walking away from Frank.",
                    "B. Run towards the harvester in white clothes.",
                    "C. Engaging in a conversation with Frank.",
                    "D. Running towards the building."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_288_real.mp4"
    },
    {
        "time": "0:08:00 - 0:08:20",
        "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:05]: The video opens with a first-person view of a character wearing a reflective safety vest standing next to a gray van with the word \"grime\" written in light blue on its side. The character is facing the van and there is an interaction menu present. [0:08:06 - 0:08:11]: The character continues to stand stationary next to the van, with the interaction menu in view. Small red objects are scattered on the ground near the van. [0:08:12 - 0:08:14]: The interaction menu remains in view as the character continues to face the van. The background shows an industrial area with another van visible in the distance. [0:08:15 - 0:08:19]: The camera angle remains consistent, maintaining focus on the character and the van. The interaction menu options show \"Wire large\" and \"Deploy trip.\" [0:08:20]: The character steps back and turns slightly away from the van, revealing more of the surrounding industrial environment. Multiple vehicles and other characters are visible in the distance, with a light blue van standing out.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What are the three people in the video doing now?",
                "time_stamp": "00:08:20",
                "answer": "A",
                "options": [
                    "A. Standing quietly.",
                    "B. Deployed a tripwire next to the van.",
                    "C. Talking to each other.",
                    "D. Interacted with another character in the background."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_288_real.mp4"
    },
    {
        "time": "0:10:20 - 0:10:39",
        "captions": "[0:10:20 - 0:10:39] [0:10:20 - 0:10:22]: The scene opens with a first-person perspective in a lush grassy area with numerous flowers. The background contains some trees and a building partially visible through the foliage. A small inset screen on the upper left displays a person in a room with various electronic equipment, appearing to stream or play a game. Various chat messages are visible on the right side of the screen. [0:10:23 - 0:10:26]: The view shifts to a road, looking at a truck in the center. This large, white vehicle has a visible license plate and is surrounded by trees and utility poles. The person in the inset screen remains in the same position, continuing to interact with their chat. [0:10:27]: A black rabbit appears on the road, positioned near the truck, and is looking toward the camera. The truck is still centered in the frame. [0:10:28 - 0:10:31]: The rabbit is now fully centered and sitting upright in the foreground. In the background, the white truck remains stationary. [0:10:32 - 0:10:34]: The rabbit continues to sit on the road while the frame remains focused on the truck with the background showing trees and utility poles. [0:10:35 - 0:10:37]: The camera zooms slightly closer to the truck. The trees to the left side become more prominent, and the rabbit is no longer in the frame. [0:10:38 - 0:10:39]: The perspective changes back to a park-like setting with large trees and bushes. In the background, buildings are visible, indicating an urban environment. The streaming individual is still present in the upper left inset with their room and setup.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the camera doing right now?",
                "time_stamp": "00:10:39",
                "answer": "C",
                "options": [
                    "A. Zooming in on the truck.",
                    "B. Focusing on the streaming individual.",
                    "C. Panning across the park-like setting.",
                    "D. Capturing the rabbit sitting on the road."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_288_real.mp4"
    }
]