[
    {
        "time": "[0:00:00 - 0:00:05]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "Right now, what is the main color of the bus in front?",
                "time_stamp": "00:00:19",
                "answer": "A",
                "options": [
                    "A. Yellow.",
                    "B. Red.",
                    "C. Blue.",
                    "D. Green."
                ],
                "required_ability": "working memory",
                "rekv": " A"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_457_real.mp4"
    },
    {
        "time": "[0:01:55 - 0:02:00]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is the yellow vehicle on the left side of the street right now?",
                "time_stamp": "00:01:56",
                "answer": "A",
                "options": [
                    "A. DHL van.",
                    "B. Police car.",
                    "C. Fire truck.",
                    "D. Food truck."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_457_real.mp4"
    },
    {
        "time": "[0:03:50 - 0:03:55]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Right now, what is the speed limit indicated on the yellow bus?",
                "time_stamp": "00:03:52",
                "answer": "A",
                "options": [
                    "A. 90 km/h.",
                    "B. 30 km/h.",
                    "C. 60 km/h.",
                    "D. 80 km/h."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_457_real.mp4"
    },
    {
        "time": "[0:05:45 - 0:05:50]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "Right now, what is the background color of the sign indicating changed traffic conditions?",
                "time_stamp": "00:05:45",
                "answer": "A",
                "options": [
                    "A. Yellow.",
                    "B. Blue.",
                    "C. Red.",
                    "D. White."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_457_real.mp4"
    },
    {
        "time": "[0:07:40 - 0:07:45]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "Right now, what architectural style is predominant in the buildings visible on the left side of the street?",
                "time_stamp": "00:07:48",
                "answer": "A",
                "options": [
                    "A. Georgian.",
                    "B. Gothic.",
                    "C. Modern.",
                    "D. Art Deco."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_457_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video begins with a logo for \"3D BotMaker Diecast Racing League\" on a textured background. [0:00:01 - 0:00:04]: The perspective switches to a miniature racing track scene. Multiple small diecast cars, including orange, yellow, and black vehicles, are seen on a curved track. The setting includes faux grass, trees, billboards, and spectators. [0:00:05 - 0:00:10]: The logo for \"3D BotMaker Diecast Racing League\" reappears, with additional text indicating it is the \"Fiero Tournament, Race 1 of 3.\" [0:00:11 - 0:00:15]: The scene shows a steep track on a hillside. The environment is a well-crafted diorama, featuring a track with railings, small trees, and detailed landscaping. [0:00:16]: A close-up of a miniature off-road scene shows several model cars on rocks, with miniature figures posed around them, emphasizing a realistic, dynamic setup. [0:00:17 - 0:00:18]: The view shifts to a camping area beside the track. There are multiple caravans, a few parked cars, and miniature figures, indicating a detailed and vibrant setting. [0:00:19]: The final frame focuses on a miniature race control tower with small figures. Two portable toilets are positioned nearby, surrounded by detailed foliage and structures.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What does the additional text indicate about the \"3D BotMaker Diecast Racing League\"?",
                "time_stamp": "00:00:10",
                "answer": "B",
                "options": [
                    "A. It is the \"Final Race\".",
                    "B. It is the \"Fiero Tournament, Race 1 of 3\".",
                    "C. It is the \"Champion's Cup\".",
                    "D. It is the \"Qualifying Round\"."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_500_real.mp4"
    },
    {
        "time": "[0:01:00 - 0:02:00]",
        "captions": "[0:01:40 - 0:02:00] [0:01:40 - 0:01:43]: The video begins with a group of model cars on a slot car racing track. The track loops through a miniature landscape with green slopes and small, evenly spaced trees. The group of cars, including a red, an orange, a green, and a yellow car, races along a curved section of the track. [0:01:44 - 0:01:45]: As the cars continue racing, they speed past a grassy hillside. The red car appears to be leading, followed by a yellow car. The landscape maintains its lush, green appearance with occasional trees along the track. [0:01:46 - 0:01:47]: The cars navigate another section of the track, moving downhill. The red car is slightly ahead of the yellow car. The greenery around the track adds a sense of realism to the miniature environment. [0:01:48 - 0:01:50]: The track circles a central area with parked miniature vehicles, spectators, and various structures. The red car leads closely followed by other cars around the bend, with a red and white barrier lining the track's edge. In the background, an electronic billboard is visible. The red car continues while the other cars join the track. [0:01:51 - 0:01:54]: The scene shifts to a high-speed straight section. The red car, followed by a yellow car, races past a series of billboards and miniature trees. The track is bordered by grassy areas and more background elements such as miniature spectator stands and signage. [0:01:55 - 0:01:57]: The track runs parallel to structures, including billboards for \"Slammin Customs\" and \"Model Cars Houston\". The red car is slightly blurred from its speed as it crosses a finishing line marked with a digital timer. [0:01:58 - 0:02:00]: The video concludes with a display showing the race results. An orange car is prominently featured on the screen with a list ranking the cars by their positions, indicating that \"Randy\" driving a red car has finished first. The setup includes more structures and a miniature race control center.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "Which car is leading right now?",
                "time_stamp": "0:01:43",
                "answer": "C",
                "options": [
                    "A. Black car.",
                    "B. Red car.",
                    "C. Yello car.",
                    "D. Green car."
                ],
                "required_ability": "episodic memory"
            },
            {
                "task_type": "Text-Rich Understanding",
                "question": "According to the race results displayed at the end, who is the driver of the red car?",
                "time_stamp": "00:02:00",
                "answer": "B",
                "options": [
                    "A. George Milidrag.",
                    "B. Randy Ferrero.",
                    "C. Crazy Jimmy.",
                    "D. Rad Cunningham."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_500_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:23]: The video starts with a first-person view looking at a model car racetrack. There is a curved section of the track with several model cars racing around it, including a bright yellow car in the foreground. In the background, there are small trees and some billboards. The left half of the frame contains sand and a small grassy area with miniature trees. [0:03:24 - 0:03:27]: As the video progresses, the view shifts to a straightaway portion of the track, where more model cars are seen racing towards a small building on the right side. The building has some figures (likely model people) standing around. Along the track, there are more spectators and trees creating a bustling scene of a model racing event. [0:03:28 - 0:03:30]: Near the finish line, the racetrack has a timing tower and several spectators gathered around, cheering for the cars as they speed by. The grassy hill remains prominent in the background. [0:03:31 - 0:03:35]: The view transitions to the starting line on an elevated track section. Four brightly colored model cars (red, green, orange, yellow) are lined up beneath a starting gate equipped with lights. The adjacent track is visible with several elevation changes and a few trees lining the sides. [0:03:36 - 0:03:39]: The starting lights turn green, signaling the start of the race. The model cars begin moving swiftly down the track, with the red car slightly leading. The track curves and descends, with more trees and a racetrack visible on the left side of the frame as well.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What colors are the four model cars lined up at the starting line?",
                "time_stamp": "00:03:35",
                "answer": "D",
                "options": [
                    "A. Blue, yellow, green, red.",
                    "B. Red, blue, orange, green.",
                    "C. Green, yellow, orange, blue.",
                    "D. Red, green, orange, yellow."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_500_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:05:47]",
        "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:05]: The video begins on a racetrack with a few cars on the track. The camera captures a tight corner with red and white barriers. Several vehicles and trailers are parked on the grassy area adjacent to the track. Three racing cars\u2014yellow, green, and orange\u2014are in the middle of taking the curve. [0:05:06 - 0:05:09]: As the frame changes, the yellow car leads on the track while the red and orange cars follow closely. The terrain is grassy, and the track smoothly transitions into a straight path. [0:05:10 - 0:05:11]: The yellow car continues to lead, followed by the red car in the left lane, while the spectators and a few structures, including a sign and a small building, appear to the left side of the track.  [0:05:12 - 0:05:14]: The yellow car remains in the lead, passing under a raised structure displaying a digital timer. The crowd stands along the track, watching as cars race by. Some tents and a porta-potty can also be seen on the sidelines. [0:05:15 - 0:05:16]: The camera zooms in on the parking area where the yellow car is parked next to a red car adorned with an American flag design. A few people stand nearby, one of them taking photos. The background includes parked cars, trees, and a sign for \"Slamman Customs.\" [0:05:17 - 0:05:19]: The view remains focused on the yellow and red cars parked side-by-side. More details of the parking area and the background, including blue porta-potties and spectators, are visible.\n[0:05:40 - 0:05:47] [0:05:40 - 0:05:47]: The video shows a close-up view of a textured black asphalt surface. Positioned horizontally across the top of the frame is a black banner with bold white text that reads \"3D BotMaker DIECAST RACING LEAGUE.\" An orange hexagon with the white letters \"3D\" is on the left side of the banner. This banner spans the width of the frame, and the asphalt surface occupies the remaining area, providing a consistent and uniform background.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What design is on the red car parked next to the yellow car right now?",
                "time_stamp": "00:04:58",
                "answer": "D",
                "options": [
                    "A. Flames.",
                    "B. Stripes.",
                    "C. Checkerboard.",
                    "D. American flag."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_500_real.mp4"
    },
    {
        "time": "0:00:00 - 0:00:20",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video begins with a first-person view of a person standing on the sidewalk. He is wearing a blue and white striped shirt and white shorts. He is looking towards a multi-lane road with several cars passing by. In the background, there is an elevated highway and a few palm trees. Other pedestrians can be seen in the vicinity. [0:00:04 - 0:00:07]: The person starts walking along the sidewalk, approaching an intersection. The traffic light is visible, and a few more cars are seen driving by on the road ahead. A tall building under construction can be seen across the street. [0:00:08 - 0:00:11]: Continuing along the sidewalk, the person moves closer to the intersection. More details of the construction site are visible, including scaffolding and cranes. Traffic continues to flow in the background. [0:00:12 - 0:00:14]: The person begins to run, heading towards the intersection on the opposite side of the street. There are still multiple cars on the road, and the elevated highway remains in view. [0:00:15 - 0:00:17]: The person runs across the street towards another section of the sidewalk. A white car is seen waiting at the intersection, and other vehicles are moving in the distance. The construction site is still prominent in the background. [0:00:18 - 0:00:20]: The person continues running past the white car and up the sidewalk, approaching a pedestrian walking ahead. The buildings and elevated highway serve as a backdrop to the scene.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the person doing right now?",
                "time_stamp": "00:00:14",
                "answer": "B",
                "options": [
                    "A. Sit down on a bench.",
                    "B. Running along the sidewalk.",
                    "C. Taking a photograph.",
                    "D. Turning around."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_271_real.mp4"
    },
    {
        "time": "0:02:40 - 0:03:00",
        "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:50]: The video begins with a first-person perspective of a person running along a sidewalk lined with a blue construction wall on the left and parked cars on the right. The person, wearing a striped shirt and shorts, is steadily moving forward. The background shows a cityscape with buildings and a traffic light in the distance. Several objects like cans and debris are scattered along the sidewalk. [0:02:50 - 0:02:55]: As the person continues running, they approach a section of the sidewalk with more scattered trash, suggesting an urban setting. The blue construction wall slightly curves ahead, indicating the person is about to turn a corner. [0:02:55 - 0:03:00]: The person turns right at the corner of the blue construction wall, heading towards a crosswalk. The view ahead shows more city buildings with advertisements, a few parked and moving cars. The person crosses the street and approaches a blue car stopped on the opposite side.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is located to the right of the person right now?",
                "time_stamp": "00:02:57",
                "answer": "B",
                "options": [
                    "A. A blue construction wall.",
                    "B. A blue car.",
                    "C. A crosswalk.",
                    "D. City buildings with advertisements."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_271_real.mp4"
    },
    {
        "time": "0:05:20 - 0:05:40",
        "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:25]: The scene takes place in a parking lot near a building labeled \"24 HR PARKING.\" The person is walking towards the building's entrance, passing by a small booth painted blue and white. The ground has white painted lines marking parking spots.  [0:05:26 - 0:05:29]: The person continues towards the entrance, veering slightly to the right. Two individuals are standing near the entrance, appearing to be engaged in a conversation. Nearby, a potted plant is placed on the ground beside them. [0:05:30 - 0:05:32]: The person pauses for a moment and looks around. The background reveals other buildings, vehicles, and some traffic signs. The setting appears urban with some construction or renovation activity visible in the distance. [0:05:33 - 0:05:37]: Continuing to walk towards the two individuals, the person breaches the edge of the parking lot and approaches the building. The two individuals are still engaged in a conversation near an emergency exit door. [0:05:38 - 0:05:40]: The person stops and stands near the two individuals, who seem to be talking.  [0:05:41 - 0:05:43]: The scene shifts, showing the person turning around to face the parking lot, where some litter and disorganized parking spaces are visible. They stand still momentarily. [0:05:44 - 0:05:47]: The perspective turns back towards the parking lot, showing the person's back to the camera. The parking lot appears somewhat rundown with scattered debris around. [0:05:48 - 0:05:59]: The view switches to a map interface displaying a detailed map of the city. Various locations and routes are marked in different colors. The map shows urban planning, streets, buildings, and some notable landmarks by the coastline. [0:06:00 - 0:06:10]: The map interface continues being displayed, with the person possibly deciding on a route or examining different marked locations. The screen remains static, focusing on the detailed map. [0:06:10]: The final frame displays the person continuing to examine the map interface, planning their next move. The map remains the central focus, showcasing various streets and points of interest throughout the city.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is located near the entrance of the building right now?",
                "time_stamp": "00:05:23",
                "answer": "B",
                "options": [
                    "A. A blue and white booth.",
                    "B. Two potted plants.",
                    "C. A small kiosk.",
                    "D. A security camera."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_271_real.mp4"
    },
    {
        "time": "0:08:00 - 0:08:20",
        "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:02]: In the video, an individual with short blond hair wearing a light blue and white striped shirt, white shorts, and white sneakers is seen running along a sidewalk which runs parallel to a blue construction wall. The sky is clear, and daylight illuminates the scene.   [0:08:03 - 0:08:07]: The character continues running, and the video reveals a street on the left with a store named \"Los Santos Vapid\" in the background. More shops and several signposts line the street as well.   [0:08:08 - 0:08:10]: The person is still moving along the sidewalk with the blue construction wall on the right and starts moving past a pole. [0:08:11 - 0:08:15]: As the character runs further down the sidewalk, a black car appears on the road to the left, heading towards an intersection. [0:08:16 - 0:08:18]: The camera angle slightly shifts, providing a close-up view of the individual\u2019s upper body and the nearby pole. [0:08:19 - 0:08:21]: The runner continues along the sidewalk, and the intersection with a visible pedestrian crossing signal can be seen ahead. [0:08:22 - 0:08:24]: As the person keeps running, the blue construction wall continues alongside him, with various signs posted on it. [0:08:25 - 0:08:27]: The person maintains their speed, approaching a bridge visible in the distance, under which the street and sidewalk continue. [0:08:28 - 0:08:30]: Both the runner and the black car progress towards the intersection where a crossing may happen. [0:08:31 - 0:08:33]: The individual approaches closer to the bridge, with a red building visible to the right beyond the blue construction wall. [0:08:34 - 0:08:36]: As the person runs past the red building, they make a slight turn, continuing along the pathway.  [0:08:37 - 0:08:40]: The runner approaches the end of the blue construction wall, revealing a stairway leading up to another level. [0:08:41 - 0:08:43]: The person moves towards the intersection, showing more signs of urban infrastructure and traffic signals. [0:08:44 - 0:08:46]: The view shifts slightly, showing the runner closer to various urban elements like streetlights, trash bins, and passing by a transit building with \"Transit\" written on it. [0:08:47 - 0:08:50]: The runner continues through the intersection area, displaying more urban scenery including traffic lights and buildings ahead. [0:08:51 - 0:08:54]: The individual crosses the road onto a pavement, moving through the cityscape with tall buildings in the background. [0:08:55 - 0:08:58]: The camera adjusts as the person makes a sharp turn, revealing a wide road ahead and more buildings and infrastructure. [0:08:59 - 0:08:20]: The runner proceeds into an open boulevard, running parallel to a row of tall buildings and finally, veers right onto a quieter side street.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is located to the right of the runner as they approach the bridge?",
                "time_stamp": "00:08:15",
                "answer": "B",
                "options": [
                    "A. A blue construction wall.",
                    "B. A red wall.",
                    "C. A pedestrian crossing signal.",
                    "D. A black car."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_271_real.mp4"
    },
    {
        "time": "0:10:00 - 0:10:15",
        "captions": "[0:10:00 - 0:10:15] [0:10:00 - 0:10:04]: The video begins with a first-person perspective of a character in a striped shirt running along a concrete sidewalk, next to a short concrete wall lined with green hedges on the right side. The sidewalk is bordered by a patch of grass on the left side, behind which there's a road. Tall buildings and trees can be seen in the distance, suggesting an urban environment. There is an overlay in the top left corner showing a different person's figure, possibly a streamer, and a chatroom interface is visible on the right side of the screen. [0:10:05 - 0:10:07]: The character slows down and comes to a stop, facing the same direction down the sidewalk. The streamer overlay in the top left remains stable, with continuous activity in the chatroom interface on the right. [0:10:08]: The character begins to walk slowly down the sidewalk again, maintaining the same direction. The scene around remains consistent, with greenery on the right and urban architecture in the background. [0:10:09]: The character temporarily stops again on the sidewalk. The overlay and chatroom interface remain unchanged in position and content. [0:10:10]: A map interface pops up, overlaying the direction and location. This map shows a detailed view of the surrounding area, highlighting the current position and path. [0:10:11]: The map disappears, returning to the first-person view of the character standing on the sidewalk, with the chatroom on the right still active. [0:10:12 - 0:10:14]: The character resumes running down the sidewalk. The urban scenery remains unchanged, displaying tall buildings, green hedges, and palm trees. [0:10:15]: The character continues to run forward, with more urban structures and greenery seen ahead. The streamer overlay remains in the same position, with vibrant chat activity ongoing.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is the position of the greenery in relation to the character right now?",
                "time_stamp": "00:10:02",
                "answer": "B",
                "options": [
                    "A. On the left side.",
                    "B. On both sides.",
                    "C. Behind the character.",
                    "D. In front of the character."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_271_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:30]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why is Mr. Bean constantly posing right now?",
                "time_stamp": "00:01:04",
                "answer": "D",
                "options": [
                    "A. Because he is trying to practice his modeling skills.",
                    "B. Because he is preparing for a photo shoot.",
                    "C. Because he is showing off his new outfit in front of a mirror.",
                    "D. Because he is imitating the characters on the television."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_237_real.mp4"
    },
    {
        "time": "[0:02:05 - 0:02:35]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why does the bed get pulled out through the window?",
                "cause": "The man ties the rope to the bed",
                "effect": "The bed gets pulled out through the window",
                "time_stamp": "00:02:34",
                "answer": "A",
                "options": [
                    "A. Because the man ties the rope to the bed.",
                    "B. Because the bed is on wheels.",
                    "C. Because the window is too large.",
                    "D. Because the bed is not secured properly to the floor."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_237_real.mp4"
    },
    {
        "time": "[0:04:10 - 0:04:40]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why does the Mr. Bean peer through the door's mail slot or window?",
                "time_stamp": "0:04:28",
                "answer": "C",
                "options": [
                    "A. Because he lost his keys and is trying to see if anyone is home.",
                    "B. Because he heard a strange noise and wants to check what it is.",
                    "C. Because he wanted to observe the activities of the four people outside.",
                    "D. Because he is waiting for an important package to be delivered."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_237_real.mp4"
    },
    {
        "time": "[0:06:15 - 0:06:45]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why does Mr.Bean drop their coat and bag?",
                "time_stamp": "00:06:27",
                "answer": "C",
                "options": [
                    "A. Because he realized he grabbed the wrong coat and bag.",
                    "B. Because he was startled by a sudden loud noise.",
                    "C. Because his disguise mission has ended.",
                    "D. Because he spotted someone he knows and wanted to quickly greet them."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_237_real.mp4"
    },
    {
        "time": "[0:08:20 - 0:08:50]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why does the police officer react and possibly make a call for backup or additional assistance?",
                "time_stamp": "0:08:45",
                "answer": "C",
                "options": [
                    "A. Because the police officer needs to record the event.",
                    "B. Because the police officer needs to clock off for the day.",
                    "C. Because the police officer believes the photographs show a potential crime.",
                    "D. Because the police officer wants to review the photos later."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_237_real.mp4"
    },
    {
        "time": "0:00:00 - 0:00:20",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:09]: A hand holds a set of keychains, displayed prominently against a beige background. The keychains are made of clear acrylic discs with different colored backdrops\u2014pink, purple, blue, and mint green. Each keychain has a name written on it: Holly on the pink disc, Katie on the purple, Millie on the blue, and Eloy on the mint green. Each name is accompanied by a small white heart underneath. The keychains are attached to rings and have tassels in various colors: pink, blue, and green, which complement the discs. The clear discs show a glossy finish, reflecting some light. The hand holding them is slightly rotated to showcase different angles of the keychains. [0:00:09 - 0:00:10]: The screen transitions to black. [0:00:10 - 0:00:13]: White text on a black background appears, reading \"Personalized Acrylic Key Chain.\" [0:00:13 - 0:00:14]: The screen remains black. [0:00:14 - 0:00:18]: More white text appears, reading \"If you would like to see how I designed these keychains, watch until the end.\" [0:00:18]: The screen fades to black again. [0:00:19]: The scene changes to a workspace. A compact, teal-colored Cricut cutting machine is positioned at the top of the frame. Below it, there's a green cutting mat, gridded with white lines, placed on a black-checkered work surface. Two hands are visible, preparing the materials for crafting.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the person doing right now?",
                "time_stamp": "00:00:20",
                "answer": "D",
                "options": [
                    "A. Holding a set of keychains.",
                    "B. Displaying a text message.",
                    "C. Operating a Cricut cutting machine.",
                    "D. Preparing materials for crafting."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_49_real.mp4"
    },
    {
        "time": "0:02:00 - 0:02:20",
        "captions": "[0:02:00 - 0:02:20] [0:02:01 - 0:02:02]: Both hands, wearing a magenta sweater, are handling several pieces of translucent, circular plastic sheets and square white papers placed on a gray grid-patterned surface. A small round piece with a light blue color is being lifted by the right hand. [0:02:03]: The left hand holds a piece of paper with a pastel pink color, while the right hand is peeling off a sticker with blue paint. [0:02:04]: The right hand now holds a purple sticker, previously peeled off from the white backing paper, while the left hand holds the same pastel pink paper. [0:02:05]: The purple sticker is now being attached to one of the circular, translucent plastic sheets with the hands carefully positioning it. [0:02:06]: The hands continue positioning and pressing down the purple sticker onto the translucent sheet, ensuring it is aligned. [0:02:07]: The hands are still pressing and adjusting the sticker to ensure it adheres correctly to the circular sheet. [0:02:08 - 0:02:09]: The hands remain focused on fixing the purple sticker onto the circular plastic sheet, smoothing out any bubbles or creases. [0:02:10]: The right hand reaches out for another piece of paper, while the left continues to handle the circle with the purple sticker. [0:02:11 - 0:02:12]: The left hand places the circular sheet with the purple sticker back onto the surface. A new piece of white backing paper with what appears to be a pink sticker is being lifted with both hands. [0:02:13 - 0:02:14]: The right hand begins peeling off a pink sticker from the backing paper while the left hand supports the paper. [0:02:15]: Both hands work together to peel off the entire pink sticker from the white backing paper. [0:02:16]: The pink sticker is now fully removed, and the right hand holds it mid-air while the left hand sets the backing paper aside. [0:02:17 - 0:02:18]: Both hands begin positioning the pink sticker onto another translucent circular sheet, ensuring it is correctly aligned. [0:02:19 - 0:02:20]: The hands continue adjusting and fixing the pink sticker onto the circular sheet, smoothing out its surface to ensure proper adhesion.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What are the hands doing right now?",
                "time_stamp": "0:02:14",
                "answer": "C",
                "options": [
                    "A. Peeling off a blue sticker.",
                    "B. Attaching a purple sticker to a circular sheet.",
                    "C. Adjusting and smoothing out a pink sticker.",
                    "D. Lifting a piece of pastel pink paper."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_49_real.mp4"
    },
    {
        "time": "0:04:00 - 0:04:20",
        "captions": "[0:04:00 - 0:04:20] [0:00:01 - 0:00:05]: From the first-person perspective, a pair of hands is visible manipulating keyrings with the use of a blue-handled tool. The backdrop features a gray grid-patterned mat. There are four round keychains near the top of the frame, each with names and pastel colors. The keychains have different colored tassels: blue, pink, green, and purple. [0:00:06 - 0:00:12]: The hands continue working on the keyring. The tool is held in the right hand, used to manipulate a silver component attached to a pink-colored keychain that bears the name \"Holly\u201d along with a small heart symbol underneath it. The activity focuses on attaching or modifying the keychain attachments. [0:00:13 - 0:00:15]: The person\u2019s left hand moves the pink keychain with \"Holly\" positioned centrally, suggesting completion of the adjustment. Meanwhile, the other hand prepares to adjust or attach another part of the keychain. [0:00:16 - 0:00:20]: The right hand with the tool moves closer to the camera, forming a more detailed view of the attachment process. Moments later, the hands present all the keychains in one hand, showing the names \"Millie\", \"Holly\", \"Betty\", and \"Katie\", each with their distinctive color and small tassel. [0:00:21 - 0:00:25]: The keychains are held more steadily, each clearly displayed. The hands briefly shift the position to allow a full view of the names and tassels against the gray background. [0:00:26 - 0:00:30]: A transition occurs where the hand moves to display the keychains against a plain background instead of the grid mat. The names and tassels remain in clear view, capturing the keychains' intricacies and details. The colors and design elements, such as small hearts beside the names, are distinctly visible.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What are the hands doing right now?",
                "time_stamp": "00:04:05",
                "answer": "C",
                "options": [
                    "A. Stitching a piece of fabric.",
                    "B. Writing on a notepad.",
                    "C. Adjusting a pink keychain.",
                    "D. Sorting different colored beads."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_49_real.mp4"
    },
    {
        "time": "0:06:00 - 0:06:20",
        "captions": "[0:06:00 - 0:06:20] [0:06:05 - 0:06:06]: The video begins with the screen displaying a software interface with a grid background. On the screen, various design elements are positioned. At the top, there are eight blobs of colors including beige, light blue, pink, pastel purple, yellow, lavender, green, and another shade of pink, all aligned in a row. Below these colored blobs, there are three large black circles placed horizontally across the middle of the screen. To the right of these black circles, the word \"Shelby\" is written in a stylish script font three times at different locations and sizes. [0:06:06 - 0:06:07]: The word \"Katie\" is typed and added beneath the second black circle, replacing one of the \"Shelby\" texts. The words \"Shelby\" and \"Katie\" appear to be used for designing purposes on this grid-based interface. [0:06:07 - 0:06:09]: \"Katie\" continues to be adjusted and repositioned under the second black circle. The text \"Shelby\" still remains under the first black circle and to the far right. [0:06:09 - 0:06:11]: No significant changes occur. The design on the screen remains mostly the same with an emphasis on the text \"Katie,\" \"Shelby,\" and the black circles. [0:06:11 - 0:06:12]: Another text box appears next to the third circle, but no text has been typed in it. The cursor may be preparing to add another name or word within the editable space. [0:06:12 - 0:06:13]: The new text box remains empty while the design involves the black circles and the various positioned text elements such as \"Shelby\" and \"Katie.\" [0:06:13 - 0:06:14]: The text box continues to exist to the right of the third black circle without any content. The design remains consistent with black circles and corresponding names. [0:06:14 - 0:06:15]: The empty text box remains. The design interface still shows no additional movements or adjustments. [0:06:15 - 0:06:16]: The word \"Millie\" is typed out within the previously empty text box to the right of the third black circle. The layout now includes \"Shelby,\" \"Katie,\" and \"Millie\" under the respective black circles. [0:06:16 - 0:06:17]: The design elements such as \"Shelby,\" \"Katie,\" and \"Millie\" are being fine-tuned possibly for alignment under each black circle within the software interface. [0:06:17 - 0:06:18]: The word Millie is now clearly seen along with \"Shelby\" and \"Katie,\" arranged under the black circles. [0:06:18 - 0:06:19]: The addition of another text box to the right of \"Shelby\" but it remains empty. Text editing and positioning tools visible on the right side of the interface. [0:06:19 - 0:06:20]: The previously empty text box next to \"Shelby\" has its cursor visible, preparing for text input. Other elements remain unchanged. [0:06:20 - 0:06:21]: The design screen with black circles, colored blobs, and texts \"Katie\", \"Shelby,\" and \"Millie\" remains mostly constant. [0:06:21 - 0:06:22]: The empty text box appears again next to \"Shelby.\" The positioning of elements on the screen remains the same. [0:06:22]: The newly added text, \"Holly,\" is placed in the previously empty text box next to \"Shelby.\" All text elements are well-aligned and visible. [0:06:22 - 0:06:23]: Enlarged view of the text shows the names are now clearly seen, with \"Katie\", \"Shelby\", \"Millie\", and newly added \"Holly\" arranged orderly. [0:06:23 - 0:06:24]: The design focuses on the three black circles while the text elements, \"Katie,\" \"Shelby,\" \"Millie,\" and \"Holly\" are arranged for a clear view. [0:06:24 - 0:06:25]: A closer look at \"Shelby,\" showing positioning and editing tools being used. Text alignment features are visible for precision in the design. [0:06:25]: The video ends with the focus on the text elements and their proper alignment. The words \"Katie\", \"Shelby\", \"Millie,\" and \"Holly\" are neatly arranged under the row of black circles.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What name is being typed out right now?",
                "time_stamp": "00:06:07",
                "answer": "C",
                "options": [
                    "A. Shelby.",
                    "B. Katie.",
                    "C. Millie.",
                    "D. Holly."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_49_real.mp4"
    },
    {
        "time": "0:08:00 - 0:08:20",
        "captions": "[0:08:00 - 0:08:20] [8:08:05 - 8:08:07]: Four circular designs are displayed horizontally across a grid background. From left to right, the circles contain names: \"Shelby\" on a mint green background, \"Katie\" on a purple background, \"Millie\" on a light blue background, and \"Holly\" on a pink background, all in black cursive writing. [8:08:08 - 8:08:11]: The \"Shelby\" circle is removed, leaving three circles. The \"Katie\" circle now has a small pink heart shape, while the \"Millie\" and \"Holly\" circles remain unchanged. [8:08:12 - 8:08:15]: A small red heart shape is added to the \"Millie\" circle. The \"Katie\" and \"Holly\" circles remain the same. [8:08:16 - 8:08:18]: Another small black heart is added to the \"Millie\" circle. The \"Katie\" and \"Holly\" circles remain the same. [8:08:19 - 8:08:21]: The \"Millie\" circle now has a total of three small hearts: one red and two black. The \"Katie\" and \"Holly\" circles remain the same. [8:08:22 - 8:08:25]: The \"Katie\" circle remains the same with its single pink heart. The \"Millie\" circle is unchanged with its three small hearts. A small black heart is added to the \"Holly\" circle. [8:08:26 - 8:08:29]: Both the \"Millie\" and \"Holly\" circles now each have an additional small heart, making three small hearts in the \"Millie\" circle and one in the \"Holly\" circle. The \"Katie\" circle remains the same. [8:08:30 - 8:08:33]: A black heart is added below the names in all three circles: \"Katie,\" \"Millie,\" and \"Holly.\" [8:08:34 - 8:08:37]: A black heart appears in all three circles. The circles remain in place on the grid background. The \"Katie\" circle is moving out of view to the left. [8:08:38 - 8:08:41]: The frames show the three circles \"Katie,\" \"Millie,\" and \"Holly,\" each with a black heart below their names. The background shows a grid layout.  [8:08:41]: The \"Katie\" circle has moved completely out of sight, leaving just the \"Millie\" and \"Holly\" circles. The grid background remains the same.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the \"Katie\" circle doing right now?",
                "time_stamp": "0:08:10",
                "answer": "A",
                "options": [
                    "A. Moving to the left.",
                    "B. Adding a heart shape.",
                    "C. Displaying a light blue background.",
                    "D. Removing the label."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_49_real.mp4"
    },
    {
        "time": "0:00:00 - 0:00:20",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:08]: Two individuals are standing beside a large black vehicle. One person is wearing a striped shirt and light-colored shorts, and the other is dressed in a blue hoodie and dark pants. They are positioned close to the rear of the vehicle, which is facing a wall. The individual in the striped shirt holds a device, possibly a smartphone, looking at it attentively. The other individual appears to be conversing with the first person. Various icons and text are displayed on the right side of the frame, suggesting a streaming interface. The background has some parked cars and urban elements visible under dim lighting, indicating it might be nighttime. [0:00:09 - 0:00:20]: The person in the striped shirt continues to interact with their device, occasionally looking up. The second individual in the blue hoodie keeps observing or talking to them. After interacting for a while, the person in the striped shirt starts to move around the vehicle, walking briskly towards the front of the large black vehicle with \u201cgrip\u201d written on the side in dark letters. The corridors seem urban with some street lighting visible, suggesting they are in a city or town at night. As the individual runs, their motion suggests urgency or purpose.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the person in the striped shirt doing right now?",
                "time_stamp": "00:00:18",
                "answer": "A",
                "options": [
                    "A. Walking briskly towards the front of the vehicle.",
                    "B. Standing beside the vehicle and looking at their device.",
                    "C. Conversing with the individual in the blue hoodie.",
                    "D. Observing the background elements."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_278_real.mp4"
    },
    {
        "time": "0:02:40 - 0:03:00",
        "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:45]: The video starts with a first-person perspective of a character walking on a sidewalk at night, accompanied by two other characters. One character is wearing a striped shirt and another is in a blue hoodie. There are buildings and street lights in the background. [0:02:46 - 0:02:50]: The characters move closer to a bus or truck parked along the sidewalk. The view shifts slightly to follow the character in the striped shirt who starts to crouch. [0:02:51 - 0:02:53]: The character in the striped shirt continues moving towards the parked vehicle, now in a crouching position. Another character follows closely behind. [0:02:54 - 0:02:56]: The perspective changes to an inventory screen showing various items. Personal information and statistics are visible on the left side, while the inventory grid is on the right. [0:02:57 - 0:03:01]: The inventory screen remains open, showing assorted items and available storage spaces. The actions of the character are not visible but seem focused on organizing or inspecting the inventory. [0:03:02 - 0:03:04]: Back to the first-person perspective, the character now stands behind the vehicle. A prompt appears on the screen offering options to \"toggle item\" or \"toggle engine.\" [0:03:05 - 0:03:08]: The character moves slightly to the side of the vehicle, and the same interaction prompt remains on the screen. [0:03:09 - 0:03:10]: The character continues to walk around the vehicle, examining it from different angles.  [0:03:11 - 0:03:12]: The interaction prompt changes as the character approaches the back side of the vehicle more closely. [0:03:13 - 0:03:14]: The character stops walking and pulls out a phone.  [0:03:15 - 0:03:17]: The character looks down at their phone, possibly interacting with it, while still standing next to the vehicle. [0:03:18 - 0:03:19]: The character puts away the phone and continues standing beside the vehicle, still looking at it. [0:03:20]: The character moves again, starting to walk around the front side of the vehicle. [0:03:21 - 0:03:23]: Now, the character proceeds towards the driver's side door and makes a motion to open it. [0:03:24 - 0:03:26]: The character opens the door and begins to climb into the driver's seat of the vehicle. [0:03:27 - 0:03:29]: With the character now seated inside the vehicle, the view switches to a third-person perspective showing the vehicle from above and behind. [0:03:30]: The vehicle starts moving forward, driving along the street. The dashboard and navigation icons are visible on the screen.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What did the character do just now?",
                "time_stamp": "00:02:54",
                "answer": "A",
                "options": [
                    "A. Put away his phone.",
                    "B. Open the driver's side door.",
                    "C. Crouch next to the vehicle.",
                    "D. Toggle the engine."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_278_real.mp4"
    },
    {
        "time": "0:05:20 - 0:05:40",
        "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:29]: A dark-colored delivery van, marked with the word \"prime\" on the side, is parked near the sidewalk of a dimly lit street. The street appears to be in a commercial area with buildings close by. The streetlights and building windows provide some illumination. A person in a blue shirt stands near the back of the van, slightly obscured by the vehicle. In the background, city lights can be seen, indicating it's nighttime. [0:05:29 - 0:05:32]: The person near the van's back door begins to open it. Their posture suggests they are preparing to load or unload something. The surrounding buildings remain the same, with the same illuminated windows and low lighting conditions. [0:05:32 - 0:05:36]: The back door of the van is fully opened. The person is now clearly visible, seemingly involved in handling something inside the van. The rest of the scene remains static, with the illuminated windows providing the primary light source in the otherwise dark environment. [0:05:36 - 0:05:40]: The person at the back of the van continues to handle items. Their movements indicate they are either loading or unloading the van. The surrounding area remains unchanged, with consistent lighting from the nearby buildings and streetlights.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the person near the van doing right now?",
                "time_stamp": "00:05:28",
                "answer": "A",
                "options": [
                    "A. Handling items in the back of the van.",
                    "B. Standing near the van without any movement.",
                    "C. Opening the van's side door.",
                    "D. Walking towards the street."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_278_real.mp4"
    },
    {
        "time": "0:08:00 - 0:08:20",
        "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: The player is driving a van in a dark alley, showing the front of the van with headlights on. The surroundings are dimly lit with some red indication markers on the ground. [0:08:02 - 0:08:04]: The van is seen from a higher angle, navigating slightly to the left in the alleyway still enclosed by the red markers. [0:08:05 - 0:08:08]: The van moves forward under a low roof or possibly a parking garage entrance; the front part is illuminated with the red and black colored route on the ground. [0:08:09 - 0:08:10]: The van continues straight, visible from the rear and slightly elevated angle, as it aligns itself correctly in the parking area. [0:08:11 - 0:08:12]: The scene shifts back to a closer view of the van's side as it approaches a bay marked in green light. [0:08:13 - 0:08:14]: The van stops in the highlighted green area. The driver seems to be aligning the vehicle with a designated point. [0:08:15 - 0:08:16]: The van is seen from the rear, parked entirely within the green-marked area, inside what appears to be a loading dock. [0:08:17 - 0:08:18]: The driver exits the van into the dimly lit garage. The player character, dressed in striped clothes, steps out of the vehicle. [0:08:19]: Walking around the van, the character stops partially hidden behind the van's side. [0:08:20]: The character approaches another person standing nearby. The interface opens to show an inventory screen with various items and storage options visible.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What did the driver do just now?",
                "time_stamp": "00:08:15",
                "answer": "A",
                "options": [
                    "A. Exiting the van.",
                    "B. Entering the van.",
                    "C. Turning on the headlights.",
                    "D. Navigating the alley."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_278_real.mp4"
    },
    {
        "time": "0:09:40 - 0:09:45",
        "captions": "[0:09:40 - 0:09:45] [0:09:40 - 0:09:41]: The video shows a first-person perspective of someone playing a driving video game at night. The camera is positioned to show the road ahead and a portion of the right side sidewalk. The urban environment is illuminated by streetlights, building lights, and headlights from the vehicle and other cars. In the top left corner, a small window shows a person playing the game, noticeable from their focused expression and gaming setup. The vehicle's rear lights are prominently visible as red outlines in a rectangular shape. [0:09:41 - 0:09:43]: The vehicle continues moving down the road. The surroundings include palm trees on the left side and tall buildings on both sides of the street. Some windows in the buildings are lit, adding to the nighttime ambiance. The road appears slightly wet, reflecting the lights over the ground, suggesting either recent rain or a general reflective surface. [0:09:43 - 0:09:44]: As the vehicle moves forward, the player keeps a steady hand on the controls. On the right side of the image, light poles and a few small trees can be seen on the sidewalk. The digital dashboard displays current speed and other driving-related information, shown in white text. [0:09:44]: The vehicle approaches an intersection with traffic lights ahead. Red traffic lights and street signs become more visible, indicating the need to stop or slow down. The player's focus remains on the screen, ensuring accurate navigation through the urban streets. [0:09:45]: The vehicle reaches the intersection, showing a clearer view of the traffic lights now turned red. Another car is visible on the left side, ready to cross the intersection. The player remains engaged with the game, reacting to the traffic signals and planning the next move. The surroundings maintain the same urban nighttime environment with illuminated buildings and slightly wet roads.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the player focusing on right now?",
                "time_stamp": "00:09:43",
                "answer": "A",
                "options": [
                    "A. Driving a truck on the road.",
                    "B. Adjusting the game settings.",
                    "C. Quiting the game.",
                    "D. Watching a cut scene."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_278_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a black screen. [0:00:01 - 0:00:05]: The video reveals a scene in an outdoor plaza on a sunny day. Prominently featured is a large metallic globe sculpture with golden letters spelling out \"UNIVERSAL\" encircling it. The globe is set inside a circular water feature with mist rising around its base. At least two people are visible in the scene; one person is capturing a photo or video using their phone, and another person is walking to the left of the globe. The background features several palm trees and street lamps. [0:00:06 - 0:00:10]: The scene remains largely the same. However, one person, dressed in black and standing to the right of the globe, is more visible now and seems to be posing for a picture, looking towards the camera. The person who was walking before is now closer to the left side of the frame. The mist from the water feature continues to rise around the globe. [0:00:11 - 0:00:13]: The camera angle begins to shift to the right, providing a broader view of the plaza. The focus moves away from the globe sculpture to reveal more people in the background, walking around the area. The red carpet starts becoming visible in later frames. [0:00:14 - 0:00:16]: The scene captures more of the walkway, bordered by palm trees, leading towards an arched entrance in the distance. Two signs with arrows indicating \"ALREADY HAVE TICKETS?\" and \"PURCHASE TICKETS\" are visible. Several people are present, walking along the pathway. The red carpet extends further, leading towards the arched entrance. [0:00:17 - 0:00:20]: The camera continues to shift right, fully showing a wide red carpet leading towards the arched entrance of a building in the distance. More pedestrians are walking along, taking in the surroundings. The streetlights and palm trees line the path, providing a picturesque and welcoming entrance. The overall scene suggests an inviting and crowded area, with people likely visiting for entertainment purposes.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What prominent feature is located in the center of the outdoor plaza?",
                "time_stamp": "0:00:05",
                "answer": "B",
                "options": [
                    "A. A large fountain.",
                    "B. A metallic globe sculpture.",
                    "C. A statue of a person.",
                    "D. A giant screen."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Text-Rich Understanding",
                "question": "What word is encircling the large metallic globe sculpture?",
                "time_stamp": "0:00:10",
                "answer": "A",
                "options": [
                    "A. UNIVERSAL.",
                    "B. GLOBAL.",
                    "C. EARTH.",
                    "D. WORLD."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_308_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:42]: People walk along a paved pathway in a theme park setting, featuring various themed buildings designed to look like a small-town street. On the left, a group of people walks toward the camera while on the right, a child in a pink shirt moves away. [0:02:43 - 0:02:45]: The scene shows the exteriors of colorful, stylized buildings, including a green storefront and a red caf\u00e9 named \"Power Up Caf\u00e9\" with a yellow question mark sign above the door. [0:02:46 - 0:02:48]: Additional elements of the setting include a sign for \"Stage 52\" on the right side and the backgrounds feature thematic posters and decorations, creating an immersive environment. [0:02:49 - 0:02:52]: The pathway is relatively empty except for occasional individuals moving through the scene. The \"Power Up Caf\u00e9\" continues to be a prominent feature, with its bright signage and adjacent red umbrella. [0:02:53 - 0:02:57]: Two people are standing near the entrance of the \"Power Up Caf\u00e9.\" One individual, wearing yellow, interacts with the door while another, dressed in red, faces inward. The background highlights more details of the caf\u00e9, including posters and decorative elements. [0:02:58 - 0:02:59]: A closer view of the \"Power Up Caf\u00e9\" shows specific details like a large billboard featuring \"Greetings from New Donk City\" and familiar-themed graphics that enhance the caf\u00e9's playful and vibrant aesthetic.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What was the name of the caf\u00e9 shown just now?",
                "time_stamp": "00:03:04",
                "answer": "C",
                "options": [
                    "A. The Green Caf\u00e9.",
                    "B. The Fun Caf\u00e9.",
                    "C. Power Up Caf\u00e9.",
                    "D. Stage 52 Caf\u00e9."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_308_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:29]: The video depicts a first-person perspective as the viewer walks down a street. The street is lined with various buildings on either side, displaying an urban setting with a mix of architectural styles. On the left, there is a laundromat with a bright blue storefront, with other shops and a fire escape above it. People walk on the zebra crossing and sidewalks, including a man pushing a stroller and others walking in groups. On the right, buildings display beige facades with detailed window frames and green awnings. The street is bustling with pedestrians, some wearing backpacks, children accompanying parents, and a mix of casual summer clothing. The sky is partly cloudy. Further ahead, more structures of different colors, including a pastel blue building and others with peach and green accents, are visible. [0:05:30 - 0:05:39]: The viewer continues to walk down the street, passing more shops and people. A building with a green awning reads \"MacGuff Cinema,\" and people gather around the area. Across the street, patrons sit under green umbrellas at a cafe. The background reveals more of the street, with buildings displaying classic facades and balconies, and people engaged in various activities. The video continues to show the continuous movement of the crowd and street features, maintaining the urban atmosphere and lively environment. The flora includes some potted plants and small trees along the road.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is one of the activities that people were doing on the zebra crossing just now?",
                "time_stamp": "0:05:29",
                "answer": "B",
                "options": [
                    "A. Jogging.",
                    "B. Pushing a stroller.",
                    "C. Riding a bicycle.",
                    "D. Selling goods."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_308_real.mp4"
    },
    {
        "time": "[0:08:00 - 0:09:00]",
        "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:03]: A staircase with wooden steps and railings on both sides is seen. The left wall is plain, while the right wall is made of red bricks. A person is visible near the top of the stairs, who moves away from the stairs. [0:08:04 - 0:08:08]: Reaching the top of the stairs, the layout transitions into a hallway with wooden flooring. A door with a blue color is on the right, opening into an adjacent room. [0:08:09 - 0:08:10]: Entering a room with brick walls, containing a small dining area with a wooden table and metal chairs. In the background, a kitchen area is visible, with people inside and various kitchen appliances. [0:08:11 - 0:08:14]: The scene moves towards a section with a bookshelf filled with books and decorative objects. Then, it progresses towards a room with a baby gate and people walking in and out. [0:08:15 - 0:08:19]: Entering a children's room, decorated with playful elements. The room features a bed, a small table, shelves with toys and books, colorful curtains, and decorative wall art. The presence of a green dinosaur-shaped lamp adds to the room's kid-friendly ambiance.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What material are the stairs made of?",
                "time_stamp": "00:08:03",
                "answer": "B",
                "options": [
                    "A. Metal.",
                    "B. Wood.",
                    "C. Marble.",
                    "D. Concrete."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_308_real.mp4"
    },
    {
        "time": "0:00:00 - 0:00:20",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: Several Christmas ornaments are showcased closely. The first ornament, prominently in the foreground, is a vivid blend of pink, blue, and gold hues with the text \"JINGLE bells\" inscribed in white. Surrounding it are other ornaments partially visible, with similarly vibrant mixtures of colors. The background is decorated with festive greenery. [0:00:04 - 0:00:06]: The view expands to show three ornaments fully. Positioned in the center is the \"JINGLE bells\" design, while to its left, an ornament reads \"Joyeux Noel\" in white cursive against a dark blue and gold mix. To the right, an ornament reads \"Merry Christmas\" in white cursive with a blue and purple base highlighted with gold. These ornaments are arranged among green pine branches adorned with red berries and golden ribbons. [0:00:07 - 0:00:08]: The frame becomes darker and transitions to a title screen. The text \"ALCOHOL INK ORNAMENTS\" appears in white on a solid black background. [0:00:09 - 0:00:10]: The title screen remains the same, with \"ALCOHOL INK ORNAMENTS\" still displayed. [0:00:11 - 0:00:13]: A new text appears against a soft pink background, stating, \"Templates and Materials used are listed in the description.\" [0:00:14]: A pair of hands wearing black gloves is seen holding a can of Cabot's Cabothane Clear. The text \"For this project you will need:\" appears at the bottom of the frame. [0:00:15]: The hands firmly grasp the top and bottom of the can, showcasing it to the camera. The design of the can prominently features yellow and blue colors with detailed text and branding. [0:00:16 - 0:00:19]: The hands continue holding the can as the text \"Water based Polyurethane\" appears below the can image.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What are the prominent colors of the ornament in the middle of the video?",
                "time_stamp": "00:00:03",
                "answer": "A",
                "options": [
                    "A. Pink, blue, and gold.",
                    "B. Red, green, and silver.",
                    "C. Black, white, and blue.",
                    "D. Yellow, orange, and purple."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_48_real.mp4"
    },
    {
        "time": "0:01:40 - 0:02:00",
        "captions": "[0:01:40 - 0:02:00] [0:01:40 - 0:01:50]: A pair of hands wearing black gloves is holding a translucent, spherical ornament with a matte, slightly cloudy surface. The ornament is being held above a small, clear plastic cup that is situated on top of an open tin containing a golden rim. The background is a blue grid mat.  [0:01:50 - 0:01:56]: The hands continue to hold the ornament as it is being tilted slightly to one side, aligning the opening of the ornament with the cup in the tin below, suggesting the intention to pour out excess liquid from the ornament.  [0:01:56 - 0:01:58]: The view remains the same, with the ornament's opening now directly above the cup, continuing to suggest the pouring action without any visible liquid. The text on the frame reads \"Pour excess liquid back into the tin.\" [0:01:58 - 0:02:00]: The ornament remains in the same position, held steadily above the cup, ensuring that any residual liquid drains out completely. [0:02:00 - 0:02:05]: The process of pouring continues as the text changes to \"Let the ornament drain for 10 minutes,\" indicating that the ornament is to be left in this position for proper drainage. The hands maintain their grip on the ornament to prevent any movement.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What are the black-gloved hands doing right now?",
                "time_stamp": "0:02:00",
                "answer": "A",
                "options": [
                    "A. Holding the ornament steadily above the cup for drainage.",
                    "B. Placing the ornament on a table.",
                    "C. Shaking the ornament to remove excess liquid.",
                    "D. Filling the ornament with a new liquid."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_48_real.mp4"
    },
    {
        "time": "0:03:20 - 0:03:40",
        "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:22]: A person wearing black gloves holds a round glass ornament, decorated with vibrant swirls of blue, purple, pink, and gold. On a white cloth beneath the ornament, several small bottles of different colors are placed alongside some bottle caps. [0:03:22 - 0:03:24]: The gloved hands position the ornament closer to the camera while also holding a small bottle. [0:03:24 - 0:03:26]: The person starts applying the contents of the small bottle to the opening of the ornament. [0:03:26 - 0:03:28]: The hands carefully manipulate the ornament and the small bottle, spreading the liquid inside. [0:03:28 - 0:03:30]: The person closely examines the ornament, ensuring that the liquid inside it is evenly distributed. [0:03:30 - 0:03:32]: The ornament is placed back onto the white cloth while the person retrieves a blue funnel. [0:03:32 - 0:03:34]: The funnel is inserted into the ornament's opening. [0:03:34 - 0:03:35]: The person adjusts the funnel and prepares to use it. [0:03:35 - 0:03:40]: The person holds a pink container labeled \u201cGlitter,\u201d and begins pouring fine white glitter into the funnel. The glitter fills the gaps inside the ornament, adding a sparkling effect.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the person doing right now with the pink container labeled \"Glitter\"?",
                "time_stamp": "0:03:38",
                "answer": "A",
                "options": [
                    "A. Pouring fine white glitter into the funnel.",
                    "B. Sprinkling glitter on the white cloth.",
                    "C. Decorating the ornament with paint.",
                    "D. Mixing different colored liquids inside the ornament."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_48_real.mp4"
    },
    {
        "time": "0:05:00 - 0:05:20",
        "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:01]: Two black-gloved hands are holding a spherical object. The sphere has various colorful patterns, including blue, purple, and gold. One hand holds a bottle with a metallic cap, currently applying a golden liquid to the sphere's opening. The sphere is positioned above a white cloth on a blue surface, with a couple of other small bottles and a white container nearby. There is a grid visible on the blue surface in the background. [0:05:01 - 0:05:02]: The hands are continuing to apply the golden liquid from the bottle to the opening of the colorful sphere. The sphere has distinctive patterns that include areas of blue, purple, teal, and golden shades blending together. The white cloth underneath, on a grid-patterned blue background, remains in place. The additional bottles and a white container are still visible on the right side of the frame. [0:05:02 - 0:05:03]: The right hand is now holding the liquid bottle away from the sphere, which the left hand is rotating slightly, showing a full view of the colorful design, including patches of gold, blue, and purple colors. The white cloth below is still visible, laying on the blue grid-patterned background. The additional materials from the previous frames are placed to the right. [0:05:03 - 0:05:04]: Holding the sphere steady, the right hand tilts the bottle, with the golden cap still attached, closer to the opening of the sphere. The colors on the sphere, including blues, purples, and gold, remain vivid and intricate. The white cloth remains spread on the blue grid-patterned background, with additional items still in their previous positions. [0:05:04 - 0:05:05]: The right hand is pouring the golden liquid into the opening of the colorful sphere. The sphere's patterns are clear, with swirling blue, purple, and gold designs. The white cloth underneath continues to lie on the blue grid background. The surrounding objects remain unaltered, positioned slightly right of the sphere. [0:05:05 - 0:05:06]: The hands hold the sphere and the bottle in a similar manner to the previous frames. The golden liquid continues to be applied to the opening of the sphere. The intricate colorful patterns, including blue, purple, and gold, are clear on the sphere. The white cloth on the blue grid-patterned surface and the surrounding bottles maintain their positions. [0:05:06 - 0:05:07]: The hands slightly rotate the sphere while continuing to apply the golden liquid from the bottle into the opening. The sphere\u2019s colorful design remains prominent, showcasing swirls of blue, purple, and gold. The white cloth still lies on the blue grid background, with additional items on the right remaining in place. [0:05:07 - 0:05:08]: The right hand holds the liquid bottle slightly away from the sphere, which is being rotated by the left hand. The colorful patterns of the sphere, consisting of blue, purple, and gold, are still visible. The white cloth on the blue grid-pattern background and the nearby small bottles and container remain unchanged. [0:05:08 - 0:05:09]: The right hand brings the liquid bottle closer to the sphere's opening, continuing to pour the golden liquid. The left hand holds and slightly rotates the colorful sphere, showcasing the various shades of blue, purple, and gold. The white cloth under the sphere lies on the blue grid background, with the additional items standing to the right. [0:05:09 - 0:05:10]: The sphere is held steady, with the right hand positioning the bottle close to its opening. The intricate patterns of blue, purple, and gold on the sphere remain visible. The white cloth on the blue grid background and the surrounding bottles and container are still present in the same arrangement. [0:05:10 - 0:05:11]: The right hand continues to pour the golden liquid into the opening of the sphere, while the left hand holds it securely. The colorful patterns, blending blue, purple, and gold, are clear. The white cloth on the blue grid background, with the additional bottles and container to the right, remain in place. [0:05:11 - 0:05:12]: The hands rotate the sphere slightly while applying the golden liquid around its opening. The colorful patterns of the sphere, featuring blue, purple, and gold hues, are visible. The white cloth, spread on the blue grid-patterned surface, and the nearby bottles and container maintain their positions. [0:05:12 - 0:05:13]: The right hand tilts the bottle, applying more golden liquid near the sphere\u2019s opening. The colorful sphere, with",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What are the hands doing right now?",
                "time_stamp": "0:05:07",
                "answer": "A",
                "options": [
                    "A. Rotating the sphere while applying golden liquid.",
                    "B. Placing the sphere on a white cloth.",
                    "C. Cleaning the sphere with a cloth.",
                    "D. Removing the golden liquid bottle."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_48_real.mp4"
    },
    {
        "time": "0:06:40 - 0:07:00",
        "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:50]: A pair of gloved hands is seen holding a circular ornament in the left hand and a bottle of metallic-colored paint in the right hand. The ornament, predominantly pink and purple with a glossy finish, has two white spots near its surface. The background is a blue, grid-patterned work mat with several bottles of colored paints arranged neatly to the right side of the frame. A small transparent plastic cup is positioned between the hands, just below the ornament. The metallic-colored paint bottle is tilted towards the ornament, and some paint is applied on its surface.  [0:06:51 - 0:06:59]: The ornament is set down on the work mat, its colorful surface now adorned with additional paint. The pair of hands then picks up a bottle containing blue-colored paint. The bottle is held close to the ornament, and its cap is being unscrewed. The scene includes the same components, such as the work mat, the paint bottles, and the small plastic cup remaining in their positions on the right side of the frame.  [0:06:53 - 0:06:56]: The gloved hands begin to apply the blue paint to the ornament, introducing new streaks and patches of color. The ornament is rotated to ensure even coverage. The background layout remains the same, with the ornament taking center stage and the paint bottle being handled with precision. [0:06:57 - 0:06:59]: The paint bottle is set back on the work mat, and the ornament, now displaying a vibrant mixture of pink, purple, and blue hues, is closely examined. The focus is on the painted ornament, while the bottles of paint and the small plastic cup stay positioned to the right of the frame. The created design on the ornament becomes more intricate and defined with each application of paint.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What did the person do just now with the metallic-colored paint bottle?",
                "time_stamp": "00:06:50",
                "answer": "A",
                "options": [
                    "A. Applied paint to the ornament.",
                    "B. Shook the bottle vigorously.",
                    "C. Poured paint into a plastic cup.",
                    "D. Closed the bottle's cap."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_48_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:05]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the predominant vehicle type visible on the right side of the street right now?",
                "time_stamp": "00:00:11",
                "answer": "B",
                "options": [
                    "A. Red truck.",
                    "B. Yellow car.",
                    "C. White van.",
                    "D. Blue bicycle."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_398_real.mp4"
    },
    {
        "time": "[0:01:56 - 0:02:01]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the weather condition right now?",
                "time_stamp": "00:01:56",
                "answer": "C",
                "options": [
                    "A. Sunny.",
                    "B. Snowy.",
                    "C. Rainy.",
                    "D. Clear."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_398_real.mp4"
    },
    {
        "time": "[0:03:52 - 0:03:57]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Which type of establishment has a visible sign on the left side of the street right now?",
                "time_stamp": "00:03:52",
                "answer": "C",
                "options": [
                    "A. A cafe.",
                    "B. A library.",
                    "C. A tea room.",
                    "D. A bookstore."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_398_real.mp4"
    },
    {
        "time": "[0:05:48 - 0:05:53]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Which company logo is visible on a truck right now?",
                "time_stamp": "00:05:50",
                "answer": "B",
                "options": [
                    "A. FedEx.",
                    "B. Ryder.",
                    "C. UPS.",
                    "D. Amazon."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_398_real.mp4"
    },
    {
        "time": "[0:07:44 - 0:07:49]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Which establishment's sign is visible in the video right now?",
                "time_stamp": "00:07:47",
                "answer": "B",
                "options": [
                    "A. Starbucks.",
                    "B. Dunkin'.",
                    "C. McDonald's.",
                    "D. Subway."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_398_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:20]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color is the traffic light now?",
                "time_stamp": "00:00:02",
                "answer": "C",
                "options": [
                    "A. Yellow.",
                    "B. Red.",
                    "C. Green.",
                    "D. Black."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_184_real.mp4"
    },
    {
        "time": "[0:01:58 - 0:02:18]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the fenced area located right now?",
                "time_stamp": "00:02:03",
                "answer": "D",
                "options": [
                    "A. Only on the left side of the road.",
                    "B. Directly behind the cyclist.",
                    "C. Directly ahead of the cyclist.",
                    "D. On both sides of the road."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_184_real.mp4"
    },
    {
        "time": "[0:03:56 - 0:04:16]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the stop sign located right now?",
                "time_stamp": "00:03:54",
                "answer": "D",
                "options": [
                    "A. On the left side of the road.",
                    "B. In the middle of the crossroad.",
                    "C. Above the traffic lights.",
                    "D. On the right side of the road."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_184_real.mp4"
    },
    {
        "time": "[0:05:54 - 0:06:14]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is placed along the side of the road right now?",
                "time_stamp": "00:05:55",
                "answer": "D",
                "options": [
                    "A. Tree barriers.",
                    "B. Cones.",
                    "C. Metal fences.",
                    "D. Hay bales."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_184_real.mp4"
    },
    {
        "time": "[0:07:52 - 0:08:12]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is located in front of the cyclists right now?",
                "time_stamp": "00:08:07",
                "answer": "D",
                "options": [
                    "A. A finishing line.",
                    "B. A straight road.",
                    "C. A roundabout.",
                    "D. A sharp left turn."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_184_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:03]: A person wearing a teal shirt stands in front of a kitchen counter beginning to cook. Two frying pans rest on the stove, with the one on the left emitting a small amount of steam. A wooden shelf filled with glass containers and dishes decorates the white-tiled backsplash behind them. The person grasps a cauliflower head with both hands, positioning it in front of them;  [0:02:04 - 0:02:10]: The person lowers the cauliflower towards one of the frying pans on the stove, tilting it to a vertical position. Various kitchen utensils, bottles, and a cutting board are visible on the left side of the counter. They place the cauliflower into the pan with both hands;  [0:02:11 - 0:02:13]: An overhead view focuses on two frying pans\u2014the left one containing the cauliflower. The person's right hand picks up a pinch of seasoning or salt from a nearby container and sprinkles it over the cauliflower;  [0:02:14 - 0:02:16]: The person's left hand retrieves another pinch of seasoning or salt from the container and again sprinkles it over the cooking cauliflower;  [0:02:17 - 0:02:19]: After seasoning, the person steps aside to grab a small plate from the left side of the counter. The kitchen is equipped with an array of utensils, jars, and bottles neatly arranged near the stove.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What does the person in the teal shirt grasp with both hands?",
                "time_stamp": "0:02:04",
                "answer": "A",
                "options": [
                    "A. A cauliflower head.",
                    "B. A frying pan.",
                    "C. A cutting board.",
                    "D. A glass container."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Action Recognition",
                "question": "What does the person do after picking up a pinch of seasoning or salt?",
                "time_stamp": "0:02:18",
                "answer": "D",
                "options": [
                    "A. Places it on the cutting board.",
                    "B. Puts it back in the container.",
                    "C. Hands it to someone else.",
                    "D. Sprinkles it over the cauliflower."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_38_real.mp4"
    },
    {
        "time": "[0:06:00 - 0:07:00]",
        "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:05]: A person is seen from a first-person perspective, holding a small bottle of golden liquid above a glass mixing bowl placed on a wooden cutting board. The person is pouring the liquid into the bowl containing chopped green ingredients while stirring with a spoon. The setting is a kitchen with a stove and a saucepan on the left side. Cabinets in blue and wooden tones are visible in the background.  [0:06:06 - 0:06:12]: The scene zooms out, showing more of the kitchen and the person. There are various kitchen items on the countertop, such as plates, and small glass containers holding spices. The person is continuing to pour the golden liquid into the mixing bowl and stirring. The background reveals a brick wall with shelves holding kitchen utensils and ingredients.  [0:06:13 - 0:06:15]: The view shifts to an overhead perspective, showing the person stirring the mixture in the glass bowl placed on the wooden cutting board with other ingredients like butter and pepper in small bowls positioned around it.  [0:06:16 - 0:06:18]: The person adds another ingredient from a small container into the mixing bowl and continues mixing. The overhead perspective provides a clear view of the various ingredients and utensils arranged around the workspace.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the person holding above the glass mixing bowl?",
                "time_stamp": "0:06:06",
                "answer": "A",
                "options": [
                    "A. A small bottle of golden liquid.",
                    "B. A jar of honey.",
                    "C. A bottle of olive oil.",
                    "D. A cup of vinegar."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Object Recognition",
                "question": "Which utensil is the person using to stir the mixture?",
                "time_stamp": "0:06:15",
                "answer": "A",
                "options": [
                    "A. Spoon.",
                    "B. Fork.",
                    "C. Whisk.",
                    "D. Spatula."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Clips Summarize",
                "question": "What is the overall activity taking place in the kitchen?",
                "time_stamp": "0:06:18",
                "answer": "A",
                "options": [
                    "A. Making a delicate cauliflower dish.",
                    "B. Baking a cake.",
                    "C. Making a sandwich.",
                    "D. Cooking pasta."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_38_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:05]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Which famous fast-food chain's logo can be seen on the left side of the street right now?",
                "time_stamp": "00:00:04",
                "answer": "D",
                "options": [
                    "A. Starbucks.",
                    "B. McDonald's.",
                    "C. Subway.",
                    "D. Chipotle."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_402_real.mp4"
    },
    {
        "time": "[0:01:58 - 0:02:03]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the color of the traffic light right now?",
                "time_stamp": "00:01:03",
                "answer": "D",
                "options": [
                    "A. Red.",
                    "B. Yellow.",
                    "C. Blue.",
                    "D. Green."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_402_real.mp4"
    },
    {
        "time": "[0:03:56 - 0:04:01]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is the name of the parking facility visible on the street sign right now?",
                "time_stamp": "00:03:56",
                "answer": "D",
                "options": [
                    "A. Central Parking.",
                    "B. Manhattan Parking.",
                    "C. New York Parking.",
                    "D. Lincoln Center Parking."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_402_real.mp4"
    },
    {
        "time": "[0:05:54 - 0:05:59]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What type of weather condition is being experienced right now?",
                "time_stamp": "00:05:57",
                "answer": "D",
                "options": [
                    "A. Sunny and clear.",
                    "B. Snowy and blowing.",
                    "C. Foggy and misty.",
                    "D. Rainy and wet."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_402_real.mp4"
    },
    {
        "time": "[0:07:52 - 0:07:57]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "Which type of vehicle can be seen parked on the left side of the street right now?",
                "time_stamp": "00:07:53",
                "answer": "C",
                "options": [
                    "A. A truck.",
                    "B. A bicycle.",
                    "C. A white car.",
                    "D. A motorcycle."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_402_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:20]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the green banner located right now?",
                "time_stamp": "00:00:04",
                "answer": "D",
                "options": [
                    "A. Hanging above the cyclists.",
                    "B. On the left side of the road.",
                    "C. Directly behind the cyclists.",
                    "D. On the both sides of the road."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_179_real.mp4"
    },
    {
        "time": "[0:02:01 - 0:02:21]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the flag located right now?",
                "time_stamp": "00:02:18",
                "answer": "D",
                "options": [
                    "A. Hanging on the left side of the road.",
                    "B. Hanging above the cyclists.",
                    "C. Hanging behind the cyclists.",
                    "D. Hanging on the right side of the road."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_179_real.mp4"
    },
    {
        "time": "[0:04:02 - 0:04:22]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the cyclist located relative to the National Championships banner right now?",
                "time_stamp": "00:04:05",
                "answer": "D",
                "options": [
                    "A. Far down the road from the banner.",
                    "B. Before reaching the banner.",
                    "C. Turned away from the banner.",
                    "D. Just crossing underneath the banner."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_179_real.mp4"
    },
    {
        "time": "[0:06:03 - 0:06:23]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is the position of the tall building with many windows right now?",
                "time_stamp": "00:06:09",
                "answer": "D",
                "options": [
                    "A. On the left side of the road.",
                    "B. Directly in front of the cyclists.",
                    "C. Behind the cyclists.",
                    "D. On the right side of the road."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_179_real.mp4"
    },
    {
        "time": "[0:08:04 - 0:08:24]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is the current positioning of the National Championships banner right now?",
                "time_stamp": "00:08:17",
                "answer": "D",
                "options": [
                    "A. Behind the cyclists.",
                    "B. On the left side of the road.",
                    "C. On the right side of the road.",
                    "D. Directly above the cyclists."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_179_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:05]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is noticeable about the item emitting smoke on the right side of the street?",
                "time_stamp": "00:00:05",
                "answer": "C",
                "options": [
                    "A. It is completely covered in white paint.",
                    "B. It has black stripes.",
                    "C. It is orange and white with caution tapes around it.",
                    "D. It is blue and white with warning signs around it."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_396_real.mp4"
    },
    {
        "time": "[0:01:35 - 0:01:40]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What type of vehicle is on the left-hand side traveling through the intersection right now?",
                "time_stamp": "00:01:35",
                "answer": "B",
                "options": [
                    "A. Sedan.",
                    "B. SUV.",
                    "C. Taxi.",
                    "D. Motorcycle."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_396_real.mp4"
    },
    {
        "time": "[0:03:10 - 0:03:15]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the traffic light color right now?",
                "time_stamp": "00:03:10",
                "answer": "C",
                "options": [
                    "A. Red.",
                    "B. Yellow.",
                    "C. Green.",
                    "D. Blinking."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_396_real.mp4"
    },
    {
        "time": "[0:04:45 - 0:04:50]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the traffic light color right now?",
                "time_stamp": "00:04:36",
                "answer": "A",
                "options": [
                    "A. Red.",
                    "B. Yellow.",
                    "C. Green.",
                    "D. Blinking."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_396_real.mp4"
    },
    {
        "time": "[0:06:20 - 0:06:25]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the man wearing on his head right now?",
                "time_stamp": "00:06:22",
                "answer": "C",
                "options": [
                    "A. A red cap.",
                    "B. A white hat.",
                    "C. A blue cap.",
                    "D. A black hat."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_396_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:20]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The scene starts with an individual, dressed in a dark blue denim shirt and white t-shirt, seated behind a desk with a black laptop bearing a green emblem in front of them. He is leaning slightly forward, hands gestured in front, seemingly speaking to the camera.  [0:00:01 - 0:00:03]: The individual is seen continuing his explanation. The background has a geometric blue pattern, and there are shelves on both sides with decorative items like awards, books, and devices. [0:00:03]: The screen turns mostly black with a small green highlighted rectangle containing three vertical circles. [0:00:04 - 0:00:07]: A close-up of a laptop screen displaying a futuristic cityscape with sci-fi elements. Several parts are highlighted in neon green lines focused from left to right on different laptop components and performance metrics. [0:00:08 - 0:00:12]: A split-screen comparison between two graphics cards, labeled \"RTX 3080\" and \"RTX 3070,\" accompanied by their respective diagrams and computational capabilities. Then shifts to a more detailed internal structure of GPU components labeled \"GA104\" and \"GA102\". [0:00:13 - 0:00:14]: Continues to show a detailed comparison between \"GA104\" and \"GA102\" with additional labels and metrics, focusing on the differences in GPU build and performance. [0:00:15 - 0:00:16]: Comparison of two laptop models. On the left, features including \"RTX 3080 Ti\", \"GA103\", \"16GB GDDR6\", \"90-150W\" are shown. On the right, the features of \"RTX 3070 Ti\", \"GA104\", \"8GB GDDR6\", \"80-125W\" are highlighted. A laptop showing a high-action war video game scene on the display is in the foreground. [0:00:17 - 0:00:20]: The comparison and graphics details continue with the gaming laptop remaining in focus. The specifications of \"RTX 3080 Ti\" and \"RTX 3070 Ti\" appear in green text on the left, indicating the types and performance levels of the graphics cards. The action game display remains unchanged.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Which graphics cards are being compared right now?",
                "time_stamp": "00:00:20",
                "answer": "A",
                "options": [
                    "A. RTX 3080 Ti and RTX 3070 Ti.",
                    "B. RTX 3060 and RTX 3070 Ti.",
                    "C. RTX 3080 Ti and RTX 3090.",
                    "D. RTX 3070 and RTX 3080 Ti."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_112_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:03:20]",
        "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:04]: The video starts with a computer screen displaying various performance monitoring software. These include readings of CPU and GPU usage, along with a list of system processes and their temperatures. There is a graph showing CPU usage over time and a separate window displaying GPU temperatures and power consumption. [0:03:05 - 0:03:08]: The video transitions to a screen with text in Chinese characters and two measurements: \"CPU Package Power: 44.996 W\" and \"GPU Board Power Draw: 150.4 W.\" [0:03:09 - 0:03:11]: The screen remains static with the same information displayed regarding CPU and GPU power consumption. [0:03:12 - 0:03:14]: The video shows a man sitting at a desk with a laptop displaying the Razer logo. He is mid-action, gesturing with his hands while speaking. [0:03:15 - 0:03:16]: The man at the desk has stopped gesturing and is now clasping his hands together while continuing to speak. Various objects are visible on the shelves behind him. [0:03:17 - 0:03:18]: The man at the desk continues to speak calmly while resting one hand on the laptop, maintaining a focus on the camera. [0:03:19 - 0:03:20]: The video shows a different screen labeled \"3DMark Ti\" and \"3DMark Time Spy,\" displaying comparison benchmarks between \"RTX 3080 Ti\" and \"RTX 3080.\" The benchmarks show performance scores for the two graphics cards with accompanying bar graphs displaying relative performance levels.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Which graphics cards are being compared right now?",
                "time_stamp": "00:03:11",
                "answer": "A",
                "options": [
                    "A. RTX 3080 Ti and RTX 3080.",
                    "B. RTX 3060 and RTX 3070 Ti.",
                    "C. RTX 3080 Ti and RTX 3090.",
                    "D. RTX 3070 and RTX 3080 Ti."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_112_real.mp4"
    },
    {
        "time": "[0:06:00 - 0:06:20]",
        "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:02]: The video starts with a static image of a benchmarking screen. The screen features performance comparisons between two graphics cards: the RTX 3080 Ti and the RTX 3080. The benchmark results display metrics in 2K and 4K resolutions for a game. Numerical values for FPS, clock speed, temperature, power, and CPU usage are shown on the right side for each card. The background of the screen shows a blurry game scene with some indistinguishable objects. [0:06:03 - 0:06:04]: The next sequence features a man in a blue shirt and glasses seated at a desk with a black laptop in front of him. He appears to be presenting or talking about the laptop, gesturing with his hands as he speaks. Behind him is a wall with a geometric blue pattern, and shelves holding various items like a globe, speakers, and awards. [0:06:05 - 0:06:08]: The man continues to talk, shifting his gaze occasionally between the laptop and the camera. His facial expressions change as he explains something, indicating engagement with his presentation. The background remains the same with slight variations in his posture and gestures. [0:06:09 - 0:06:13]: The scene transitions to an in-game view from the God of War video game. The camera follows the main character, a muscular man with a bald head, wearing Norse-inspired armor and carrying an axe. He walks through a rugged, snowy landscape, crossing a river. Water splashes around, and rocky terrain is visible in the background. [0:06:14 - 0:06:18]: The character continues to navigate through the environment, making his way towards a large cave entrance. He climbs into a small boat and rows forward, illuminated by a torchlight that adds a warm glow contrasting with the cold surroundings. The graphics showcase detailed textures and dynamic lighting effects in the game environment. [0:06:19 - 0:06:20]: As the character rows deeper into the cave, the surrounding environment becomes darker, with stalactites hanging from the ceiling and light reflections on the water surface. The character's movements are smooth, and the overall ambiance suggests exploration and adventure. The screen also displays performance metrics similar to those shown earlier in the benchmarking segment.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What metrics are displayed on the benchmarking screen for each graphics card right now?",
                "time_stamp": "00:06:00",
                "answer": "A",
                "options": [
                    "A. FPS, clock speed, temperature, power, and CPU usage.",
                    "B. Memory usage, CPU temperature, and power draw.",
                    "C. Ping rate, jitter, and packet loss.",
                    "D. Frame time, GPU load, and VRAM usage."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_112_real.mp4"
    },
    {
        "time": "[0:09:00 - 0:09:20]",
        "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:02]: The video displays a bar chart with the title \"\u300a\u8352\u91ce\u5927\u9556\u5ba2\uff1a\u6551\u8d4e2\u300b\u6e38\u620f\u5e73\u5747\u5e27\" (Red Dead Redemption 2 - Game Average FPS). Three graphics card setups are compared in terms of maximum frame rates (fps) using 2K+DLSS and 4K+DLSS settings. The RTX 3080 Ti with Intel i9 12900K and 32G DDR5 4800 achieves 125 fps at 2K+DLSS and 91 fps at 4K+DLSS. The RTX 3080 Ti in the Razer Blade 17 setup records 91 fps at 2K+DLSS and 66 fps at 4K+DLSS. The RTX 3080 in the Lenovo Y900K setup achieves 84 fps at 2K+DLSS and 58 fps at 4K+DLSS.  [0:09:03 - 0:09:08]: The scene remains consistent, with the same bar chart and data displayed, focusing on clarity and readability. The background has a gradient effect, adding a polished appearance to the graphical data presentation. [0:09:09 - 0:09:12]: The video transitions to another bar chart, this one with the title \"\u300a\u6218\u795e4\u300b\u6e38\u620f\u5e73\u5747\u5e27\" (God of War 4 - Game Average FPS). In this chart, the RTX 3080 Ti with Intel i9 12900K and 32G DDR5 4800 achieves the highest frame rates, recording 135 fps at 2K+DLSS and 101 fps at 4K+DLSS. The RTX 3080 Ti in the Razer Blade 17 setup records 91 fps at 2K+DLSS and 66 fps at 4K+DLSS. The RTX 3080 in the Lenovo Y900K setup achieves 87 fps at 2K+DLSS and 63 fps at 4K+DLSS.  [0:09:13 - 0:09:17]: This segment continues to display the same bar chart and data for \"God of War 4\", maintaining focus on the comparative performance of the different setups. The background also includes a blurred depiction of scenes from the game, giving context to the performance data. [0:09:18]: The video transitions to another benchmark comparison chart titled \"\u300a\u6781\u9650\u7ade\u901f\uff1a\u5730\u5e73\u7ebf5\u300b\u6e38\u620f\u5e73\u5747\u5e27\" (Forza Horizon 5 - Game Average FPS). In this chart, the RTX 3080 Ti with Intel i9 12900K and 32G DDR5 4800 achieves 109 fps at 2K+DLSS and 82 fps at 4K+DLSS. The Razer Blade 17 setup records 78 fps at 2K and 57 fps at 4K. The Lenovo Y900K setup achieves 78 fps at 2K and 58 fps at 4K. The background of this chart includes an image of a racing car, giving context to the gaming performance data provided.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What frame rate does the RTX 3080 Ti with Intel i9 12900K and 32G DDR5 4800 achieve for \"God of War 4\" at 4K+DLSS right now?",
                "time_stamp": "00:09:20",
                "answer": "A",
                "options": [
                    "A. 101 fps.",
                    "B. 135 fps.",
                    "C. 91 fps.",
                    "D. 87 fps."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_112_real.mp4"
    },
    {
        "time": "[0:11:00 - 0:11:17]",
        "captions": "[0:11:00 - 0:11:17] [0:11:00 - 0:11:05]: The video starts with a close-up shot of a computer motherboard, focusing on the NVIDIA GPU which is surrounded by several small components like capacitors and resistors. The GPU is rectangular with a shiny silver surface that displays the NVIDIA logo and some specifications. Nearby, a secondary chip can be seen within a red frame, and several circular mounting points are positioned around the components. The motherboard has a dark blue or black background with detailed circuit patterns. [0:11:05 - 0:11:08]: The scene shifts to a man sitting at a desk in a modern, well-lit room. He is wearing glasses and a denim jacket over a graphic t-shirt. He appears to be explaining something, gesturing with his hands while a laptop with a green logo on the back is open in front of him. The background has a blue geometric pattern with shelves on either side, holding various items like books, speakers, and a helmet. [0:11:08 - 0:11:13]: The man continues speaking, emphasizing his points with hand gestures. His expression is attentive, and he occasionally looks directly at the camera. Overlay graphics appear on the screen showing various icons, such as a thumbs-up button, a coin with a slash through it, a star, and an arrow, suggesting options like liking, voting, saving, and sharing the content. These graphics move across the screen as he speaks. [0:11:13 - 0:11:17]: The video transitions to an animated outro featuring a spinning logo and Chinese characters. The logo resembles a gear and a piece of electronic equipment, glowing in white against a black background filled with small, star-like dots. The words next to the logo also glow brightly, pulsating as the logo moves. The outro gives a dynamic and energetic feel, suggesting the end of the content.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What appears in the overlay graphics on the screen right now?",
                "time_stamp": "00:11:13",
                "answer": "A",
                "options": [
                    "A. User interaction options.",
                    "B. Weather icons.",
                    "C. Application logos.",
                    "D. Navigation tools."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_112_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:10]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the content of the video just shown?",
                "time_stamp": "00:00:10",
                "answer": "A",
                "options": [
                    "A. An individual adjusts the settings on a coffee grinder, preparing it for use.",
                    "B. An individual sets the timer on a toaster oven, ready to bake.",
                    "C. An individual calibrates an oven's temperature for baking pastries.",
                    "D. An individual measures the exact amount of coffee grounds for a recipe."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_361_real.mp4"
    },
    {
        "time": "[0:01:55 - 0:02:05]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "What is the best summary of the actions performed just now?",
                "time_stamp": "00:02:05",
                "answer": "A",
                "options": [
                    "A. The individual prepared a coffee by grinding the beans and operating an espresso machine.",
                    "B. The individual assembled a sandwich with various ingredients and served it to a customer.",
                    "C. The individual organized a stationary set, placing pens and pencils in a container.",
                    "D. The individual arranged some tools on a workbench, preparing for a repair task."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_361_real.mp4"
    },
    {
        "time": "[0:03:50 - 0:04:00]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the content of the video just shown?",
                "time_stamp": "0:04:00",
                "answer": "A",
                "options": [
                    "A. A person is preparing for steaming the milk.",
                    "B. A barista prepares and serves a complex coffee drink, adjusting various machines and ingredients to ensure perfection.",
                    "C. An individual prepares various ingredients for a large meal, organizing and setting them on a counter.",
                    "D. A chef expertly places different kitchen tools in an organized manner to prepare for a cooking session."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_361_real.mp4"
    },
    {
        "time": "[0:05:45 - 0:05:55]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions of the individual just now?",
                "time_stamp": "0:05:55",
                "answer": "A",
                "options": [
                    "A. The individual ground coffee beans, tamped the coffee grounds, and prepared an espresso shot.",
                    "B. The individual operated a juicer, filled a glass with orange juice, and served it to a customer.",
                    "C. The individual brewed a pot of tea, poured it into cups, and arranged cookies on a plate.",
                    "D. The individual prepared a milkshake, added toppings, and served it with a straw."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_361_real.mp4"
    },
    {
        "time": "[0:07:40 - 0:07:50]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the action just taken by the individual?",
                "time_stamp": "00:07:50",
                "answer": "A",
                "options": [
                    "A. The individual is steaming the milk.",
                    "B. The individual is arranging paper cups for use.",
                    "C. The individual is preparing a drink by adding a mixer.",
                    "D. The individual is cleaning a stainless steel jug."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_361_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:30]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why are they two people shaking hands now?",
                "time_stamp": "0:01:57",
                "answer": "B",
                "options": [
                    "A. Because they just reached an important business agreement.",
                    "B. Because they appeared together in a photo when they were young.",
                    "C. Because they are meeting for the first time after many years.",
                    "D. Because they are congratulating each other on a job well done."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_238_real.mp4"
    },
    {
        "time": "[0:02:09 - 0:02:39]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why is Mr. Bean very angry now?",
                "time_stamp": "00:03:00",
                "answer": "A",
                "options": [
                    "A. Because another person ate all the food in the refrigerator.",
                    "B. Because someone spilled a drink on his favorite chair.",
                    "C. Because his television stopped working during his favorite show.",
                    "D. Because he just realized he lost his wallet."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_238_real.mp4"
    },
    {
        "time": "[0:04:18 - 0:04:48]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why does the person in the smaller bed become annoyed and awake?",
                "cause": "The person with a larger build interacts with the other person.",
                "effect": "The person in the smaller bed becomes visibly annoyed and awake.",
                "time_stamp": "0:04:28",
                "answer": "B",
                "options": [
                    "A. Because the person with a larger build speaks loudly.",
                    "B. Because the snoring sound from sleeping on the ground is too loud.",
                    "C. Because the person in the smaller bed has a nightmare.",
                    "D. Because a noise from outside wakes them up."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_238_real.mp4"
    },
    {
        "time": "[0:06:27 - 0:06:57]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why is Mr. Bean very angry now?",
                "time_stamp": "0:06:42",
                "answer": "B",
                "options": [
                    "A. Because someone borrowed his car without asking.",
                    "B. Because another person also ate the ice cream that belonged to Mr. Bean.",
                    "C. Because his favorite TV show was canceled.",
                    "D. Because his favorite shirt got ruined in the wash."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_238_real.mp4"
    },
    {
        "time": "[0:08:36 - 0:09:06]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why does Mr. Bean want to escape?",
                "time_stamp": "0:10:17",
                "answer": "B",
                "options": [
                    "A. Because he accidentally broke something in the store.",
                    "B. Because he doesn't want to pay the bill.",
                    "C. Because he saw someone he is trying to avoid.",
                    "D. Because he is late for an important appointment."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_238_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:05]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the color of the bus zone sign visible right now?",
                "time_stamp": "00:00:04",
                "answer": "A",
                "options": [
                    "A. Red and white.",
                    "B. Blue and white.",
                    "C. Green and white.",
                    "D. Black and yellow."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_453_real.mp4"
    },
    {
        "time": "[0:03:20 - 0:03:25]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the color of the traffic light right now?",
                "time_stamp": "00:03:19",
                "answer": "A",
                "options": [
                    "A. Green.",
                    "B. Yellow.",
                    "C. Red.",
                    "D. Blue."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_453_real.mp4"
    },
    {
        "time": "[0:06:40 - 0:06:45]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What can be identified about the current traffic light status for vehicles?",
                "time_stamp": "00:06:42",
                "answer": "C",
                "options": [
                    "A. Red light.",
                    "B. Yellow light.",
                    "C. Green light.",
                    "D. Flashing light."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_453_real.mp4"
    },
    {
        "time": "[0:10:00 - 0:10:05]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is the time displayed on the dashboard right now?",
                "time_stamp": "00:10:06",
                "answer": "B",
                "options": [
                    "A. 7:10:30.",
                    "B. 7:11:02.",
                    "C. 7:12:15.",
                    "D. 7:11:45."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_453_real.mp4"
    },
    {
        "time": "[0:13:20 - 0:13:25]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is the current speed limit for this road?",
                "time_stamp": "00:13:24",
                "answer": "C",
                "options": [
                    "A. 50.",
                    "B. 60.",
                    "C. 70.",
                    "D. 80."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_453_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: A white rectangular shape and a round white clock with a fork and knife at the center are displayed on a blue background. [0:00:01 - 0:00:03]: A person in a white chef's coat stands with arms crossed in front of a blue background, with the words \"RAMSAY in 10\" displayed prominently in bold red letters. [0:00:04 - 0:00:06]: The scene changes to a modern kitchen with blue cabinets, glass-fronted upper cabinets, a tiled backsplash, and various kitchen appliances such as a microwave, toaster, and coffee maker on the counter. There are also books and utensils. [0:00:05 - 0:00:07]: In the foreground, a steel countertop with a stove has two large burners and a frying pan on top, along with a griddle. The camera angle changes slightly, showing a hand moving into the frame on the left side. [0:00:07 - 0:00:08]: The person wearing a navy blue t-shirt enters the frame, standing beside the stove, and appears to be engaging with the camera, possibly speaking. [0:00:08 - 0:00:11]: The person is leaning forward, closer to the camera, showing a closer view of the stove and kitchen behind. They seem animated and expressive, indicating a dynamic interaction. [0:00:09 - 0:00:13]: The camera captures the person pausing momentarily, with the text \"Previously Recorded Live\" appearing at the top right of the screen. [0:00:13 - 0:00:14]: The person resumes speaking and moves slightly, gesturing with their hands. The kitchen setting remains consistent in the background. [0:00:15 - 0:00:17]: They continue talking, occasionally looking at the stove and then back at the camera. The frying pan on the stove remains central in the frame. [0:00:17 - 0:00:18]: The person looks directly at the camera and continues to engage, with their hands resting on the countertop. The background stays the same with the kitchen in view. [0:00:18 - 0:00:19]: In the final frames, the person steps back slightly, continuing to speak with an expressive demeanor, maintaining the focus on the kitchen environment.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What text appears at the top right of the screen right now?",
                "time_stamp": "00:00:13",
                "answer": "B",
                "options": [
                    "A. \"Live Recording\".",
                    "B. \"Previously Recorded Live\".",
                    "C. \"Recording Now\".",
                    "D. \"Live Now\"."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_28_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:21]: A man is standing in a kitchen with blue cabinets and a white tile backsplash, placing sliced potatoes into a black frying pan on a gas stove. He is wearing a dark blue t-shirt and a black wristwatch. Various kitchen appliances and utensils are visible on the counter behind him. [0:05:21 - 0:05:22]: The man continues to place additional potato slices into the frying pan. The potatoes are cut into thin rounds and are being laid flat in a single layer. [0:05:22 - 0:05:24]: He adjusts the positions of the potato slices within the pan, ensuring they are evenly spread out for cooking. His left hand holds a few more potato slices ready to be placed in the pan. [0:05:24 - 0:05:27]: The man meticulously arranges the potato slices while occasionally looking down at the pan to check their placement. In the background, the kitchen counter holds a variety of cooking items, including bottles and a knife block. [0:05:27 - 0:05:28]: He reaches over to the side, possibly to grab more potato slices from a wooden cutting board located next to the stove. His movements are precise and careful, indicating attention to detail in his cooking process. [0:05:28 - 0:05:29]: A close-up view of the man placing more potato slices into the frying pan. There are audible sizzling sounds as the potatoes come in contact with the hot surface. [0:05:29 - 0:05:30]: He adjusts the heat on the gas stove, ensuring the temperature is optimal for cooking the potatoes. His focus remains on the pan as he works. [0:05:30 - 0:05:31]: The man arranges the last few potato slices in the pan, making sure they are not overlapping and have enough space to cook evenly. His hand movements are quick and practiced. [0:05:31 - 0:05:32]: He shifts his gaze from the pan to another part of the kitchen, possibly checking on another cooking task. Behind him, there are glass cabinets with neatly arranged glassware and other kitchen items. [0:05:32 - 0:05:35]: The man briefly steps away from the pan, walking over to the cutting board where there are a few more potato slices left. He picks them up and returns to the pan, placing them inside it carefully. [0:05:35 - 0:05:36]: He stirs the potatoes slightly, ensuring they are all evenly cooked. The kitchen has a modern and clean ambiance, with everything in its place and well-organized. [0:05:36 - 0:05:37]: The man wipes his forehead with his forearm, showing that he is engaged in an activity that requires focus and effort. He then resumes his attention to the stove. [0:05:37 - 0:05:39]: The camera focuses on the frying pan showing the potatoes sizzling in oil. The potatoes are starting to turn golden brown on the edges indicating they are cooking well. The scene ends with the man giving the pan a slight shake to ensure even cooking.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What action does the man take after placing the potato slices in the frying pan?",
                "time_stamp": "0:05:24",
                "answer": "B",
                "options": [
                    "A. He adds seasoning to the potatoes.",
                    "B. He sprinkles seasonings on the potatoes.",
                    "C. He washes his hands.",
                    "D. He prepares another dish."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Attribute Recognition",
                "question": "What does the man do to ensure the potatoes cook evenly in the pan?",
                "time_stamp": "0:05:31",
                "answer": "B",
                "options": [
                    "A. He stirs them constantly.",
                    "B. He arranges them to avoid overlapping.",
                    "C. He covers the pan.",
                    "D. He adds more oil."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_28_real.mp4"
    },
    {
        "time": "[0:10:00 - 0:11:00]",
        "captions": "[0:10:40 - 0:11:00] [0:10:40 - 0:10:52]: A person wearing a dark blue t-shirt stands at a kitchen counter, chopping green onions on a wooden cutting board. Various objects are present on the counter including a frying pan to the left and a white bowl with vegetables on the right. Behind the person, a grey cupboard with glass doors contains various dishes and a few bottles. The countertop itself is white with a small portion of a sink visible. The person continues chopping and adjusting the green onions with a focused expression;  [0:10:53]: The person shifts their focus from the cutting board to the bowl on the counter, holding both items. They are captured in mid-motion, appearing to smile or laugh while engaging with someone off-screen;  [0:10:54 - 0:10:56]: The person momentarily steps backward from the counter, facing away from the camera towards a kitchen shelf and microwave in the background. In this shot, more of the kitchen is visible, including a large clock mounted on the back wall. Another individual seated at a table is semi-visible in the distant background through a doorway;  [0:10:57 - 0:10:58]: The camera angle shifts, zooming back in on the countertop area. Numerous kitchen utensils and a stovetop with partially visible food being cooked are captured. The person resumes their previous position at the counter, holding the bowl with vegetables;  [0:10:59]: The person lifts the bowl towards the camera while using a spoon to mix the contents. The expression on their face is cheerful and engaging, as they look towards the camera. The kitchen setup in the background remains the same with the grey cupboards and various kitchen items.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is this person doing right now?",
                "time_stamp": "0:10:39",
                "answer": "B",
                "options": [
                    "A. He is chopping green onions.",
                    "B. He is slicing carrots.",
                    "C. He is dicing tomatoes.",
                    "D. He is mincing garlic."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Event Understanding",
                "question": "What is the person doing with the bowl right now?",
                "time_stamp": "0:11:00",
                "answer": "C",
                "options": [
                    "A. Putting it in the microwave.",
                    "B. Washing it in the sink.",
                    "C. Mixing its contents with a spoon.",
                    "D. Placing it back on the counter."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_28_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:10]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions taken just now?",
                "time_stamp": "00:00:10",
                "answer": "B",
                "options": [
                    "A. An individual browsed through items in a refrigerator section of a store with a cloth in hand.",
                    "B. An individual walked through an establishment, apparently a caf\u00e9, holding a cloth and prepared to clean a table.",
                    "C. An individual picked up a jacket from a chair and folded it neatly to place it back.",
                    "D. An individual took a tray of food to serve a seated customer at a table."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_352_real.mp4"
    },
    {
        "time": "[0:02:05 - 0:02:15]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions taken just now?",
                "time_stamp": "00:02:15",
                "answer": "C",
                "options": [
                    "A. An individual was washing their hands and drying them with a towel.",
                    "B. An individual was organizing various kitchen utensils and cutlery in a drawer.",
                    "C. An individual was washing and rinsing a batch of dishes and cups in a sink.",
                    "D. An individual was preparing ingredients for a meal by chopping vegetables."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_352_real.mp4"
    },
    {
        "time": "[0:04:10 - 0:04:20]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions taken just now?",
                "time_stamp": "00:04:20",
                "answer": "B",
                "options": [
                    "A. An individual rearranged items in a refrigerator and then wiped the floors.",
                    "B. An individual disposed of trash, retrieved cleaning supplies, and started mopping the floor.",
                    "C. An individual washed dishes, emptied the trash, and cleaned the sink.",
                    "D. An individual served food to a customer and then started cleaning the countertops."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_352_real.mp4"
    },
    {
        "time": "[0:06:15 - 0:06:25]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions taken just now?",
                "time_stamp": "00:06:25",
                "answer": "A",
                "options": [
                    "A. An individual adjusted various settings on an electronic device, selected payment options.",
                    "B. An individual searched through web pages on a tablet, bookmarked a few articles, and saved them for later reading.",
                    "C. An individual played a game on a tablet, paused to check messages, and then defeated the final boss in the game.",
                    "D. An individual followed a recipe on a tablet screen, added ingredients to a bowl, and began the baking process."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_352_real.mp4"
    },
    {
        "time": "[0:08:20 - 0:08:30]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions taken just now?",
                "time_stamp": "00:09:33",
                "answer": "A",
                "options": [
                    "A. An individual made a cappuccino and a flat white with oat milk, and served them in takeaway cups.",
                    "B. An individual made an espresso and a latte, serving them in ceramic mugs for dine-in.",
                    "C. An individual prepared tea and hot chocolate for takeaway, using regular milk for both.",
                    "D. An individual brewed a strong filter coffee and an americano, serving them in large takeaway containers."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_352_real.mp4"
    },
    {
        "time": "0:00:00 - 0:00:20",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a completely black screen.  [0:00:01 - 0:00:04]: The screen transitions to display the text \"RESIN WITH ME\" centered with a thin horizontal line beneath it. [0:00:03 - 0:00:04]: The text \"Acrylic Floral Sign\" appears below the horizontal line.  [0:00:05]: The perspective changes to a work surface covered with a blue grid mat. Two plastic containers filled with small flowers and other colorful decorations are held by a person wearing black gloves. A round brown object is placed in the center of the mat. [0:00:06 - 0:00:11]: The camera zooms in to the two plastic containers. On close inspection, the containers each hold multiple compartments with various dried flowers of different colors and shapes arranged neatly. [0:00:12]: The view shifts slightly, showing both containers being set down around the round object placed in the center of the mat. [0:00:13]: The person removes a craft knife from the left side of the frame. [0:00:14 - 0:00:17]: The person's gloved hands use the knife to carefully cut along the line dividing the brown round object, methodically scoring through the material. [0:00:18 - 0:00:19]: The person peels off a section of the brown object, revealing the surface underneath.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the person doing right now?",
                "time_stamp": "00:00:19",
                "answer": "C",
                "options": [
                    "A. Zooming in on the plastic containers.",
                    "B. Cutting along the line dividing the brown object.",
                    "C. Peeling off a section of the brown object.",
                    "D. Arranging flowers in the containers."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_68_real.mp4"
    },
    {
        "time": "0:01:40 - 0:02:00",
        "captions": "[0:01:40 - 0:02:00] [0:00:00 - 0:00:01]: The scene opens to a blue grid-mat background with a round, tan-colored coaster-like object in the center. Various small, decorative elements, such as flowers and leaves in different colors (including red, purple, and green), are placed on the coaster. In the top left corner, there is a small circular container filled with a translucent liquid. To the right of the coaster, there is a rectangular container with multiple compartments, each holding tiny, colorful embellishments. A skewer stick is placed diagonally near the top right of the coaster. The view suggests the perspective of someone working on a craft project, possibly resin art. A gloved hand holding another small plastic box filled with colorful embellishments is seen entering the frame from the bottom. [0:00:02 - 0:00:03]: The gloved hand adjusts the small plastic box, revealing compartments with various small embellishments, including flowers and leaves. The focus is on selecting the embellishments for placement on the coaster. [0:00:04 - 0:00:06]: The gloved hand picks a flower embellishment from the small plastic box using a white tool with a flat edge. The other hand continues to hold the plastic box steady. [0:00:07]: The flower selected is yellow. The gloved hand uses the white tool to lift the flower out of the compartment. [0:00:08 - 0:00:10]: The gloved hand carefully places the yellow flower onto the coaster. It is positioned near the bottom right of the arrangement of existing embellishments. [0:00:11 - 0:00:13]: The gloved hand continues to hold the white tool and adjust the yellow flower to the desired position on the coaster. [0:00:14 - 0:00:16]: Additional adjustment is made to ensure the yellow flower fits well with the surrounding embellishments. The hand continues to refine the placement using the white tool. [0:00:17 - 0:00:19]: After securing the yellow flower in place, the gloved hand moves back towards the small plastic box to select another embellishment. The frame includes a clear view of the utensil, flowers, and leaves in the plastic box. [0:00:20]: The gloved hand opens another compartment of the plastic box, revealing more embellishments to choose from. The arrangement on the coaster remains in view, displaying a deliberate and meticulous crafting process.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the gloved hand doing right now?",
                "time_stamp": "00:02:00",
                "answer": "C",
                "options": [
                    "A. Adjusting the heat on the stove top.",
                    "B. Painting the coaster into a solid color.",
                    "C. Adjusting the position of the flower on the brown ring.",
                    "D. Picking up the coaster."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_68_real.mp4"
    },
    {
        "time": "0:03:20 - 0:03:40",
        "captions": "[0:03:20 - 0:03:40] [0:00:20 - 0:00:21]: A round wooden disk is centered on a grid-patterned blue cutting mat. The surface of the disk is adorned with various small, colorful items including flowers and leaves in hues of pink, purple, yellow, red, green, and white. A white ceramic bowl with a paintbrush standing in it sits in the top left corner of the frame. [0:00:22 - 0:00:23]: The setup remains unchanged with the wooden disk and ceramic bowl on the blue grid mat.  [0:00:24 - 0:00:25]: A black-gloved hand holding a bamboo stick appears from the right side of the frame. The stick is aimed at a yellow flower near the center of the disk. [0:00:26 - 0:00:27]: The black-gloved hand continues holding the bamboo stick as it approaches the yellow flower on the disk. [0:00:28 - 0:00:29]: The bamboo stick makes contact with the yellow flower, gently pressing or stirring it. [0:00:30 - 0:00:31]: The black-gloved hand continues to press or stir the yellow flower delicately with the bamboo stick. [0:00:32 - 0:00:33]: The bamboo stick remains pressed or stirring the yellow flower, and the black glove remains unchanged in its position. [0:00:34 - 0:00:35]: The bamboo stick continues its interaction with the yellow flower, demonstrating some movement but remaining focused on the same spot. [0:00:36 - 0:00:37]: The bamboo stick still interacts with the yellow flower with slight movement, showing refined attention to detail by the black-gloved hand. [0:00:38 - 0:00:39]: The black-gloved hand presses or stirs a little more firmly but continues its fine-tuned work on the yellow flower with cautious motions. [0:00:40 - 0:00:41]: The bamboo stick slightly shifts upwards, focusing on the adjacent green leaves. The hand adjusts its position slightly. [0:00:42 - 0:00:43]: The stick repositions further towards the green leaves, but its movement remains subtle and precise. [0:00:44 - 0:00:45]: The bamboo stick moves over to interact with the pink flower near the edge of the disk, maintaining the same gentle and detailed approach. [0:00:46 - 0:00:47]: The bamboo stick continues to interact with the pink flower carefully, maintaining its gentle touch. [0:00:48 - 0:00:49]: The stick shifts towards the orange flower on the disk, with the black glove maintaining control and precision. [0:00:50 - 0:00:51]: The black-gloved hand holds the bamboo stick while focusing on the green leaf beside the orange flower, showing meticulous detail in every movement. [0:00:52 - 0:00:53]: The bamboo stick and black-gloved hand stay attentive on the green leaf beside the orange flower, continuing smooth, delicate movements. [0:00:54 - 0:00:55]: The stick moves back towards the yellow flower near the center of the disk, continuing its delicate and meticulous intervention. [0:00:56 - 0:00:57]: The bamboo stick remains over the yellow flower, and the black gloved hand continues its careful, detailed work. [0:00:58 - 0:00:59]: The hand and bamboo stick maintain their position over the yellow flower, demonstrating consistent and detailed attention throughout the interaction.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the black-gloved hand doing right now?",
                "time_stamp": "0:03:26",
                "answer": "C",
                "options": [
                    "A. Repositioning the white ceramic bowl.",
                    "B. Stirring the paintbrush in the bowl.",
                    "C. Adjusting the flowers with a bamboo stick.",
                    "D. Adjusting the blue cutting mat."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_68_real.mp4"
    },
    {
        "time": "0:05:00 - 0:05:20",
        "captions": "[0:05:00 - 0:05:20] [0:00:00 - 0:00:19]: The video begins with a pair of hands wearing black gloves, holding a small, transparent plastic cup in the left hand and a thin wooden stick in the right hand. The hands are positioned over a circular, flat surface with a blue grid mat in the background. The circular surface appears to be a mold filled with various colorful elements, including flowers and leaves, embedded within. These elements are arranged in an aesthetically pleasing pattern with colors such as pink, purple, green, orange, and blue. The surface also includes scattered gold flakes.  Initially, the right hand uses the thin wooden stick to move and position the elements within the mold, ensuring they are evenly spaced and well-aligned. As the video progresses, the left hand tilts the plastic cup, pouring a clear liquid\u2014likely resin\u2014into the mold. The right hand continues to adjust the positions of the elements with the wooden stick, occasionally stirring the liquid to distribute it evenly across the mold.  Throughout the video, the actions are carried out meticulously, with the focus on achieving a balanced arrangement within the mold. The clear liquid slowly fills the spaces, surrounding the colorful elements and gold flakes, giving the appearance of creating a decorative piece.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the person doing right now?",
                "time_stamp": "00:04:55",
                "answer": "A",
                "options": [
                    "A. Pouring resin into the circular mold.",
                    "B. Mixing colors in a bowl.",
                    "C. Arranging flowers on a table.",
                    "D. Stacking plastic cups."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_68_real.mp4"
    },
    {
        "time": "0:06:40 - 0:07:00",
        "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:43]: The video begins with a pair of hands holding a translucent circular object adorned with various multicolored floral and leaf designs. The surface of the object glistens and reflects light. The right hand is carefully peeling off a piece of gold-colored material from the circular object\u2019s surface. This scene is set against a backdrop of a gray, grid-patterned work surface. [0:06:44 - 0:06:46]: As the gold-colored material continues to be peeled away, more of the floral and leaf designs are revealed. These designs include shades of purple, pink, blue, green, yellow, and orange. Some leaves are also visible among the intricate floral patterns. [0:06:47 - 0:06:48]: The circular object is now fully uncovered, revealing the entire colorful design beneath. The hand continues to hold the object steady against the gray grid-patterned surface, capturing the details of the vivid flowers and leaves embedded in the clear material. [0:06:49 - 0:06:51]: A closer view is shown, with the object held directly in front of the camera. This view highlights the various colorful floral and leaf designs more prominently. The designs are scattered evenly across the circular object, with a slight golden hue accentuating the edges of the flowers and leaves. [0:06:52 - 0:06:53]: The camera captures the object slightly rotated between the fingers. The floral patterns, which appear three-dimensional, shimmer under the light. The intricate details and vibrant colors of each flower and leaf are clearly visible. [0:06:54 - 0:06:55]: The object is held up closer to the camera, giving a much more detailed view. The round surface showcases flowers in purple, pink, yellow, blue, and orange, along with green leaves and small golden accents scattered throughout. [0:06:56 - 0:06:57]: The camera maintains focus on the circular object, held steadily by the hand. The intricate designs and shimmering colors are clearly visible against the consistent gray grid-patterned background. [0:06:58 - 0:07:00]: The perspective shifts as the circular object is placed on the gray grid-patterned work surface. The hand reaches down to grab a white rectangular piece of paper, placing it in front of the circular object. [0:07:01 - 0:07:02]: The hand picks up a pen-like tool and begins to manipulate the rectangular piece of paper. The tool is placed precisely at one corner of the paper, indicating the start of a new crafting process. [0:07:03 - 0:07:04]: The hand carefully makes precise movements with the pen-like tool along the edge of the paper, while the other hand holds the paper steady. The circular object remains visible in the background. [0:07:05 - 0:07:06]: The pen-like tool continues to work its way along the edge of the paper, creating a series of cut-out shapes. The focus remains on the careful and deliberate hand movements guiding the tool. [0:07:07 - 0:07:08]: The hand shifts, adjusting the paper slightly to ensure a precise cut. The pen-like tool continues to create detailed cut-out shapes along the edge of the rectangular paper. [0:07:09 - 0:07:12]: The hand partially lifts the paper to reveal the cut-out shapes. The focus remains on the delicate crafting process, with the pen-like tool carefully following the pre-determined path. Both the cut-out shapes and the circular object in the background are clearly visible. [0:07:13 - 0:07:15]: The camera captures the detailed cut-out shapes forming on the paper. The pen-like tool is lifted momentarily while the hand adjusts the paper, revealing the progress made thus far. The cut-outs are intricate and complement the floral designs on the circular object. [0:07:16 - 0:07:17]: The hand continues to manipulate the pen-like tool, ensuring that the shapes being cut out are precise and clean. The paper, now partially cut, begins to show the developed pattern as the other hand holds it in place. [0:07:18 - 0:07:20]: The paper is fully lifted, showing the intricate pattern created by the cut-out shapes. The camera captures both the detailed cut-outs and the floral designs on the circular object in the background, emphasizing the craftsmanship involved. [0:07:21 - 0:07:22]: The rectangular paper, now featuring several cut-out shapes, is held in place while the pen-like tool is repositioned. The hand prepares to further refine the design. The circular object remains a constant in the background, showcasing its vibrant floral",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What action is being performed with the hands right now?",
                "time_stamp": "0:06:43",
                "answer": "B",
                "options": [
                    "A. Painting the circular object.",
                    "B. Peeling off a gold-colored material.",
                    "C. Polishing the circular object.",
                    "D. Cutting the circular object into pieces."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_68_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins in a commercial storage room or a supermarket stock room. A person is looking at a metal shelving unit filled with various goods, mainly food products. The lower shelves contain multiple yogurt containers. A distinctive red crate is visible on the floor. [0:00:03 - 0:00:04]: The person moves down the aisle and their hand, wearing a black glove, becomes visible as they push a shopping cart filled with more yogurt containers. There is a cardboard box with an image of an egg on it nearby, and a pallet jack indicating a storage or stocking area. [0:00:05 - 0:00:06]: The perspective returns to the metal shelving, focusing on rows of dairy products. The shelves are divided into sections, each labeled with clear signage. Small cartons and containers are neatly lined up. [0:00:07 - 0:00:08]: Attention shifts to a lower shelf where several yogurt cups are arranged. Further down, larger yogurt containers are stacked. The floor is mostly clean but shows signs of heavy foot traffic typical of storage areas. [0:00:09 - 0:00:10]: The person's gloved hands pick up a blue yogurt container from the shelf and appear to restock it or examine it closely. More yogurt containers are visible on the shelves, stacked in rows. [0:00:11 - 0:00:12]: A close-up look at the yogurt containers indicates various brands and flavors, all neatly organized on the shelf. The clear labeling helps in easy identification of the products, which are stacked from front to back. [0:00:13 - 0:00:14]: The view pans back out to show a wider area of the shelving unit, with a focus on the middle shelf. The yogurt containers are mostly white with different label colors. The lower shelves also hold some boxed items. [0:00:15 - 0:00:16]: Close-up of the middle shelf with more yogurt containers, and a hand reaches in to adjust or inspect one. The products are well-lit, emphasizing the cleanliness and organization of the storage area. [0:00:17 - 0:00:18]: Movement again to the shelving with the person actively restocking yogurt. Each container is placed carefully next to others, maintaining the orderly arrangement. [0:00:18 - 0:00:20]: The video concludes with the perspective widening to include more of the storage room. Shelves filled with products line the walls, and multiple boxes and pallets are spread around, showing the context of the space as a busy, well-organized storage area.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What type of container is the person placing on the shelves?",
                "time_stamp": "0:00:18",
                "answer": "A",
                "options": [
                    "A. Blue and white yogurt containers.",
                    "B. Small cardboard boxes.",
                    "C. Large glass jars.",
                    "D. Metal cans."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "storage_manage",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_442_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:20 - 0:04:40] [0:04:20 - 0:04:23]: The footage shows boxes stacked on the floor in a storage room. The boxes are labeled \"BRAVO\" and a person wearing gloves is visible, arranging or removing boxes. Cartons of eggs can be seen. [0:04:24]: Shelves filled with various items like packaged juice, milk products, and other cartons are visible. Price tags can be seen on the shelves. [0:04:25 - 0:04:29]: The camera turns back to focus on an area with more stacked boxes, the same gloved hands arranging and handling boxes. The gloved individual is adjusting cartons on top of the boxes. [0:04:30 - 0:04:32]: The gloved person places items on the shelf next to juice cartons. There are various flavors of juice with price tags. [0:04:33]: The footage becomes momentarily blurry as the camera moves. [0:04:34 - 0:04:35]: The camera focuses back on the storage area with stacked boxes. One large box has cut-outs, possibly handles, and the gloved hands are moving another box. [0:04:36 - 0:04:38]: The view captures more of the storage room, showing shelves on the right side with various items organized on them. The person continues handling boxes. [0:04:39]: Again, more of the storage room is visible, showing a variety of stacked items on the left side.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What label is visible on the boxes stacked on the floor in the storage room right now?",
                "time_stamp": "00:04:19",
                "answer": "C",
                "options": [
                    "A. ALPHA.",
                    "B. CHARLIE.",
                    "C. Valio and BRAVO.",
                    "D. DELTA."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "storage_manage",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_442_real.mp4"
    },
    {
        "time": "[0:13:00 - 0:14:00]",
        "captions": "[0:13:00 - 0:13:20] [0:13:00 - 0:13:02]: The video begins with a first-person perspective focusing on a metal shelf stocked with large juice cartons labeled \"EXOTISK JUICE.\" The cartons are green with some orange detailing. The person's black-gloved hands are visible as they handle the cartons, seemingly restocking or organizing them. The shelf is labeled with a price of 28.50. [0:13:03 - 0:13:04]: The camera angle moves downward, revealing a brown liquid spill on the white floor next to a cardboard box filled with more cartons. The contents of the box include similar green cartons, possibly the same as those being placed on the shelf. [0:13:05 - 0:13:07]: The motion continues as the person transfers more cartons from the box on the floor to the shelf, making sure they are positioned neatly. More green juice cartons are added to the row on the lower shelf. [0:13:08 - 0:13:12]: The perspective shifts and the person picks up another carton from the box on the floor and places it on the shelf, organizing the items carefully. The upper shelf holds several pink cartons, contrasting with the green cartons on the lower shelf. [0:13:13 - 0:13:15]: The camera moves to show the wider storage area, which includes a partially open door, tall shelves, and more large boxes filled with various items. The flooring is white, maintaining a clean and organized warehouse appearance. [0:13:16 - 0:13:19]: The video concludes with the person moving slightly away from the shelf and placing the remaining box lid onto the box labeled \"BRAVO.\" Other full boxes labeled \"BRAVO\" are stacked on a dolly, hinting that the person is in a stockroom or inventory space used for organizing product deliveries.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What brand of juice cartons is the person handling?",
                "time_stamp": "00:13:02",
                "answer": "A",
                "options": [
                    "A. BRAVO.",
                    "B. EXOTISK JUICE.",
                    "C. TROPICANA.",
                    "D. DELIGHT JUICE."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Attribute Recognition",
                "question": "What are the colors of the juice cartons being handled by the person?",
                "time_stamp": "00:13:14",
                "answer": "A",
                "options": [
                    "A. Green with white and yello detailing.",
                    "B. Blue with yellow detailing.",
                    "C. Red with white detailing.",
                    "D. Pink with green detailing."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "storage_manage",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_442_real.mp4"
    },
    {
        "time": "[0:16:00 - 0:16:50]",
        "captions": "[0:16:40 - 0:16:50] [0:16:40 - 0:16:41]: The scene shows a storage room with shelves of brightly colored packaged goods on the left and a slightly open door at the back. There's a cart with dairy products in the foreground. [0:16:42 - 0:16:45]: The view shifts to focus on refrigerated shelves stocked with cartons of yogurt. Two hands wearing black gloves are seen rearranging or picking up the yogurt cartons. [0:16:46]: The perspective returns to the broader storage room. The shelves with colorful packages remain to the left, the cart is in the foreground, and the door at the back center is fully visible. [0:16:47 - 0:16:48]: The perspective moves towards a corner where multiple boxes are stacked on the floor and a wire rack holds more boxes and some green products. [0:16:49 - 0:16:50]: A shelving unit stocked with containers, possibly dairy products, is visible on the right. In the foreground, there are open cardboard boxes on the floor.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color are the gloves seen in the scene?",
                "time_stamp": "00:16:45",
                "answer": "A",
                "options": [
                    "A. Black.",
                    "B. Red.",
                    "C. White.",
                    "D. Blue."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "storage_manage",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_442_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:22]: In a well-lit kitchen with tiled walls and light blue cupboards, a person is seen wearing a dark blue shirt. The person is using a box grater to grate a yellow ingredient, likely cheese, over a white plate on a wooden cutting board. To the left, there's a frying pan on the stove, and to the right are some kitchen utensils like a pair of forks and possibly a bottle or jar. [0:03:23 - 0:03:27]: The grating continues methodically, with the person applying pressure and moving the yellow ingredient downward against the grater. More of the ingredient accumulates on the plate. The actions remain consistent with grating the yellow substance. [0:03:28 - 0:03:31]: The kitchen settings including the surrounding shelves with glassware come into clearer focus. The person's focused expression is visible as they continue grating the yellow ingredient vigorously. [0:03:32 - 0:03:35]: A small timer appears in the bottom right corner, counting down from 10:00 minutes. Meanwhile, the grating continues with consistent motion. The left arm holds the grater steady while the right arm moves the yellow ingredient across its surface. [0:03:36 - 0:03:39]: The grating process continues steadily, with the person looking focused on the task. The kitchen background features a sink area with various kitchen items orderly placed. The grated yellow ingredient is now significantly piling up on the plate, indicating considerable progress in the grating task.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color are the kitchen cupboards?",
                "time_stamp": "00:03:22",
                "answer": "B",
                "options": [
                    "A. Dark blue.",
                    "B. Light green.",
                    "C. White.",
                    "D. Yellow."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Action Recognition",
                "question": "What is the person doing with the box grater?",
                "time_stamp": "00:03:27",
                "answer": "D",
                "options": [
                    "A. Slicing bread.",
                    "B. Chopping vegetables.",
                    "C. Peeling potatoes.",
                    "D. Grating a yellow ingredient."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_36_real.mp4"
    },
    {
        "time": "[0:06:00 - 0:07:00]",
        "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:44]: A person in a blue shirt is seen chopping red bell pepper on a wooden cutting board. The chopping action is quick and precise, with the knife moving smoothly and consistently. The person's arms are positioned to stabilize the pepper while slicing it into thin strips. The kitchen background is clearly visible, with various kitchen utensils, a sink, and some cooking ingredients placed on a countertop;  [0:06:45 - 0:06:48]: The chopping continues as the person focuses intently on the task. Here, the sliced pieces of bell pepper start to accumulate on the board. On the left side of the frame, part of the stove with a pan visible. The person maintains a steady rhythm, ensuring each slice is uniform;  [0:06:49 - 0:06:51]: After chopping the bell pepper, the person shifts slightly to the side, looking down, presumably to inspect the slices. More kitchen tools and ingredients come into view in the background, neatly arranged on shelves or the countertop;  [0:06:52 - 0:06:55]: The person gathers the chopped bell pepper with the knife, transferring the pieces into a pan on the stove. The other pan on the stove has some ingredients in it, likely already cooking. The person moves with efficiency and purpose;  [0:06:56 - 0:06:59]: The person starts stirring the contents of the pan with a wooden spoon, ensuring even cooking and mixing of ingredients. Visible on the countertop is a microwave and additional cooking supplies, with the person's focus now entirely on the stove. The food in the pan looks vibrant and colorful, indicating a mix of ingredients. The entire kitchen setup portrays a well-organized and functional cooking space.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the person in a blue shirt chopping on the wooden cutting board?",
                "time_stamp": "0:06:42",
                "answer": "A",
                "options": [
                    "A. Red bell pepper.",
                    "B. Carrots.",
                    "C. Green bell pepper.",
                    "D. Onions."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_36_real.mp4"
    },
    {
        "time": "[0:10:00 - 0:11:00]",
        "captions": "[0:10:00 - 0:10:20] [0:10:00 - 0:10:02]: A person is standing in a modern kitchen, holding a wooden spoon and stirring food in a black frying pan on the stove. The kitchen features dark blue cabinets with glass doors showcasing dishware. Behind the person, there is a white tile backsplash and various kitchen utensils on the countertop. To the person's left, there is a stand mixer and a kettle. The person is wearing a navy blue shirt and green pants and a checkered cloth is tucked into the waistband. [0:10:02 - 0:10:06]: The person continues to stir the food in the frying pan, now using both the wooden spoon and another utensil to mix the ingredients. Their attention is focused on the pan. They briefly gesture with their free hand while still stirring. [0:10:06 - 0:10:09]: The video angle changes slightly, revealing a closer view of the pan. The food in the pan appears to be a mixture of vegetables and possibly some protein, with vibrant colors including yellow, red, and green. The person continues to mix the ingredients, lifting the pan slightly off the stove at one point. [0:10:09 - 0:10:11]: The person places the pan back on the stove, and the camera focuses closely on the pan, capturing detailed view of the ingredients being stirred. [0:10:11 - 0:10:13]: The person picks up a small bowl filled with a red liquid, possibly a sauce, and begins to pour it into the frying pan. They use the wooden spoon to ensure all the sauce is added. [0:10:13 - 0:10:16]: The person continues to empty the bowl into the pan, using the spoon to scrape out any remaining sauce. They then discard the empty bowl and resume mixing the contents of the frying pan. [0:10:16 - 0:10:19]: The camera returns to focus on the person\u2019s face, who briefly looks up and then continues to stir the food meticulously, ensuring it is well-mixed with the sauce.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the person holding right now?",
                "time_stamp": "0:10:02",
                "answer": "A",
                "options": [
                    "A. A wooden spoon.",
                    "B. A metal spatula.",
                    "C. A whisk.",
                    "D. A ladle."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_36_real.mp4"
    },
    {
        "time": "[0:13:00 - 0:14:00]",
        "captions": "[0:13:20 - 0:13:40] [0:13:20 - 0:13:21]: The scene shows an individual in a kitchen, holding a spatula in their right hand while standing in front of a stovetop. There are two frying pans on the stovetop; one is empty, and the other contains a yellowish dish cooking. The background features cabinets with glass doors, some utensils on the countertop, and a microwave. [0:13:21 - 0:13:23]: The person turns slightly to their left, moving an item away from the stovetop. The lower part of the kitchen, including drawers and cabinets, is visible. [0:13:23 - 0:13:24]: The individual is seen working on the countertop on their right while facing the stovetop on the left. There is a bowl of green leaves, a glass, and a ladle on the countertop. [0:13:24 - 0:13:25]: They are seen handling some dough on the wooden cutting board, which is lightly floured. The individual\u2019s hands are covered in flour, and they are using a wooden rolling pin. [0:13:25 - 0:13:26]: The person continues to roll out the dough, flattening it into a circular shape. The countertop now shows additional items: a bottle of oil, a stack of plates, and some jars. [0:13:26 - 0:13:27]: The individual places the flattened dough onto the wooden cutting board and continues rolling another piece of dough. The stovetop and cooking pan with the yellowish contents are still active, with steam visible. [0:13:27 - 0:13:28]: Moving the rolling pin back and forth, the person flattens another piece of dough. Several prepared balls of dough are visible on a plate nearby. [0:13:28 - 0:13:29]: The individual rolls out the last piece of dough, ensuring it is thin and even. Other kitchen items remain undisturbed in their places. [0:13:29 - 0:13:30]: They place the dough aside and adjust the rolling pin. The person is focused on the preparation, with the cooking pans emitting steam. [0:13:30 - 0:13:31]: The individual continues their dough preparation, leaning slightly forward over the countertop. The background shelves with glassware and kitchen supplies are visible. [0:13:31 - 0:13:32]: Rolling another piece of dough, the person maintains a steady pace. The organized kitchen setting remains consistent, with various utensils and jars positioned around the countertops. [0:13:32 - 0:13:33]: The individual carefully presses the dough, ensuring it is properly flattened. Their attention stays on the task, with the stovetop still visible in the background. [0:13:33 - 0:13:34]: The person continues working with precision, rolling out the dough. The shelves behind them hold various kitchen equipment and glassware. [0:13:34 - 0:13:35]: They examine the dough, stretching it gently to achieve the desired shape and consistency. Items on the countertop remain neatly arranged. [0:13:35 - 0:13:36]: The individual then places the dough on the board and readjusts the rolling pin. The kitchen background and countertop setup remain unchanged. [0:13:36 - 0:13:37]: They work systematically, ensuring each piece of dough is evenly processed. The organized kitchen setting continues to provide a clear workspace. [0:13:37 - 0:13:38]: Their focus stays on rolling the dough, with all necessary utensils within easy reach. The kitchen remains tidy and functional, with all tools in their designated spots. [0:13:38 - 0:13:39]: Finally, the individual completes the dough preparation, with the countertop and surrounding utensils remaining orderly. The background shelves still display kitchenware and glass items.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the person holding in the right hand right now?",
                "time_stamp": "0:13:19",
                "answer": "A",
                "options": [
                    "A. A pen.",
                    "B. A ladle.",
                    "C. A knife.",
                    "D. A rolling pin."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_36_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:10]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which option best summarizes what was just shown?",
                "time_stamp": "00:00:10",
                "answer": "B",
                "options": [
                    "A. A person was preparing and serving hamburgers to a line of customers at a busy street corner.",
                    "B. A mobile food truck opened its serving window near a shopping area in the morning.",
                    "C. A vendor inside a food truck was grilling burgers on a barbecue and handing them to customers.",
                    "D. A food truck got stuck in traffic and had to be manually pushed to the side of the road by several people."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_409_real.mp4"
    },
    {
        "time": "[0:02:38 - 0:02:48]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which option best summarizes what was just shown?",
                "time_stamp": "00:02:47",
                "answer": "B",
                "options": [
                    "A. A person was cleaning the kitchen area and restocking spices and sauces.",
                    "B. A vendor was arranging different types of drinks on the countertop in a food truck.",
                    "C. Several people were unloading supplies from a truck into a storage area.",
                    "D. A chef was heating oil in a pan to start cooking a meal."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_409_real.mp4"
    },
    {
        "time": "[0:05:16 - 0:05:26]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which option best summarizes what was just shown?",
                "time_stamp": "00:05:26",
                "answer": "C",
                "options": [
                    "A. A person was unloading kitchen equipment from a truck and placing it on a counter.",
                    "B. A person was stocking a refrigerator with beverages and condiments.",
                    "C. A person was organizing cooking ingredients.",
                    "D. A person was preparing ingredients for a recipe by mixing spices in different containers."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_409_real.mp4"
    },
    {
        "time": "[0:07:54 - 0:08:04]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which best summarizes what was just shown?",
                "time_stamp": "00:08:04",
                "answer": "A",
                "options": [
                    "A. A burger was being freshly seasoned and prepared for customers.",
                    "B. A chef was preparing a vegetarian dish by adding spices.",
                    "C. A worker was cleaning a grill after a long day of cooking.",
                    "D. An ice cream vendor was serving customers on a sunny day at the park."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_409_real.mp4"
    },
    {
        "time": "[0:10:32 - 0:10:42]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which option best summarizes what was just shown?",
                "time_stamp": "00:10:42",
                "answer": "C",
                "options": [
                    "A. A chef was slicing vegetables and placing them on a plate.",
                    "B. A person was manually operating a fryer to prepare French fries.",
                    "C. Several patties and buns were being grilled on a hot surface by a worker.",
                    "D. A person was cleaning up the kitchen area, wiping counters and washing dishes."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_409_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:05]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What kind of car is parked on the left side of the road now?",
                "time_stamp": "00:00:07",
                "answer": "A",
                "options": [
                    "A. Police car.",
                    "B. Bus.",
                    "C. Ambulance.",
                    "D. Taxi."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_393_real.mp4"
    },
    {
        "time": "[0:01:45 - 0:01:50]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the color of the traffic light now?",
                "time_stamp": "00:02:32",
                "answer": "A",
                "options": [
                    "A. Red.",
                    "B. Green.",
                    "C. Yellow.",
                    "D. Orange."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_393_real.mp4"
    },
    {
        "time": "[0:03:30 - 0:03:35]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "Right now, what type of vehicle is the black car on the left?",
                "time_stamp": "00:03:34",
                "answer": "A",
                "options": [
                    "A. Infiniti.",
                    "B. Toyota.",
                    "C. Honda.",
                    "D. Lexus."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_393_real.mp4"
    },
    {
        "time": "[0:05:15 - 0:05:20]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the color of the traffic light now?",
                "time_stamp": "00:05:08",
                "answer": "A",
                "options": [
                    "A. Red.",
                    "B. Green.",
                    "C. Yellow.",
                    "D. Orange."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_393_real.mp4"
    },
    {
        "time": "[0:07:00 - 0:07:05]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Which street name is visible on the green sign right now?",
                "time_stamp": "0:07:00",
                "answer": "C",
                "options": [
                    "A. Broadway.",
                    "B. Canal St.",
                    "C. Centre St.",
                    "D. East Broadway."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_393_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:30]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why does the rabbit strike the door with its fist several times?",
                "time_stamp": "0:00:25",
                "answer": "D",
                "options": [
                    "A. Because the rabbit is practicing its strength.",
                    "B. Because the rabbit is knocking to check if someone is inside.",
                    "C. Because the rabbit wants to open the door.",
                    "D. Because the rabbit wants to get food."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_233_real.mp4"
    },
    {
        "time": "[0:01:29 - 0:01:59]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why would the rabbit inside the iron gate be very panicked now?",
                "time_stamp": "00:02:14",
                "answer": "D",
                "options": [
                    "A. Because he just realized the gate is locked, and he can't get out.",
                    "B. Because he heard loud footsteps approaching from outside.",
                    "C. Because the lights suddenly went out, and he is afraid of the dark.",
                    "D. Because the toilet paper he was holding in his hand fell."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_233_real.mp4"
    },
    {
        "time": "[0:02:58 - 0:03:28]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "What causes the bunny character to react with shock and sweat?",
                "time_stamp": "0:03:28",
                "answer": "C",
                "options": [
                    "A. Because the bunny just realized it forgot something important.",
                    "B. Because a loud noise startled the bunny from behind.",
                    "C. Because the rabbit wearing red and white clothes is perfectly fine.",
                    "D. Because the bunny suddenly noticed a large shadow looming over it."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_233_real.mp4"
    },
    {
        "time": "[0:04:27 - 0:04:57]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why do the rectangular characters arm themselves?",
                "time_stamp": "0:04:41",
                "answer": "B",
                "options": [
                    "A. Because they are excited to see the rabbit-like figure.",
                    "B. Because the rabbit-like figure has glowing eyes and a menacing demeanor.",
                    "C. Because they want to celebrate with the rabbit-like figure.",
                    "D. Because they see another group approaching."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_233_real.mp4"
    },
    {
        "time": "[0:05:56 - 0:06:26]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why does the rabbit break through the wall to escape?",
                "time_stamp": "00:06:16",
                "answer": "D",
                "options": [
                    "A. Because the rabbit wants to catch a thief.",
                    "B. Because the rabbit is afraid of a predator.",
                    "C. Because the rabbit wants to retrieve the advertisement.",
                    "D. Because the rabbit is inspired by the advertisement for new shoes."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_233_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:05]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "What is the sticker on the windshield saying right now?",
                "time_stamp": "00:00:04",
                "answer": "B",
                "options": [
                    "A. PLEASE PAY FARE.",
                    "B. PLEASE HAND UP.",
                    "C. PLEASE EXIT.",
                    "D. PLEASE TAKE SEATS."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_459_real.mp4"
    },
    {
        "time": "[0:01:59 - 0:02:04]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What device is located to the right side of the steering wheel right now?",
                "time_stamp": "00:02:03",
                "answer": "A",
                "options": [
                    "A. A digital device.",
                    "B. A water bottle.",
                    "C. A map.",
                    "D. A book."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_459_real.mp4"
    },
    {
        "time": "[0:03:58 - 0:04:03]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What street is visible on the signpost right now?",
                "time_stamp": "00:04:02",
                "answer": "D",
                "options": [
                    "A. Oak Street.",
                    "B. Maple Street.",
                    "C. Elm Street.",
                    "D. Jersey Street."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_459_real.mp4"
    },
    {
        "time": "[0:05:57 - 0:06:02]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What sign is visible to the right of the intersection right now?",
                "time_stamp": "00:06:00",
                "answer": "D",
                "options": [
                    "A. STOP.",
                    "B. YIELD.",
                    "C. NO ENTRY.",
                    "D. GIVE WAY."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_459_real.mp4"
    },
    {
        "time": "[0:07:56 - 0:08:01]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color is the first car on the left side of the road right now?",
                "time_stamp": "00:07:58",
                "answer": "D",
                "options": [
                    "A. Green.",
                    "B. Blue.",
                    "C. Silver.",
                    "D. Red."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_459_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:30]",
        "questions": [
            {
                "task_type": "Prospective Reasoning",
                "question": "What might the speaker discuss next?",
                "time_stamp": "00:00:24",
                "answer": "C",
                "options": [
                    "A. Calculating the perimeter of shapes.",
                    "B. Understanding different geometric figures.",
                    "C. The concept of Area.",
                    "D. The concept of volume."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "math tutorials",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_214_real.mp4"
    },
    {
        "time": "[0:02:09 - 0:02:39]",
        "questions": [
            {
                "task_type": "Prospective Reasoning",
                "question": "What formula might the speaker likely introduce next?",
                "time_stamp": "00:02:03",
                "answer": "D",
                "options": [
                    "A. Perimeter of a rectangle.",
                    "B. Volume of a cube.",
                    "C. Circumference of a circle.",
                    "D. Area of a triangle."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "math tutorials",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_214_real.mp4"
    },
    {
        "time": "[0:04:18 - 0:04:48]",
        "questions": [
            {
                "task_type": "Prospective Reasoning",
                "question": "What might the speaker explain next?",
                "time_stamp": "00:04:45",
                "answer": "D",
                "options": [
                    "A. How to convert between different units of length.",
                    "B. The concept of volume measurement.",
                    "C. The perimeter of different shapes.",
                    "D. How to calculate the area of another shapes."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "math tutorials",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_214_real.mp4"
    },
    {
        "time": "[0:06:27 - 0:06:57]",
        "questions": [
            {
                "task_type": "Prospective Reasoning",
                "question": "What might the speaker explain next?",
                "time_stamp": "00:06:55",
                "answer": "D",
                "options": [
                    "A. The angle of the base.",
                    "B. The formula for area.",
                    "C. The hypotenuse of the triangle.",
                    "D. The height of the triangle."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "math tutorials",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_214_real.mp4"
    },
    {
        "time": "[0:08:36 - 0:09:06]",
        "questions": [
            {
                "task_type": "Prospective Reasoning",
                "question": "What might the speaker discuss next?",
                "time_stamp": "00:08:45",
                "answer": "D",
                "options": [
                    "A. Discussing the properties of different quadrilaterals.",
                    "B. Solving for the perimeter of the triangle.",
                    "C. Simplifying algebraic expressions.",
                    "D. Calculating the area of a different triangle."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "math tutorials",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_214_real.mp4"
    },
    {
        "time": "0:00:00 - 0:00:20",
        "captions": "[0:00:00 - 0:00:20] [0:00:02 - 0:00:08]: A close-up view reveals four circular keychains held in a hand. Each keychain features an intricately designed acrylic disc attached to a metal ring, which is further connected to a metal cap and a colored tassel. The design on each disc is detailed and unique: the first keychain has a teal background with a geometric pattern and a decorative name \"Kayla\" written in white. The second keychain has a pink background with a chevron pattern and a monogram \"L\" in a fancy script. The third keychain has a purple background with a quatrefoil pattern, featuring the name \"Natalie\" in white alongside a heart and a monogram \"N\" in black with a heart. The tassels for the first, second, and third keychains are teal, pink, and lavender, respectively. The four keychains are neatly arranged in the hand, providing a clear view of each decorative element. [0:00:09 - 0:00:13]: The screen goes black, and the text \"Vinyl on Acrylic Key Chains\" appears, centered and in white font. [0:00:14]: The screen remains black. [0:00:15 - 0:00:17]: Another set of white text appears, reading \"How to personalized them in Cricut Design Space,\u201d centered and in the same white font. [0:00:18 - 0:00:19]: The text changes to \"To watch the assembly skip to 2:08,\" maintaining the same centered, white font.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What was shown just now?",
                "time_stamp": "00:00:08",
                "answer": "A",
                "options": [
                    "A. A detailed view of three circular keychains held in a hand.",
                    "B. A person assembling a keychain.",
                    "C. A tutorial on Cricut Design Space.",
                    "D. A black screen with white text."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_50_real.mp4"
    },
    {
        "time": "0:02:00 - 0:02:20",
        "captions": "[0:02:00 - 0:02:20] [0:00:00 - 0:00:05]: The video begins with a selection interface on a screen, presenting options for how to load materials for a project. The screen features a white background with black and grey text, and four icons in blue outlines, associated with options: \"Without Mat,\" \"On Mat,\" \"On Card Mat,\" and \"Multiple Ways.\" [0:00:06 - 0:00:05]: The interface remains on the screen, with no changes in the selection. Below the options, instructions in black font urge to \"Choose settings according to your Vinyl type and cut out.\" [0:00:06 - 0:00:09]: The video transitions to a table with a dark grid pattern as the backdrop. At the center of the frame is a teal-colored Cricut device with the word \"Cricut\" engraved prominently on its front. Positioned beneath it is a standard green Cricut mat with a grid layout for aligning materials. [0:00:10 - 0:00:12]: A pair of hands, assumed to be the viewer\u2019s, come into view. They hold a small piece of teal vinyl. The viewer positions the vinyl on the green Cricut mat, aligning it within the grid. [0:00:13 - 0:00:14]: With careful placement, the hands smooth out the teal vinyl on the mat, ensuring it sticks properly without any bubbles or creases. The hands then gently press down to secure it. [0:00:15 - 0:00:16]: The viewer lifts the mat slightly and adjusts it, preparing to insert it into the Cricut device. [0:00:17 - 0:00:15]: The hands move the Cricut mat towards the Cricut device and align it with the machine's entrance. [0:00:16 - 0:00:18]: The viewer carefully inserts the mat into the open slot of the Cricut machine, ensuring it is correctly aligned and ready for cutting. [0:00:19]: The video ends with the Cricut mat fully loaded into the machine, with the vinyl piece visible and ready for the cutting process to commence.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the person doing right now?",
                "time_stamp": "00:02:20",
                "answer": "D",
                "options": [
                    "A. Arranging a selection of materials on a screen.",
                    "B. Smoothing out the vinyl on the mat.",
                    "C. Pressing down the teal vinyl to secure it.",
                    "D. Inserting the mat into the machine named \"Cricutjoy \"."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_50_real.mp4"
    },
    {
        "time": "0:04:00 - 0:04:20",
        "captions": "[0:04:00 - 0:04:20] [0:00:00 - 0:00:01]: Two keychains with tassels attached lie at the top of the frame on a grid-patterned surface. One keychain is purple with the name \"Natalie\" written on it, and the other is pink with a design resembling a knot. Below the keychains is a small white card. A pair of hands, wearing a purple sweater, are positioned at the bottom center of the frame, holding a semicircular piece of transparent paper with a teal design. [0:00:01 - 0:00:02]: The hands move the transparent paper with the teal design upwards, closer to the white card. The hands still hold the paper gently with their fingers on either side of the paper. [0:00:02 - 0:00:03]: The transparent paper with the teal design is now almost touching the white card. The detailed teal design on the paper becomes more apparent against the white background. [0:00:03 - 0:00:04]: The hands are now adjusting the position of the transparent paper over the white card. Both hands have fingers positioned symmetrically, ensuring careful alignment. [0:00:04 - 0:00:05]: The hands continue to adjust the transparent paper over the white card, ensuring it is perfectly aligned. The teal design, now fully visible, fits neatly over the white card. [0:00:05 - 0:00:06]: Once the transparent paper is aligned, the hands gently press down on it. The fingertips make sure the paper is secured onto the white card without any creases. [0:00:06 - 0:00:07]: The hands maintain their position as they apply even pressure across the entire surface of the transparent paper to ensure it sticks properly to the white card. [0:00:07 - 0:00:08]: The hands slowly begin to release the pressure, having successfully affixed the transparent paper to the white card.  [0:00:08 - 0:00:09]: The hands are now lifting slightly away from the card to inspect if the transparent paper with the teal design has properly adhered. [0:00:09 - 0:00:10]: The hands move the now-secured transparent paper and white card slightly sideways for further inspection. [0:00:10]: The hands continue adjusting the position of the teal design on the white card to ensure accuracy and avoid any misalignment. [0:00:11]: The hands continue to secure the transparent paper while slightly adjusting and pressing where needed.  [0:00:12]: With the card and transparent paper now firmly in place, the hands carefully move away, indicating that adjustments are complete. [0:00:13]: The hands return to the card, holding a new piece of transparent paper with a purple design, getting ready to repeat the process. [0:00:14]: The hands position the new paper over the white card. The delicate purple design contrasts against the teal design. [0:00:15]: The hands closely align this new transparent paper over the previously affixed teal design, ensuring it layers correctly. [0:00:16]: The hands press down gently again, ensuring the new layer of transparent paper adheres neatly over the first design. [0:00:17]: The hands work to remove any air bubbles and secure the new layer firmly, making sure the design remains intact. [0:00:18]: With both layers now firmly attached, the hands lift away for a final inspection, ensuring everything is smooth and aligned. [0:00:19]: The hands make minor adjustments to ensure the designs align perfectly, before taking a step back, revealing the completed layered design on the white card.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the hands doing right now?",
                "time_stamp": "0:04:07",
                "answer": "B",
                "options": [
                    "A. Move the hands closer to the white card.",
                    "B. Lift slightly away from the card to inspect the adhesion.",
                    "C. Secure the paper by pressing down firmly.",
                    "D. Place a new piece of transparent paper with a purple design."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_50_real.mp4"
    },
    {
        "time": "0:06:00 - 0:06:20",
        "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:07]: In the first sequence, a hand is shown holding a set of keychains with tassels. There are four keychains in total, each with a different name: \"Holly,\" \"Katie,\" \"Millie,\" and \"Elby.\" The keychains are circular with transparent edges and are held together by metal rings. Each keychain has a unique background color behind the name: \"Holly\" has a light pink background, \"Katie\" has a purple background, \"Millie\" has a light blue background, and \"Elby\" has a mint green background. A small heart graphic is displayed beneath each name. The tassels attached to the keychains match the respective background colors and hang in an organized fashion. [0:06:08]: The scene transitions to show a Cricut cutting machine with a green Cricut mat positioned below it on a grey grid background. The focus is on the setting, suggesting preparation for a crafting task. [0:06:09]: A pair of hands in a purple sweater begins placing a small piece of mint green material at the top right corner of the green Cricut mat, aligning it carefully with the grid lines. [0:06:10]: The hands position the mint green piece precisely on the mat, ensuring alignment within the grid. [0:06:11]: A piece of pink material is now being positioned in the grid\u2019s bottom right section, following the same careful alignment process. [0:06:12 - 0:06:13]: The hands place a purple piece of material at the bottom left section of the mat, ensuring it sticks properly to the adhesive mat surface. Alignment follows the grid lines meticulously to avoid any overlap. [0:06:14]: A blue piece of material is added above the purple piece, completing the arrangement on the mat. The green mat now displays four different colored pieces in each quadrant: mint green in the top right, blue in the top left, pink in the bottom right, and purple in the bottom left. [0:06:15]: The hands make final touches to the blue piece to ensure it is securely adhered to the mat. The overall arrangement shows a well-organized pattern, suggesting an upcoming multi-color crafting project. [0:06:16]: The hands pick up the mat and start guiding it into the Cricut cutting machine's opening slot, preparing it for the cutting process. [0:06:17 - 0:06:18]: The green mat with the four pieces of colored material is now partially inserted into the machine's slot, ready for the cutting operation. [0:06:19]: The mat is fully inserted into the machine, with the four pieces of material perfectly aligned, indicating readiness for the Cricut to begin the cutting task. The setup is complete, transitioning into the crafting action phase.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What did the person do just now?",
                "time_stamp": "00:06:12",
                "answer": "B",
                "options": [
                    "A. Arrange a pink material at the top right.",
                    "B. Place a purple piece of material at the bottom left.",
                    "C. Position a blue piece of material at the bottom right.",
                    "D. Set a mint green piece at the top left."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_50_real.mp4"
    },
    {
        "time": "0:08:00 - 0:08:20",
        "captions": "[0:08:00 - 0:08:20] [0:08:07 - 0:08:09]: Several square and circular pieces of translucent material, each bearing a spot of color, are arranged on a grid-patterned work surface. The squares display hues of pink, blue, and other pastels, while the circles remain plain. A pair of hands, one holding a circular piece with a purple design and the other using a small tool, are positioned in the lower half of the frame. The fingers precisely apply pressure to attach or adjust a piece of strip tape onto the circular material. [0:08:09 - 0:08:13]: The hands continue their meticulous work by placing the purple-colored circle onto the work surface. Then, with the tool in one hand, they apply a white rectangular label to one of the squares, ensuring the label's edges align perfectly with the square's grid pattern. [0:08:13 - 0:08:21]: The hands adjust the position of the purple circlular piece, verifying its attachment. Concentrated efforts are directed towards affixing the purple circle further, ensuring its adherence to the surface. Subsequently, the focus shifts from the purple circle to a blue-colored piece. With deliberate movements, they peel off the label from the blue square, securing it onto another transparent circular material. The process demonstrates a methodical approach to applying the materials for a precise finish.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What did the person do just now?",
                "time_stamp": "00:08:19",
                "answer": "B",
                "options": [
                    "A. Attach a purple circular piece with a strip tape.",
                    "B. Apply a blue pattern to a square.",
                    "C. Arrange pieces of pastel colors on the grid-patterned surface.",
                    "D. Hold a tool to adjust the position of the purple circle."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_50_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:30]",
        "questions": [
            {
                "task_type": "Prospective Reasoning",
                "question": "What concept might the speaker explain next?",
                "time_stamp": "00:00:40",
                "answer": "C",
                "options": [
                    "A. How to visualize data on a graph.",
                    "B. The types of charts used in data representation.",
                    "C. Differences between continuous and discrete data.",
                    "D. Examples of data collection methods."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "math tutorials",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_216_real.mp4"
    },
    {
        "time": "[0:02:36 - 0:03:06]",
        "questions": [
            {
                "task_type": "Prospective Reasoning",
                "question": "What might the speaker explain next?",
                "time_stamp": "00:02:59",
                "answer": "B",
                "options": [
                    "A. How to create a bar graph.",
                    "B. How to fill in the data in the data table.",
                    "C. How to interpret the data in the data table.",
                    "D. How to convert the data table into a pie chart."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "math tutorials",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_216_real.mp4"
    },
    {
        "time": "[0:05:12 - 0:05:42]",
        "questions": [
            {
                "task_type": "Prospective Reasoning",
                "question": "What might the speaker explain next?",
                "time_stamp": "00:05:38",
                "answer": "B",
                "options": [
                    "A. The average precipitation in May.",
                    "B. The meaning of the scale on the graph.",
                    "C. The total annual precipitation.",
                    "D. The method used to calculate the average precipitation."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "math tutorials",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_216_real.mp4"
    },
    {
        "time": "[0:07:48 - 0:08:18]",
        "questions": [
            {
                "task_type": "Prospective Reasoning",
                "question": "What might the speaker explain next?",
                "time_stamp": "00:08:01",
                "answer": "D",
                "options": [
                    "A. The use of the negative coordinate axis.",
                    "B. How the fluctuation in investment income impacts yearly analysis.",
                    "C. Strategies to improve investment returns.",
                    "D. The overall trend of investment income throughout the year."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "math tutorials",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_216_real.mp4"
    },
    {
        "time": "[0:10:24 - 0:10:54]",
        "questions": [
            {
                "task_type": "Prospective Reasoning",
                "question": "What might the speaker explain next?",
                "time_stamp": "0:10:09",
                "answer": "A",
                "options": [
                    "A. Interpret the trends shown in the graphs.",
                    "B. Introduce new data sets for comparison.",
                    "C. Calculate the mean of the data sets.",
                    "D. Convert the temperatures to Celsius."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "math tutorials",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_216_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video shows a small, white canvas placed on a flat surface. A transparent palette with mixed colors is positioned on the right side of the canvas. A hand appears holding a tube of dark paint, dispensing it onto the palette, and another hand holding a lighter-colored paint tube is shown moments later. [0:00:03 - 0:00:05]: The palette contains blobs of pink and white paint. The person dips a brush into the pink paint and begins to paint a horizontal stroke of pink at the top edge of the canvas. [0:00:06 - 0:00:08]: The individual continues to paint horizontally, moving slightly down the canvas, adding more pink paint with the brush. The strokes are even and consistent. [0:00:09 - 0:00:12]: The brush is reloaded with paint from the palette, and additional horizontal strokes are made below the previous layer. The color appears to be blended slightly with white, creating a gradient effect with a softer pink hue. [0:00:13 - 0:00:17]: The process continues with more paint being added gradually. The brushstrokes blend smoothly, and the gradient becomes more noticeable as the lighter pink transitions to white. [0:00:18 - 0:00:20]: The gradient effect becomes more pronounced as the brush moves lower on the canvas. The painting now exhibits a seamless transition from pink at the top to white at the bottom, with each stroke carefully blended to eliminate any harsh lines.",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why does the gradient effect become more pronounced?",
                "time_stamp": "0:00:20",
                "answer": "C",
                "options": [
                    "A. Because more dark paint is added.",
                    "B. Because the brushstrokes become vertical.",
                    "C. Because the brushstrokes blend smoothly from pink to white.",
                    "D. Because the person stops adding paint."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following best summarizes the painting process just shown in the video?",
                "time_stamp": "0:00:20",
                "answer": "A",
                "options": [
                    "A. The person paints a gradient from pink to white with smooth horizontal strokes.",
                    "B. The canvas is covered with random colors and patterns.",
                    "C. The palette is used to mix blue and green colors for a landscape.",
                    "D. The person uses a sponge to create a textured effect."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_140_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:19]: The video shows the painting of a landscape on a small square canvas. The scene includes a pink sky with a gradient transitioning to light yellow near the horizon. In the background, there is a silhouette of a distant mountain in light blue. Beneath the mountain, there are hills painted in various shades of green. The artist uses a thin brush to add fine details to the foreground hills, blending yellows and greens to create texture and depth. The artist\u2019s hand meticulously moves the brush, applying small strokes to enhance the grassy area on the right side of the canvas. A palette with mixed green and yellow paint is visible on the right edge of the frames. The workspace is well-lit, and the focus remains on the painting process, with the artist's hand occasionally obscuring parts of the canvas momentarily as they work.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What action is the artist performing right now?",
                "time_stamp": "00:02:19",
                "answer": "C",
                "options": [
                    "A. Mixing colors on the palette.",
                    "B. Framing the finished painting.",
                    "C. Applying fine details to the grass lawn.",
                    "D. Cleaning the brush with water."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_140_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:20]: The video shows a close-up view of someone painting on a small canvas using a thin paintbrush. The canvas depicts a landscape scene with a path leading through green fields towards a small white house with a red roof, positioned at the center. The background features blue mountains under a pink and yellow gradient sky. The painter\u2019s hand holding the brush is visible at the bottom right of the frames, and the brush is applying red paint to the roof of the house. A palette with mixed paint colors, including shades of yellow, white, blue, and brown, is positioned on the right side of the video frames.  Throughout the frames, the movement of the brush strokes is observed, adding fine details and adjusting the color on the roof. The video is shot from a first-person perspective, emphasizing the process of painting the roof of the miniature house to blend with the rest of the serene landscape. A section of text displaying paint color names \u201cBurnt umber\u201d and \u201cWhite\u201d appears in the top left corner of the image starting from the frame at [0:04:11] and continuing until the end. There are no significant background elements visible other than the painting and the palette.",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following best summarizes the painting just shown in the video?",
                "time_stamp": "00:04:20",
                "answer": "A",
                "options": [
                    "A. A landscape painting with a path leading to a small white house with a red roof.",
                    "B. A portrait of an elderly man being painted with fine details.",
                    "C. A close-up of a flower being painted with vibrant colors.",
                    "D. An abstract painting with various geometric shapes."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_140_real.mp4"
    },
    {
        "time": "[0:06:00 - 0:07:00]",
        "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:20]: In a detailed view, a person uses a small paintbrush with a light purple handle marked \"3/0 ART RIGHT\" to paint on a small canvas. The scene being painted features a landscape with a large dark brown tree with slender branches and a pinkish-red foliage. The background shows a gradient sky, transitioning from pink at the top to light blue towards the horizon. There is a mountain silhouette shaded in a light blue hue. Below the tree, a winding pathway takes a curved route towards a small, reddish-brown roofed house. The foreground comprises lush green grass, adding a vibrant touch to the pastoral setting.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is located below the large dark brown tree in the painting?",
                "time_stamp": "00:06:20",
                "answer": "D",
                "options": [
                    "A. A lake with a boat.",
                    "B. A group of animals.",
                    "C. A river with a bridge.",
                    "D. A grass lawn."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_140_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:41]: At the beginning of the scene, the camera is positioned at a viewing area that leads to an escalator covered by a green and glass structure. The sky is predominantly blue with white clouds. In the foreground, there is a food stand selling \"REEL TREATS\" to the left, and a few people are seen walking towards the escalator. [0:02:42 - 0:02:48]: The camera perspective shifts to the descent of the escalator. The semi-transparent green and glass arched roofing covers the escalator. Several people are seen ahead of the camera, descending the escalator at a steady pace. A mix of greenery and buildings is visible through the glass structure. The white flooring at the bottom of the escalator is gradually becoming more visible as the descent continues. [0:02:49 - 0:02:52]: The camera continues to descend, drawing closer to the bottom. More people are visible walking on the ground level, including a couple dressed in matching red and black ensembles. They are heading past some signs indicating directions for the \"STUDIO TOUR\" and \"RESTROOMS.\" [0:02:53 - 0:02:59]: As the camera nears the bottom of the escalator, more people appear walking in various directions, and the camera captures the bustling activity. At the bottom, several directional signs and another escalator entrance are clearly seen. The people walking by vary in their movement - some individuals are heading towards the signs, while others are seen walking away from it, highlighting the busy and dynamic atmosphere of the area.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What was the name of the food stand shown just now?",
                "time_stamp": "00:02:44",
                "answer": "B",
                "options": [
                    "A. TASTY TREATS.",
                    "B. REEL TREATS.",
                    "C. SWEET SNACKS.",
                    "D. FOOD HAVEN."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_312_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:30]: The video shows a journey on a tram or similar vehicle moving along a road adjacent to a wooded area. The view begins inside the tram with passengers seated, looking out the window which provides an outside view. The road is bordered by metallic guardrails and trees with thick foliage. The seats are facing forward with passengers looking towards a screen at the front of the tram. The exterior area visible through the windows includes a mixture of leafless and leafy trees surrounded by dense greenery. As the vehicle advances, the perspective gradually shifts from inside the tram to focus more on the unfolding scenery, highlighting the transition; [0:05:31 - 0:05:39]: There is an increase in the visibility of the outside scenery, including an area with clearer views past the trees, revealing a distant urban environment with buildings and a mountainous landscape in the background. The road continues to follow a gentle curve, with the surrounding trees slowly giving way to a more open view. The sky is clear with a few wispy clouds, and the environment looks bright and serene. The perspective shows fewer close-up views of the tram's interior and more of the open surroundings as the journey continues.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What kind of area is visible through the tram's windows during the journey?",
                "time_stamp": "0:05:30",
                "answer": "B",
                "options": [
                    "A. A sandy beach.",
                    "B. A wooded area with trees.",
                    "C. A bustling city street.",
                    "D. A snowy mountain path."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Object Recognition",
                "question": "How is the road next to the tram bordered?",
                "time_stamp": "0:05:46",
                "answer": "C",
                "options": [
                    "A. With a wooden fence.",
                    "B. With stone walls.",
                    "C. With metallic guardrails.",
                    "D. With hedges."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_312_real.mp4"
    },
    {
        "time": "[0:08:00 - 0:09:00]",
        "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:02]: The view begins with a first-person perspective driving along a road. On the left, there is a large building with the word \"Justice\" written in bold white and black letters. Directly in front, several billboards are visible, including one advertising \"MILK\" featuring a man\u2019s face. There\u2019s a small vehicle moving quickly on the left side of the road. [0:08:03 - 0:08:06]: The drive continues, revealing more billboards. These advertise movies such as \"Bridesmaids\" and \"ted\", among others. The background consists of tall buildings with reflective windows and a clear blue sky. [0:08:07 - 0:08:08]: As the vehicle moves further along, additional billboards become visible, such as an advertisement for \"Les Mis\u00e9rables\" and a couple more with unknown titles. [0:08:09 - 0:08:10]: The perspective shows the edge of the vehicle from which the recording is done, with additional billboards promoting what appears to be other movies or media. The surrounding area includes more buildings, some painted in light colors. [0:08:11 - 0:08:16]: The path continues past more advertisements and a large, long building on the right. There is a mix of concrete and other structures along the roadside. The road slightly curves. [0:08:17 - 0:08:19]: The video concludes with a view of a large grey building on the right side, several trucks and vehicles are parked along the side, and large movie posters appear on the building\u2019s exterior walls.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What types of advertisements were seen on the billboards just now?",
                "time_stamp": "0:08:16",
                "answer": "C",
                "options": [
                    "A. Food products.",
                    "B. Clothing brands.",
                    "C. Movie promotions.",
                    "D. Electronics."
                ],
                "required_ability": "episodic memory"
            },
            {
                "task_type": "Action Recognition",
                "question": "What is the perspective of the view as the vehicle drives along the road?",
                "time_stamp": "0:08:10",
                "answer": "B",
                "options": [
                    "A. Aerial view.",
                    "B. First-person perspective.",
                    "C. Third-person perspective.",
                    "D. Rear view."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_312_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The scene begins with a person holding a folded, transparent umbrella in their hand, walking up a set of wet stairs tiled with a safety pattern. The background shows a tiled wall and a handrail. [0:00:03 - 0:00:05]: As the person ascends, they begin to open the umbrella. The street-level background becomes more visible, with a few pedestrians walking under umbrellas. [0:00:06 - 0:00:09]: The person holding the umbrella approaches the street, surrounded by various buildings and objects. Signs and maps are visible on the wall, with a yellow-tactile paving and wet pavement leading onto a zebra crossing. [0:00:09 - 0:00:12]: The person starts walking over the wet zebra crossing while holding the opened umbrella. Other people are walking with umbrellas, and a few cars are seen on the road. Some greenery and road signs are visible to the left. [0:00:12 - 0:00:15]: The person continues to walk on the zebra crossing, heading towards a sidewalk. More pedestrians with umbrellas are visible, along with another building with various store signs. [0:00:15 - 0:00:19]: As the person reaches the sidewalk, they turn right and continue walking along a row of shops. The pathway ahead shows more pedestrians with their umbrellas open. The building textures, signs, and greenery to the left reflect the wet conditions due to the rain. [0:00:19 - 0:00:20]: The scene continues with the person walking along the sidewalk, passing storefronts with closed shutters and illuminated signs. Other pedestrians walk ahead under their umbrellas, and more of the cityscape is visible in the background with damp sidewalks and overcast skies.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What object is the person holding in the hand?",
                "time_stamp": "0:00:03",
                "answer": "A",
                "options": [
                    "A. A folded umbrella.",
                    "B. A map.",
                    "C. A shopping bag.",
                    "D. A book."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_324_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:07]: The scene is set on a rainy day in an urban environment. The camera view shows a wet street with various individuals walking under umbrellas. A row of buildings lines the left side of the street, and there is a sidewalk adjacent to the buildings. Some buildings are several stories high with modern facades. People are walking along the sidewalk, some carrying umbrellas. A police car is driving along the street with its headlights on. The street itself has two lanes divided by a crosswalk marked with white stripes. There are metal railings along the sidewalk to separate pedestrians from the road. In the distance, more buildings and vehicles are visible, indicating a bustling city environment. [0:03:08 - 0:03:13]: The frame captures more pedestrians walking along the sidewalk and crossing the street. The camera moves forward along the sidewalk. The street to the right shows parked cars and more buildings. The wet ground reflects the surroundings, enhancing the rainy atmosphere. Leafy green trees in metal planters are placed intermittently along the sidewalk. A pedestrian can be seen riding a bicycle down the sidewalk. The crosswalk in view appears to be frequently used as more people approach and cross it.  [0:03:14 - 0:03:20]: The camera angle shifts slightly to capture more of the crosswalk and the view further down the street. The buildings continue to line both sides of the street, with shops, offices, and residential windows visible. Pedestrians with umbrellas continue to walk briskly, trying to avoid the rain. The street surface is reflective from the rain, and the crosswalk lines are clear and distinct. The overall movement of pedestrians and occasional vehicles creates a sense of active city life despite the dreary weather conditions.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the distinctive feature of the crosswalk?",
                "time_stamp": "00:03:20",
                "answer": "B",
                "options": [
                    "A. It has red and white stripes.",
                    "B. It is marked with white stripes.",
                    "C. It is marked with yellow stripes.",
                    "D. It has green and white stripes."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_324_real.mp4"
    },
    {
        "time": "[0:06:00 - 0:07:00]",
        "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:03]: On a rainy day, a first-person perspective video captures a wet sidewalk with red brick pavement. In the background, a mix of modern and older buildings line the street. Several people are visible carrying umbrellas while walking down the sidewalk. A blue truck is parked on the right side of the road, and a few pedestrians can be seen walking near it. [0:06:04 - 0:06:07]: As the person moves along the sidewalk, the view shifts slightly to the left, showing more pedestrians crossing a wet street at a crosswalk. A white van is waiting at the crosswalk, and people holding umbrellas are walking in various directions. [0:06:08 - 0:06:11]: The movement continues down the sidewalk, where more people with umbrellas are seen. A black taxi with its lights on is driving down the wet road beside the sidewalk. The sidewalk is separated from the road by a metal railing. [0:06:12 - 0:06:15]: The rain continues to pour, visible by the droplets on the camera lens. The camera captures more people walking both on the sidewalk and across the street, all holding different colored umbrellas. Some people walk alone while others walk in pairs or small groups. [0:06:16 - 0:06:19]: The video continues to show more of the sidewalk, with the rain still falling. Pedestrians are seen walking toward and away from the camera. The sidewalk is lined with green plants and trees, adding a bit of nature to the urban setting. The black taxi is still visible in the distance, moving down the street past the blue truck.",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "What is the overall weather condition depicted in the video?",
                "time_stamp": "0:06:19",
                "answer": "C",
                "options": [
                    "A. Sunny.",
                    "B. Snowy.",
                    "C. Rainy.",
                    "D. Foggy."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_324_real.mp4"
    },
    {
        "time": "[0:09:00 - 0:10:00]",
        "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:02]: The scene begins on a rainy street viewed from a first-person perspective. Multiple vehicles, including a black car and a grey van, are driving on the wet road, lined with modern buildings and lush green trees. Some pedestrians hold umbrellas. [0:09:03 - 0:09:04]: The camera angle shows more of the sidewalk, presenting several bicycles parked against the pavement railing. There are also more pedestrians in the scene, some carrying umbrellas. [0:09:05 - 0:09:08]: The perspective shifts further down the sidewalk, revealing more of the street's expanse and the arrangement of trees, vehicles, and buildings. The rain is clearly visible on the umbrella and wet pavement, indicating ongoing drizzle. [0:09:09 - 0:09:10]: The angle now targets a biking area with parked bikes, showing more details of the wet conditions as the rain continues to fall. The empty space on the sidewalk highlights the rainfall's effect on the typically busy street. [0:09:11 - 0:09:13]: As the video progresses, the viewer's gaze shifts towards colorful signs written in Japanese on handrails and building walls, emphasizing the vibrant urban environment. [0:09:14 - 0:09:15]: The first-person perspective navigates towards a crosswalk, revealing more parked vehicles, including two white vans, and additional Japanese signage. The atmosphere remains consistently wet and rainy. [0:09:16 - 0:09:18]: Further down, the video showcases a more extended part of the sidewalk. The presence of rain significantly impacts the visual clarity and creates reflections from the pavement. [0:09:19 - 0:09:20]: The video wraps up with a view down the street, prominently featuring multiple parked vehicles, signage, and a clear path extending into the urban environment under persistent rain.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What was a noticeable feature of the street just now?",
                "time_stamp": "0:09:20",
                "answer": "C",
                "options": [
                    "A. It is lined with colorful flowers.",
                    "B. The road is empty.",
                    "C. It has a lot of parked bicycles and is wet due to rain.",
                    "D. It is empty and dry with clear skies."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_324_real.mp4"
    },
    {
        "time": "[0:11:00 - 0:11:59]",
        "captions": "[0:11:40 - 0:11:59] [0:11:40 - 0:11:54]: The video takes place in a large, open courtyard surrounded by tall buildings. Trees line the courtyard, with two prominent trees at the center. The trees have thick trunks and lush green leaves. To the right, there is a pathway with some people walking. The ground is wet from rain, creating a reflective surface. There is a small, white building at the far end of the courtyard. The viewer's perspective moves forward slowly, and it appears to be raining, as indicated by the raindrops visible on an umbrella held above the camera.  [0:11:55 - 0:11:59]: As the perspective continues to move forward, more details of the surroundings become visible. On the left, there is a playground with slides and climbing structures, while the buildings in the background are varied in height and architectural style. The trees, still in focus, add a natural element to the urban environment. The overall setting remains consistent, with wet surfaces and raindrops on the umbrella.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the small white building located in the courtyard?",
                "time_stamp": "00:11:59",
                "answer": "B",
                "options": [
                    "A. To the left of the pathway.",
                    "B. At the far end of the courtyard.",
                    "C. Next to the playground.",
                    "D. In the center of the courtyard."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_324_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:01]: A person is standing in a kitchen by a large countertop. They are stirring food in a black frying pan with a red spatula. Various ingredients and utensils are spread out on the counter, including a lemon and some herbs. The background includes a refrigerator and cabinets. [0:02:01 - 0:02:02]: The person continues to stir the contents of the frying pan, which appears to contain sliced onions. They are holding the pan with their left hand and the spatula with their right hand. [0:02:02 - 0:02:03]: A close-up view of the frying pan shows the sliced onions being stirred and saut\u00e9ed. The person is using the spatula to mix the onions while holding the pan over the stove. [0:02:03 - 0:02:04]: The person continues stirring the onions in the frying pan. The camera remains focused on the pan and their hands, showing a controlled and steady stirring motion. [0:02:04 - 0:02:05]: The person gestures with their left hand while holding the red spatula upright in their right hand. They seem to be explaining something while maintaining eye contact, with the frying pan still on the stove. [0:02:05 - 0:02:06]: The person continues using the spatula in the frying pan with their right hand, looking down at the pan. The countertop and objects on it remain visible. [0:02:06 - 0:02:07]: The person continues stirring the contents in the frying pan. The background, including the cabinets and window, remains unchanged. [0:02:07 - 0:02:08]: The person steps away from the stove, moving towards the counter. They reach for something on the counter while the pan remains on the stove. [0:02:08 - 0:02:09]: The person picks up a utensil from the counter, holding it with their left hand. The frying pan remains on the stove while they prepare to use the utensil. [0:02:09 - 0:02:10]: A close-up of the person sharpening a knife. They hold the sharpening tool with their left hand and the knife with their right hand, moving it steadily across the tool. [0:02:10 - 0:02:11]: The person holds the sharpened knife up, displaying it. They appear to be explaining something, with their attention focused ahead. [0:02:11 - 0:02:12]: The person begins to gesture with their left hand while still holding the knife. The green cup and the bottle of oil on the counter remain in their positions. [0:02:12 - 0:02:13]: The person continues to gesture with their left hand, appearing to explain further. The countertop with the various ingredients and utensils is visible behind them. [0:02:13 - 0:02:14]: The person reaches towards the counter with their left hand, appearing to pick up one of the ingredients. The pan is still on the stove. [0:02:14 - 0:02:15]: The person picks up an ingredient from a small bowl on the counter. They continue to explain something while holding the ingredient. [0:02:15 - 0:02:16]: The person moves back towards the stove with the ingredient in their left hand. They are preparing to add it to the frying pan. [0:02:16 - 0:02:17]: The person adds the ingredient to the frying pan with a precise motion. The green cup and bottle of oil are still visible on the counter. [0:02:17 - 0:02:18]: The person continues to focus on the frying pan, stirring the contents with the red spatula. [0:02:18 - 0:02:19]: The person continues stirring the frying pan\u2019s contents with the added ingredient, ensuring it mixes well. Their attention is fully on the frying pan.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the person using to stir the contents in the frying pan?",
                "time_stamp": "00:02:01",
                "answer": "C",
                "options": [
                    "A. A wooden spoon.",
                    "B. A metal spatula.",
                    "C. A red spatula.",
                    "D. A plastic spoon."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_27_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:02]: A man stands in a modern, brightly lit kitchen in front of a stainless steel refrigerator and white subway-tiled wall. He places a bottle of white wine on the marble countertop, next to a variety of ingredients, including a lemon, fresh herbs, and a plate of prawns. [0:04:02 - 0:04:04]: The man picks up the bottle of white wine and begins pouring it into a frying pan on the stove. The pan contains a mixture of ingredients, including cherry tomatoes and herbs, which begin to sizzle as the wine is added. [0:04:04 - 0:04:06]: He continues to pour wine into the frying pan while stirring the contents with a red spatula. The contents in the pan are a vibrant mix of colors, primarily red and yellow from the cherry tomatoes. [0:04:06 - 0:04:08]: The camera zooms in to show a close-up of the pan. As the wine is added, steam rises, and the tomatoes and other ingredients continue to cook, releasing more aroma. [0:04:08 - 0:04:10]: The man briefly stops pouring wine and places the bottle back on the countertop. He then stirs the ingredients in the pan with a red spatula, ensuring even cooking and mixing. [0:04:10 - 0:04:12]: He continues to stir the contents of the frying pan, which now have a rich, simmering texture. Steam continues to rise from the pan, indicating the heat and ongoing cooking process. [0:04:12 - 0:04:13]: The camera shows a closer view of the frying pan, focusing on the tomatoes and other ingredients as they simmer and combine with the wine, creating a bubbling effect. [0:04:13 - 0:04:15]: The man stirs the mixture with the red spatula, ensuring that all ingredients are well-combined and cooked evenly. The sizzling sound intensifies, and more steam rises as the mixture cooks down. [0:04:15 - 0:04:17]: The man continues stirring and then briefly gestures with his free hand. He speaks, possibly explaining what he\u2019s doing or offering cooking tips. The background remains consistent, with a well-organized kitchen and natural light streaming in from a window. [0:04:17 - 0:04:19]: The man then moves away from the stove, briefly placing the spatula down. He reaches out to grab another ingredient from the countertop, preparing to add it to the frying pan. The assortment of kitchen tools and ingredients spread out on the counter reflects a well-prepared cooking setup.",
        "questions": [
            {
                "task_type": "Prospective Reasoning",
                "question": "What is the man likely to do next after reaching out to grab another ingredient from the countertop?",
                "time_stamp": "0:04:19",
                "answer": "B",
                "options": [
                    "A. Serve the dish immediately.",
                    "B. Add the new ingredient to the frying pan.",
                    "C. Clean the countertop.",
                    "D. Turn off the stove."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_27_real.mp4"
    },
    {
        "time": "[0:06:00 - 0:07:00]",
        "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:01]: The video opens with a kitchen scene where a man in a navy blue shirt is preparing ingredients on a wooden cutting board placed on a marble countertop. Several items, including lemons, mushrooms, a green ceramic container, and a bowl, are placed on the counter. Stainless steel appliances and white tiled walls are visible in the background. The man is focused on chopping or arranging something on the board. [0:06:01 - 0:06:02]: The man reaches towards the bowl on his left and starts to move it closer to him, positioning it better for the next step of his preparation. His attention seems to be fully on the bowl and its contents. [0:06:02 - 0:06:03]: He uses both hands to handle the bowl, ensuring it is steady. Meanwhile, the countertop with the various ingredients and utensils remains organized, aiding in a smooth cooking process. [0:06:03 - 0:06:04]: Concentration remains high as he adjusts the position of the bowl. The lemons and other ingredients on the counter stay intact but await their purpose in the recipe. Natural light from the window nearby illuminates the scene, indicating it's daytime. [0:06:04 - 0:06:05]: After adjusting the bowl, the man resumes his focus on the cutting board, this time seemingly ready to incorporate the contents of the bowl into his preparation. The hand movements are deliberate and precise. [0:06:05 - 0:06:06]: His attention shifts as he places a piece from the bowl onto the cutting board, maintaining the cleanliness and order of his workspace. The bright natural light from outside continues to light up the kitchen. [0:06:06 - 0:06:07]: With the bowl properly positioned, the man retrieves another ingredient or utensil from the side, perhaps preparing to add it to what he was handling previously. His movements are calm and measured, highlighting his experience. [0:06:07 - 0:06:08]: Next, he begins seasoning the food item on the board with a pepper mill, suggesting that the preparation is nearing the cooking stage. The countertop remains organized, with ingredients lying in wait for further use. [0:06:08 - 0:06:09]: A close-up view zooms in on the shrimp on the cutting board as it starts getting seasoned with black pepper. The detail of the pepper falling onto the shrimp is clearly visible, indicating careful seasoning. [0:06:09 - 0:06:10]: The hand holding the pepper mill is shown up close while peppering the shrimp, emphasizing the action and detail involved in ensuring even seasoning across all pieces. [0:06:10 - 0:06:11]: Back to the wider kitchen view, the man places the pepper mill back on the counter and reaches for another ingredient, perhaps salt, as he continues the seasoning process. [0:06:11 - 0:06:12]: He picks up a container of salt and starts sprinkling it onto the shrimp, ensuring they are properly seasoned. His meticulous attention to detail is evident. [0:06:12 - 0:06:13]: The camera focuses closely again on the shrimp being seasoned with salt. The granules are visible as they land on the seafood, adding to the flavoring process. [0:06:13 - 0:06:14]: A pile of seasoned shrimp on the cutting board is shown up close, with the mix of black pepper and salt visible on them. The man's fingers slightly adjust the shrimp to ensure even coverage of spices. [0:06:14 - 0:06:15]: The man continues to handle the shrimp, giving them one last adjustment to ensure they are evenly coated with the pepper and salt. Each movement is purposeful and controlled. [0:06:15 - 0:06:16]: The man adjusts the shrimp one last time to make sure the seasoning is spread well, readying the seafood for cooking.  [0:06:16 - 0:06:17]: Another close-up shows the shrimp resting on the cutting board with seasoning all around, ready for the next step in preparation. The man's fingers are visible, indicating his preparedness for subsequent actions. [0:06:17 - 0:06:18]: Returning to the broader kitchen view, the man might be discussing or considering the next steps in the recipe. His hands rest on the cutting board near the shrimp, indicating a brief pause before continuing. [0:06:18 - 0:06:19]: The man resumes a more involved stance, appearing to talk or explain while pointing towards something near the shrimp. The kitchen remains orderly, with ingredients and utensils ready for further use.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What is the main activity the man is engaged in right now?",
                "time_stamp": "00:06:24",
                "answer": "A",
                "options": [
                    "A. Cooking shrimp.",
                    "B. Preparing a salad.",
                    "C. Washing vegetables.",
                    "D. Baking bread."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_27_real.mp4"
    },
    {
        "time": "[0:08:00 - 0:09:00]",
        "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: In a bright kitchen with white tiled walls and cabinets, there is a large stainless steel refrigerator on the left side. A chef in a navy blue shirt stands behind a marble kitchen island. Various cooking utensils and ingredients are placed on the countertop and stoves. He gestures with his hands above a pot on the stove, which has steam rising from it. Two pans and a bottle of olive oil are also on the stove.  [0:08:01 - 0:08:02]: The chef bends slightly toward the pot, stirring it with a spoon. His left hand rests on the counter. The stainless steel refrigerator and kitchen counters are visible in the background. [0:08:02 - 0:08:03]: The chef's right hand continues stirring the pot while his left hand is now raised, possibly gesturing for emphasis. The countertops surround the cooking area with another stove containing a metal bowl and bottle of olive oil. [0:08:03 - 0:08:04]: The chef leans forward, focusing on the pot he is stirring. The kitchen behind him remains orderly, featuring various kitchen tools. He reaches over the pot positioned on the stove. [0:08:04 - 0:08:05]: With his right hand, the chef raises a utensil from the pot, inspecting its contents. His left hand is now positioned on the countertop for support. [0:08:05 - 0:08:06]: The chef looks toward the small pan on his left, and his hand reaches toward it. Olive oil bottles and metal bowls are still on the counter. [0:08:06 - 0:08:07]: The chef picks up the small pan and carefully flips the food inside. Behind him, natural light streams in from the window, illuminating the space. [0:08:07 - 0:08:08]: He continues to flip the food in the pan, ensuring even cooking. The kitchen's neat and organized setup with essential utensils and ingredients is visible around. [0:08:08 - 0:08:09]: The chef places the flipped food back onto the burner, adjusting its position on the stovetop. The kitchen background remains consistent, with the window and cabinets well designed. [0:08:09 - 0:08:10]: The chef's focus remains on the small pan as he continuously flips the food inside, ensuring it's perfectly cooked. The pot, utensils, and other cooking items are orderly on the countertop. [0:08:10 - 0:08:11]: As he flips the food inside the pan, the chef checks its cooking progress. His movements are precise and controlled, indicative of his professional expertise. [0:08:11 - 0:08:12]: A close-up shot of a different pan reveals shrimp and vegetables sizzling inside, releasing steam upward. [0:08:12 - 0:08:13]: The close-up continues, showing the sizzling shrimp and vegetables, with bits of green and red indicating spices and herbs. [0:08:14 - 0:08:15]: The chef steps back, holding a white cloth, gesturing toward the cooking food. The kitchen's clean and organized setup with essential implements remains visible. [0:08:15 - 0:08:16]: The chef picks up the pan, the white cloth in his left hand, focused on maintaining the food's presentation. Kitchen elements like the refrigerator and cabinets remain in the frame. [0:08:16 - 0:08:17]: He adjusts the position of the pan with the white cloth, ensuring that the food cooks evenly, maintaining the neat setting of the kitchen. [0:08:17 - 0:08:18]: Another close-up shot of the sizzling shrimp and vegetables in the pan on the stovetop, continuing to cook. [0:08:18 - 0:08:19]: The continuing close-up shows the same pan angle, highlighting the intense cooking action and rich colors of the shrimp and vegetables. [0:08:19 - 0:08:20]: The chef leans over a pot on the stove, using a pair of tongs to handle the food inside, with the kitchen in the background.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the chef stirring in the pot?",
                "time_stamp": "0:08:02",
                "answer": "C",
                "options": [
                    "A. A sauce.",
                    "B. Vegetables.",
                    "C. noodles.",
                    "D. Shrimp."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Object Recognition",
                "question": "What item is used by the chef to handle the food inside the pot?",
                "time_stamp": "0:08:25",
                "answer": "C",
                "options": [
                    "A. A spoon.",
                    "B. A spatula.",
                    "C. A pair of tongs.",
                    "D. A fork."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_27_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:05]: Two sketchbooks are on a table. One is open with a black elastic strap laying across the left page. Behind them is an out-of-focus portrait of a person. A hand holding a tube with the text \"GOLDEN 100\" is at the right side; A paintbrush is placed on the table between the sketchbooks. [0:00:06 - 0:00:07]: A small bottle dispenses a liquid onto the open page of the sketchbook. [0:00:08 - 0:00:09]: The liquid continues to collect in a small pool on the paper. [0:00:10 - 0:00:15]: The paintbrush is being used to spread the liquid onto the entire page of the open sketchbook in horizontal motions.  [0:00:16 - 0:00:17]: The page is now evenly coated with the liquid, and the brush moves away. [0:00:18 - 0:00:19]: Overhead, a black object (possibly a light or another tool) is near the open sketchbook. A pencil and pen are on the table next to the book, and the page appears dry.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is the position of the paintbrush relative to the sketchbooks initially?",
                "time_stamp": "0:00:20",
                "answer": "B",
                "options": [
                    "A. In front of the open sketchbook.",
                    "B. Between a sketchbook and a white item.",
                    "C. Behind the closed sketchbook.",
                    "D. To the left of the open sketchbook."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_133_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:24]: The video shows a close-up of a person's hand painting a portrait. The portrait is partially completed with a focus on the face, which features a mix of vibrant colors including blue, yellow, and brown. The painting is on a white paper, with the artist using a flat brush to add color to the bottom part of the face, near the mouth and chin. The artist's hand is visible holding a brush. [0:03:25 - 0:03:31]: The hand continues to apply paint to the portrait, focusing on blending the colors around the mouth and chin area. The strokes are precise, adding depth and texture to the portrait. The person's features in the portrait become more defined with each brushstroke, and the use of multiple colors gives the portrait a dynamic and expressive feel. [0:03:32 - 0:03:39]: The artist makes final adjustments and details to the face, smoothing and blending the colors around the chin and neck. The background remains plain, keeping the focus entirely on the portrait's face. The expressive use of color and the careful brushwork make the portrait appear more vivid and lifelike as the video progresses.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What is the artist focusing on right now?",
                "time_stamp": "00:03:39",
                "answer": "B",
                "options": [
                    "A. Adding a background.",
                    "B. Painting the portrait's face.",
                    "C. Sketching the portrait.",
                    "D. Cleaning the palette."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_133_real.mp4"
    },
    {
        "time": "[0:06:00 - 0:07:00]",
        "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:59]: The video captures the process of painting a detailed portrait of a man with dark hair and a serious expression. The canvas is white with shades of teal, blue, and green used for the background and the man's shirt. The artist's hand is visible holding a brush, applying strokes to the portrait. The brush moves mainly around the man's face and background, adding details and blending colors. The book, in which the painting is set, shifts slightly as the artist works, and the brush strokes become more precise over time. The scene appears to be set on a flat surface, probably a table, and occasional glimpses of other painting tools can be seen in the background.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the painting set while the artist works on it?",
                "time_stamp": "0:07:00",
                "answer": "C",
                "options": [
                    "A. On an easel.",
                    "B. On the floor.",
                    "C. On a table.",
                    "D. On a wall."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following best summarizes the painting process shown just now?",
                "time_stamp": "0:07:00",
                "answer": "B",
                "options": [
                    "A. A rough sketch of a landscape.",
                    "B. Add background color to the portrait.",
                    "C. An abstract painting with vibrant colors.",
                    "D. A still life painting of fruits and flowers."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_133_real.mp4"
    },
    {
        "time": "[0:10:00 - 0:11:00]",
        "captions": "[0:10:00 - 0:10:20] [0:10:00 - 0:10:09]: The video shows a close-up view of a painting in progress. The painting is of a person's face, characterized by dark hair and a serious expression, rendered in shades of green, blue, and white. The brushstrokes are bold, with noticeable texture. The artwork is being created in an open sketchbook with white pages. A paintbrush is actively applying paint to the canvas, moving around the facial features, particularly focusing on the areas near the eyes and cheeks. A dark-colored mug with the text \"THAT'S TRUST IN PIXIE DUST\" in golden letters is visible to the right side of the sketchbook. The background surface is light and neutral, likely a desk or table. [0:10:10 - 0:10:19]: The camera continues to focus on the painting while the brush moves, touching up the details around the eyes and other facial features. The perspective remains consistent, maintaining a clear view of the painting and the ongoing artistic process. The brush techniques vary slightly, applying different shades and blending colors to enhance the texture and depth of the portrait. The overall appearance of the painting remains vibrant and expressive.",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following best summarizes the painting process in the video?",
                "time_stamp": "00:10:19",
                "answer": "C",
                "options": [
                    "A. Bold brushstrokes on a colorful landscape.",
                    "B. Fine details on a still life painting.",
                    "C. Detailing a portrait of a man with dark hair and a serious expressions.",
                    "D. Subtle shading on an abstract design."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_133_real.mp4"
    },
    {
        "time": "0:00:00 - 0:00:20",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:00:18",
                "answer": "A",
                "options": [
                    "A. 0.",
                    "B. 1.",
                    "C. 3.",
                    "D. 4."
                ],
                "required_ability": "episodic memory"
            },
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:00:58",
                "answer": "A",
                "options": [
                    "A. 0.",
                    "B. 1.",
                    "C. 3.",
                    "D. 4."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_71_real.mp4"
    },
    {
        "time": "0:02:40 - 0:03:00",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:02:18",
                "answer": "A",
                "options": [
                    "A. 0.",
                    "B. 1.",
                    "C. 3.",
                    "D. 4."
                ],
                "required_ability": "episodic memory"
            },
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:02:58",
                "answer": "A",
                "options": [
                    "A. 0.",
                    "B. 1.",
                    "C. 3.",
                    "D. 4."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_71_real.mp4"
    },
    {
        "time": "0:05:20 - 0:05:40",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:05:18",
                "answer": "A",
                "options": [
                    "A. 0.",
                    "B. 1.",
                    "C. 3.",
                    "D. 4."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_71_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:20]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Which side of the trail has more dense foliage right now?",
                "time_stamp": "00:00:10",
                "answer": "C",
                "options": [
                    "A. The left side.",
                    "B. The right side.",
                    "C. Both sides equally.",
                    "D. Neither side has foliage."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_166_real.mp4"
    },
    {
        "time": "[0:01:47 - 0:02:07]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the large tree located right now?",
                "time_stamp": "00:02:06",
                "answer": "D",
                "options": [
                    "A. On the left side of the road.",
                    "B. Ahead, slightly to the left.",
                    "C. Overhead, providing shade.",
                    "D. On the right side of the road."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_166_real.mp4"
    },
    {
        "time": "[0:03:34 - 0:03:54]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the shadowed area located right now?",
                "time_stamp": "00:03:53",
                "answer": "B",
                "options": [
                    "A. On the left side of the road.",
                    "B. Across the entire road.",
                    "C. On the right side of the road.",
                    "D. No shadow at all."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_166_real.mp4"
    },
    {
        "time": "[0:05:21 - 0:05:41]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is located immediately ahead in the clearing right now?",
                "time_stamp": "00:05:33",
                "answer": "A",
                "options": [
                    "A. An intersection.",
                    "B. A steep hill.",
                    "C. A large boulder.",
                    "D. A narrow bridge."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_166_real.mp4"
    },
    {
        "time": "[0:07:08 - 0:07:28]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is mainly visible on the left side of the road right now?",
                "time_stamp": "00:07:20",
                "answer": "B",
                "options": [
                    "A. A lake.",
                    "B. Trees and dense foliage.",
                    "C. A building complex.",
                    "D. Farmland."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_166_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:10]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the activity that transpired just now?",
                "time_stamp": "00:00:10",
                "answer": "D",
                "options": [
                    "A. The person cooked fish filets, garnished them with vegetables, and presented them for serving.",
                    "B. The person assembled burgers by adding patties, placing condiments, and wrapping them.",
                    "C. The person restocked sandwich ingredients, such as lettuce and tomatoes, in preparation areas.",
                    "D. The person cleaned a used tray by discarding waste, and preparing it for reuse."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_343_real.mp4"
    },
    {
        "time": "[0:01:15 - 0:01:25]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the activity that transpired just now?",
                "time_stamp": "00:01:25",
                "answer": "D",
                "options": [
                    "A. The worker inspected the kitchen utensils, adjusted the oven temperature, and cleaned the grill.",
                    "B. The worker arranged cooked patties on a slide, cleaned the grill surface, and prepared for the next batch.",
                    "C. The worker deep-fried chicken, assembled it on a plate, and garnished it with herbs.",
                    "D. The worker flipped burger patties, placed them on a tray,."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_343_real.mp4"
    },
    {
        "time": "[0:02:30 - 0:02:40]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the activity that transpired just now?",
                "time_stamp": "00:02:40",
                "answer": "D",
                "options": [
                    "A. The worker arranged hotdogs on a grill, added condiments, and wrapped them for serving.",
                    "B. The worker cooked and seasoned bacon strips and placed them into the fryer for crispiness.",
                    "C. The worker assembled fish fillets, sprinkled them with seasoning, and placed them in the oven for baking.",
                    "D. The worker transferred cooked onion onto a tray, cleaned the surface of the grill, and prepared for the next cooking batch."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_343_real.mp4"
    },
    {
        "time": "[0:03:45 - 0:03:55]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the activity that transpired just now?",
                "time_stamp": "00:03:55",
                "answer": "D",
                "options": [
                    "A. The individual prepared a batch of scrambled eggs, added herbs, and placed them on a hot plate.",
                    "B. The individual restocked a freezer with packages of frozen vegetables, organized them by color, and labeled each bin.",
                    "C. The individual cleaned and sanitized a workstation, organized cooking utensils, and prepared for the next task.",
                    "D. The individual prepared strips of bacon by separating them from frozen packaging."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_343_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:05:10]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the activity that transpired just now?",
                "time_stamp": "00:05:10",
                "answer": "D",
                "options": [
                    "A. The worker cleaned the grill, prepared eggs by cracking and whisking them, and started cooking pancakes.",
                    "B. The worker poured liquid eggs into a container, organized kitchen supplies, and cleaned the preparation area.",
                    "C. The worker cracked eggs into a bowl, mixed them with spices, and set them aside for cooking later.",
                    "D. The worker filled a container with liquid eggs."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_343_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:06]: A person with long curly hair is standing against a rough, pale green wall. The individual is wearing a loose, beige garment. Initially, they are looking down and appear to be adjusting their clothes with their hands near their hair.  [0:00:07 - 0:00:11]: The person turns slightly and begins to pick up a large piece of beige fabric. The background includes a draped cloth, some wooden beams, and a glimpse of darker fabric or objects near the floor. [0:00:12 - 0:00:15]: The person lifts the cloth and starts draping it over themselves, threading their arms through the sleeves. Their movements are deliberate, and they seem focused on their task. [0:00:16 - 0:00:20]: The person continues adjusting the garment, ensuring it is properly positioned. Their arms move in a fluid manner as they finalize the arrangement, looking slightly upwards towards the end. The background remains consistent, with the same draped cloth and wooden elements visible.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the person picking up in the video?",
                "time_stamp": "00:00:11",
                "answer": "B",
                "options": [
                    "A. A small piece of fabric.",
                    "B. A large piece of beige fabric.",
                    "C. A wooden beam.",
                    "D. A piece of darker fabric."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Action Recognition",
                "question": "What action does the person perform with the large piece of fabric?",
                "time_stamp": "00:00:15",
                "answer": "B",
                "options": [
                    "A. Ties it around their waist.",
                    "B. Drapes it over themselves and threads arms through the sleeves.",
                    "C. Folds it neatly and places it on the floor.",
                    "D. Wraps it around their head."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "getting_dressed",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_160_real.mp4"
    },
    {
        "time": "[0:01:00 - 0:02:00]",
        "captions": "[0:01:20 - 0:01:40] [0:01:20 - 0:01:22]: A person with long curly hair wearing a light-colored dress stands in a rustic room with wooden walls and a window. They are holding and inspecting a small object, possibly a necklace, with greenery visible through the window. [0:01:23 - 0:01:25]: The person bends over a wooden table to place or pick up several small objects. A wicker basket containing various items is on the table, and a dark red cloak is hanging on the wall nearby. [0:01:26 - 0:01:33]: The person stands upright again, facing the window, and begins adjusting or tying a braided belt around their waist. The window provides natural light, illuminating the interior of the room and highlighting the person's concentrated expression. [0:01:34 - 0:01:35]: The person secures a small brown pouch to their belt. The pouch hangs on their left hip, displaying ornamental detailing and embroidery on the belt. [0:01:36 - 0:01:40]: Close-up shots of a metallic pendant or key attached to the braided belt provide a detailed view of its intricate design and craftsmanship. The pendant/key is long with a distinctive pattern and multiple holes, adding a decorative touch to the overall attire.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What does the person do after picking up or placing several small objects?",
                "time_stamp": "0:01:33",
                "answer": "C",
                "options": [
                    "A. Opens the window.",
                    "B. Sits down at the table.",
                    "C. Adjusts a braided belt around their waist.",
                    "D. Leaves the room."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "getting_dressed",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_160_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:45]: Hands are tying a belt with an attached pouch and a key at the waist. A person, seen from the torso down, is adjusting a burgundy and beige garment. The sleeves of the outfit have decorative patterns, and the hands are in the process of fastening the belt securely; [0:02:46 - 0:02:51]: The perspective changes to show the person\u2019s face as they pull a white fabric over their head, likely a hood or headscarf. The person has long, dark hair and appears focused on adjusting their attire. The background reveals a room with dim lighting and wooden wall paneling; [0:02:52 - 0:02:54]: The individual, now fully covered with the fabric that drapes over their head and shoulders, turns away from the camera and faces a closed wooden door. The room shows a mix of medieval or historical decor elements, including a loom and other rustic furnishings; [0:02:55 - 0:02:57]: The person opens the wooden door, allowing natural light to flood into the dark room. They step outside, revealing part of a sunny, green outdoor area. The doorway frames this transition from the dim, cozy interior to the bright exterior; [0:02:58 - 0:02:59]: The scene changes to an outdoor setting with the person standing in front of a closed, weathered wooden door. The exterior of the building features rough, wooden siding, indicating an old or rustic structure. They are seen from behind, with the light fabric covering their head and shoulders flowing down their back.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the person doing right now?",
                "time_stamp": "0:02:57",
                "answer": "D",
                "options": [
                    "A. Adjusting their belt.",
                    "B. Pulling a hood over their head.",
                    "C. Opening a wooden door.",
                    "D. Stepping outside."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "getting_dressed",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_160_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:02]: The video shows the lower part of a person lying down. The individual is wearing a cream-colored garment underneath a blue one, both adorned with red embroidered edges. There is a dark object, possibly a strap or belt, positioned horizontally across the image, likely part of the person's attire. The person's lower body remains still during this time. [0:04:03 - 0:04:17]: A hand appears in the frame from the right side, reaching towards the dark object. The hand begins to touch and adjust the strap or belt, checking its buckle and positioning. The background remains dimly lit, focusing on the textures and colors of the fabric and the hand's movement. [0:04:18 - 0:04:20]: The scene zooms out slightly, revealing more of the person's upper body and part of the head, which is covered in hair. The person is lying down on a dark surface, possibly a bed or a cushioned area, wearing a dark robe with fur trim around the neck. The hand moves away from the buckle, and the person remains still.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What are the colors of the garments worn by the person lying down?",
                "time_stamp": "00:04:28",
                "answer": "A",
                "options": [
                    "A. Cream and blue with red embroidered edges.",
                    "B. White and blue with gold embroidered edges.",
                    "C. Cream and green with silver embroidered edges.",
                    "D. White and red with blue embroidered edges."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "getting_dressed",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_160_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:03:12",
                "answer": "A",
                "options": [
                    "A. 2.",
                    "B. 3.",
                    "C. 4.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_73_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:04:31",
                "answer": "B",
                "options": [
                    "A. 1.",
                    "B. 4.",
                    "C. 2.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_73_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:05:18",
                "answer": "D",
                "options": [
                    "A. 1.",
                    "B. 3.",
                    "C. 2.",
                    "D. 5."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_73_real.mp4"
    },
    {
        "time": "[0:07:00 - 0:08:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:07:18",
                "answer": "C",
                "options": [
                    "A. 1.",
                    "B. 3.",
                    "C. 7.",
                    "D. 5."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_73_real.mp4"
    },
    {
        "time": "[0:12:00 - 0:13:00\u3011",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:12:21",
                "answer": "B",
                "options": [
                    "A. 10.",
                    "B. 11.",
                    "C. 9.",
                    "D. 12."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_73_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: A bustling street scene in a city at night. Various vehicles, including a double-decker bus, are visible in the road to the right. Across the street, buildings with brightly lit windows and advertisements add vibrancy. Individuals on the left sidewalk appear to be exploring or walking around. [0:00:03 - 0:00:06]: The street scene continues to develop, with more emphasis on the surrounding architecture. To the left, a building with a distinct slanted design becomes more prominent. Streetlights illuminate the path, and the iconic clock tower stands out in the background. [0:00:07 - 0:00:10]: The first-person perspective progresses forward along the bustling sidewalk. The path is flanked by palm trees, faint silhouettes of pedestrians, and a prominent clock tower creating focal points in the background. Vibrant lights and modern skyscrapers are visible in the distance. [0:00:11 - 0:00:14]: Moving closer to the clock tower, more details of the surrounding architecture and environment are observed. The modern buildings contrasting with the historical clock tower create a dynamic cityscape. More pedestrians and dynamic urban elements add life to the scene. [0:00:15 - 0:00:18]: Continuing along the sidewalk, a slight turn reveals more of the city skyline and passing vehicles. Advertisements on the side of the building highlight cultural events. The street remains busy with pedestrians and lined with palm trees, adding to the urban environment. [0:00:19 - 0:00:20]: Approaching the tall clock tower, the view captures the rich combination of modern urban life and historical architecture. Individuals continue to fill the bustling street, contributing to the lively city atmosphere.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What type of bus is visible right now?",
                "time_stamp": "00:00:16",
                "answer": "C",
                "options": [
                    "A. Single-decker bus.",
                    "B. Minibus.",
                    "C. Double-decker bus.",
                    "D. School bus."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_336_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:23]: The scene starts in a well-lit, covered pedestrian walkway with a view to the right of a waterfront cityscape at night. The ceiling has bright light fixtures arranged in a row. To the left, there's a poster with unclear details. Several pedestrians can be seen in the distance walking toward the camera, and a person on the right is looking out over the water. [0:02:24 - 0:02:26]: As the camera moves forward, more details of the people become clearer. A group of three people wearing light-colored tops and shorts are walking toward the camera, others are scattered along the path, some standing and some walking. There are also people on the waterfront, engaged in looking out over the water. [0:02:27 - 0:02:28]: The clarity improves as the group of three people draws nearer, with some people walking by on the left side of the walkway. A sitting area with benches appears on the left, used by some resting individuals. The waterfront cityscape is vivid with skyscrapers illuminated. [0:02:29 - 0:02:30]: The group of three has passed by the camera, and two men in casual clothes are now prominent on the left side of the frame. People continue walking along the path, with more benches and people visible on the left. The illuminated buildings across the water remain visible in the background. [0:02:31 - 0:02:34]: The scene reveals a broader view of the walkway as the camera progresses. More people are visible, including those sitting on benches to the left and others walking in the distance. A vibrant blue poster is more discernible on a column. The walkway is bustling with activity. [0:02:35 - 0:02:39]: As the camera angle shifts slightly to the right, a street performer or speaker dressed in orange comes into view. This performer is seen adjusting equipment near a microphone stand. Pedestrians, including families and individuals, continue to move along, viewing the performance. The urban skyline in the background remains illuminated and provides a striking contrast against the night sky.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What is the primary activity of the pedestrians seen in the walkway?",
                "time_stamp": "00:02:26",
                "answer": "B",
                "options": [
                    "A. Running.",
                    "B. Walking.",
                    "C. Cycling.",
                    "D. Skating."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_336_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:59]: The video captures a vibrant cityscape from a first-person perspective at night, showcasing a panoramic view over a body of water. The sky is dark, interspersed with a few clouds. The city in the background is brightly lit with numerous multicolored lights from various buildings, including blue, red, green, and yellow lights reflecting on the water's surface, adding a dynamic array of colors to the scene. A high-rise building with a distinct, well-lit spire is prominent among the skyline. In the foreground, near the bottom-left corner, there is a portion of a person's hand and arm, suggesting they are on a boat or near the water's edge. The water gently ripples, reflecting the city lights in a dance of colors. The scene is static, primarily focusing on the lively, illuminated city skyline against the dark, night backdrop.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What is the setting of the video scene?",
                "time_stamp": "0:05:00",
                "answer": "C",
                "options": [
                    "A. A busy market street during the day.",
                    "B. A quiet village at sunset.",
                    "C. A vibrant cityscape at night.",
                    "D. A beach during the evening."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_336_real.mp4"
    },
    {
        "time": "[0:07:00 - 0:08:00]",
        "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:19]: The video shows a panoramic view of a city skyline at night, reflecting vibrantly in the water in the foreground. The scene is filled with numerous high-rise buildings adorned with colorful lights, creating a dazzling display against the dark sky. One prominent building with a distinctively lit spire stands tall in the center, serving as a focal point amidst the other illuminated structures. The water reflects the city\u2019s myriad lights, causing a beautiful interplay of colors on its surface. The sky above is partly cloudy, with some clouds visible but not obstructing the view. The video appears to be static, with no significant movement or changes happening throughout.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What is the overall setting depicted in the video?",
                "time_stamp": "0:07:20",
                "answer": "A",
                "options": [
                    "A. A city skyline at night.",
                    "B. A rural landscape at sunset.",
                    "C. A beach during daytime.",
                    "D. A forest in the morning."
                ],
                "required_ability": "episodic memory"
            },
            {
                "task_type": "Spatial Understanding",
                "question": "Where are the colorful lights primarily reflected?",
                "time_stamp": "0:07:20",
                "answer": "C",
                "options": [
                    "A. In the sky.",
                    "B. On the buildings.",
                    "C. In the water in the foreground.",
                    "D. On the streets."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_336_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a view of someone holding and breaking down several cardboard boxes on a floor with a grayish texture. Shelves with various products, including milk cartons and red plastic crates, are on the right. [0:00:01 - 0:00:07]: The person shifts their focus towards a shelf, eventually kneeling down and using their gloved hands to organize boxes of milk. A nearby metal cart filled with red plastic crates is visible to the right of the shelves. [0:00:08 - 0:00:11]: The person picks up a box labeled \"Valio\" and proceeds to place milk cartons from the box onto the shelf. The milk cartons are predominantly green and white and labeled with prices, such as \"20.95.\" [0:00:11 - 0:00:12]: The person continues placing milk cartons on the lower shelves, ensuring there is an orderly arrangement. [0:00:12 - 0:00:17]: The person picks up additional milk cartons from the box on the floor and places them on the shelf. The action alternates between reaching into the box and arranging the cartons on the shelf. The surrounding floor has scattered pieces of cardboard. [0:00:18 - 0:00:20]: The person continues to organize the remaining cartons. A more comprehensive view shows the shelves filled with various milk products and other items arranged neatly. The dismantled cardboard boxes lie on the ground to the left.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What action does the person perform after kneeling down?",
                "time_stamp": "0:00:08",
                "answer": "B",
                "options": [
                    "A. Breaking down cardboard boxes.",
                    "B. Placing and organizing boxes of milk on a shelf.",
                    "C. Placing cartons on a metal cart.",
                    "D. Removing gloves."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "storage_manage",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_447_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:42]: The video shows a person in a storage room or supermarket aisle, reaching for yogurt cups on a shelf. The individual is using both hands, and an assortment of dairy products, mainly yogurts, is visible on the shelves. The person's hands are gloved, suggesting they may be handling cold items. [0:02:43 - 0:02:45]: The person continues to reach for different yogurt cups on the shelf. A wider view of the area shows more yogurt cups and other dairy products neatly organized on the shelves. The shelves have product labels and price tags, indicating a retail environment. [0:02:46 - 0:02:49]: The perspective shifts downward, showing a shopping cart with various items including a few yogurt cups and other packaged goods. The floor is grey and looks like the back storage area or warehouse of a retail store. The cart is partially filled. [0:02:50 - 0:02:53]: The individual appears to be picking up a box from a nearby stack of items. Several boxes and a red cart filled with more products are visible. The camera captures the person\u2019s gloved hands as they handle the box. [0:02:54 - 0:02:56]: The person places the box into the shopping cart which already contains several similar boxes. The cart is pulled slightly closer while repositioning the items for better organization. [0:02:57 - 0:02:59]: The individual continues organizing items in the shopping cart, picking up and placing a smaller box that contains multiple white containers with red caps. The shelves are stocked with various food items, and more carts with similar products are in the vicinity.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What kind of items is the person reaching for on the cart?",
                "time_stamp": "0:02:43",
                "answer": "B",
                "options": [
                    "A. Cheese blocks.",
                    "B. Yogurt cups.",
                    "C. Milk cartons.",
                    "D. Bread loaves."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "storage_manage",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_447_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:39]: The video shows a person, presumably the viewer, stocking shelves in a store. The perspective is first-person, and the viewer's gloved hands are visible in almost every frame as they handle items. The shelves are filled with various products, primarily cartons and boxes of what appears to be food or beverage items. The items have different colors and labels, while the shelves are white and metallic.  A cart or some sort of mobile rack is positioned to the left side of the viewer, loaded with the items to be stocked, and the floor has a gray texture. The video captures the viewer fitting products into the shelves, removing items from cardboard boxes, and organizing them. The viewer's arms extend occasionally to the shelves to place the items accurately. Throughout the segment, there are additional cardboard boxes and packaging materials on the floor around the viewer. As the viewer stocks the shelves, they move products from the cart to their designated spots on the shelf. The area seems to be part of a stockroom or a storage section of a retail store based on the item arrangement and the environment. [0:05:20 - 0:05:29]: The viewer is fitting small containers, possibly yogurt or similar products, into a cardboard packaging box on the floor before placing the entire box on the shelf above. [0:05:30 - 0:05:31]: The viewer reaches toward a part of the shelf and adjusts their positioning. [0:05:32 - 0:05:36]: The viewer shifts their attention to a stack of larger cardboard boxes on the floor beside them. They begin to interact with these boxes, possibly identifying what items to stock next. [0:05:37 - 0:05:39]: The viewer resumes stocking the shelf, placing items carefully in their respective positions, reaching slightly upwards to fit the items in the correct spots.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What type of items is the viewer primarily handling in the video?",
                "time_stamp": "00:05:40",
                "answer": "B",
                "options": [
                    "A. Electronic gadgets.",
                    "B. Dairy products.",
                    "C. Clothing accessories.",
                    "D. Stationery products."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "storage_manage",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_447_real.mp4"
    },
    {
        "time": "[0:08:00 - 0:09:00]",
        "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:11]: The video shows a first-person perspective of a person working in what appears to be a storage room or warehouse. The individual is wearing gloves and is placing items on a shelf. There are cardboard boxes on the floor and a metal trolley cart next to the individual. On the shelf, there are various items including milk cartons. The person is systematically picking up and placing items from the box onto the shelf. The shelves have price labels, indicating different price points for the items.  [0:08:12 - 0:08:16]: The scene shifts as the individual moves away from the shelf, carrying a cardboard box. The camera follows a movement towards a stack of boxes on the floor. The stack appears to be organized with some boxes already opened and others still sealed. The individual seems to be preparing to stack more boxes or rearrange them. [0:08:17 - 0:08:20]: The individual picks up a manual pallet jack, indicating they might be preparing to move the stack of boxes. The camera position and movements suggest the person is making preparations to transport or rearrange the boxes. The video concludes with the individual still in the process of handling the pallet jack and preparing for further actions in the storage room.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What does the individual do after moving away from the shelf?",
                "time_stamp": "00:08:16",
                "answer": "B",
                "options": [
                    "A. Picks up another item to place on the shelf.",
                    "B. Carries a cardboard box towards a stack of boxes.",
                    "C. Sits down to take a break.",
                    "D. Leaves the storage room."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "storage_manage",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_447_real.mp4"
    },
    {
        "time": "[0:09:00 - 0:09:59]",
        "captions": "[0:09:40 - 0:09:59] [0:09:40 - 0:09:41]: The video shows a person wearing a dark jacket and gloves with a gold bracelet on their wrist. They are reaching towards a white shelf filled with cartons labeled \"Julebrus.\"  [0:09:41 - 0:09:42]: The person continues to grab cartons from the shelf. Visible below are some cardboard boxes and the ground. [0:09:42 - 0:09:43]: The person has taken a carton off the shelf and is moving it towards a cart.  [0:09:43 - 0:09:44]: The camera angle changes to show the person's perspective as they move cartons from a shelf into a large, red shopping cart.  [0:09:44 - 0:09:45]: The person takes more cartons and begins placing them on the cart. Boxes are scattered on the ground nearby. [0:09:45 - 0:09:46]: More cartons are placed in the cart, while the person continues moving amidst a pile of cardboard boxes.  [0:09:46 - 0:09:47]: The person picks up a large, flat piece of cardboard from the pile on the floor and places it against the side of their cart. [0:09:47 - 0:09:48]: The view shifts back to the person organizing the boxes in the cart, making sure that they are stacked securely. [0:09:48 - 0:09:49]: The person adjusts several cartons in the cart, preparing to stack more boxes on top. [0:09:49 - 0:09:50]: They continue to organize the boxes, ensuring the pile is stable. Shelves filled with products can be seen in the background. [0:09:50 - 0:09:51]: The person stacks another box on the cart, pressing it down to make sure it is secure. [0:09:51 - 0:09:52]: They reach for another carton and begin to move it towards the cart, placing it carefully within the existing stack. [0:09:52 - 0:09:53]: Another carton is being held and positioned by the person above a large red basket and a pile of flattened boxes on the ground. [0:09:53 - 0:09:54]: The camera angle changes to show a shelf filled with product, with the person reaching up to grab an item. [0:09:54 - 0:09:55]: The person has taken a few cartons from the shelf and is securing them in the cart. [0:09:55 - 0:09:56]: They continue to stack the cartons in the cart, being more cautious with placement. [0:09:56 - 0:09:57]: They pull out another set of cartons from the shelf and place them on the cart. [0:09:57 - 0:09:58]: The person adjusts and makes sure the cartons are securely placed on the lower shelf. [0:09:58 - 0:09:59]: The person is seen moving more cardboard pieces towards the cart, managing the boxes efficiently.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the person wearing on his right wrist?",
                "time_stamp": "00:09:41",
                "answer": "C",
                "options": [
                    "A. A silver watch.",
                    "B. A leather bracelet.",
                    "C. A gold bracelet.",
                    "D. A rubber band."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "storage_manage",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_447_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video starts with a close-up view of a cluttered worktable. In the foreground, a paintbrush with a golden handle is out of focus. In the background, various art supplies such as paintbrushes and tools are scattered on the tabletop.  [0:00:02 - 0:00:03]: The perspective shifts to a top-down view of a wooden board that appears to be a canvas with a sketch outlined on it. A hand is holding a small paintbrush, applying red paint to a specific area of the sketch. [0:00:04 - 0:00:05]: The camera moves closer to the painting, focusing on a detailed section where the artist is painting a human ear. The hand holding the paintbrush adds precision to the brush strokes, enhancing the contours of the ear with shades of brown and pink. [0:00:06]: The scene transitions to showing a hand carefully peeling off a sheet of gold leaf. The gold leaf is placed on a piece of fabric on the table, with other sheets of similar material nearby. [0:00:07 - 0:00:08]: The camera shows the process of applying gold leaf to a painted surface. A brush gently presses the thin gold material onto the surface, which already has some gold leaf applied. [0:00:09 - 0:00:13]: The video returns to the previously seen painting, now showing more of the canvas. The painting appears to be a portrait of a person with leaves in their hair. The lighting creates a warm reflection on the freshly applied paint, which glistens under the light. The scene gradually darkens. [0:00:14]: The scene changes, showing a hand about to start painting on a blank white board or canvas. The surroundings indicate an indoor studio setting. [0:00:15 - 0:00:19]: The hand begins to apply a base layer of golden paint onto the white board. The process is shown with broad strokes, as the paint covers more of the surface. The detail and consistent motion of the brush are captured, illustrating the beginning stages of a new artwork.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the artist doing with the gold leaf in the video?",
                "time_stamp": "00:00:08",
                "answer": "A",
                "options": [
                    "A. Applying it to a painted surface.",
                    "B. Cutting it into smaller pieces.",
                    "C. Mixing it with paint.",
                    "D. Discarding it."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_126_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:45]: The video shows a detailed close-up view of a painting in progress. It is a portrait of a person's side profile with a background of textured gold color. A hand is holding a brush, applying paint to the area near the nose of the figure. The brush is flat and moves smoothly over the canvas, adding details to the side of the nose and cheek area.  [0:02:46 - 0:02:50]: The brush continues to move, refining the contours of the face. The colors used include soft skin tones that blend into the gold background. The lighting highlights the texture of the brush strokes both in the background and the portrait. [0:02:51 - 0:02:56]: The hand changes the brush's position, now working on adding depth and shading to the cheek and lower face area. The figure\u2019s ear, hair, and part of the eye are clearly visible, emphasizing the precision of the painting process. [0:02:57 - 0:02:59]: The artist focuses on painting the area near the forehead and the top of the face. The strokes appear careful and measured, with the brush applying varying shades to create a realistic representation of the face.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What is the sequence of actions just performed by the artist?",
                "time_stamp": "00:03:00",
                "answer": "B",
                "options": [
                    "A. Painting the background, then the nose, followed by the cheek, and finally the forehead.",
                    "B. Painting the nose, then the cheek, followed by the forehead.",
                    "C. Painting the forehead, then the nose, followed by the cheek.",
                    "D. Painting the cheek, then the forehead, followed by the nose."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_126_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:39]: A detailed close-up of an artist painting a human ear on a canvas can be observed. The scene showcases the artist's hand holding a green paintbrush, applying meticulous strokes to the ear area. The painting itself features a side profile view of a person's face, predominantly painted in warm tones such as shades of brown, beige, and some golden hues. The ear is central in the frames, while the surrounding area displays segments of the cheek, jawline, and neck, illustrating an evident attention to detail. The hair is depicted in dark, almost black, brushstrokes, complemented by lighter highlights. A yellow and slightly red segment, possibly a part of the background or the person's clothing, is visible above the ear. The artist continues to refine the ear's contours and shadows, making precise adjustments to enhance its realism. As each second passes, subtle progressions in the ear's depiction are noticeable, highlighting the artist's technique and patience. The brush moves slowly and deliberately, adding depth and dimension to the painted ear. [0:05:20 - 0:05:39]: The artist eventually completes the ear, and the scene focuses on the finished painting, indicating the culmination of this segment's work.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What did the artist do just now?",
                "time_stamp": "00:05:38",
                "answer": "B",
                "options": [
                    "A. Sketching the ear.",
                    "B. Applying meticulous strokes to the ear area.",
                    "C. Cleaning the paintbrush.",
                    "D. Painting the background."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_126_real.mp4"
    },
    {
        "time": "[0:08:00 - 0:09:00]",
        "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:05]: The video shows a close-up of a hand using a small paintbrush to add details to an illustration of a woman's side profile. The woman has dark hair adorned with a series of colorful leaves, primarily in shades of yellow and orange. The background is light grey. There are other brushes and pencils laying on the table around the artwork.  [0:08:06 - 0:08:12]: The camera zooms in closer to the part where the leaves meet the hair. The hand now swaps to a smaller brush, carefully painting intricate details on the orange leaves, ensuring each edge and vein of the leaves are well defined. [0:08:13 - 0:08:17]: The perspective zooms out slightly, showing the whole illustration from a somewhat wider view. Another brush with a long handle is used to refine the shading in the hair and leaves, making sure the blend between colors is seamless. [0:08:18 - 0:08:20]: The camera angle remains the same, focusing on the artist gently blending the leaves into the hair. The gentle strokes of the brush create a natural transition between the subject\u2019s hair and the colorful leaves. The background remains consistent, maintaining the focus on the detailed work of the illustration.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where are the other brushes and pencils located in relation to the artwork?",
                "time_stamp": "00:08:10",
                "answer": "B",
                "options": [
                    "A. On a shelf above the artwork.",
                    "B. Scattered around the artwork on the table.",
                    "C. Inside a drawer next to the table.",
                    "D. Held in the artist's other hand."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following best summarizes the painting shown in the video?",
                "time_stamp": "00:08:20",
                "answer": "C",
                "options": [
                    "A. A detailed painting of a woman with blonde hair on a dark blue background with a full moon and clouds.",
                    "B. A landscape painting with mountains and rivers.",
                    "C. A detailed illustration of a woman's profile with leaves in her hair being painted.",
                    "D. A close-up portrait of an elderly man."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_126_real.mp4"
    },
    {
        "time": "0:00:00 - 0:00:20",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:09]: The video starts by showcasing an assortment of six boxed keychains laid out on a flat surface. Each keychain is housed in a distinct, pastel-colored box with a cut-out window design on the lid. From left to right on the top row, there is a pink box containing a pink keychain with the name \"Alli\" written in white script, black polka dots, and a white tassel. Next, a white box holds a silver-glittered keychain with a small RV design and a white tassel. The final box on the top row is light green, containing an orange round keychain with the name \"Peter\" written in white and a palm tree image in the background. On the bottom row, there is a yellow box with a yellow keychain bearing the name \"Wendy\" encircled by a floral pattern and a yellow tassel. Next, a purple box holds a purple keychain with \"bella\" written in white scripted text and decorated with a purple tassel. The last box on the right is light pink, featuring a pink heart-shaped keychain with the name \"Lisa\" written in white and a pink tassel. [0:00:10 - 0:00:14]: The scene changes to a close-up view of a pair of hands opening the pink box with the \"Alli\" keychain. One hand holds the box open while the other gently removes the keychain from the packaging, showing the intricate details of the decoration and the tassel attached to it. The hands carefully place the keychain back into the box, orienting it properly for display. [0:00:15 - 0:00:17]: The focus remains on the \"Alli\" keychain now resting neatly inside the open pink box. The soft lighting accentuates the shiny and smooth surface of the keychain, highlighting its decorative features and making the text and polka dots stand out distinctly. The video then transitions to a black screen. [0:00:18 - 0:00:19]: The words \"Keychain Display Box\" appear centered on the black screen in white text, providing a title or description for the display shown in the video. The title remains on the screen as the video concludes.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What are the hands doing right now?",
                "time_stamp": "00:00:16",
                "answer": "D",
                "options": [
                    "A. Writing the name \"Alli\" on a keychain.",
                    "B. Holding the box closed.",
                    "C. Removing the keychain from the packaging.",
                    "D. Placing the keychain into the packaging."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_64_real.mp4"
    },
    {
        "time": "0:01:40 - 0:02:00",
        "captions": "[0:01:40 - 0:02:00] [0:01:40 - 0:01:41]: The video begins with a person holding a flat, rectangular, light pink cardboard piece with several flaps folded in different directions. Next to it, on a flat surface, is another cardboard piece with a small plastic sheet on top of it. The surface is a muted grey color. [0:01:42 - 0:01:46]: The person then picks up another piece of cardboard of the same color, this one having a big rectangular cutout in the middle. In one of their hands, they hold a cylindrical bottle of clear adhesive with a blue and white label. [0:01:47 - 0:01:53]: The person proceeds to apply the adhesive to the other flat cardboard piece with the large rectangular cutout. They are careful to apply it along the inner edges of the cutout, ensuring coverage around the entire border. [0:01:54 - 0:01:57]: After the adhesive is applied, the person positions a transparent plastic sheet over the cutout area, aligning it perfectly with the edges of the cardboard. They press it down to ensure it sticks firmly. [0:01:58 - 0:01:59]: The person then lifts the glued piece, giving the viewer a good look at the results. The transparent sheet stays flat and well-aligned with the cardboard.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the person doing right now?",
                "time_stamp": "0:01:57",
                "answer": "C",
                "options": [
                    "A. Picking up a light pink cardboard piece.",
                    "B. Applying adhesive to a cardboard piece.",
                    "C. Pressing down a transparent plastic sheet.",
                    "D. Holding a cylindrical bottle."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_64_real.mp4"
    },
    {
        "time": "0:03:20 - 0:03:40",
        "captions": "[0:03:20 - 0:03:40] [0:03:24 - 0:03:25]: The video begins with a view of the Cricut Design Space user interface in a browser window. The screen displays a grid workspace with tools on the left side for operations such as \"New,\" \"Templates,\" and \"Projects,\" along with a top toolbar with option buttons like \"Undo\" and \"Image\" and a header showing the project name \"Untitled\". [0:03:25 - 0:03:27]: The view remains fixed on the Cricut Design Space interface. The grid workspace with its accompanying tools and options continues to be displayed. [0:03:27 - 0:03:28]: The screen changes to a file management window showing the contents of a folder named \u201cDIY Craft Tutorials-KeychainBox-Set3\u201d. The folder contains several items: three PDF documents named \"Design Space Instructions,\" \"License-Terms-and-Guide,\" and \"YouTube Tutorial\", four subfolders labeled \"DXF Files,\" \"PDF Files,\" \"PNG Files,\" and \"SVG Files,\" along with an additional blank document icon. The background of the file manager is white, with each item displayed at the top of the window. [0:03:28 - 0:03:29]: The contents of the folder remain the same. The cursor hovers over the icons, but no selection or opening action occurs yet. [0:03:29 - 0:03:30]: The cursor clicks and selects multiple icons, highlighting them in blue. The selected items remain the same. [0:03:30 - 0:03:31]: The selected items stay highlighted. The screen doesn't change, and no new actions occur. [0:03:31 - 0:03:32]: All selected items stay highlighted. No new action occurs yet. [0:03:32 - 0:03:33]: The view focuses back on the file management window. All selected items remain highlighted without any further interaction. [0:03:33 - 0:03:34]: The file management window still displays the same highlighted items. The cursor hovers over them without interacting. [0:03:34]: There is no change in the display. The file management window still shows the same view with the selected items. [0:03:35 - 0:03:36]: The screen transitions to a detailed instructional PDF document opened from the file management window. The document, titled \"Keychain Box Template,\" shows various images of keychain box templates in pastel colors at the top, followed by a paragraph describing how to access the YouTube tutorial and other details. The layout of the document includes headings, a text section, and visual guides. [0:03:36 - 0:03:37]: The PDF document remains on the screen, displaying further details about the keychain box template. It shows measurements and various parts that constitute the template. [0:03:37 - 0:03:38]: The PDF document continues to display more information about the keychain box template, including additional visual guides and specifications. [0:03:38 - 0:03:39]: The view stays fixed on the PDF document detailing the keychain box template, showing comprehensive instructional graphics and dimensions of the template parts. [0:03:39 - 0:03:40]: The PDF document shows further parts and measurements of the keychain box template. No interaction with the document is detected. [0:03:40 - 0:03:41]: Another section of the PDF document is shown, with more illustrations and size guides for various parts needed for the assembly, orienting predominantly horizontal and some vertical components. [0:03:41 - 0:03:42]: The document displays additional components with detailed guides and segments showing specific parts along with their exact dimensions and orientations, still mainly focused on horizontal layouts. [0:03:42 - 0:03:43]: More details about the keychain box template's sections, showing the necessary dimensions and assembly instructions in detail, complete with illustrations for each specific element. [0:03:43 - 0:03:44]: Displays extensive viewable instructions, including parts and images, dimensions, and notes on the keychain box template making process, all displayed between horizontal and vertical segregations. [0:03:44 - 0:03:45]: Another page of the PDF, focusing on more detailed segments of the template. More extensive guides illustrate the parts involved in crafting the keychain box model. [0:03:45 - 0:03:46]: An extended display section of various segments of the template in the PDF, each with their independent measurements and associated visuals, continuing the keychain box's detailed creation process. [0:03:46 - 0:03:47]: The",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the cursor doing right now?",
                "time_stamp": "0:03:44",
                "answer": "B",
                "options": [
                    "A. Clicking the \"Undo\" button.",
                    "B. Selecting multiple icons.",
                    "C. Opening a subfolder.",
                    "D. Deleting a document."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_64_real.mp4"
    },
    {
        "time": "0:05:00 - 0:05:20",
        "captions": "[0:05:00 - 0:05:20] [0:00:00 - 0:00:04]: The video starts by displaying a software interface, specifically the design space of Cricut Design Space on a computer screen. On the canvas, there are drawing outlines of various components of a keychain box; most of these shapes are pink except for one vertical rectangle, which is orange. The left rectangular shape appears to be the main body of the box, with distinct flap areas, and the other shapes are additional parts, including a tag and other sections. The time displayed on the screen indicates 05:49:53. [0:00:05 - 0:00:08]: The elements on the canvas have not changed, and the timestamp now reads 05:49:54. There are corresponding layers shown on the right side of the screen with keychain-box-1a-svg labels for each shape. The same label appears multiple times with the 'Basic Cut' operation described underneath each. [0:00:09 - 0:00:14]: The visual state remains consistent, with no new actions on the canvas. The timestamp continues to advance. The arrow pointing to various layers on the right shows the current selection within the software. [0:00:15 - 0:00:20]: The software interface maintains the same view, with the cursor visible at times, indicating user interaction or selection. The labeled layers on the right correspond to the shapes on the left, with several layers representing different parts of the keychain box design. The visual focus remains clear on the design elements of the keychain box.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What did the cursor indicate just now?",
                "time_stamp": "0:05:09",
                "answer": "C",
                "options": [
                    "A. Selecting shapes on the canvas.",
                    "B. Displaying a pop-up menu.",
                    "C. Moving to the Layers panel on the right.",
                    "D. Highlighting text input fields."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_64_real.mp4"
    },
    {
        "time": "0:06:40 - 0:07:00",
        "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:43]: The video begins with a computer screen displaying the Cricut Design Space application. The main window shows a design with two pink-colored shapes arranged on a grid mat. The larger shape is positioned at the top left, and the smaller one is at the bottom right. Both shapes have dashed lines inside them. To the left of the screen, under the \"Basic Cut\" and \"Score\" tabs, two rectangular icons depicting a cut and a score tool are visible. [0:06:44 - 0:06:48]: The screen remains static, still showing the design with the two pink shapes on the Cricut mat. The shapes are unchanged, and the tool options on the left side remain the same with no new actions occurring. [0:06:49 - 0:06:52]: The design on the screen updates. The positions of the shapes shift, and the design now includes an additional cut-out at the bottom left corner of the smaller pink shape. The icons on the left side remain the same as earlier, with the \"Basic Cut\" and \"Score\" options visible. [0:06:53 - 0:06:55]: The shapes on the mat appear larger, zooming in slightly. The positions of the shapes remain the same with the larger shape at the top and the smaller one at the bottom, still showing the detailed cut-out pattern. [0:06:56 - 0:06:58]: The screen transitions to a new interface in the Cricut Design Space application. The top shows a message: \"To continue, please connect your Maker 3.\" The rest of the screen is predominantly white, awaiting further user interaction. [0:06:59 - 0:07:01]: The screen remains static with the message about connecting the Maker 3. No additional changes are made in the interface, and it waits for the device connection. [0:07:02 - 0:07:04]: The screen remains unchanged, still showing the message to connect the Maker 3, with no further action taking place. [0:07:05 - 0:07:08]: The interface refreshes, showing an option to set the base material. Several colored tabs for different materials such as Popular, Favorites, and Browse All Materials are visible. Below these tabs, a list of material options appears, waiting for selection. [0:07:09 - 0:07:12]: The screen shows the \"All Materials\" tab selected, displaying a list of various materials on the Cricut Design Space. Materials like Chipboard, Corrugated Cardboard, and Foil Poster Board are listed in a scrollable menu. [0:07:13 - 0:07:16]: The view zooms in slightly on the materials list, focusing on items like Poster Board and Art Board. The search bar at the top allows for filtering among the materials listed. [0:07:17 - 0:07:21]: The perspective shifts again, focusing now on the \"Cardstock\" section. Multiple options for cardstock materials with various characteristics such as thickness and type are displayed. [0:07:22 - 0:07:27]: The list under the \"Cardstock\" section is detailed, showing different types like Deluxe Paper, Heavy Cardstock, and Light Cardstock. Each item has a checkbox for selection. [0:07:28 - 0:07:31]: The user scrolls down to find the \"Medium Cardstock - 80 lb (216 gsm)\" option, highlighted with a red arrow. This item is located towards the bottom of the list under the \"Cardstock\" section. [0:07:32 - 0:07:35]: The \"Medium Cardstock - 80 lb (216 gsm)\" option remains highlighted. The user prepares to select it, indicated by the cursor hovering over the checkbox next to it. [0:07:36 - 0:07:39]: The user selects the \"Medium Cardstock - 80 lb (216 gsm)\" option as the desired material. The checkbox next to it is ticked, confirming the selection. [0:07:40 - 0:07:42]: With the material selected, the screen updates to reflect the chosen \"Medium Cardstock - 80 lb (216 gsm)\" as the base material. The \"Set Base Material\" section at the top now includes this selected material. [0:07:43 - 0:07:46]: The bottom of the screen shows options to load tools and material, with icons depicting different tools needed for the selected task. The \"Press Go\" button is visible but not yet activated.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What action did the user perform just now?",
                "time_stamp": "00:07:00",
                "answer": "C",
                "options": [
                    "A. Selected the \"Score\" tool.",
                    "B. Added a new shape to the design.",
                    "C. Chose the \"Medium Cardstock - 80 lb (216 gsm)\" as the base material.",
                    "D. Connected the Maker 3."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_64_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:20]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the hotel in relation to the cyclist right now?",
                "time_stamp": "00:00:19",
                "answer": "A",
                "options": [
                    "A. On the right side.",
                    "B. On the left side.",
                    "C. Behind the cyclist.",
                    "D. Directly ahead."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_297_real.mp4"
    },
    {
        "time": "[0:03:32 - 0:03:52]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the yellow vehicle located right now?",
                "time_stamp": "00:03:44",
                "answer": "A",
                "options": [
                    "A. On the right side of the road.",
                    "B. Directly behind the cyclist.",
                    "C. Directly in front of the cyclist.",
                    "D. On the left side of the cyclist."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_297_real.mp4"
    },
    {
        "time": "[0:07:04 - 0:07:24]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is located on the left side, parallel to the cyclists right now?",
                "time_stamp": "00:07:18",
                "answer": "A",
                "options": [
                    "A. A safety railing.",
                    "B. A parking lot.",
                    "C. A rest area.",
                    "D. A grass field."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_297_real.mp4"
    },
    {
        "time": "[0:10:36 - 0:10:56]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is located to the right side of the cyclist right now?",
                "time_stamp": "00:10:55",
                "answer": "A",
                "options": [
                    "A. An open field.",
                    "B. A dense forest.",
                    "C. A lake.",
                    "D. A mountain range."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_297_real.mp4"
    },
    {
        "time": "[0:14:08 - 0:14:28]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is on the right side of the cyclists right now?",
                "time_stamp": "00:14:07",
                "answer": "A",
                "options": [
                    "A. A brick wall.",
                    "B. A bush.",
                    "C. A field.",
                    "D. A dense forest."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_297_real.mp4"
    },
    {
        "time": "0:00:00 - 0:00:20",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: A circular ornament with the name \"Harlow\" written in white script is presented. The ornament is made of resin with a glittery gold and pink ombre effect. It hangs from a braided gold thread. The background includes wooden elements and a sprig of pink and white flowers on the left side. [0:00:03]: The screen transitions to black. [0:00:04 - 0:00:06]: The text \"Ombre Resin Ornament\" is displayed against a black background. [0:00:07]: The screen transitions to black again. [0:00:08 - 0:00:12]: The text \"Links to the materials I used are in the video description\" is displayed on a light pink background. [0:00:13]: The same text continues but with a change in background to a white hue. The words \"DIY CRAFT TUTORIALS\" appear in the bottom right corner. [0:00:14 - 0:00:15]: A pair of hands holds a grey respirator mask with two white filters on each side, with the brand \"KISCHERS\" visible in the center. [0:00:16 - 0:00:19]: The text \"A Respirator Mask\" appears on the left side while the hands continue to hold and slightly adjust the mask, showing close-up details of its components.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What are the hands doing right now?",
                "time_stamp": "00:00:19",
                "answer": "D",
                "options": [
                    "A. Painting the ornament.",
                    "B. Hanging the ornament on a tree.",
                    "C. Mixing resin components.",
                    "D. Holding and adjusting a mask."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_54_real.mp4"
    },
    {
        "time": "0:01:40 - 0:02:00",
        "captions": "[0:01:40 - 0:02:00] [0:01:40 - 0:01:43]: A hand holding a transparent plastic cup containing several wooden sticks and a pair of tweezers is the main focus. The background is a plain, light-colored wall. The hand appears to belong to a person wearing a dark-colored sleeve. The text \"Cups, wood sticks and tweezers\" is displayed on the upper left side of the frame for these three seconds. [0:01:44 - 0:01:45]: The scene changes, showing a setup on a flat surface covered in a textured, white material. A hand wearing a black glove is pouring a liquid into a clear cup, which is held by another gloved hand. In the background, there is a beaker filled with a golden liquid and a wooden stick, alongside another small beaker. [0:01:46 - 0:01:47]: The pouring action continues, and a white circular object is placed horizontally on the surface. Another beaker and a small jar lid are visible on the right side. The setup remains consistent with the beaker of golden liquid and wooden stick. [0:01:48 - 0:01:51]: The cup being poured contains resin mixed with pink mica, as indicated by the text that appears in the top left corner reading \"Resin with Pink Mica.\" The pouring continues in a smooth, controlled manner. [0:01:52 - 0:01:57]: As the resin pours into the white circular object, the gloved hand uses a stick to help direct the flow. The golden liquid in the beaker and the arranged setup remain constant in the background. [0:01:58 - 0:01:59]: The remaining resin with pink mica in the cup is almost fully poured into the white object. The scene shows a close-up of the object as the resin spreads inside it, with both gloved hands maintaining their positions.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the gloved hand doing right now?",
                "time_stamp": "0:01:53",
                "answer": "D",
                "options": [
                    "A. Shaking a beaker filled with golden liquid.",
                    "B. Stirring the resin with a wooden stick.",
                    "C. Holding a pair of tweezers.",
                    "D. Pouring resin with pink mica into the white circular object."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_54_real.mp4"
    },
    {
        "time": "0:03:20 - 0:03:40",
        "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:25]: A pair of hands, covered in black gloves, is seen holding a circular white ornament with a small hole at the top, positioned centrally over a metallic gold mat. The ornament is half pink and half gold, with a clear boundary line. The right hand holds a wooden stick at an angle, hovering over the golden section of the ornament, while the left hand holds the ornament securely. Two plastic cups, one filled with a pink liquid and the other with a golden liquid, are placed on the mat above the ornament. A white absorbent paper towel covers the surface beneath the mat. The pink liquid appears as droplets below the ornament on the mat. [0:03:25 - 0:03:32]: The right hand steadily moves the wooden stick, distributing the resin over the ornament\u2019s surface. Gradually, the clear boundary starts to blur as the pink and gold resin mix subtly. The left hand remains steady, ensuring the ornament stays centered. The background remains consistent with the utensils and materials in their original positions. [0:03:32 - 0:03:37]: The manipulation continues with the right hand skillfully moving the wooden stick in smooth motions, occasionally dipping it back into the golden liquid in the cup. Several resin droplets accumulate below the ornament. The left hand adjusts the ornament slightly, ensuring an even application. The focus remains on achieving a seamless blend of the two colors on the ornament. [0:03:37 - 0:03:39]: As the blending proceeds, the right hand puts the wooden stick back into the cup containing the golden liquid. The left hand lifts the ornament slightly off the mat to inspect the smoothness of the resin layer. Both hands continue to handle the ornament carefully, striving for a smooth and even finish across the entire surface.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What did the right hand do just now?",
                "time_stamp": "00:03:29",
                "answer": "D",
                "options": [
                    "A. Adjusted the position of the ornament.",
                    "B. Lifted the ornament to inspect it.",
                    "C. Held the ornament securely.",
                    "D. Steadily moved the stick to distribute the resin."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_54_real.mp4"
    },
    {
        "time": "0:05:00 - 0:05:20",
        "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:19]: Two hands wearing black gloves are seen from a first-person perspective, working on a circular object placed on a brown mat. The background is a white, textured surface, possibly a padded table cover. The object appears to be a resin craft, displaying a pink and gold striped pattern. The left hand holds a small, clear plastic cup filled with a glittery substance. In the right hand, a wooden stick is used to spread and manipulate the substance over the surface of the circular object. There are a few small droplets on the brown mat around the main object, indicating potential drips during the crafting process. Nearby, two other cups with different substances are visible, one of which contains a pink liquid with a stick inside. The process of spreading the glittery material continues steadily throughout the sequence, with the left hand adjusting the cup's position and the right hand carefully applying the substance in a consistent manner. The object remains centered and the camera angle unchanged, maintaining a focus on the detailed work being performed.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "How is the glittery substance being applied to the circular object right now?",
                "time_stamp": "00:05:10",
                "answer": "D",
                "options": [
                    "A. Using a paintbrush.",
                    "B. Pouring it directly from the cup.",
                    "C. Applying it with fingers.",
                    "D. Spreading it with a wooden stick."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_54_real.mp4"
    },
    {
        "time": "0:06:40 - 0:07:00",
        "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:44]: A hand, dressed in a black sleeve, places a circular gold ornament on a grey, grid-patterned cutting mat. The ornament has the word \"Marlow\" written in white in the center. Another hand, also in a black sleeve, enters the frame and starts to peel off a piece of tape from the ornament. [0:06:45]: Both hands continue to work on peeling off the tape from the ornament. One hand holds the ornament steady, while the other carefully removes the tape. [0:06:46 - 0:06:48]: With the tape now removed, one hand stretches a piece of thin golden string in preparation. The ornament remains centered on the grey, grid-patterned cutting mat. [0:06:49 - 0:06:50]: The string is threaded through a small hole at the top of the ornament. One hand holds the string while the other hand guides it through the hole. [0:06:51]: The string is pulled through the hole, and one hand holds both ends of the string together. [0:06:52 - 0:06:53]: The person begins to tie the two ends of the string into a knot, creating a loop. [0:06:54 - 0:06:57]: The knot is tightened and secured by the person's hands, completing the attachment of the string to the ornament.  [0:06:58 - 0:06:59]: The person gives the string one final tug to ensure it is secure. The ornament is now ready to be hung, with the \"Marlow\" text clearly visible in the center.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the person doing right now?",
                "time_stamp": "0:06:47",
                "answer": "D",
                "options": [
                    "A. Placing the ornament on the mat.",
                    "B. Peeling off tape from the ornament.",
                    "C. Tying a knot in the string attached to the ornament.",
                    "D. Giving the string one final tug to secure it."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_54_real.mp4"
    },
    {
        "time": "0:00:00 - 0:00:20",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:11]: Two Christmas ornaments are held up close to the camera from a first-person perspective. The one on the left is purple with streaks of darker and lighter shades, and has the name \"Avery\" written in white cursive letters, along with a small holly design beneath the text. The one on the right is teal with similar streaks and has the name \"Jackson,\" also in white cursive letters with a holly design. Both ornaments have silver tops with white ribbons tied into bows. The holder's hands move slightly, and at times, rotate the ornaments, displaying their colors and features from different angles. [0:00:12]: The screen goes black. [0:00:13]: The text \"D ORN\" appears centered on the black screen. [0:00:14 - 0:00:17]: The text \"PAINTED ORNAMENTS,\" centered on the black background, is displayed as the screen remains dark. [0:00:18 - 0:00:19]: The screen transitions to a light pink background with the text \"Templates and Materials used are listed in the description\" centered on the screen.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What was held up close to the camera right now?",
                "time_stamp": "00:00:09",
                "answer": "A",
                "options": [
                    "A. Two Christmas ornaments.",
                    "B. A pair of painted canvases.",
                    "C. A set of colorful ribbons.",
                    "D. Two Christmas cards."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_69_real.mp4"
    },
    {
        "time": "0:01:00 - 0:01:20",
        "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:04]: A pair of hands, visible in the lower part of the frame, are adding acrylic paint into a transparent plastic container with a clear lid. The container is positioned in the middle of the frame, situated on a white, textured surface which contrasts with a dark grid-patterned mat beneath. On the top left and middle parts of the white surface lies another transparent container, similar in design but with two white components. Off to the right side, two tubes of acrylic paint, one green and one blue, rest with their caps removed. Another paint tube, white in color, is being held in the left hand, and a green paint tube is held in the right.  [0:01:05]: The right hand brings the tube of green paint closer to the transparent plastic container in the middle; the left hand is slightly open, positioned above the white surface. The left hand is now holding the previously opened white paint tube. The clear plastic containers and paint tubes remain in their prior positions.  [0:01:06 - 0:01:11]: The right hand starts to squeeze and add the acrylic paint from the green tube into the transparent container, which already contains some blue paint. The left hand, still holding the white paint tube, is slightly relaxed and positioned near the edge of the white surface. The green and blue tubes, with their caps off, still rest on the right side of the white surface. Two small, white plastic caps lie near the right edge of the white surface.  [0:01:12 - 0:01:15]: The right hand places the green paint tube on the right side of the white textured surface, close to the blue paint tube. The white paint tube held by the left hand is now placed near the center of the white textured surface. The container with paint in the middle remains unchanged, and the surrounding items, including the two small white caps and the other paint container, are in their original positions.  [0:01:16 - 0:01:19]: The left hand picks up a transparent plastic piece with white components seen previously on the upper part of the white surface. The left hand positions the plastic piece directly over the container holding the blue and green paint, gently lowering it as the right hand stabilizes the container from the side. The right hand continues to hold the container in place while the left hand carefully lowers the additional plastic piece with white components into the paint mixture. The other items on the white surface maintain their positions.  [0:01:20]: The hand slightly swirls the container, causing the paints inside to mix subtly without blending completely. The container and surrounding tubes and caps are in the same positions as before.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What did the right hand do just now?",
                "time_stamp": "0:01:19",
                "answer": "B",
                "options": [
                    "A. Picked up the blue paint tube.",
                    "B. Picked up shaked and the transparent plastic sphere.",
                    "C. Squeezed the white paint tube.",
                    "D. Put down the green paint tube."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_69_real.mp4"
    },
    {
        "time": "0:02:00 - 0:02:20",
        "captions": "[0:02:00 - 0:02:20] [0:02:04 - 0:02:24]: A pair of hands, positioned at the center and oriented towards the camera, firmly grips a spherical object. The sphere, predominantly blue with hints of green and white, is being carefully wiped clean using a white cloth held between the thumb and fingers. The surface appears glossy, reflecting light with multiple high points of reflection. Meanwhile, on the gray grid-patterned work surface beneath the hands are three tubes of paint\u2014one black, one green, and one blue\u2014 arranged in a slightly scattered manner on a white paper towel. Close by, part of a clear plastic container, thin and fragile-looking, occupies the top left corner. The right hand keeps holding the sphere while the cloth gently moves over the surface of the sphere\u2019s opposite side. The repeated motion smooths out the painted surface, revealing a glossy finish while the left hand occasionally moves to adjust the hold. A few plastic caps associated with the paint tubes lie scattered and disorganized near the top and right sides of the workspace. The background remains consistently occupied by the grid-patterned surface, accentuating the organized environment of a craft activity.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the person doing right now?",
                "time_stamp": "00:02:24",
                "answer": "B",
                "options": [
                    "A. Painting the sphere.",
                    "B. Wiping the sphere's surface.",
                    "C. Mixing paint colors.",
                    "D. Discarding the paint tubes."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_69_real.mp4"
    },
    {
        "time": "0:03:00 - 0:03:20",
        "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:01]: A hand, most likely wearing a recording device, is placed over a white paper towel. The surface beneath the towel appears to be a grid-patterned worktable. Several small paint tubes are scattered around, including one with white paint and another with black paint situated nearer than the rest. The pink paint tube is held securely in the left hand, positioned over the transparent bauble. The bauble contains a mix of black and pink paint at the bottom. A small, blue-green globe-like object sits off to the side, contributing to the scene's creative ambiance. [0:03:01 - 0:03:05]: The hand squeezes a few drops of bright pink paint from the tube into the bauble, adding to the existing colors inside the bauble. The hand continues to firmly grasp the tube while angling it back toward the base. This action brings the tube approximately halfway into view. [0:03:05 - 0:03:06]: With the right hand still holding the pink paint tube upright, the left hand returns the tube to its designated spot on the paper towel. At the same time, another, larger purple paint tube, partially capped, lays on the paper towel alongside the white paint tube. [0:03:06 - 0:03:08]: The left hand picks up the white paint tube. This results in a slight camera shift as the focus centers on the white paint tube. Meanwhile, the discarded pink paint tube partially rolls on its side but stays on the towel. [0:03:08 - 0:03:11]: With the white paint tube in hand, a small stream of white paint is released into the bauble. The center is forming a small mound of colors mixing at the bottom. The hands continue to apply redirection and control of the color placement, ensuring white approximates other colors. [0:03:11 - 0:03:13]: After adding the white paint to the bauble, the left hand sets the white paint tube on the towel. This tube is angled, rolling slightly before halting. The left hand then reaches to grasp the bauble's base, holding it steady. [0:03:13 - 0:03:15]: The bauble is lifted, and the components inside are clearly visible through the transparent glass. This perspective offers a direct view inside the bauble, highlighting the contrasting hues of pink, white, and black paint blended while still retaining their unique shapes.  [0:03:15 - 0:03:19]: While holding the position, the camera shifts slightly. Both hands rotate the bauble gently. The white and pink paint swirl together in a mix with the black base, creating a marbled effect. The purple hues become more visible as the hands carefully shift the bauble to ensure a balanced, integrated blend. Careful handling ensures the paint layers remain distinct, avoiding overmixing.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the person doing right now?",
                "time_stamp": "00:03:15",
                "answer": "C",
                "options": [
                    "A. Add blue-green paint to the bauble.",
                    "B. Squeeze more black paint into the bauble.",
                    "C. Lift the bauble to show the paint inside.",
                    "D. Clean the paint tubes."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_69_real.mp4"
    },
    {
        "time": "0:04:00 - 0:04:20",
        "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:09]: In a first-person perspective, the camera captures a scene where two hands hold a spherical ornament covered in vibrant purple paint with irregular patches of white and black. The ornament rests on a white paper towel, which is slightly stained with purple paint. To the left of the ornament, there are two tubes of paint lying on the paper towel, one purple and one black, both uncapped. In the background, another blue ornament with white patches is visible on a grid-patterned surface. The video shows slight movements of the hands as the ornament is rotated, revealing additional perspectives of the painted surface. Meanwhile, the caption at the bottom reads, \"Depending on your temperature, it might take a little longer to dry.\" [0:04:10]: As the video progresses, the hands continue to rotate the ornament, exposing more of its colorful surface. The camera captures the details of the purple paint with white and black patches distinctly. The blue ornament remains stationary in the background, placed slightly to the right on the grid-patterned surface. [0:04:11 - 0:04:16]: The camera zooms in slightly on the purple ornament held by the hands, offering a closer look at the paint's texture and colors. The tubes of paint remain visible on the paper towel. The blue ornament in the background stays in its position, providing a contrast to the purple ornament in the foreground. [0:04:17 - 0:04:19]: The scene fades to black, and white text appears on the screen that reads, \"Personalizing the Ornaments.\" The text remains steady as the captions [0:04:17], [0:04:18], and [0:04:19] display sequentially with no other visual changes.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What are the hands doing right now?",
                "time_stamp": "00:03:53",
                "answer": "B",
                "options": [
                    "A. Applying paint to the ornament.",
                    "B. Rotating the ornament to reveal different perspectives.",
                    "C. Placing the ornament back on the towel.",
                    "D. Cleaning the paint tubes."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_69_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00]: The screen is completely black, with the timestamp \"00:00\" in white text in the top left corner; [0:00:01]: An abstract, colorful background with a mix of purple, pink, and black hues appears, along with horizontal glitch lines; [0:00:02 - 0:00:07]: A logo with the words \"GRAVITY THROTTLE RACING\" is displayed in white letters against a yellow, red, and blue triangular background; [0:00:08]: The scene transitions to a vibrant diorama of a Texaco gas station with several miniature cars, including a prominent red race car and a pink car labeled \"Charms.\" There are mountains in the background; [0:00:09 - 0:00:16]: Several miniature figurines are positioned around the gas station and cars. The scene shows various activities, including the cars getting refueled or serviced. The bright, sunny setting contrasts with the bright green landscape, and a large Texaco sign lists gas prices; [0:00:17]: The camera angle shifts slightly to reveal the full breadth of activity, with more emphasis on the cars and figurines interacting within the scene; [0:00:18 - 0:00:19]: The view transitions again to a field with green hills. There are miniature dinosaurs and figurines, also showing several cars and a helicopter within the scene.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What significant change happened just now?",
                "time_stamp": "0:00:19",
                "answer": "A",
                "options": [
                    "A. The scene transitions to a field with green hills.",
                    "B. The logo with \"GRAVITY THROTTLE RACING\" reappears.",
                    "C. The abstract background becomes darker.",
                    "D. The Texaco gas station disappears."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_493_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:01]: The video showcases a winding mountain road with a steep drop-off on one side, surrounded by snow-covered terrain and evergreen trees. A blue car is seen driving around a sharply curved incline. [0:04:01 - 0:04:03]: The camera follows the car's progress, showing it navigating the curves of the road. The background features rocky outcroppings and additional roadways traversing the mountainous region. [0:04:03 - 0:04:04]: As the car continues along the road, the landscape transitions to drier, more desert-like conditions. Various vehicles traverse different routes, with a black car dominating the scene. [0:04:04 - 0:04:06]: The black car is seen exiting a tunnel and continuing its path through a winding, elevated road. The road's design integrates with the rocky, arid terrain. [0:04:06 - 0:04:08]: An overhead view shows multiple cars, red and black, navigating the winding roadway. The roadways cross over one another, highlighting the complex, multi-level design. [0:04:08 - 0:04:10]: The camera angle shifts to emphasize a primary black car on the road while other cars maintain distance behind. The road weaves through diverse topographies, including rocky and desert-like environments. [0:04:10 - 0:04:11]: The viewpoint transitions to an overview of an airport, featuring hangars, a small plane, and a few vehicles parked or moving near the structures. A rocky outcrop stands prominently in the foreground. [0:04:11 - 0:04:13]: Vehicles, including two yellow cars, maneuver around a rocky structure near the airport. The road swerves and curves tightly around the terrain. [0:04:13 - 0:04:15]: The video focuses on a black car driving along a smoother section of the winding road. The background consists of barren, rocky hills and elevated tracks. [0:04:15 - 0:04:17]: The black car continues its route, passing under a sign and converging with other paths. Road surface variations are evident, adding texture to the visual experience. [0:04:17 - 0:04:19]: The video depicts a speed checkpoint displaying race information. The black car approaches and eventually passes the display, revealing time and tracking details. The surrounding environment consists of sparse vegetation and sandy terrain.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "Which car won first place in this competition?",
                "time_stamp": "00:04:25",
                "answer": "D",
                "options": [
                    "A. The red car.",
                    "B. The orange car.",
                    "C. The yellow car.",
                    "D. The black car."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_493_real.mp4"
    },
    {
        "time": "[0:08:00 - 0:09:00]",
        "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:02]: A few colored toy cars are racing on a beige terrain with tire tracks, resembling a snowy landscape with scattered toy trees. The camera follows the cars from a first-person perspective as they navigate the track.  [0:08:03 - 0:08:05]: The cars continue driving up a curved hill with more toy trees placed sporadically along the track. The terrain elevation changes as the cars move along the winding road. [0:08:06 - 0:08:07]: The cars make a sharp turn around a steep curve. The landscape appears rocky with elevated sections, and the toy trees are denser along the edge of the track. [0:08:08 - 0:08:09]: The track descends slightly as the cars race down the bending path, surrounded by a rocky and steep landscape. The toy trees continue to line the path, and some toy greenery is seen around the curves. [0:08:10]: One of the cars enters a small tunnel at the base of a rocky hill, while the background shows more of the rocky model terrain and a small bridge overhead. [0:08:11 - 0:08:13]: The scene transitions to an adjacent part of the track, where a model train passes behind a rocky backdrop, and toy vehicles are visible on another part of the track. The landscape resembles a desert area. [0:08:14 - 0:08:15]: Two toy cars continue to race alongside a model train on a parallel track. The cars pass a billboard, and the desert-themed landscape has model trees and shrubs. [0:08:16 - 0:08:18]: One car overtakes another car as they continue on a winding, slightly elevated road, with the desert backdrop featuring rail tracks and a colorful painted cliffside. [0:08:19]: The scene seamlessly transitions to a different setup, where different toy cars are positioned on separate tracks, ready for another race. The background showcases the starting line and racing details. [0:08:20]: The scene features a group of parked toy cars in different colors on a snowy terrain. The track is clearly defined with tire marks, and the terrain retains its white snowy appearance.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "How is the terrain described where the toy cars are racing at the beginning?",
                "time_stamp": "00:08:02",
                "answer": "A",
                "options": [
                    "A. Snowy with scattered toy trees.",
                    "B. Rocky and steep.",
                    "C. Desert-themed with rail tracks.",
                    "D. Grassy with ponds."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_493_real.mp4"
    },
    {
        "time": "[0:12:00 - 0:13:00]",
        "captions": "[0:12:00 - 0:12:20] [0:12:00 - 0:12:05]: The video begins with a view of a paved road curving to the right in a desert-like landscape. A red car leads the scene, followed by a black car, both driving along the road. Rail tracks run parallel to the road on the right side. In the distance, a tunnel is visible in the mountains, with two trains approaching from the left. The background features a clear, blue sky with white clouds and rocky mountain structures. [0:12:06 - 0:12:07]: The scene transitions to a close-up of a section of the road with a concrete sidewalk next to it, resembling a miniature setup. Two tripods are visible standing on the ground. The camera captures the white car's movement to the left. [0:12:08]: A further zoomed-in view shows a white car continuing its way across the miniature setup, with the backdrop of a grey wall. [0:12:09 - 0:12:12]: The video captures a list of racing teams and their ranking points. The prominent teams are \u201cLarro,\u201d \u201cJay Bo,\u201d \u201cPacific Pirate,\u201d and \u201cSisters of the Heavy.\u201d The number of points for each round (R1-R7) along with totals are displayed. Larro leads with 23 points, followed by Jay Bo with 17, Pacific Pirate with 15, and Sisters of the Heavy scoring significantly lower. Different colors and fonts highlight each team\u2019s name. [0:12:13 - 0:12:20]: The final frames show five toy cars lined up on a white, snowy track, ready for the race. The names \"Jay Bo,\" \"Larro,\" \"Sisters of the H.M.,\" and \"Pacific Pirate\" are visible, indicating the competitors.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Which team is leading in the racing points?",
                "time_stamp": "00:12:14",
                "answer": "D",
                "options": [
                    "A. Jay Bo.",
                    "B. Pacific Pirate.",
                    "C. Sisters of the Heavy.",
                    "D. Larro."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_493_real.mp4"
    },
    {
        "time": "[0:15:00 - 0:15:43]",
        "captions": "[0:15:40 - 0:15:43] [0:15:40 - 0:15:43]: The video centers on a colorful logo that occupies the majority of the screen. The logo is shaped like an elongated, pointed oval, with a combination of yellow, blue, and red colors. The yellow background is prominent, with a blue triangular shape extending from the left into the center, overlaid by a smaller red triangle. The text \"GRAVITY THROTTLE RACING\" is written across the logo in three lines, with each word stacked vertically. The text is bold and white, standing out clearly against the colored background. The background of the entire frame is dark, possibly indicating a low-light environment or outer space theme, and faint horizontal lines are visible, creating a dynamic backdrop.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "Which car won first place in this competition?",
                "time_stamp": "00:15:15",
                "answer": "D",
                "options": [
                    "A. The red and bluecar.",
                    "B. The black car.",
                    "C. The yellow car.",
                    "D. The brown and blue car."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_493_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:20]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video starts with a person sitting at a desk with a graphics card in front of them. The individual has a calm expression, wearing a grey t-shirt. The background is dimly lit with some modern furniture visible, including a dark cabinet and a monitor to the right. The desk is light-colored, contrasting with the darker background. The person gestures with their hands while talking, and a large \"$300\" appears on screen in bright yellow text, emphasizing the cost of the item being discussed. [0:00:04 - 0:00:06]: The individual continues discussing the graphics card, moving their hands expressively. The graphics card itself, mostly white with three large cooling fans, is positioned on the desk. At one point, the person holds up the graphics card to showcase it to the camera. [0:00:07 - 0:00:08]: The video transitions to a top-down view of the graphics card labeled \"GEFORCE RTX\". The card lies flat on the desk, with the person's hands on either side, providing a clear view of its design and branding. [0:00:09 - 0:00:12]: The person reaches to pick up another graphics card from the desk, holding two in hand for comparison. The second graphics card is dark in color with a different fan design. They place both cards next to each other on the desk for a side-by-side comparison, continuing to explain the differences. [0:00:13 - 0:00:17]: The focus shifts to a close-up of the darker graphics card being taken out of its packaging. The hands are seen carefully removing it from a black foam tray, with the branding \"INSPIRED BY GAMERS. BUILT BY NVIDIA.\" clearly visible on the packaging. The person examines the card, turning it around to show various angles and details. [0:00:18 - 0:00:20]: The final part of the video shows a web page featuring listings of different graphics cards for sale, including their prices and conditions. The screen displays a variety of GPUs with prices ranging from $224 to $300 and more, illustrating the options available for purchase. The user interface includes search filters, images of the products, and detailed information about each listing.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What text is prominently shown on the packaging of the darker graphics card right now?",
                "time_stamp": "00:00:15",
                "answer": "C",
                "options": [
                    "A. DESIGNED BY GAMERS, BUILT BY NVIDIA.",
                    "B. CREATED BY GAMERS, DEVELOPED BY NVIDIA.",
                    "C. INSPIRED BY GAMERS. BUILT BY NVIDIA.",
                    "D. ENGINEERED BY GAMERS, MADE BY NVIDIA."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_122_real.mp4"
    },
    {
        "time": "[0:01:20 - 0:01:40]",
        "captions": "[0:01:20 - 0:01:40] [0:01:20 - 0:01:24]: The video begins with a comparison chart showcasing average frames per second (FPS) for different titles under the label \"NVIDIA 3070 vs. 4060\" at \"1440p high/max.\" The chart shows various FPS values represented by green bars, with titles listed at the bottom, including \"DOOM Eternal,\" \"F1 22,\" \"Shadow of the Tomb Raider,\" \"Horizon Zero Dawn,\" \"Forza Horizon 5,\" \"COD: MW II,\" \"RDR2,\" \"God of War,\" \"Control,\" \"Dying Light 2,\" \"Hogwarts Legacy,\" and \"Cyberpunk 2077.\" The background is dark with a subtle gradient and some graphical detailing. [0:01:25 - 0:01:36]: As the video progresses, the chart remains on the screen, showing FPS values for the same titles mentioned earlier. The positions and values of the green bars do not change, indicating a static comparison without dynamic updates. The background continues to be dark with some stylistic grain or noise. [0:01:37 - 0:01:40]: From timestamp 0:01:37, the chart gradually fades out, and the scene transitions to a close-up view of a person seated at a desk. The individual is speaking and gesturing with their right hand. Two graphics card models are prominently displayed on the desk in front of them. The one on the left has a black design with dual fans, while the one on the right is white with triple fans. The environment is well-lit, modern, and clean, with background elements including a dark wall, a light source, and parts of the room.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What titles are not listed on the comparison chart shown right now?",
                "time_stamp": "00:01:36",
                "answer": "A",
                "options": [
                    "A. GTA V.",
                    "B. Shadow of the Tomb Raider.",
                    "C. Forza Horizon 5.",
                    "D. God of War."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_122_real.mp4"
    },
    {
        "time": "[0:02:40 - 0:03:00]",
        "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:43]: The video begins with a close-up shot of a graphics card on a white table. The card is black with red accents, and the brand name \"RADEON\" is visible. The lighting is soft with a dark background. Following this, a person holds a different graphics card, displaying both the front and the back. The card is black with dual fans, and the person's hands are prominently featured in the frame against a red and black background. [0:02:44 - 0:02:49]: The scene shifts to a medium shot of a person seated at a table, holding and pointing to the graphics card with dual fans. The person appears to be explaining something about the card. The room is well-lit with a modern, dark-themed decor. Another graphics card lies on the table beside the person, indicating a comparison might be taking place. The person then reaches toward the side to pick up another graphics card and brings it into the frame. [0:02:50 - 0:02:55]: The video transitions to a graphical display comparing the average frames per second (fps) of different games between the AMD Radeon card and another card. The graph shows data points for several games such as DOOM Eternal, Shadow of the Tomb Raider, Horizon Zero Dawn, and others. The data is displayed with varying fps numbers, with the AMD card showing higher performance. [0:02:56 - 0:02:59]: The performance comparison continues with the graph displaying detailed statistics that AMD Radeon\u2019s fps is 16% faster than the average of another card. The list of games and their corresponding fps remain constant, showing the performance of the AMD card in various high-resolution scenarios. The background remains dark with a slight texture, highlighting the data points.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is the brand name of the graphics card shown right now?",
                "time_stamp": "00:03:00",
                "answer": "C",
                "options": [
                    "A. NVIDIA.",
                    "B. GE FORCE.",
                    "C. AMD.",
                    "D. RADEON."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_122_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:04:20]",
        "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:11]: A man in a grey t-shirt is seated at a table with two large graphics cards placed in front of him. He is explaining something, using hand gestures to emphasize his points. The background is dimly lit with modern, minimalistic room decor, including a standing light to the left and a glimpse of a computer setup on the right. The left graphics card is black with dual fans, while the right graphics card has a distinct design with three fans and angular edges. The man points to the graphics cards, moving his hands in a descriptive manner. [0:04:12 - 0:04:13]: The view shifts to a close-up overhead shot of the two graphics cards on the table. The left card has a sleek design with two large fans, whereas the right card features three fans with a geometric, aggressive design. The man's hands briefly appear in the lower part of the frame, possibly indicating features on the cards. [0:04:14 - 0:04:19]: The video returns to the man as he continues to explain, periodically using hand gestures. He is now pressing his fist into his open palm and occasionally pointing towards one of the graphics cards on the table. His expressions are engaged, suggesting he is explaining something in detail about the graphical hardware in front of him.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the design feature of the graphics card on the right right now?",
                "time_stamp": "00:04:11",
                "answer": "B",
                "options": [
                    "A. It has dual fans with a sleek design.",
                    "B. It has three fans with a geometric, aggressive design.",
                    "C. It features a single large fan in the center.",
                    "D. It is completely fanless with a passive cooling system."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_122_real.mp4"
    },
    {
        "time": "[0:04:40 - 0:04:47]",
        "captions": "[0:04:40 - 0:04:47] [0:04:40 - 0:04:47]: The video features an individual in a well-lit room, sitting at a desk with two graphics cards displayed in front of them. The individual gestures with their hands while speaking, illustrating points about the graphics cards. The graphics card on the left is black with dual large fans, while the one on the right is white with triple fans. The person wears a grey shirt, and the background is a combination of dark and light areas, with some vertical lines and dark-colored equipment or furniture.",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many fans does the graphics card on the right have right now?",
                "time_stamp": "00:04:46",
                "answer": "C",
                "options": [
                    "A. One.",
                    "B. Two.",
                    "C. Three.",
                    "D. Four."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_122_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video begins with a close-up view of a painting that depicts a stylized, abstract figure in black with yellow color accents on what appear to be gloves. The background of the painting is a mix of white, gray, and patches of red and yellow. The painting is framed in black and hung on a white wall. [0:00:03 - 0:00:05]: The camera pans slightly to the right, revealing another painting next to the first one. This second painting also depicts an abstract figure, but this time the figure is in pale white, outlined with dark lines and featuring stripes on its neck and limbs. The figure is set against a dark background with vertical stripes and geometric shapes. [0:00:05 - 0:00:13]: As the camera continues to pan right, it captures more of the second painting, showing the entire figure, which is abstract and elongated. Another painting becomes visible to the right, showcasing two figures with red hair and pale bodies against a dark background. These figures are also abstract, with one featuring a spiral on the left arm. [0:00:13 - 0:00:16]: The video focuses on the third painting depicting two red-haired, pale figures. Both figures are stylized and abstract, with red and pink hues accentuating their bodies. The painting has a dark background and is framed in black. [0:00:16 - 0:00:19]: The camera moves further right, showing another abstract painting. This work features two black figures in a domestic scene set against a gray background. One figure is reclining on a couch or bed, while the other stands nearby. There's also a stark contrast with bright colors, including a pink section at the bottom of the painting. People are visible in the background as the video concludes.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is the position of the third painting in relation to the second painting?",
                "time_stamp": "0:00:13",
                "answer": "B",
                "options": [
                    "A. To the left.",
                    "B. To the right.",
                    "C. Above.",
                    "D. Below."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Attribute Recognition",
                "question": "What distinguishes the figures in the third painting?",
                "time_stamp": "0:00:16",
                "answer": "B",
                "options": [
                    "A. They have blue hair and dark bodies.",
                    "B. They have orange hair and pale bodies.",
                    "C. They have green hair and yellow bodies.",
                    "D. They have black hair and white bodies."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "art_show",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_471_real.mp4"
    },
    {
        "time": "[0:08:00 - 0:09:00]",
        "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: The video begins showing an art display on a white wall, featuring various framed artworks. Prominently, there is a large square piece in the center with a purple abstract design. Surrounding it are smaller rectangular, square, and circular frames with different abstract and nature-themed artwork. To the right, there is an artwork depicting a child riding a turtle. [0:08:01 - 0:08:03]: The camera moves slightly to the right, providing a slightly better view of the painting of the child on the turtle. More people can be seen in the background, suggesting the setting is an art gallery. [0:08:03 - 0:08:04]: The camera pans down, giving a clearer focus on the lower part of the display, which includes more abstract art in both circular and rectangular frames. Flowers and greenery designs become visible on the left side. [0:08:04 - 0:08:05]: The view shifts to focus more on the four square pieces on the left side of the display, each showing colorful abstract depictions of flowers and plants. The artworks are vivid with rich greens, reds, and purples. [0:08:05 - 0:08:06]: The camera remains on the four square pieces, but moves slightly downward, concentrating on them more closely. The top two pieces have a prominent greenish-yellow hue, while the bottom two are primarily red and green. [0:08:06 - 0:08:08]: The camera continues moving slightly downward and to the right, maintaining focus on the four square floral artworks. The details of the artworks, such as individual brush strokes and textures, become more visible. [0:08:08 - 0:08:10]: The camera angle shifts rightwards to include additional smaller abstract pieces on a wooden background. These pieces feature white, cloud-like forms on wooden canvases ranging in size and shape. [0:08:10 - 0:08:12]: The camera pans right to reveal more of the wooden panel designs, each featuring minimalist white shapes. The background display\u2019s arrangement seems deliberate, emphasizing the simplicity of the white forms against the natural wood. [0:08:12 - 0:08:14]: The view centers on an ensemble of six wooden panel pieces, highlighting their abstract and minimalistic nature. Each piece includes a solitary white shape, varying from oval to spherical. [0:08:14 - 0:08:16]: The camera continues to focus on the six wooden artworks, where the white shapes appear slightly raised, casting subtle shadows on the wood surface. The arrangement adds depth and texture to the display. [0:08:16 - 0:08:19]: The camera zooms in on the largest wooden panel piece at the top left of the display. This rectangular piece features a white, spherical shape subtly raised from the surface. The grainy texture of the wood and the delicate pattern surrounding the white shape become evident as the video concludes.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What characteristic is shared by the white shapes on the wooden panel pieces?",
                "time_stamp": "0:08:16",
                "answer": "B",
                "options": [
                    "A. They are all triangular.",
                    "B. They are all raised and cast shadows.",
                    "C. They all have a blue background.",
                    "D. They all include floral designs."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "art_show",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_471_real.mp4"
    },
    {
        "time": "[0:10:00 - 0:10:43]",
        "captions": "[0:10:40 - 0:10:43] [0:10:40 - 0:10:42]: The video showcases an art exhibition. There are four pieces of artwork mounted on the white wall. The two on the left involve colorful strings creating patterns over a background of intricate black and white lace-like textures. These are rectangular and placed one above the other. To the right, there are two square black canvases. Each features a line of colorful spherical smiley-face objects arranged in rows. Some faces are arranged to form a pattern, while others appear more randomly placed. The right side of the frame shows part of another artwork featuring a painting with green foliage and white flowers on a blue background. In the background, visible parts of the gallery include artworks hanging on the walls and some visitors in the space. The area is well-lit, and the artworks are displayed neatly. [0:10:42 - 0:10:43]: The camera angle remains fairly consistent while slightly adjusting to provide a better view of the four artworks. The details in the background become clearer, showcasing additional paintings on the wall with varying colors and frames. The visitor in white pants and a darker top is partially visible to the left, suggesting an open gallery space. The lighting continues to highlight the artworks effectively.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Compared to the colorful string patterns, where are the black canvases with smiley decorations on the top of these two squares?",
                "time_stamp": "00:10:43",
                "answer": "C",
                "options": [
                    "A. Directly above them.",
                    "B. To the left of them.",
                    "C. To the right of them.",
                    "D. Below them."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Causal Reasoning",
                "question": "What can be inferred about the layout of the gallery space based on the video content?",
                "time_stamp": "00:10:43",
                "answer": "C",
                "options": [
                    "A. The gallery is very crowded with limited space.",
                    "B. The artworks are displayed haphazardly.",
                    "C. The gallery is open with artworks neatly displayed and visible visitors.",
                    "D. The lighting is dim and focused on certain artworks."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "art_show",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_471_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:10]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions taken just now?",
                "time_stamp": "00:00:10",
                "answer": "D",
                "options": [
                    "A. The individual makes an espresso shot, adds water to it, and prepares it for serving.",
                    "B. The individual heats water, measures coffee grounds, and prepares coffee in a French press.",
                    "C. The individual collects various ingredients, chops them, and prepares a fresh smoothie.",
                    "D. The individual picks up a milk jug, pours milk into a metal pitcher."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_372_real.mp4"
    },
    {
        "time": "[0:01:19 - 0:01:29]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions taken just now?",
                "time_stamp": "00:01:29",
                "answer": "D",
                "options": [
                    "A. The individual washed the espresso machine parts and prepared fresh coffee grounds.",
                    "B. The individual brewed tea leaves and poured tea into a cup.",
                    "C. The individual mixed ingredients in a blender to prepare a smoothie.",
                    "D. The individual steamed milk using an espresso machine's steam wand and wiped the steam wand clean."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_372_real.mp4"
    },
    {
        "time": "[0:02:38 - 0:02:48]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions taken just now?",
                "time_stamp": "00:02:45",
                "answer": "D",
                "options": [
                    "A. The individual prepares espresso shots while organizing coffee cups and saucers.",
                    "B. The individual brews tea, rinses teapots, and places them on a drying rack.",
                    "C. The individual purchases new coffee supplies and stores them in the kitchen cabinets.",
                    "D. The individual cleans several milk frothing pitchers, rinsing them under running water."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_372_real.mp4"
    },
    {
        "time": "[0:03:57 - 0:04:07]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions taken just now?",
                "time_stamp": "00:04:50",
                "answer": "D",
                "options": [
                    "A. The individual arranges milk jugs and rinses them under running water, then cleans the countertop.",
                    "B. The individual organizes kitchen utensils, prepares ingredients, and washes dishes in the sink.",
                    "C. The individual identifies dirty items in the workspace, organizes them, and prepares for the next task by collecting utensils.",
                    "D. The individual adjusted coffee machine settings, added coffee to cups while checking printed receipts."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_372_real.mp4"
    },
    {
        "time": "[0:05:16 - 0:05:26]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions taken just now?",
                "time_stamp": "00:05:26",
                "answer": "D",
                "options": [
                    "A. The individual collects various bakery items, assembles them on a tray, and prepares to serve them to customers.",
                    "B. The individual picks up a milk jug, prepares milk for steaming, and pours it into a cup for serving.",
                    "C. The individual clears dirty dishes, washes them, and arranges them on a drying rack.",
                    "D. The individual operates the espresso machine, prepares coffee, and serves the freshly brewed coffee to a customer."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_372_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a close-up shot of a wooden bowl filled with various fruits, including a pomegranate split open to reveal its seeds, green and purple grapes, and figs. The fruits are positioned neatly in the bowl, which is set against a backdrop of green leaves. The lighting is soft and natural. [0:00:02 - 0:00:06]: As the camera angle shifts downward, the hem of a red garment is visible on a wooden floor. The focus then changes to bare feet touching the floor, eventually revealing more of the red garment and another white piece of fabric nearby. [0:00:07 - 0:00:10]: The scene transitions to show a woman with curled hair, wearing a white dress, in the process of getting her hair styled by another person. The background is plain, with a focus on the woman\u2019s face and the person styling her hair. [0:00:11 - 0:00:12]: The camera then pans out to display a wider view of the room. The woman is seated on a wooden chair, still getting her hair done. The room is decorated in a traditional style with patterned wallpaper and sconces holding lit candles. A small table with fruits and candles is visible in the background. [0:00:13 - 0:00:15]: There is a close-up shot of the woman\u2019s face again, showing her smiling gently as her hair is being styled.  [0:00:16 - 0:00:19]: The camera view expands to show another person, dressed in a traditional outfit with a white cap and red dress, adjusting the seated woman\u2019s gown and apron in the same traditionally decorated room. The interaction is nurturing and gentle.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the woman in the white dress doing when the scene transitions?",
                "time_stamp": "0:00:10",
                "answer": "C",
                "options": [
                    "A. Dancing.",
                    "B. Styling another person's hair.",
                    "C. Getting her hair styled by another person.",
                    "D. Eating."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "getting_dressed",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_151_real.mp4"
    },
    {
        "time": "[0:01:00 - 0:02:00]",
        "captions": "[0:01:40 - 0:02:00] [0:01:40 - 0:01:46]: In a cozy, vintage-style room lit by several wall-mounted sconces, two women are engaged in the process of dressing. The woman on the left, wearing a white undershirt, stands facing the other woman, who appears to be helping her with a black outer garment. The woman on the left raises her arms while the woman on the right, dressed in a red and white outfit with a cap, secures the garment around her. The surroundings are modestly decorated with patterned wallpaper and a wooden bench. [0:01:47 - 0:01:54]: The dressing process continues as the woman on the right diligently adjusts the black dress over the white undershirt of the woman on the left. She lifts and arranges the garment, ensuring it fits snugly. The black dress has intricate patterns or embroidery visible on its surface. Throughout these actions, they remain in the same positions within the room. [0:01:55 - 0:01:56]: The focus shifts more closely as the woman in red works on fastening the dress, securing various adjustments to ensure a proper fit. The expressions of both women remain calm and concentrated on the task at hand. [0:01:57 - 0:01:59]: A close-up perspective captures the meticulous actions of fastening the black lace on the garment. The hands of the woman in red carefully work on the lacing, showing attention to detail. The other woman's contented expression is visible, indicating the completion of the dressing process.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the color of the undershirt worn by the woman on the left?",
                "time_stamp": "00:01:55",
                "answer": "B",
                "options": [
                    "A. Black.",
                    "B. White.",
                    "C. Red.",
                    "D. Blue."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Event Understanding",
                "question": "What activity are the two women primarily engaged in?",
                "time_stamp": "00:02:00",
                "answer": "C",
                "options": [
                    "A. Cooking.",
                    "B. Cleaning.",
                    "C. Dressing.",
                    "D. Sewing."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "getting_dressed",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_151_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:26]: A woman is standing still as another woman, dressed in period attire, assists her in adjusting a long, dark, flowing garment. The first woman has curled hair and is wearing a dark dress with golden accents, while the assistant, who has curly hair tied back, wears a red dress with a white apron and white headpiece. They are in a room with cream-colored walls, decorated with wall sconces and floral patterns near the ceiling and at chair rail height. Furniture like chairs and a dresser is present in the background. [0:03:27 - 0:03:36]: Close-up views focus on the flowing dark fabric of the dress, emphasizing its texture and movement as it is being adjusted. Another person\u2019s arm, dressed in a light blue sleeve, occasionally comes into view, pulling and aligning the dress. [0:03:37 - 0:03:39]: Finally, a wide view shows the first woman standing poised while the assistant completes final adjustments on the front of the dress, maintaining her focused expression. The room's furnishings and decor remain consistent in the background.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color is the assistant's dress?",
                "time_stamp": "0:03:26",
                "answer": "B",
                "options": [
                    "A. Blue and red.",
                    "B. Red and white.",
                    "C. Green and red.",
                    "D. Purple and white."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "getting_dressed",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_151_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:02]: The video opens with a close-up shot of a man with medium-length, curly, white hair. He has a full beard and is wearing a black top with a white neckline. The background is plain black. He initially has a neutral expression before gradually beginning to smile. [0:05:03 - 0:05:14]: The focus shifts to another person wearing a dark, possibly black robe with long, yellow-gold sleeves. Their hands are clasped together in front of their lap, with their fingers interlocked. The background here is simple and undecorated. [0:05:15 - 0:05:19]: The camera captures the upper part of a person's body, focusing on an ornately designed neckline of their dress. The dress is black with intricate gold stitching around the neckline. The person has curly hair that is partly visible. The background shows a window, through which blurred greenery can be seen.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the man with medium-length, curly, white hair wearing?",
                "time_stamp": "00:05:19",
                "answer": "B",
                "options": [
                    "A. A white top with a black neckline.",
                    "B. A black top with a white neckline.",
                    "C. A dark robe with yellow-gold sleeves.",
                    "D. A dress with intricate gold stitching."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "getting_dressed",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_151_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:07]: In a cityscape featuring a bustling urban atmosphere, the video begins with a view of a cobblestone street. Tall, multi-story buildings in various shades of gray and beige line both sides of the street, reflecting European architectural styles with large windows and ornate balconies. The left side of the frame shows two people, one wearing a blue shirt and black shorts, and the other in a black top and yellow shorts, walking along the sidewalk. Street lamps and tram tracks are prominent features along the road. Various pedestrians are walking, engaging in casual activities, and some are heading towards the backdrop, where more buildings can be seen. [0:00:08 - 0:00:14]: As the footage continues, the camera captures more pedestrians crossing or navigating along the street. A street pole with graffiti appears in the middle of the frame, adding an urbane touch. The crosswalk is clearly visible with its distinctive white stripes against the dark cobblestone. On the right side of the frame, a woman in a patterned top and jeans, wearing a mask, approaches while carrying a blue bag. The background showcases storefronts with vibrant window displays and a moderate amount of foot traffic. [0:00:15 - 0:00:20]: The scene shifts focus slightly, panning from left to right. A crosswalk and a street corner with modern streetlights and signage come into view. Pedestrians continue to populate the area, moving in different directions. More historical buildings with large windows, some with balconies, occupy the background. Trees add a touch of greenery, and a kiosk stands at the corner. The final frames reveal a more expansive view of the square, with a statue and additional buildings in the distance, including one under construction. Three individuals walk close to the camera, adding a dynamic element to the concluding moments.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color shorts is the person wearing who is walking with someone in a blue shirt?",
                "time_stamp": "0:00:07",
                "answer": "B",
                "options": [
                    "A. Black.",
                    "B. Yellow.",
                    "C. Red.",
                    "D. Green."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_331_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:01]: At the start, the video showcases a beautifully paved, expansive square with a central statue surrounded by buildings with classic European architecture. The sky above is bright and clear. [0:02:02 - 0:02:04]: As the video progresses, the camera angle shifts slightly to the right. A green bench with several people seated on it becomes more visible. Additionally, cars and buses are parked along the street to the left of the square. [0:02:05 - 0:02:08]: The camera continues to pan slightly to the right, enhancing the central positioning of the green bench. The bench has three people seated on it, engaging in what appears to be casual conversation. The street to the left is active with more vehicles and a visible pedestrian crossing. [0:02:09 - 0:02:12]: The orientation of the view is now almost directly centered on the square. The buildings on the left and right edges of the square frame the scene, with pedestrians walking towards the central area. The architecture of the buildings is ornate, featuring tall windows and elegant facades. [0:02:13 - 0:02:15]: As the panning continues, the camera slightly adjusts to include more details of the buildings on the left, with trees lining the street. The bus and car traffic remains visible, indicating an active urban setting. [0:02:16 - 0:02:18]: The focus shifts further to the right, revealing more of the right-side buildings. A cyclist rides through the square, heading towards the central area. The fountain and statue are still visible in the lower right corner. [0:02:19 - 0:02:20]: Finally, the camera settles to provide a comprehensive view of the square's expanse. Both sides of the square are framed by grand architectural buildings, and the central pathway is dotted with pedestrians, a cyclist, and arranged benches. The atmosphere is lively and bustling.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the cyclist heading as they ride through the square?",
                "time_stamp": "00:02:18",
                "answer": "C",
                "options": [
                    "A. Towards the pedestrian crossing.",
                    "B. Away from the central area.",
                    "C. Towards the central area.",
                    "D. Along the street to the left."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_331_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:05]: The video opens with a wide view of a large plaza paved with cobblestones. In the center background stands a grand, historical building with a tall clock tower topped with a green spire. The sky is clear and blue. The plaza is lined with neatly arranged trees, and some scaffolding can be seen on the left. A white van is parked on the right side of the plaza. The perspective is from a central point facing directly towards the building, and there are a few scattered pedestrians. [0:04:06 - 0:04:17]: As the seconds pass, the perspective remains focused on the grand building, but the camera slowly moves forward. Trees line both sides of the pathway leading up to the building. More details of the plaza become visible, including a circular brown area near the center and additional people walking around. The scaffolding on the left appears to be related to ongoing construction or an event setup. The white van is still parked, but there is slight movement towards the building in the background. Pedestrians are seen walking in both directions, and their positions change slightly as they move. [0:04:18 - 0:04:20]: Near the end of the video, more people are visible walking closer to the camera. Two people are walking on the left side of the path, and another person walks on the right. The historic building becomes more prominent, and details like the statues and carvings on its fa\u00e7ade are clearer. The urban setting's activity is subtly dynamic with people strolling and some vehicles visible in peripheral areas. The weather remains clear and the video maintains a bright, sunny atmosphere.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is parked on the right side of the plaza?",
                "time_stamp": "00:04:05",
                "answer": "C",
                "options": [
                    "A. A red bicycle.",
                    "B. A blue car.",
                    "C. A white van.",
                    "D. A black motorcycle."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the scaffolding located in the plaza?",
                "time_stamp": "00:04:10",
                "answer": "C",
                "options": [
                    "A. On the right side.",
                    "B. Near the center.",
                    "C. On the left side.",
                    "D. In front of the clock tower."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_331_real.mp4"
    },
    {
        "time": "[0:06:00 - 0:07:00]",
        "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:06]: The video showcases a street scene near a large ornate building with a classical architectural style. The building is on the right side of the visuals and features tall windows and intricate stonework. In the center of the frame, there are several large trees with lush green foliage providing shade over an adjacent parking and pedestrian area. Several vehicles are visible, including a white van that starts maneuvering from a stationary position, and a police car parked on the right side. Pedestrians are casually walking or standing near the white van, and other parked vehicles are visible under the trees.  [0:06:07 - 0:06:09]: The white van continues driving from left to right, reaching the pedestrian crossing area marked with white stripes on the road. Another dark green vehicle follows from the left. The pedestrians near the white van begin walking away from the scene. [0:06:10 - 0:06:12]: The white van has now left the scene while other vehicles continue moving in the central area. The dark green vehicle proceeds further from the left part of the frame towards the right along the crosswalk. Another silver car appears and is driving across the crossing. [0:06:13 - 0:06:16]: The silver car continues to move to the right, while additional pedestrians walk near the ornate building on the right side. Left of the screen, a van marked 'ATM' comes into view, moving down towards the crosswalk. [0:06:17 - 0:06:19]: The 'ATM' van continues its path towards the right across the pedestrian crossing. More people are seen walking on the sidewalk adjacent to the building steps. The scene is picturesque with well-maintained surroundings and a clear blue sky visible above all structures and trees. [0:06:20]: The scene encapsulates the final movement of the 'ATM' van as it veers slightly, with pedestrians continuing their activities around the picturesque location.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the color of the vehicle that follows the white van?",
                "time_stamp": "00:06:10",
                "answer": "C",
                "options": [
                    "A. Red.",
                    "B. Black.",
                    "C. Green.",
                    "D. Blue."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_331_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:20]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is located on the right side of the cyclist right now?",
                "time_stamp": "00:00:14",
                "answer": "B",
                "options": [
                    "A. A supermarket entrance.",
                    "B. A row of flags.",
                    "C. A bus stop.",
                    "D. A park."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_172_real.mp4"
    },
    {
        "time": "[0:02:04 - 0:02:24]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What structure is located straight ahead on the right side of the road, visible right now?",
                "time_stamp": "00:02:21",
                "answer": "C",
                "options": [
                    "A. A bridge.",
                    "B. A gym.",
                    "C. A billboard.",
                    "D. A roundabout."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_172_real.mp4"
    },
    {
        "time": "[0:04:08 - 0:04:28]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is located on the right side of the road right now?",
                "time_stamp": "00:04:23",
                "answer": "A",
                "options": [
                    "A. A series of road signs.",
                    "B. A parked car.",
                    "C. A bus stop.",
                    "D. A pedestrian crossing."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_172_real.mp4"
    },
    {
        "time": "[0:06:12 - 0:06:32]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the cyclist right now?",
                "time_stamp": "00:06:14",
                "answer": "C",
                "options": [
                    "A. Turning a corner.",
                    "B. Going uphill.",
                    "C. On a straight road.",
                    "D. About to reach a roundabout."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_172_real.mp4"
    },
    {
        "time": "[0:08:16 - 0:08:36]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the big red truck?",
                "time_stamp": "00:09:20",
                "answer": "A",
                "options": [
                    "A. On the left side of the cyclist.",
                    "B. Behind the cyclist, on the right.",
                    "C. On the right side of the main path.",
                    "D. Ahead, to the left."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_172_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The scene begins on a flat, light-colored surface with a yellow wall in the background, featuring some marks. The top left corner shows a timestamp.  [0:00:01 - 0:00:04]: A hand appears from the right, holding a yellow rectangular block. A \"SUBSCRIBE\" button graphic with a red background is displayed in the top left corner of the screen. The hand places the block on the flat surface. Another logo in grey resembling the subscription notification bell appears next to the text. [0:00:04 - 0:00:07]: The hand adjusts and then releases the block on the surface. The graphics, including the subscribe button and bell, remain on screen as the hand starts to move away.  [0:00:07 - 0:00:09]: The hand is now out of frame, and the graphics disappear, leaving the block centered on the surface.  [0:00:09 - 0:00:13]: The scene remains mostly static until another hand appears from the right, holding a green block. The hand moves towards the already placed yellow block to position the green block on top. [0:00:13 - 0:00:15]: The hand successfully places the green block onto the yellow block and starts to move away. [0:00:15 - 0:00:17]: The hand now holds another green block and approaches the stacked blocks to add the second green block on top.  [0:00:17 - 0:00:20]: The hand firmly adjusts the blocks, ensuring they are stacked securely. The hand slowly moves away and finishes with a thumbs-up gesture.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What happens after the first green block is placed on the yellow block?",
                "time_stamp": "00:00:25",
                "answer": "D",
                "options": [
                    "A. The hand removes the green block.",
                    "B. The hand moves away and does nothing.",
                    "C. The hand removes the yellow block.",
                    "D. The hand adds another green block."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "block_building",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_209_real.mp4"
    },
    {
        "time": "[0:01:00 - 0:02:00]",
        "captions": "[0:01:40 - 0:02:00] [0:01:40 - 0:01:43]: A pair of hands is constructing a structure using brightly colored plastic building blocks on a white surface with a yellow wall in the background. The structure, primarily consisting of yellow and green pieces with some blue blocks, is being adjusted by the hands. [0:01:44 - 0:01:47]: One of the hands picks up and places another yellow block. The blocks appear to be interconnected and stacked on top of each other. [0:01:48 - 0:01:51]: The individual adjusts and adds a green block to the structure, ensuring the pieces fit tightly together. The structure's arrangement slowly becomes more stable and intricate. [0:01:52 - 0:01:55]: Both hands work on pressing down the blocks to ensure they are securely connected. The structure has an even distribution of yellow, green, and blue blocks. [0:01:56 - 0:01:58]: The hands briefly move away from the structure, showcasing the partially built assembly, which stands out prominently against the plain white and yellow background. [0:01:59]: The pair of hands then reappears, holding another blue block, ready to further add to the existing structure.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What action do the hands perform just before moving away from the structure?",
                "time_stamp": "0:01:58",
                "answer": "D",
                "options": [
                    "A. Adding a yellow block.",
                    "B. Adding a blue block.",
                    "C. Adjusting the blue block.",
                    "D. Pressing down the green blocks."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "block_building",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_209_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:21]: A hand is reaching towards a colorful structure made of interlocking plastic blocks on a white table. This structure consists of blocks of various colors including blue, green, yellow, and orange. The hand appears to be adjusting or grabbing the blocks. The background is a plain yellow wall. [0:03:22]: The hand is no longer in the frame, and the structure is unchanged. [0:03:23 - 0:03:24]: The hand reappears to the right side of the frame, holding a green plastic block, and is moving it towards the table. [0:03:25]: The green plastic block is now positioned alone on the table in front of the original structure. The hand is no longer visible. [0:03:26 - 0:03:28]: A hand reappears on the right side of the frame, this time empty. It appears to be in a position as if to make another adjustment. [0:03:29]: The hand is in the process of moving back, seemingly completing a placement or an adjustment. [0:03:30 - 0:03:32]: A new yellow and green block has been placed on the table next to the original single green block. [0:03:33 - 0:03:35]: The hand picks up and arranges the new yellow block next to the existing green block. [0:03:36 - 0:03:37]: With the yellow and green blocks now together, the hand is no longer in the frame. [0:03:38 - 0:03:39]: The hand reappears, reaching towards the blocks on the table, and seems to be gathering additional blocks.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What sequence of actions occured with the green and yellow blocks just now?",
                "time_stamp": "00:03:30",
                "answer": "A",
                "options": [
                    "A. The green block is placed first, followed by the yellow block.",
                    "B. The yellow block is placed first, followed by the green block.",
                    "C. The green block is removed, then the yellow block is placed.",
                    "D. The yellow block is adjusted, then the green block is placed."
                ],
                "required_ability": "episodic memory"
            },
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the new yellow and green block placed relative to the original structure?",
                "time_stamp": "00:03:32",
                "answer": "D",
                "options": [
                    "A. Behind the original structure.",
                    "B. Next to the original structure.",
                    "C. On top of the original structure.",
                    "D. In front of the original structure."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "block_building",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_209_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:01]: The video shows two hands working with interlocking plastic building blocks on a white surface. A stack of blue, yellow, and green blocks is in the background near a yellow wall, and a green block is in the foreground. [0:05:01 - 0:05:04]: The person\u2019s hands move the green block towards the bottom of the frame, and they reach for additional blocks. [0:05:04 - 0:05:07]: The hands grasp another yellow block and place it beside the green block.  [0:05:07 - 0:05:09]: One hand continues to place and adjust yellow blocks while the other hand stabilizes the green block. [0:05:09 - 0:05:11]: The person\u2019s hand secures the yellow blocks into the green block, creating a structure consisting of both yellow and green blocks. [0:05:11 - 0:05:15]: The person's right hand further secures the yellow blocks, pressing down to ensure they are tightly interlocked with the green block. [0:05:15 - 0:05:17]: They continue placing yellow blocks on the green base and adjusting the blocks to ensure alignment. [0:05:17 - 0:05:18]: The right hand makes minor adjustments to the yellow block structure, ensuring all pieces are correctly connected. [0:05:18 - 0:05:19]: The hand then pulls away, and the yellow and green block arrangement is completed, while the pre-constructed set remains in the background.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What did the person do with the yellow block just now?",
                "time_stamp": "0:05:15",
                "answer": "D",
                "options": [
                    "A. Removed them from the structure.",
                    "B. Arranged them on the blue block and adjusted for alignment.",
                    "C. Stacked them on top of the blue blocks.",
                    "D. Placed them above the green block and another yellow block."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "block_building",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_209_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:05]: The video captures a busy city street in the evening, showcasing a bustling atmosphere with various activities occurring simultaneously. The scene is set along a sidewalk where people are walking. The left side of the video frames features a tall building with a large storefront displaying a bright red and white 'Virgin' logo. The right side of the video frames reveals a row of bikes parked at a bike stand and several buildings that extend into the background. [0:00:06 - 0:00:10]: The video highlights the movement of pedestrians along the sidewalk. Some individuals are seen carrying umbrellas, hinting at recent or imminent rain. Streetlights and buildings are illuminated, contributing to the urban evening ambience. Vehicles, including taxis and cars, move along the street, with their headlights adding to the overall lighting of the scene. [0:00:11 - 0:00:15]: As the video progresses, more pedestrians come into view, some of whom are entering and exiting shops. A few people continue to walk with umbrellas open. The street remains busy with moving vehicles, and the background of tall buildings and vibrant lights create a dense metropolitan feel. [0:00:16 - 0:00:20]: The final segment continues to capture the livelihood of the city street. Pedestrians maintain their course on the sidewalk, some stopping momentarily. The buildings' illuminated signs and window displays remain vibrant, and the street stays active with flowing traffic. The overall scene encapsulates the urban life of a city street in the evening.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What are some pedestrians seen doing as they walk along the sidewalk?",
                "time_stamp": "0:00:15",
                "answer": "C",
                "options": [
                    "A. Riding bikes.",
                    "B. Taking pictures.",
                    "C. Carrying umbrellas.",
                    "D. Jogging."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_306_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:23]: The video takes place on a city sidewalk at night, covered with scaffolding. The sidewalk is busy with people walking in both directions. To the left, there's a store with lit windows, while the right side has parked cars, including a black SUV and a food truck adorned with bright lights and a sign reading \"Shanif's Famous.\" [0:02:23 - 0:02:27]: A man wearing a yellow shirt walks toward the camera, while others in dark clothing walk in the opposite direction. The food truck continues to be prominently visible, parked along the street near the curb. [0:02:27 - 0:02:29]: More individuals, including a man in a beige shirt and another in a gray jacket, walk past the camera. The food truck's brightly lit front is also visible. Additional pedestrians are visible further down the sidewalk. [0:02:29 - 0:02:32]: The camera continues down the sidewalk, passing the food truck. Two women in patterned dresses and orange hijabs stand near the food truck. One of them points towards the truck\u2019s brightly lit menu. [0:02:32 - 0:02:34]: As the camera moves past the truck, the focus shifts onto the bustling sidewalk ahead. The area is lit by a combination of street lights and the lights from various shops and vehicles. [0:02:34 - 0:02:37]: The path ahead continues under scaffolding, which is supported by metal beams. People walk toward the camera and in the distance, a mix of shops and illuminated signs are visible. [0:02:37 - 0:02:39]: The camera approaches the end of the scaffolding structure, revealing more pedestrians and street signs. Several vehicles are parked along the street, and the skyline of taller buildings is visible in the background.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the name on the food truck?",
                "time_stamp": "00:02:29",
                "answer": "B",
                "options": [
                    "A. Marco's Delights.",
                    "B. Shanif's Famous.",
                    "C. Tasty Treats.",
                    "D. City Eats."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_306_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:41]: The video shows a bustling city street during twilight, with tall buildings on either side and lots of lit windows. Several vehicles, including taxis, are on the road, and illuminated signs are visible, including one with a large star on it. [0:04:41 - 0:04:43]: As the camera moves forward, the cityscape remains consistently busy, with pedestrians on the sidewalk, streetlights illuminating the area, and people walking with umbrellas. Decorative plants in pots are positioned along the sidewalk. [0:04:43 - 0:04:45]: The video continues along the same street, with a clear view of both the busy vehicular traffic and the lively pedestrian activity. The camera captures another storefront with bright lighting. [0:04:45 - 0:04:47]: The scene remains lively with the continuous movement of traffic and people. A yellow taxi is prominent in the traffic, and the background reveals a mix of old and modern buildings. [0:04:47 - 0:04:49]: The street is crowded with both cars and people. The brightly lit storefront signs and window displays create a vibrant atmosphere. The sidewalk is lined with more potted plants. [0:04:49 - 0:04:51]: The camera still traverses the busy street, with taxis and other vehicles making up the traffic. Street signs and shop banners are visible, contributing to the city's energetic ambiance. [0:04:51 - 0:04:53]: The view continues along the busy street, capturing more vehicles and pedestrians. The large star sign becomes obscured as the camera moves forward. [0:04:53 - 0:04:55]: The footage shows continuous movement, with the street bustling with various activities. Pedestrians are visible near shop entrances, enjoying the evening. [0:04:55 - 0:04:57]: The video captures more of the lively street, including taxis, streetlights, and shop fronts. The pedestrians continue to move along the sidewalk. [0:04:57 - 0:04:58]: The street remains crowded with evening activity, with vehicles and pedestrians maintaining a constant presence in the video. Decorative potted plants line the sidewalk. [0:04:58 - 0:05:00]: More of the bustling evening street scene is visible, with bright lights from shop windows, moving traffic, and passing pedestrians. The camera continues its forward movement along the street.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the general atmosphere of the street right now?",
                "time_stamp": "0:04:54",
                "answer": "B",
                "options": [
                    "A. Calm and quiet.",
                    "B. Busy and vibrant.",
                    "C. Deserted and eerie.",
                    "D. Tense and chaotic."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_306_real.mp4"
    },
    {
        "time": "[0:07:00 - 0:08:00]",
        "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:06]: The video begins on a busy city street during early evening, likely in New York based on recognizable signage. The sky is darkening, and there are numerous buildings, both tall and short, in the distance. In particular, an iconic, tall black skyscraper stands out among the lit-up buildings. The street is bustling with cars, some with headlights on, and numerous pedestrians are on the sidewalks. On the right side of the screen, there is a Macy's department store sign hanging from a large awning. Streetlights and storefronts are illuminated, reflecting off the wet streets. The camera moves forward smoothly, maintaining its focus straight down the street. [0:07:07 - 0:07:10]: As the camera continues moving forward, the perspective shifts slightly to show more of the people walking on the sidewalk. A person in a light gray hoodie and shorts walks ahead, and a variety of advertisements and signs are visible on storefronts. The camera passes by a Sunglass Hut store, featuring large poster advertisements of people wearing sunglasses. More pedestrians come into view, including a woman in a white shirt and cap walking briskly and a person in a black outfit looking at their phone. The environment buzzes with city activity as people move in different directions. [0:07:11 - 0:07:15]: The camera continues past the storefronts. One poster depicts a person in a red outfit with sunglasses. Pedestrians continue walking in both directions, and the scene is now closer to the entrance of a subway station named \"34th Street-Herald Sq Station.\" The entrance is marked with MTA signs indicating the subway lines running through this station. A woman is seen descending the stairs with a suitcase. Another brightly lit advertisement can be seen across from the station entrance. [0:07:16 - 0:07:18]: The camera now fully focuses on the subway entrance. Several people are going down the stairs into the station, including the woman with the suitcase and another person in a blue dress carrying a white tote bag. The station's fluorescent lights illuminate the descending stairs. A Burger King sign and caution sign are visible on the left wall of the entryway, indicating the presence of a fast-food restaurant nearby. The environment transitions from the bright, busy street to a more enclosed and calmer underground setting. [0:07:19 - 0:07:20]: Descending further, the camera captures more of the subway station, showing the tiled walls and steps more clearly. The atmosphere inside the station is lit with fluorescent lighting. As the camera tightly follows the descent, the people ahead continue walking down the stairs, some with bags, moving further into the subway station. The perspective provides a look into the city's underground transit system, blending the dynamic street-level activity with the structured calmness of the subway.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "Which station entrance was shown just now?",
                "time_stamp": "0:07:18",
                "answer": "B",
                "options": [
                    "A. Times Square Station.",
                    "B. 34th Street-Herald Sq Station.",
                    "C. Grand Central Station.",
                    "D. Union Square Station."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_306_real.mp4"
    },
    {
        "time": "[0:08:00 - 0:08:55]",
        "captions": "[0:08:40 - 0:08:55] [0:08:40 - 0:08:54]: A subway train is seen arriving at a station platform at 34th Street. The train, primarily silver with yellow accents, is visible from a first-person perspective standing on the platform. The station is illuminated with multiple fluorescent lights, and passengers can be seen inside the train as it approaches. The yellow tactile paving on the edge marks the boundary for safe standing. Gradually, more of the platform becomes visible, including a sign indicating the station name \"34 St\" and a garbage can with an anti-litter message. At 0:08:54, the rear end of the train is visible as it continues down the track, with its red tail lights illuminated and a 'Q' sign shown.  [0:08:55]: Another subway train, traveling in the opposite direction, is visible on the adjacent track.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the primary color of the subway train?",
                "time_stamp": "00:08:54",
                "answer": "C",
                "options": [
                    "A. Blue.",
                    "B. Green.",
                    "C. Silver.",
                    "D. Red."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_306_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:05]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color is the taxi visible right now?",
                "time_stamp": "00:00:38",
                "answer": "C",
                "options": [
                    "A. Blue.",
                    "B. Green.",
                    "C. Yellow.",
                    "D. Red."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_382_real.mp4"
    },
    {
        "time": "[0:02:05 - 0:02:10]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What type of vehicle is shown in the foreground right now?",
                "time_stamp": "00:02:06",
                "answer": "B",
                "options": [
                    "A. A truck.",
                    "B. A taxi.",
                    "C. A motorcycle.",
                    "D. A bicycle."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_382_real.mp4"
    },
    {
        "time": "[0:04:10 - 0:04:15]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What type of lanes is indicated by the road markings right now?",
                "time_stamp": "00:04:14",
                "answer": "C",
                "options": [
                    "A. Carpool lanes.",
                    "B. HOV lanes.",
                    "C. Bus lanes.",
                    "D. Bike lanes."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_382_real.mp4"
    },
    {
        "time": "[0:06:15 - 0:06:20]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What time is displayed on the clock visible right now?",
                "time_stamp": "00:06:17",
                "answer": "D",
                "options": [
                    "A. 12:50.",
                    "B. 13:10.",
                    "C. 12:00.",
                    "D. 12:25."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_382_real.mp4"
    },
    {
        "time": "[0:08:20 - 0:08:25]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Which brand's store is visible on the right side of the street right now?",
                "time_stamp": "00:08:22",
                "answer": "B",
                "options": [
                    "A. Nike.",
                    "B. GAP.",
                    "C. Adidas.",
                    "D. H&M."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_382_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:20]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video starts with a black screen. [0:00:01 - 0:00:04]: The scene shows a man with glasses and short, dark hair, wearing a black sweater, seated behind a white table. In front of him on the table is a black graphics card with three cooling fans. Behind him is a blue gradient background and two dark shelves on each side, holding items such as books, decorative figures, and a lamp. The man gestures with his hands, picking up and presenting a model of a futuristic object, possibly a component or a gadget, while he speaks directly to the camera. [0:00:05 - 0:00:19]: The video transitions to a black background with animated graphics showcasing various GPU models and specifications. Initially, two graphics cards are displayed: - The RTX 3080 Ti with 10240 CUDA cores, a boost clock of 1665 MHz, and 12GB of memory. - The RTX 3090 with 10496 CUDA cores, a boost clock of 1695 MHz, and 24GB of memory. After a few seconds, these graphics cards are replaced by newer models: - The RTX 4090 with 16384 CUDA cores, a boost clock of 2520 MHz, and 24GB of memory. - Another GPU, possibly a professional workstation model, with 18176 CUDA cores, a boost clock of 2505 MHz, and 48GB of memory. Prices for these GPUs are also shown, with the RTX 4090 listed at 15000 yuan and the professional model at 60000 yuan.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is the boost clock speed of the RTX 4090 right now?",
                "time_stamp": "00:00:20",
                "answer": "C",
                "options": [
                    "A. 1665 MHz.",
                    "B. 1695 MHz.",
                    "C. 2520 MHz.",
                    "D. 2505 MHz."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_106_real.mp4"
    },
    {
        "time": "[0:03:20 - 0:03:40]",
        "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:21]: The video begins with a comparison chart showing performance metrics for different graphics cards, specifically RTX 4080, RTX 4070 Ti SUPER, RTX 4070 Ti, and RTX 4070, in both 2K and 4K resolutions. The document displays two sections: \"3DMark Time Spy\" and \"3DMark Time Spy Extreme.\" [0:03:22 - 0:03:26]: The scene transitions to a person in a studio setting discussing the performance comparison. The person is seated at a white table with a large graphics card beside them. The background is blue, with shelves housing various electronic devices and decorations. [0:03:27]: The video switches to a first-person perspective in a video game. The on-screen information shows the frames per second (FPS), GPU details, and other performance stats. The player is navigating a narrow, city-like alley, with a red \"B\" painted on one of the walls. [0:03:28]: The player continues moving through the alleys, observing surroundings with detailed textures on the buildings. [0:03:29]: The player navigates towards an open area with a yellow building on the left and red building in the distance. A chicken is seen on the ground, adding a lively touch to the environment. [0:03:30]: The player advances deeper into the area, closely observing a yellow building with weathered textures on the right. The player's hand holding a weapon is visible. [0:03:31]: The player walks past a stone archway, with graffiti on walls and potted plants around, enhancing the urban setting. [0:03:32]: The perspective shifts to the player holding a pistol in a different alley. The ambient lighting and detailed architecture are evident. [0:03:33 - 0:03:34]: The player moves through the narrow alley, taking cover behind walls and cautious of potential threats. [0:03:35 - 0:03:36]: Smoke grenades create a smoky environment, reducing visibility. The player cautiously proceeds, keeping the pistol ready. [0:03:37 - 0:03:40]: The player engages in a confrontation within the smoke, with increased intensity and actions. The visibility is low due to the smoke cloud, adding tension to the gameplay.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Which performance metrics are being shown right now?",
                "time_stamp": "00:03:40",
                "answer": "C",
                "options": [
                    "A. CPU benchmarks for various processors.",
                    "B. FPS results for different games.",
                    "C. CPU clock, CPU Usage, VRAM Usage, GPU Power, GPU Temp, and FPS for 4070TiS.",
                    "D. RAM speed metrics for different memory types."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_106_real.mp4"
    },
    {
        "time": "[0:06:40 - 0:07:00]",
        "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:44]: The scene depicts two individuals wearing dark jackets with large yellow \"FBI\" lettering on the back. They are walking briskly down a wet street. Various statistics regarding GPU and VRAM usage are displayed at the top of the frames. The background shows buildings with signs and some autumn trees. [0:06:44 - 0:06:52]: The scene shifts to a graphical performance comparison. The comparisons are for different graphics cards (RTX 4080, RTX 4070 Ti SUPER, etc.) playing a specific game (\u5fc3\u7075\u6740\u624b2). The bar graphs illustrate performance differences in frames per second (fps) at various graphics settings. [0:06:53 - 0:06:59]: The performance comparison continues with details on average fps and specific settings. More detailed breakdowns are shown, comparing the capability of each graphics card across different resolutions and settings. The data remains consistent, providing comprehensive information on graphical performance for each card displayed.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Which graphics card is labeled in green in the performance comparison right now?",
                "time_stamp": "00:06:59",
                "answer": "C",
                "options": [
                    "A. RTX 3060 Ti.",
                    "B. RTX 4060 Ti.",
                    "C. RTX 4070 Ti SUPER.",
                    "D. RTX 2070."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_106_real.mp4"
    },
    {
        "time": "[0:10:00 - 0:10:20]",
        "captions": "[0:10:00 - 0:10:20] [0:10:00 - 0:10:01]: The video begins with a first-person perspective interface of a computer screen displaying an image editing software. On the right side, a photo of a person wearing a white bear hat, blue scarf, and beige coat pointing upwards is shown on a bright blue background. Various editing tools and options are visible on the left side of the screen. [0:10:02]: The scene transitions to a darker setting, where a person is seated at a workstation in a dimly lit room, facing a collection of colorful screens. They appear to be working on a computer. [0:10:03 - 0:10:20]: The video now displays a static screen with a bar graph titled \"Stable Diffusion AI \u56fe\u6d4b\u8bd5\". The graph compares the performance of different GPUs (RTX 4070, RTX 4070 Ti, RTX 4080) in terms of \"SD1.5(512x512)\" and \"SDXL(512x512)\" and \"SDXL(1024x1024)\" measured by \"TensorRT\". Numbers on the y-axis represent values (14, 15, 28, 32, 120, 112). This screen remains consistent with changes in some details, such as highlighting memory configurations (12GB for the RTX 4070 and RX 4070 Ti, 16GB for the RTX 4080).",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Which GPU shows the best performance for \"SDXL(512x512)\" right now?",
                "time_stamp": "00:10:20",
                "answer": "C",
                "options": [
                    "A. RTX 4070.",
                    "B. RTX 4070 Ti.",
                    "C. RTX 4080.",
                    "D. SD1.5(512x512)."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_106_real.mp4"
    },
    {
        "time": "[0:12:40 - 0:12:53]",
        "captions": "[0:12:40 - 0:12:53] [0:12:40 - 0:12:45]: A man is seated behind a white table, facing the camera with a neutral expression. He is wearing glasses and a black shirt with an electronic component illustration on it. To his left, a large graphics card with three cooling fans is standing upright on the table. The background is a solid gradient of blue, with shelves on either side holding various objects, including gadgets and potted plants. As he speaks, icons representing \"like,\" \"comment,\" \"favorite,\" and \"share\" appear around him and the graphics card, indicating an interaction with the video content or a demonstration of features. [0:12:45 - 0:12:49]: A smartphone user interface overlay appears on the left side of the frame, showing an online shopping application. The display showcases various products, including clothing and electronics. The man continues to talk, occasionally gesturing towards the interface while maintaining eye contact with the camera. The interactive icons remain visible around him. [0:12:50 - 0:12:53]: The scene transitions to a black background filled with small white stars, giving a space-themed effect. A graphic logo with an electronic device icon appears on the left along with Chinese characters, likely representing the brand or video series title. The logo and text pulsate slightly with a glowing effect, set against the starry background.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What emojis are currently displayed around the man and the graphics card?",
                "time_stamp": "00:12:43",
                "answer": "C",
                "options": [
                    "A. \"like,\" \"comment,\" \"follow,\" and \"share\".",
                    "B. \"like,\" \"save,\" \"comment,\" and \"share\".",
                    "C. \"like,\" \"insert coins,\" \"favorite,\" and \"share\".",
                    "D. \"like,\" \"comment,\" \"subscribe,\" and \"share\"."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_106_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video starts inside a walk-in cold storage room with shelves on either side filled with dairy products. On the left, various milk cartons and beverage containers are arranged on metal shelves. On the right, there are boxes and crates stacked, holding more dairy items. The floor is metallic and appears clean. In the center, there is a trolley with crates of stacked yogurt cups. The lighting is bright, reflecting off the metallic surfaces, creating a well-lit environment. [0:00:04 - 0:00:06]: The camera moves closer to the stack of yogurt cups on the trolley, and the left shelving unit full of dairy products becomes more apparent. The focus is on these yogurt cups which are white with blue labels. Some of the products from the bottom shelves of the left-side shelving can also be seen more clearly, including milk cartons and various other dairy items. [0:00:06 - 0:00:17]: A hand, wearing black gloves, picks up a tray of yogurt cups from the top of the stack on the trolley. The person then moves towards the left shelving unit, reaching for an empty spot on the shelf. The hand places one of the yogurt cups on the shelf, next to a column of other similarly-stacked dairy products. The hand continually places more yogurt cups neatly in a row. The camera angle shows the upper shelves which hold additional dairy items like bottled beverages and yogurt packs. The items in the background include various cartons, bottles, and packages of dairy products, maintaining a consistent theme. [0:00:17 - 0:00:19]: The camera angle shifts slightly, showing more of the surrounding environment. The section on the left contains more milk cartons and other dairy goods. The person continues to place yogurt cups on the shelf, close to other similar products and directly beside more columns of stacked yogurt cups. [0:00:19 - 0:00:20]: The video concludes with a wider view of the left shelving unit filled with a variety of dairy products like different sizes of milk cartons and bottled beverages. On the right are stacks of crates and boxes, possibly for restocking the shelves. The action focuses on the final placement of yogurt cups, ensuring that the products are aligned and positioned correctly on the shelf.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What distinguishing feature is described about the yogurt cups?",
                "time_stamp": "0:00:06",
                "answer": "C",
                "options": [
                    "A. They have a green label.",
                    "B. They are stacked in red trays.",
                    "C. They are white with blue labels.",
                    "D. They are placed in a metal container."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Action Recognition",
                "question": "What action does the person perform with the yogurt cups?",
                "time_stamp": "0:00:17",
                "answer": "D",
                "options": [
                    "A. The person sorts the cups by flavor.",
                    "B. The person moves the yogurt cups to the right shelving unit.",
                    "C. The person stacks the cups in a corner.",
                    "D. The person places the yogurt cups neatly on the shelf."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "storage_manage",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_437_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:41]: The scene begins in a store aisle where a person wearing black gloves is holding a carton of milk and positioning it on an empty shelf among similarly shaped cartons. The setting includes shelves lined with various milk cartons, predominantly white with some green accents;  [0:02:42 - 0:02:43]: The individual's hand retrieves another milk carton from a nearby metal cart that is partially filled. There is a sense of repetitive action as multiple cartons are being restocked on the shelf;  [0:02:44 - 0:02:46]: Additional cartons are taken from the cart and methodically placed on the shelf. The individual continues to place the milk cartons on the empty shelf while visibly arranging them to align with the existing stock;  [0:02:47 - 0:02:49]: A broader view of the cart loaded with milk cartons is shown, positioned adjacent to the shelves. The person\u2019s movements indicate a systematic approach to restocking the shelf;  [0:02:50 - 0:02:51]: Further cartons are picked from the shelf by the individual. The majority of the cartons show uniform packaging, and their consistent positioning emphasizes the methodical stocking process;  [0:02:52 - 0:02:55]: The person continues to arrange the milk cartons on the lower shelf. The camera captures gloved hands rearranging and placing the cartons carefully;  [0:02:56 - 0:02:59]: More cartons are picked from the cart and placed onto the shelf, maintaining the organized layout. The individual continues to fill the gaps on the shelf with the cartons from the metal cart. \u041fhroughout the frames, the organized process of restocking becomes more evident, showing the individual aligning and adjusting the cartons to fit neatly into the space.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the person holding right now?",
                "time_stamp": "0:03:00",
                "answer": "B",
                "options": [
                    "A. A bottle of juice.",
                    "B. A carton of milk.",
                    "C. A box of cereal.",
                    "D. A can of soda."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "storage_manage",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_437_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:23]: In a storeroom with multiple metal shelves, the person wearing black gloves and a dark jacket places a tray of yogurt cups on the top shelf. The shelves hold various boxed and packaged items. [0:05:24 - 0:05:25]: The person continues arranging yogurt cups from the tray onto the shelf. More packaged goods are visible on the lower shelves. [0:05:26]: The tray of yogurt cups is now fully placed on the shelf. Boxes and other containers are neatly stacked around them. [0:05:27 - 0:05:29]: The person proceeds to the next tray of yogurt cups positioned on a wheeled cart. They lift the tray and begin arranging it on the shelf. [0:05:30 - 0:05:31]: The view shifts, showing the aisle with the wheeled cart full of yogurt trays. Shelves stocked with various dairy products and other groceries line both sides of the aisle. [0:05:32 - 0:05:33]: The perspective focuses on the cart and the surrounding stock of groceries. The scene is orderly, with different dairy products neatly arranged on the shelves. [0:05:34 - 0:05:35]: The person places another tray of yogurt cups from the cart onto the shelf, performing the same organizing action. [0:05:36 - 0:05:37]: The view once more shows the aisle with dairy products stocked neatly on both sides. The cart, now partially emptier, still has several trays of yogurt cups. [0:05:38 - 0:05:39]: The person completes positioning the last yogurt cups on the shelf, ensuring the items are organized. The storage area remains tidy, with goods systematically placed on the metal shelves.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What did the person pick up on the top shelf just now?",
                "time_stamp": "0:05:27",
                "answer": "C",
                "options": [
                    "A. Boxes of cereal.",
                    "B. Cans of soup.",
                    "C. Yogurt cups.",
                    "D. Bottled water."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "storage_manage",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_437_real.mp4"
    },
    {
        "time": "[0:10:00 - 0:10:45]",
        "captions": "[0:10:40 - 0:10:45] [0:10:40 - 0:10:45]: The video shows a first-person perspective of someone stocking shelves in a refrigerated section of a grocery store. The shelves are full of various dairy products, including several stacks of round yogurt containers. The person's right hand is wearing a dark glove and a jacket. The hand reaches out to pick up and arrange yogurt containers, lifting them from a lower shelf and placing them onto an upper shelf. The refrigeration unit's lights provide a bright, even illumination over the products. The scene remains consistent with the person methodically organizing the products, ensuring they are stacked neatly and labeled correctly.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What action is the person performing right now?",
                "time_stamp": "0:10:45",
                "answer": "C",
                "options": [
                    "A. Cleaning the shelves.",
                    "B. Taking inventory.",
                    "C. Organizing yogurt containers.",
                    "D. Restocking fresh vegetables."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "storage_manage",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_437_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:20]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where are the trees located right now?",
                "time_stamp": "00:00:20",
                "answer": "A",
                "options": [
                    "A. On the both sides of the road.",
                    "B. On the left side of the road.",
                    "C. In the middle of the road.",
                    "D. On the right side of the road."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_177_real.mp4"
    },
    {
        "time": "[0:01:59 - 0:02:19]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is located on the right side of the path right now?",
                "time_stamp": "00:02:19",
                "answer": "A",
                "options": [
                    "A. An open field.",
                    "B. A dense forest.",
                    "C. A farmhouse.",
                    "D. A shopping plaza."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_177_real.mp4"
    },
    {
        "time": "[0:03:58 - 0:04:18]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the house with the brown roof located right now?",
                "time_stamp": "00:04:09",
                "answer": "A",
                "options": [
                    "A. On the right side of the road.",
                    "B. On the left side of the road.",
                    "C. Directly behind the cyclist.",
                    "D. In the middle of the forest."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_177_real.mp4"
    },
    {
        "time": "[0:05:57 - 0:06:17]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where are the trees located right now?",
                "time_stamp": "00:06:16",
                "answer": "A",
                "options": [
                    "A. On the both sides of the road.",
                    "B. On the left side of the road.",
                    "C. In the middle of the road.",
                    "D. On the right side of the road."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_177_real.mp4"
    },
    {
        "time": "[0:07:56 - 0:08:16]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is the black car parked right now?",
                "time_stamp": "00:07:57",
                "answer": "A",
                "options": [
                    "A. On the left side of the road.",
                    "B. On the both sides of the road.",
                    "C. On the right side of the road.",
                    "D. No black car."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_177_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:20]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video begins with an entirely black frame. [0:00:01 - 0:00:05]: A person is seated at a white table in a studio setup with a blue background, black shelves, and various decorative items. On the table in front of them are two large graphics cards. The person is looking into the camera and occasionally at the cards, gesturing with their hands. [0:00:06 - 0:00:08]: The person continues speaking, making more hand gestures while looking at the graphics cards periodically. The light and decor on the shelves remain the same, providing a consistent background. [0:00:09]: The person appears to be smiling while still discussing the content. [0:00:10 - 0:00:12]: The focus zooms slightly closer. The person uses more expressive gestures with their hands, likely emphasizing certain points about the graphics cards. [0:00:13 - 0:00:15]: A graphic overlay appears on the screen, showing the text \u201c9499\u5143\u201d along with a label indicating \"RTX 4080\". The person continues talking energetically, possibly explaining the price or features. [0:00:16]: Another graphic overlay appears, this time indicating \"RTX 3080\" with the price \u201c5499\u5143\u201d. The individual maintains an engaging demeanor, looking directly at the camera. [0:00:17 - 0:00:19]: The person speaks in a conclusive tone, their hand movements suggesting a wrap-up of the explanation. The shelves and background decor remain unchanged, providing a consistent setting. [0:00:20]: The person looks slightly off-camera, transitioning into a more relaxed pose, possibly indicating the end of the video segment.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is the price of the RTX 4080 shown right now?",
                "time_stamp": "00:00:15",
                "answer": "B",
                "options": [
                    "A. 6499\u5143.",
                    "B. 9499\u5143.",
                    "C. 7499\u5143.",
                    "D. 5499\u5143."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_108_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:04:20]",
        "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:06]: The video begins by showing a benchmark comparison for 3DMark Time Spy Extreme scores across three different graphics cards: RTX 4090, RTX 4080, and RTX 3090 Ti. The scores are displayed as horizontal bar graphs with the RTX 4090 achieving the highest score, followed by the RTX 4080, and then the RTX 3090 Ti. A percentage difference in score between the RTX 4080 and RTX 4090 is also indicated.   [0:04:07 - 0:04:09]: The scene changes to a first-person perspective where a person is seated behind a table with various items on it. The person is gesturing and appears to be explaining something about the items in front of him, which are two large graphics cards, each with multiple fans. The background is a neatly organized room with shelves holding tech gadgets and decor. [0:04:10 - 0:04:17]: The discussion continues, with the person consistently making gestures to emphasize points about the two graphics cards in front of them. The individual's facial expressions and actions suggest a detailed explanation is being provided, possibly comparing the features, performance, or other aspects of the graphics cards. [0:04:18 - 0:04:19]: The focus shifts back to a benchmark comparison, this time for 3DMark Port Royal scores, again comparing the three graphics cards: RTX 4090, RTX 4080, and RTX 3090 Ti. As before, the RTX 4090 leads, followed by the RTX 4080 and the RTX 3090 Ti, with numerical values displayed to highlight the differences.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is highlighted as the top-performing graphics card right now according to the 3DMark Time Spy Extreme scores?",
                "time_stamp": "00:04:20",
                "answer": "B",
                "options": [
                    "A. RTX 3070.",
                    "B. RTX 4090.",
                    "C. RTX 4080.",
                    "D. RTX 3090 Ti."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_108_real.mp4"
    },
    {
        "time": "[0:08:00 - 0:08:20]",
        "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:04]: The video displays a benchmarking comparison between different GPUs: RTX 4090, RTX 4080, and RTX 3090 Ti. On the left side, a bar chart shows the average and 1% low FPS scores for each GPU at a resolution of 2560x1440. RTX 4090 shows an average FPS of 129, RTX 4080 has 118, and RTX 3090 Ti displays 119. On the right side, real-time statistics for clock, power, temperature, and VRAM usage of these GPUs are shown in separate columns. [0:08:05 - 0:08:09]: The chart on the left side changes to display new information for the resolution of 3840x2160. The data shows that the RTX 4090 has an average FPS of 124, RTX 4080 has 100, and RTX 3090 Ti shows 82. Real-time statistics of clock, power, temperature, and VRAM usage are still visible on the right side, revealing fluctuations among the GPUs. [0:08:10 - 0:08:14]: The video transitions to another benchmarking comparison featuring the game \"Cyberpunk 2077\". On the left side, a bar chart indicates performance metrics with the game at 2560x1440 resolution for RTX 4090, RTX 4080, and RTX 3090 Ti GPUs. RTX 4090 showcases an FPS of 147, RTX 4080 shows 131 FPS, and RTX 3090 Ti has 98 FPS. Real-time statistics for each GPU remain visible on the right side. [0:08:15 - 0:08:20]: The chart on the left continues to display performance metrics, with notable highlight boxes around the RTX 4080's FPS scores. Real-time statistics on the right continually update showing clock speeds, power consumption, temperatures, and VRAM usage for each GPU. The video consistently maintains comparative benchmarking information between these GPUs.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is the average FPS score of the RTX 4080 at a resolution of 3840x2160 right now?",
                "time_stamp": "00:08:09",
                "answer": "C",
                "options": [
                    "A. 124.",
                    "B. 118.",
                    "C. 100.",
                    "D. 82."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_108_real.mp4"
    },
    {
        "time": "[0:12:00 - 0:12:20]",
        "captions": "[0:12:00 - 0:12:20] [0:12:00 - 0:12:07]: The video starts with a graph showing a performance comparison of RTX 4090, RTX 4080, and RTX 3090 Ti graphics cards. The graph has a black background with the title in white, and the performance bars are in green and white. The RTX 4090 is marked at 166%, the RTX 4080 at 121%, and the RTX 3090 Ti at 100%. [0:12:07 - 0:12:10]: The prices of the graphics cards are displayed on the graph. RTX 4090 is priced at 12999 yuan, RTX 4080 at 9499 yuan, and RTX 3090 Ti does not have a price listed. [0:12:10]: The graph maintains the same details as previously captured, emphasizing performance and price comparison of the three graphics cards. [0:12:11]: The text in the graph remains unchanged, continuing to show the comparison between RTX 4090, 4080, and 3090 Ti in terms of percentage performance and price in yuan. [0:12:12]: A person appears on screen, seated behind a white table with two graphics cards placed in front of him. He has glasses and wears a black shirt. There are two boxes with prices on either side of him\u2014displaying prices of RTX 3080, 3090, 4080, and 4090. [0:12:13 - 0:12:16]: The person starts explaining the details of the graphics cards, gesturing with his hands. The RTX 3080 is priced at 5499 yuan, RTX 3090 at 11999 yuan, RTX 4080 at 9499 yuan, and RTX 4090 at 12999 yuan. [0:12:17]: The person continues talking and gesturing, with the graphics cards and their prices still prominently displayed on the screen beside him. [0:12:18]: The person adjusts his glasses and continues explaining the differences, occasionally pointing towards the cards on the table and the prices on the screen. [0:12:19 - 0:12:20]: The person holds one of the graphics cards in his hand while continuing to speak, explaining its features and benefits. The prices of the graphics cards are still displayed prominently on the screen.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is the marked performance percentage of the RTX 4090 right now?",
                "time_stamp": "0:12:01",
                "answer": "D",
                "options": [
                    "A. 100%.",
                    "B. 121%.",
                    "C. 129%.",
                    "D. 166%."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is the price of the RTX 4080 graphics card in yuan?",
                "time_stamp": "0:12:15",
                "answer": "B",
                "options": [
                    "A. 5499 yuan.",
                    "B. 9499 yuan.",
                    "C. 11999 yuan.",
                    "D. 12999 yuan."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "evaluation_of_electronic_products",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_108_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:30]",
        "questions": [
            {
                "task_type": "Prospective Reasoning",
                "question": "What might the speaker explain next?",
                "time_stamp": "00:00:20",
                "answer": "A",
                "options": [
                    "A. How to add mixed numbers.",
                    "B. The subtraction of mixed numbers.",
                    "C. The multiplication of fractions.",
                    "D. The properties of whole numbers."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "math tutorials",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_212_real.mp4"
    },
    {
        "time": "[0:01:52 - 0:02:22]",
        "questions": [
            {
                "task_type": "Prospective Reasoning",
                "question": "What might the speaker explain next?",
                "time_stamp": "00:02:10",
                "answer": "A",
                "options": [
                    "A. How to add those two mixed numbers.",
                    "B. The process of converting mixed numbers to improper fractions.",
                    "C. Subtracting mixed numbers.",
                    "D. The definition of mixed numbers."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "math tutorials",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_212_real.mp4"
    },
    {
        "time": "[0:03:44 - 0:04:14]",
        "questions": [
            {
                "task_type": "Prospective Reasoning",
                "question": "What might the speaker do next?",
                "time_stamp": "00:04:08",
                "answer": "A",
                "options": [
                    "A. How to add those two mixed numbers.",
                    "B. Subtract the fractions.",
                    "C. Multiply the fractions.",
                    "D. Divide the fractions."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "math tutorials",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_212_real.mp4"
    }
]