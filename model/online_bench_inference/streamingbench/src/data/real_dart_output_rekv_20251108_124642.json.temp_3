[
    {
        "time": "0:00:00 - 0:00:20",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:11]: A scene unfolds in what appears to be a gun shop with two people conversing in the center. The space is well-lit by ceiling lights, highlighting the extensive array of firearms mounted on the walls behind them. The person on the left wears a high-visibility vest, dark shirt, and headphones, with short blonde hair. The individual on the right, who has dark hair and sunglasses, is engaged in conversation. Various display cases filled with accessories are present, including one between the two individuals. The background includes framed certificates and signs, notably a prominent sign saying \"LOS SANTOS GUN CLUB\". [0:00:12 - 0:00:20]: The perspective shifts slightly while the two individuals continue their discussion. The person on the left occasionally looks towards the camera, as if addressing it or responding to prompts. The view then transitions to a broader perspective of the room. An American flag, additional shelves, and a vending machine come into view, with the final frames capturing a door with a sign labeled \"THANKS FOR EXIT\". The overall atmosphere remains the same, focused on the interaction between the two people in the gun shop setting.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the both persons doing right now?",
                "time_stamp": "00:00:20",
                "answer": "B",
                "options": [
                    "A. Looking at the display cases.",
                    "B. Having a face-to-face conversation while standing.",
                    "C. Walking towards the exit.",
                    "D. Talking on the phone."
                ],
                "required_ability": "working memory",
                "rekv": " B"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_293_real.mp4"
    },
    {
        "time": "0:03:20 - 0:03:40",
        "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:23]: The video begins with a first-person perspective of driving a blue car on a road at night. City lights illuminate the street, and there are markings and signs along the road. The car is heading straight with a clear view of an intersection ahead. [0:03:24 - 0:03:27]: As the car moves forward, the perspective shows the road more clearly lined with red and yellow street markings. The driver approaches the stop sign, and the surroundings include streetlights and buildings. [0:03:28 - 0:03:36]: The vehicle turns to the right, driving past a small green grass section with a red border and a palm tree. The road seems mostly empty except for some parked cars and a few buildings with lighted windows in the background. [0:03:37 - 0:03:40]: The car continues along the road, showing more items to its side like a signpost and streetlights. The vehicle aligns towards a minor intersection with clearer visibility of nearby buildings and parked vehicles.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the car doing right now?",
                "time_stamp": "00:03:30",
                "answer": "C",
                "options": [
                    "A. Driving straight through a major intersection.",
                    "B. Turning left onto a side road.",
                    "C. Keep going straight on the road.",
                    "D. Stopping at a stop sign."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_293_real.mp4"
    },
    {
        "time": "0:06:40 - 0:07:00",
        "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:42]: A person in a reflective vest stands next to a small blue car parked on the side of a street. The background features palm trees and a house with pink neon lights along the roofline. [0:06:43 - 0:06:44]: The person opens the car door and begins to get inside the blue car. The street is well-lit with artificial lights, and there are various neon lights visible in the distance. [0:06:45 - 0:06:47]: The person is now fully inside the blue car, which is still parked on the side of the street. The neon light decorations of nearby houses are visible. [0:06:48]: The blue car\u2019s brake lights are illuminated. The car starts to move along the street with a person visible inside, driving. [0:06:49]: The blue car continues down the street. Another person is seen walking towards the vehicle on the passenger side. [0:06:50]: The walker raises their hand, likely to signal or communicate with the driver. [0:06:51 - 0:06:52]: The walker approaches the car and opens the passenger door while the blue car is still on the street. [0:06:53 - 0:06:54]: The person gets into the car, and the vehicle is stationary on the road. [0:06:55]: The car door closes, and the blue car appears ready to resume moving. [0:06:56 - 0:06:58]: The car starts moving forward down the street, passing houses decorated with lights. [0:06:59 - 0:07:01]: As the car progresses along the street, it passes several decorated houses on both sides of the road. [0:07:02]: The car continues to drive forward, reaching an intersection. [0:07:03 - 0:07:04]: The blue car slows down as it approaches a stop sign at the intersection. [0:07:05]: The car comes to a complete stop at the intersection before turning left. [0:07:06]: The blue car makes a left turn at the intersection onto a cross street. [0:07:07 - 0:07:08]: The car continues straight down the new street, which is lined with more houses and some parked vehicles. [0:07:09 - 0:07:10]: The car proceeds down the road, driving past additional houses and streetlights. [0:07:11]: The blue car drives straight, moving past another intersection with no traffic. [0:07:12 - 0:07:14]: The car continues down a dimly lit road in a residential area, with various street and house lights illuminating the path. [0:07:15]: The blue car keeps driving straight, maintaining a consistent speed through the residential neighborhood.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What did the walker do just now?",
                "time_stamp": "00:06:54",
                "answer": "C",
                "options": [
                    "A. Adjusted the car's mirror.",
                    "B. Walked past the blue car.",
                    "C. Opened the passenger door and entered the car.",
                    "D. Signaled another car to stop."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_293_real.mp4"
    },
    {
        "time": "0:10:00 - 0:10:20",
        "captions": "[0:10:00 - 0:10:20] [0:10:00 - 0:10:04]: The scene is set at night in an industrial area with several people standing and talking. The video is from a first-person perspective showing a group of three individuals. The person in the foreground has a reflective vest on. The background features parked cars, trailers, and lit buildings.  [0:10:05 - 0:10:08]: The focus remains on the group of three. One person, dressed in bright pink attire, looks downwards while the others seem engaged in conversation. A blue car is parked nearby.  [0:10:09 - 0:10:11]: The person in the reflective vest continues to face the group. The person in black and white attire is the center of attention. They are wearing a hood and sunglasses. [0:10:12 - 0:10:14]: The person in the black and white attire makes a gesture, possibly continuing the conversation, and moves slightly. The person in pink stands still. The background reveals more of the parked cars. [0:10:15 - 0:10:16]: The scene remains largely unchanged with the focus on the conversation amongst the group. There is movement from the person in black and white, suggesting interaction or animated discussion. [0:10:17 - 0:10:18]: The chat on the stream updates rapidly while the group continues their interaction. The background shows the industrial setting, including machinery and storage units. [0:10:19 - 0:10:20]: The scenes transition smoothly with continued dialogue among the individuals. The person in pink is focused on the conversation, and the video provides a coherent view of the industrial night setting.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the person in the pink attire doing right now?",
                "time_stamp": "00:10:20",
                "answer": "D",
                "options": [
                    "A. Looking downwards.",
                    "B. Making a gesture.",
                    "C. Moving slightly.",
                    "D. Standing still."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_293_real.mp4"
    },
    {
        "time": "0:13:20 - 0:13:23",
        "captions": "[0:13:20 - 0:13:23] [0:13:20]: A person is walking towards another individual standing against a brick wall in a dark setting. The scene includes a backdrop featuring a large brick wall with a building's corner visible; [0:13:21]: The individual nears the person standing against the brick wall, who appears to be engaged in conversation. Interactive icons are displayed above the head of the person against the wall; [0:13:22]: The camera is oriented towards the standing person, and an interaction menu titled \"Frank Miller\" appears, offering options for further engagement. The scene remains dark with the brick wall as the main background.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the person doing right now?",
                "time_stamp": "00:13:23",
                "answer": "B",
                "options": [
                    "A. Walking away from the brick wall.",
                    "B. Engaging with the interaction menu.",
                    "C. Standing and observing the environment.",
                    "D. Adjusting their position against the wall."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_293_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:04]: The video begins with a first-person perspective in a kitchen setting. A man standing in front of a white brick wall is speaking. The wall has two shelves: the upper shelf holds green and clear jars on the right side; the lower shelf holds plates, bowls, and cups, neatly arranged. The man wears a green shirt and appears engaged in conversation. [0:00:05 - 0:00:09]: The man maintains eye contact with the camera, alternating between holding his hands together and using expressive hand gestures to emphasize his points. Various kitchen tools, including knives and cutting boards, are seen on the right side of the background. [0:00:10 - 0:00:13]: The man continues talking, occasionally looking thoughtful. His expressions convey that the topic is interesting and requiring explanation. The camera angle remains steady, keeping the focus on him and the neatly arranged kitchen background. [0:00:14]: The scene shifts to a close-up of the number \"10\" and a stylized depiction of a kitchen utensil, slightly tilted to the right. The overall tone is sleek and modern. [0:00:15 - 0:00:17]: The next frames feature a man wearing a darker shirt than initially, standing with a broad smile. Behind him, the words \"RAMSAY in 10\" are prominently displayed in blue against a white and gray gradient background. The transition suggests a shift in focus from detailed discussion to a more formal introduction. [0:00:18]: A graphic slides across the screen, partially covering the kitchen from earlier. The word \"LOOK\" made of large red letters is visible on a high shelf. [0:00:19]: The final scene returns to the man, now in a gray shirt, continuing his conversation. The background remains consistent with the earlier frames, featuring the same organized kitchen setting. The camera captures the man's final gestures and expressions.\n[0:00:20 - 0:00:40] [0:00:20 - 0:00:23]: A man in a dark gray shirt stands in front of a kitchen counter with a wide stance, his arms outstretched. The background shows a white brick wall with wooden shelves holding various kitchen items, including dishes, glassware, and utensils. On the top shelf on the left, large red letters spell out \"COOK\". The man then lowers his arms and begins to speak, his facial expression serious and engaged. [0:00:24 - 0:00:28]: The man turns his body slightly to the left while continuing to speak, his attention directed toward something off-camera. His movements include subtle hand gestures emphasizing his speech. The shelves behind him contain jars of ingredients and kitchen tools, arranged neatly. [0:00:29 - 0:00:33]: The man returns to face the camera directly, bringing his hands together in a pleading gesture while continuing his dialogue. His expression is earnest, and the background remains consistent with the white brick wall and organized kitchen shelves. [0:00:34 - 0:00:36]: Shifting his body again to his left, the man extends his right arm forward, his hand open as if making a point. He continues to speak animatedly, conveying a sense of urgency or importance. [0:00:37 - 0:00:39]: The man faces the camera again, bringing both hands close to his body as he continues speaking. His facial expression intensifies, underscoring his message. The kitchen in the background remains a consistent element, providing a well-organized and clean setting.\n[0:00:40 - 0:01:00] [0:00:40 - 0:00:42]: In a kitchen environment, a person wearing a dark grey shirt holds a white plate with a piece of raw steak. The background reveals white bricks, kitchen shelves with various dishes and utensils, and a countertop with different cooking items. [0:00:43 - 0:00:45]: The person continues to hold the steak on the plate, showing its features in more detail. The steak is marbled and red. The kitchen backsplash, cupboards, and some kitchen utensils remain visible in the background. [0:00:46 - 0:00:50]: The person rotates the steak slightly, revealing different angles and features. The individual also seems to be explaining or discussing the steak, using their other hand to gesture or point at specific parts of the steak. A shelf in the background has various colorful items, including bottles and boxes. [0:00:51 - 0:00:55]: The person keeps the steak prominent, holding it near the center of the frame while continuing to make hand gestures. The dark shirt contrasts with the white plate and red steak, emphasizing the visual features of the steak. The utensils and other kitchen items are consistently visible in the background. [0:00:56]: The person looks down at the steak while holding the plate closer to the counter. The background continues to show the white brick wall and kitchen shelving. [0:00:57 - 0:00:58]: The person extends their right arm towards the counter, pointing at several limes and other kitchen items. The focus shifts slightly towards the counter\u2019s various items, including bowls and a grater. The person\u2019s left hand still holds the plate with the steak. [0:00:59]: The focus shifts to a bowl of rice as the person pointedly gestures towards it with a fork. There are several other items around the bowl, including a jar of spices, pieces of vegetables, and a mortar and pestle. The background remains a bit blurred, keeping the attention on the countertop items.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color shirt is the man wearing when he holds the white plate with a piece of raw steak?",
                "time_stamp": "00:00:57",
                "answer": "A",
                "options": [
                    "A. Dark gray.",
                    "B. Green.",
                    "C. Blue.",
                    "D. White."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_12_real.mp4"
    },
    {
        "time": "[0:01:00 - 0:02:00]",
        "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:03]: Various ingredients are neatly arranged on a countertop. A jar of chili flakes sits on a white cutting board, surrounded by a bowl of white rice, limes, and other small bowls of ingredients. A hand is pointing near the jar, indicating towards the prepared items. [0:01:03 - 0:01:09]: With a top-down view, the camera captures a bowl of whole red radishes, alongside bowls containing sauces, seasoning, and other ingredients essential for cooking. The hand transitions across the frame, indicating different elements on the table. [0:01:09 - 0:01:16]: The frame shifts to show a person standing in a modern kitchen, with white brick walls and kitchen items arranged on shelves. The person is actively gesturing and explaining something while occasionally interacting with frying pans on the countertop. [0:01:16 - 0:01:19]: Transitioning focus back to the cooking area, the person picks up a frying pan and holds it out while continuing to speak, suggesting a demonstration or explanation of cooking techniques taking place. The background includes detailed kitchen elements like shelves, cookware, and counter space.\n[0:01:20 - 0:01:40] [0:01:20 - 0:01:21]: The video opens with a person standing in a modern kitchen, facing the camera. The kitchen has a blue and white color palette with wooden countertops and white subway tile backsplash. Various kitchen utensils and ingredients are neatly arranged on the counter. The person is wearing a dark grey t-shirt and is looking directly at the camera, appearing to be in the midst of a cooking demonstration.   [0:01:21 - 0:01:22]: The person continues to speak while gesturing with their hands. They then turns their head slightly to the left, possibly looking at something off-camera. [0:01:22 - 0:01:23]: A wider shot of the kitchen shows more details of the surroundings. The word \"COOK\" is prominently displayed in large, red letters on a shelf behind the person. Several bottles, jars, and other kitchen items are neatly organized on the shelves. [0:01:23 - 0:01:24]: The person turns their body to the right and reaches toward the counter, picking up a plate with their left hand. [0:01:24 - 0:01:25]: They hold up a plate containing a piece of raw meat, holding it in front of their chest with their left hand and gesturing towards it with their right hand as they continue to speak. [0:01:25 - 0:01:26]: A closer view shows the person focusing on the piece of meat, explaining something in detail while making hand motions over it. [0:01:26 - 0:01:27]: The person continues speaking, still holding the plate of meat, and making more small gestures with their right hand. [0:01:27 - 0:01:28]: The view shifts slightly downwards, focusing on the countertop area where the person is about to put the plate down. The person's torso is in the frame, and partially obscured by the countertop. [0:01:28 - 0:01:29]: The person places the plate of meat on the wooden cutting board in front of them. The kitchen counter features several bowls, bottles, and other ingredients. [0:01:29 - 0:01:30]: The person raises their hands slightly and gestures towards the camera, explaining the next steps of the cooking process. The kitchen in the background displays more details such as cutting boards, knives on a magnetic strip, and various kitchen tools. [0:01:30 - 0:01:31]: The person continues to gesture with their hands while explaining the cooking process, looking down at the counter and occasionally glancing at the camera. [0:01:31 - 0:01:32]: They reach for a bottle of olive oil from the counter and start pouring it over the raw meat on the plate, holding the bottle with their right hand. [0:01:32 - 0:01:33]: The person looks intently at the plate as they continue to pour olive oil over the meat, ensuring it is thoroughly coated. [0:01:33 - 0:01:34]: The person continues to pour the oil, covering the meat evenly. The plate is positioned on a wooden cutting board, with various kitchen utensils and ingredients nearby. [0:01:34 - 0:01:35]: The person moves the bottle away from the plate, finishing the task of coating the meat with olive oil, and places the bottle back on the counter. [0:01:35 - 0:01:36]: They grab a small bowl of salt and begin to sprinkle it over the oiled meat using their right hand. [0:01:36 - 0:01:37]: The top-down view provides a closer look at the meat now coated with olive oil, and the person continues to sprinkle salt over it. [0:01:37 - 0:01:38]: The person continues to season the meat, ensuring it is evenly coated with salt while still maintaining the focus on the demonstration. [0:01:38 - 0:01:39]: The view remains centered on the meat and the person's hands as they complete the seasoning step, preparing the meat for the next stage of the cooking process.\n[0:01:40 - 0:02:00] [0:01:40 - 0:01:44]: The video opens with a view of a kitchen countertop from a first-person perspective. On the wooden cutting board, a hand is seen placing a piece of steak in a large white plate to the center right of the frame. To the left, there is another white bowl containing some food remnants. In the background, a stovetop with four burners is visible on the right. Behind the cutting board, several ingredients are neatly arranged, including a bowl with three eggs and bottles containing oil and seasoning. [0:01:45 - 0:01:46]: Transitioning to another angle, the viewer now sees a person in a gray shirt holding the piece of steak over the plate with one hand. We're provided with a wider view of the countertop, which includes a skillet on the stovetop burner to the left and an assortment of ingredients spread out on the counter, including bottles, bowls, and chopped items. The stovetop and adjacent parts of the counter are positioned in the left section of the frame with the man in the center. [0:01:47 - 0:01:51]: The man, still holding the steak, appears to be placing it carefully on the plate. He then adjusts the placement and adds some seasonings from nearby containers. The oil and other condiments are still visible on the counter, while the background shows the lower half of blue and wooden cabinets. The person is focused on preparing the meat and ensuring it is well-coated with the seasoning. There are also additional kitchen utensils nearby. [0:01:52 - 0:01:56]: The camera now shifts to a slightly frontal view of the kitchen setup. The person in the gray shirt stands behind the counter, preparing the food. Behind him, the white-tiled wall has various kitchen tools and shelves holding bowls, glasses, and other kitchen essentials. Distinctively, a sign that reads \"COOK\" in bright red letters hangs on the top shelf. The setup emphasizes a well-organized kitchen environment. [0:01:57 - 0:01:58]: As the action progresses, the man is shown still actively involved in the cooking process. He appears to be offering instructions, with his hands moving expressively. Surrounding him are the various tools and ingredients he is using. His attention is divided between the food he is cooking and the additional commentary or instructions he seems to be providing. The pans on the stovetop are now clearly visible, and the organized kitchen environment continues to play a significant background role. [0:01:59]: The video concludes with a close-up view of the man as he seems to wrap up his cooking process. He continues to gesture with his hands, possibly detailing the final steps or providing additional tips related to the cooking. The background retains the blue cabinetry and variety of kitchen items previously described, maintaining a consistent and organized theme throughout the video.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What object is the person holding and gesturing towards right now?",
                "time_stamp": "0:01:25",
                "answer": "D",
                "options": [
                    "A. A frying pan.",
                    "B. A bowl of radishes.",
                    "C. A jar of chili flakes.",
                    "D. A plate with raw meat."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_12_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:03]: The person, wearing a dark-colored t-shirt, is positioned in a kitchen setting with a blue and yellow color scheme. There are kitchen utensils and ingredients spread across the counter. They are holding red tongs and appear to be handling food on a plate. [0:03:01 - 0:03:04]: The camera captures the array of ingredients on the counter. Several bowls contain items such as cherry tomatoes, spices, eggs, and limes. The person moves their hands, using the red tongs to manipulate something on the plate in front of them. [0:03:02 - 0:03:05]: The person continues to work with the tongs, moving food around on a plate positioned on a cutting board. The kitchen counter displays an assortment of utensils, including a mortar and pestle, a cheese grater, and bottles of oil and sauces. [0:03:03 - 0:03:06]: Focusing on the plate, the individual uses the tongs to rearrange the food on it. The counter remains cluttered with various kitchen implements and ingredients, presenting a busy cooking environment. [0:03:04 - 0:03:07]: From an overhead perspective, the person\u2019s hands adjusted the food with the red tongs, positioning it on a plate next to bowls containing eggs and other ingredients. [0:03:05- 0:03:08]: Using the tongs, the person prepares a dish on a white plate. The camera captures this action from above, showing a wooden cutting board, bowls with ingredients, and a stovetop nearby. [0:03:06 - 0:03:09]: The individual continues handling the food with tongs on a plate, which involves drizzling or sprinkling some ingredients. The ingredients and utensils on the counter remain prominently in view. [0:03:07 - 0:03:10]: The person uses the tongs to make final adjustments to the dish on the plate. Surrounding items include various bowls, bottles, and kitchen tools spread across the counter. [0:03:08 - 0:03:11]: After rearranging the meal on the plate with tongs, the individual places the plate on the counter. They reach towards another ingredient, preparing to add more to the dish. [0:03:09 - 0:03:12]: Utilizing the tongs, the person carefully holds a food item over the plate before placing it down. The countertop features a range of kitchen items, including bottles of oil and spices. [0:03:10 - 0:03:13]: Presented from close range, the individual\u2019s hands are seen using red tongs to position food on the plate. The countertop displays a well-arranged selection of utensils and ingredients. [0:03:11 - 0:03:14]: With the plate positioned in front of them, the individual adds finishing touches, lifting food using the tongs. A variety of cooking tools and ingredients remain visible on the counter. [0:03:12 - 0:03:15]: In a more zoomed-out view, the kitchen\u2019s full length is visible. Shelves lined with cookbooks, jars, and kitchen tools create a background as the person speaks and gestures with their hands in front of the stove. [0:03:13 - 0:03:16]: The person talks animatedly in front of the stove, making hand movements to emphasize their points. Kitchen shelves in the background hold various cooking items and decor. [0:03:14 - 0:03:17]: Engagement with the audience continues as the person speaks energetically, likely explaining a recipe or cooking process. Shelves filled with kitchen items and a large \u201cCook\u201d sign form a backdrop. [0:03:15 - 0:03:18]: Still in front of the stove, the individual gestures expressively while speaking, seemingly in the midst of a cooking demonstration. The kitchen backdrop displays a mix of decor and functional items. [0:03:16 - 0:03:17]: With the person nearing the end of their speech, they use succinct hand gestures to conclude their point. The organized kitchen shelves and clean design provide a structured setting. [0:03:17 - 0:03:18]: The individual turns their attention back to the countertop, ready to proceed with the next step of their cooking process. Kitchen appliances and items continue to form an orderly and functional workspace. [0:03:18 - 0:03:19]: Returning focus to the dish, the person uses tongs to lift a piece of food, preparing to place it onto a plate with an array of ingredients positioned nearby. The setting continues to emphasize a dynamic and active cooking scene.\n[0:03:20 - 0:03:40] [0:03:20 - 0:03:23]: A person is holding a piece of marinated meat with red tongs over a plate that has a sauce on it. The person\u2019s right hand is tilted slightly downwards towards a large skillet on a stovetop. The background features a kitchen with blue cabinets and a counter full of various kitchen tools and ingredients;  [0:03:24 - 0:03:27]: The person places the meat into the heated skillet. The countertop has several other skillets and cooking utensils visible, and the blue cabinets continue along the back wall. Mounted on the wall behind the stove is a set of knives and multiple shelves with kitchen items aligned on them; [0:03:28 - 0:03:30]: The person begins wiping their hands with a towel while glancing towards the stove. The shelves in the background have stacks of plates, glass jars, and other kitchen items neatly arranged; [0:03:31 - 0:03:33]: The camera focuses on the pan, with the meat sizzling inside. The person holds a bottle of oil above the skillet, dripping oil onto the meat. Steam rises from the skillet as the oil hits the hot surface. A kitchen timer graphic appears in the bottom right corner of the frame, showing \u201c08:00\u201d at first and then counting down to \u201c07:56\u201d; [0:03:34 - 0:03:37]: The person steps back from the stove briefly, still holding the towel. The kitchen remains brightly lit with a large window to the left letting in natural light. There are potted plants near the window, adding a touch of greenery to the space. The shelves and counters continue to display a variety of kitchen utensils and ingredients; [0:03:38 - 0:03:39]: The camera angle shifts to an overhead view, showing three skillets on the stove. One of the skillets holds the piece of meat which is now sizzling slightly. The person\u2019s hands are seen adjusting the position of one of the empty skillets on the stove.\n[0:03:40 - 0:04:00] [0:03:40 - 0:03:53]: A hand is holding a pan with a piece of meat sizzling inside it. The pan is on a stovetop next to two other empty pans. The setting is a kitchen, with a tiled floor visible underneath the stovetop. The hand is adjusting the pan's position on the stove. The stove appears metallic and has three cooking areas as well as five black knobs on the right side.  A person is standing in a modern kitchen, facing the camera while speaking. The kitchen features a white brick wall with shelves holding various items, including jars, dishes, and utensils. The word \"COOK\" is prominently displayed in red letters on one of the shelves. Two pendant lights hang from the ceiling, illuminating the scene. The person gestures with their hands as they speak.  The person continues speaking and gestures towards the stovetop in front of them, which has three pans placed on it. The kitchen counter in front of the person is filled with several items, including some bowls, utensils, a bottle of oil, and a few small containers with various ingredients.  The person turns towards the stovetop and appears to engage with one of the pans, possibly adjusting something in it. Their focus seems to be on cooking, and their movements are deliberate. The kitchen's organization and cleanliness are well-maintained, with everything neatly arranged. The person returns their attention to the camera and continues speaking, gesturing slightly with their hands. The stovetop remains in the frame, with steam or smoke rising from one of the pans. The kitchen's decor, including the white brick wall, open shelves, and pendant lights, provides a modern and welcoming atmosphere. The person leans slightly to the right, maintaining their focus on the cooking process. They appear to be checking or adding something to one of the pans on the stovetop. The surrounding kitchen items, including a variety of utensils and ingredients, suggest a professional cooking environment. The person's hands are visible as they handle utensils and work on the stovetop. Their actions are precise and controlled, indicating experience and familiarity with the cooking process. The focus remains on the interaction with the pans and the ongoing culinary activity. [0:03:54 - 0:03:59]: The top-down view of the stovetop shows the person holding a bottle of oil and pouring it into one of the empty pans. The layout features a piece of meat in the left pan, while the right pan receives a swirl of oil. The countertop surrounding the stovetop is clean and spacious, with some kitchen utensils visible at the edges. The person's hand and arm are steady as they handle the bottle and distribute the oil evenly in the pan. The overall scene emphasizes a careful and methodical approach to cooking.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the person holding in their hand?",
                "time_stamp": "0:03:05",
                "answer": "D",
                "options": [
                    "A. A spatula.",
                    "B. A wooden spoon.",
                    "C. A fork.",
                    "D. Red tongs."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_12_real.mp4"
    },
    {
        "time": "[0:09:00 - 0:10:00]",
        "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:04]: A man wearing a dark t-shirt is in a kitchen setting with white brick walls, a countertop, and various kitchen utensils and ingredients around him. He is preparing food on a wooden cutting board positioned on the countertop. There are several pans on the stove, with one containing a piece of meat, and various bowls and ingredients including a jar of salt, half a lemon, and small red vegetables are scattered around. The man is holding a spoon in his right hand and a small white bowl in his left hand. [0:09:05]: The man continues mixing something in the small white bowl while the meat sizzles in the pan on the stove. A voice in the background says, \"2 minutes!\" [0:09:06 - 0:09:09]: The man looks up and starts moving to the right while still holding the bowl and spoon. He adjusts his stance and facial expression as if responding to the announcement. The kitchen in the background features two large green hanging lamps, a shelf with dishes, bottles, and jars, and a large red sign that reads \"COOK\" hanging on the wall. The stove has multiple pans and cooking items on it. [0:09:10 - 0:09:12]: The man picks up a pan with a towel and examines its contents. He tilts the pan towards himself while holding it with his right hand and a cloth in his left hand. His face shows focused attention on the cooking process, and the blue cabinets, various utensils, and plants near the windows are visible in the background. [0:09:13 - 0:09:15]: The man moves back to the center of the counter, still holding the towel and pan. He reaches out with his right hand to grab another pan or utensil, indicating that he is multitasking in the kitchen. The white brick wall and shelves brimming with kitchen equipment serve as a backdrop. [0:09:16 - 0:09:19]: The man continues cooking and adjusting the food in the pans on the stovetop. He opens and closes various drawers and cabinets, takes out utensils, and occasionally glances up, possibly responding to someone or something outside the camera's view. He places a white plate under the counter, handles the food with precision, and starts plating the dish. The background remains consistent with the arrangement of knives, dishes, and decorative items on the shelves and countertops.\n[0:09:20 - 0:09:40] [0:09:20 - 0:09:22]: The video begins with someone holding a pan over a stove with a flame beneath it. The person is tilting the pan to pour its contents into a larger serving pan positioned on the stovetop. The stovetop is surrounded by various kitchen items, including a plate, bowls, and utensils. [0:09:23 - 0:09:26]: The perspective shifts to an overhead view. The person continues to pour the contents from the pan into a serving dish. Various kitchen ingredients and tools are laid out, such as a bowl of chopped vegetables, utensils, and spices. [0:09:27 - 0:09:29]: The camera angle changes to a person in front of a tiled wall with shelves holding kitchen items. The person is wearing a dark shirt, and steam rises from the pan they are holding while they add the food to a serving dish. [0:09:30 - 0:09:32]: The person continues to pour food from the frying pan into the serving dish. They adjust the pan and use a spoon to help transfer the contents. The background shows a well-organized kitchen with a sign above the shelves reading \u201cCOOK.\u201d [0:09:33 - 0:09:35]: The focus is again on transferring the food from the pan to a plate. The food appears to be some kind of stir-fry with vegetables. The surrounding kitchen items remain in place on the counter. [0:09:36]: The person stands behind the counter, wiping the area with a towel. The kitchen shelves, with various items placed neatly, are visible in the background. [0:09:37 - 0:09:39]: The camera shifts to an overhead view once more, showing a stovetop with multiple frying pans. One pan contains a piece of meat, another has two fried eggs, and a third has remnants of the recently cooked food. The person's hands are in motion, preparing to handle the frying pans on the stove.\n[0:09:40 - 0:10:00] [0:09:40 - 0:09:41]: The video begins with a view of a kitchen setup. The scene captures a person in a black t-shirt standing behind a kitchen counter. The kitchen has a white brick backsplash with wooden shelves holding various utensils and plates. The individual is holding a white plate with a dish and seems to be focused on the preparation process. [0:09:42]: The person tilts to the left, placing the plate down on the counter. Kitchen appliances like a sink and stove are visible in the background, indicating a busy cooking scene. [0:09:43]: The camera shifts its focus slightly lower, displaying a close-up of two frying pans on the stove. The person is handling a piece of cooked meat with tongs and a knife, about to transfer it. [0:09:44 - 0:09:49]: Now with a top-down perspective, the individual moves the cooked meat onto a wooden cutting board adjacent to a dish of rice and vegetables. The person starts to slice the meat with a knife, carefully making several precise cuts. [0:09:50 - 0:09:52]: The sliced meat is then picked up and placed on top of the rice dish. The person arranges it neatly, ensuring an even distribution over the bed of rice and vegetables. [0:09:53]: The view zooms out to reveal more of the kitchen setup. Various ingredients, bowls, and utensils are spread across the counter. The person is leaning forward, adding final touches to the dish with a spoon. [0:09:54 - 0:09:56]: Continuing from the previous actions, the person picks up a frying pan and tilts it to pour the remaining sauce or drippings onto the dish, adding a glossy finish to the plated meat. [0:09:57 - 0:09:58]: The camera captures a close-up of the sauce being drizzled over the meat. The dish is almost ready for serving, with the person handling the final steps with precision. [0:09:59]: The final frame zooms out to an overhead shot showing the entire work area. The countertop is cluttered with various ingredients and cooking tools, indicating a busy and vibrant cooking environment. The person appears to be frying eggs in another pan, showcasing the diversity of activities in the kitchen.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the man holding in his right hand while preparing food on the cutting board?",
                "time_stamp": "0:09:04",
                "answer": "D",
                "options": [
                    "A. A jar of salt.",
                    "B. Half a lemon.",
                    "C. A red vegetable.",
                    "D. A small white bowl."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_12_real.mp4"
    },
    {
        "time": "[0:11:00 - 0:11:06]",
        "captions": "[0:11:00 - 0:11:06] [0:11:00]: A person wearing a green t-shirt stands in a kitchen with a backdrop of white tiled walls. Kitchen utensils, dishes, glasses, and a shelf lined with cookbooks are visible behind him. The individual appears to be in mid-speech, with hands clasped together and a serious facial expression.  [0:11:01]: The scene transitions with a large graphic arrow, partially obscuring the previous view but hinting at continued kitchen themes behind it. [0:11:02 - 0:11:04]: A cookbook titled \"Gordon Ramsay in 10: Delicious Recipes Made in a Flash\" is featured prominently on a wooden surface. The cover includes images of various dishes and a picture of someone cooking, emphasizing the theme of quick and delicious recipes.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is the title of the cookbook prominently featured?",
                "time_stamp": "00:11:04",
                "answer": "A",
                "options": [
                    "A. \"Gordon Ramsay in 10: Delicious Recipes Made in a Flash\".",
                    "B. \"Jamie Oliver's 30-Minute Meals\".",
                    "C. \"Gordon Ramsay's Ultimate Cookery Course\".",
                    "D. \"Nigella Express: Good Food Fast\"."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_12_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:11]: The video starts with a view of an amusement park themed like a colorful, fantasy game world. Various characters, vibrant objects, and structures are featured, including large red and yellow mushrooms and green pipes. The scene is busy with people walking, heading toward a large green tunnel labeled \"Mario Kart: Bowser's Challenge.\" The left side features an entrance manned by staff in red attire, while the right side showcases different colorful sections, including checkered patterns and a grey stone statue with a menacing face near the top of the structure. [0:00:11 - 0:00:20]: The perspective moves closer to the Mario Kart ride entrance. The details of the tunnel's bright green structure and the surrounding vibrant landscape become more apparent. People continue to walk into the tunnel, passing under a red and white umbrella near the entrance. Inside, the tunnel gradually reveals a dimly lit interior with green walls and additional decorations and lighting that enhance the fantasy atmosphere. The video ends with the first-person view partly inside the tunnel, moving forward.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is labeled on the large green tunnel people are heading toward?",
                "time_stamp": "0:00:11",
                "answer": "A",
                "options": [
                    "A. \"Mario Kart: Bowser's Challenge\".",
                    "B. \"Super Mario World\".",
                    "C. \"Luigi's Mansion\".",
                    "D. \"Princess Peach's Castle\"."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_320_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:26]: The video begins with a group of people, including a person in a red uniform, a middle-aged person wearing a white shirt, and a child, walking through a dimly lit corridor. The corridor is adorned with multiple screens displaying animated characters and vibrant colors. The group is moving towards an open doorway leading into a brightly lit room;  [0:02:27 - 0:02:29]: As they progress, the brightly lit room becomes more visible, revealing a lively environment filled with themed decorations, neon lights, and colorful signs. The floor has a large emblem with the words \"Universal Studios\" and \"Mario Kart\" written on it; [0:02:30 - 0:02:34]: The group continues forward, passing themed displays with character costumes and toys enclosed in glass cases along the walls. These display shelves contain oversized character suits and colorful, cartoonish objects. The room is vibrant with reds, greens, and yellows, adding to the energetic atmosphere; [0:02:35 - 0:02:36]: The focus shifts towards the further end of the room where a collection of character costumes is neatly arranged in shelves. The shelves contain suits resembling oversized animated characters, a frog-like character, and a dinosaur-green character holding a mushroom; [0:02:37 - 0:02:39]: The video transitions to a new scene where a group of people, who appear to be seated in what looks like a theme park ride with cars stylized after characters from a popular franchise. The environment is dark but illuminated by small light sources, and the walls are designed to look like stone bricks with a large logo that reads \"Universal Studios Mario Kart\" hanging above an arched doorway. The overall ambiance suggests a fun and adventurous theme park experience.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What was the group doing just now?",
                "time_stamp": "0:02:26",
                "answer": "D",
                "options": [
                    "A. Standing in a queue.",
                    "B. Sitting on a theme park ride.",
                    "C. Entering a theater.",
                    "D. Walking through a dimly lit corridor."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_320_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:42]: An underwater scene with vibrant, colorful marine life and a submarine-like vehicle moving through the water. The background has rocky formations and floating sea plants. A large, red fish with a menacing expression appears to be chasing the submarine. [0:04:42]: A white, cartoonish squid with big round eyes faces the viewer, seemingly startled. The large red fish remains in the background. [0:04:43 - 0:04:45]: The screen is mostly dark with a few tiny, glowing lights, giving the impression of deep sea creatures or bioluminescence. [0:04:46 - 0:04:49]: The scene brightens back up, showing characters resembling an armored spiky creature surrounded by various dangerous-looking sea creatures. Blue spotlights illuminate parts of the scene, emphasizing some of the creatures as they swim. [0:04:50 - 0:04:52]: The armored spiky character moves forward as more jellyfish-like creatures hover above. Bright spotlights and glowing effects create a dynamic, intense atmosphere. [0:04:53]: The character encounters a large skeletal fish with a glowing eye, giving a sense of impending danger. [0:04:54 - 0:04:56]: The skeletal fish appears to attack, while the armored character tries to evade. The background shows a mix of dark rocky formations and the glow from the skeletal fish's eye. [0:04:57 - 0:04:59]: The armored character continues to navigate the underwater environment amidst bubbles and glowing elements. The skeletal fish is present in the background, adding to the sense of urgency and action.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the large fish chasing the small fishes?",
                "time_stamp": "00:04:42",
                "answer": "C",
                "options": [
                    "A. Blue fish.",
                    "B. Green fish.",
                    "C. Red fish.",
                    "D. Yellow fish."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_320_real.mp4"
    },
    {
        "time": "[0:07:00 - 0:08:00]",
        "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:03]: In the video, vibrant, multicolored streaks of light stretch outwards on a dark background, creating a sense of rapid movement, resembling a warp speed effect. Two animated characters, Bowser and Bowser Jr., are visible, appearing to move quickly alongside the light streaks, with their distinct colors and shapes becoming more prominent. [0:07:03 - 0:07:06]: The view shifts to a rainbow-colored track that Bowser and Bowser Jr. are steering onto. Strange multicolored lines and patterns surround them. They seem to be gaming characters engaged in an intense action sequence. The track consists of vividly colored tiles, and Bowser Jr. is seen unfurling a glider. [0:07:06 - 0:07:13]: As they zoom ahead, the video shows them nearing the finish line, which is marked by a broad banner. Bowser Jr. glides towards it amidst the colorful background. Tanooki Mario appears on the finish platform. Excited celebratory signs and sparkles highlight the cross-line moments, emphasizing their victory. [0:07:13 - 0:07:15]: The scene transitions to the prize podium. Bowser Jr. and teammates celebrate under bright, colorful spotlights and signs. The \u201cTeam Bowser Wins!\u201d banner is prominently displayed amidst the surrounding confetti and sparkles. [0:07:15 - 0:07:17]: The camera angle shifts to showcase the team's celebration from a different perspective, displaying vibrant celebratory graphics and more of the gaming environment's dark background. [0:07:17 - 0:07:20]: The view changes, focusing on a castle-like structure adorned with checkered flags and digital screens on its walls. The lighting in the scene is significantly dimmed, giving a sense of conclusion to the event. The area is predominantly dark with minimal light accents highlighting parts of the structure.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What action does Bowser Jr. perform as they steer onto the rainbow-colored track?",
                "time_stamp": "0:07:06",
                "answer": "D",
                "options": [
                    "A. Jumps.",
                    "B. Throws a shell.",
                    "C. Activates a boost.",
                    "D. Unfurls a glider."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Event Understanding",
                "question": "What is the main event happening as Bowser Jr. reaches the finish line?",
                "time_stamp": "0:07:13",
                "answer": "D",
                "options": [
                    "A. Bowser Jr. avoids obstacles.",
                    "B. Bowser Jr. prepares to glide.",
                    "C. Tanooki Mario joins the race.",
                    "D. Excited celebratory signs and sparkles appear."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_320_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:05]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Which store's name is visible on the left side of the street?",
                "time_stamp": "00:00:04",
                "answer": "A",
                "options": [
                    "A. Bed Bath & Beyond.",
                    "B. Barnes & Noble.",
                    "C. Target.",
                    "D. Best Buy."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_385_real.mp4"
    },
    {
        "time": "[0:02:02 - 0:02:07]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Which brand logo is visible on the store awnings?",
                "time_stamp": "00:02:03",
                "answer": "B",
                "options": [
                    "A. Walmart.",
                    "B. Trader Joe's.",
                    "C. Whole Foods.",
                    "D. Starbucks."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_385_real.mp4"
    },
    {
        "time": "[0:04:04 - 0:04:09]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is the Citibank sign located right now?",
                "time_stamp": "00:04:07",
                "answer": "B",
                "options": [
                    "A. On the right side of the road.",
                    "B. On the left side of the road.",
                    "C. In the front of the car.",
                    "D. Behind the car."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_385_real.mp4"
    },
    {
        "time": "[0:06:06 - 0:06:11]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the notable color of the taxi right now?",
                "time_stamp": "00:06:10",
                "answer": "C",
                "options": [
                    "A. Red.",
                    "B. Blue.",
                    "C. Yellow.",
                    "D. Green."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_385_real.mp4"
    },
    {
        "time": "[0:08:08 - 0:08:13]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is the brand of the visible pharmacy right now?",
                "time_stamp": "00:08:10",
                "answer": "B",
                "options": [
                    "A. Walgreens.",
                    "B. Duane Reade.",
                    "C. Rite Aid.",
                    "D. CVS."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_385_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:10]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the content of the video just shown?",
                "time_stamp": "00:00:10",
                "answer": "D",
                "options": [
                    "A. An employee prepares a breakfast sandwich, adds condiments, and serves it to the customer.",
                    "B. An employee prepares a sandwich, toasts the buns, adds sauce, lettuce, and serves it in a box.",
                    "C. An employee prepares a salad, chops vegetables, and arranges them in a bowl.",
                    "D. An employee prepares a burger, adds mayo, lettuce, and packs it in a box."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_349_real.mp4"
    },
    {
        "time": "[0:01:39 - 0:01:49]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the worker's actions just now?",
                "time_stamp": "00:01:49",
                "answer": "A",
                "options": [
                    "A. An employee prepares a burger, adds cheese, lettuce, and sauce, and packs it in a box.",
                    "B. An employee fries a chicken patty, adds mayo and lettuce, and packs it in a bag.",
                    "C. An employee prepares a sandwich, adds cucumbers, tomatoes, and sauce, and packs it in a wrap.",
                    "D. An employee prepares a vegan burger, adds vegan cheese, lettuce, and sauce, and packs it in a box."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_349_real.mp4"
    },
    {
        "time": "[0:03:18 - 0:03:28]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the worker's actions just now?",
                "time_stamp": "00:03:28",
                "answer": "A",
                "options": [
                    "A. The worker toasts a burger bun, adds tartar sauce and lettuce, and places the assembled burger back on the counter.",
                    "B. The worker grills a patty, adds ketchup and onions to the bun, and places the assembled burger in a bag.",
                    "C. The worker fetches ingredients from the refrigerator, cuts tomatoes, and adds them to a bowl.",
                    "D. The worker fries a fish fillet, adds mayo and pickles to a bun, and places the assembled sandwich in a wrap."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_349_real.mp4"
    },
    {
        "time": "[0:04:57 - 0:05:07]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the employee's actions just now?",
                "time_stamp": "00:05:21",
                "answer": "B",
                "options": [
                    "A. The employee prepared a chicken sandwich by toasting buns, adding mayonnaise and lettuce, and packing it in a box.",
                    "B. The employee prepared a fish burger by toasting buns, adding cheese, and placing it in a box.",
                    "C. The employee prepared a fish sandwich by toasting buns, adding tartar sauce and lettuce, and packing it in a box.",
                    "D. The employee prepared a veggie burger by toasting buns, adding vegetables, and packing it in a box."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_349_real.mp4"
    },
    {
        "time": "[0:06:36 - 0:06:46]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the content of the video just shown?",
                "time_stamp": "00:06:46",
                "answer": "C",
                "options": [
                    "A. The employee cooked and served fries, adding salt and packing them in a container.",
                    "B. The employee grilled a chicken patty, added lettuce and mayo, and packed the sandwich in a box.",
                    "C. The employee packed chicken nuggets into boxes.",
                    "D. The employee prepared a fish sandwich, adding tartar sauce and lettuce, and packed it in a paper wrap."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_349_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:00:16",
                "answer": "D",
                "options": [
                    "A. 2.",
                    "B. 3.",
                    "C. 4.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_88_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:03:08",
                "answer": "A",
                "options": [
                    "A. 4.",
                    "B. 5.",
                    "C. 6.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_88_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:04:11",
                "answer": "B",
                "options": [
                    "A. 2.",
                    "B. 4.",
                    "C. 3.",
                    "D. 6."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_88_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:05:01",
                "answer": "B",
                "options": [
                    "A. 2.",
                    "B. 5.",
                    "C. 4.",
                    "D. 6."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_88_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:06:19",
                "answer": "D",
                "options": [
                    "A. 4.",
                    "B. 8.",
                    "C. 6.",
                    "D. 7."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_88_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:20]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where are the power lines located right now?",
                "time_stamp": "00:00:08",
                "answer": "D",
                "options": [
                    "A. Crossing diagonally over the road.",
                    "B. Parallel to the right side of the road.",
                    "C. Intersecting the road perpendicularly.",
                    "D. Running parallel to the left side of the road."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_171_real.mp4"
    },
    {
        "time": "[0:02:05 - 0:02:25]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the red graffiti?",
                "time_stamp": "00:02:16",
                "answer": "D",
                "options": [
                    "A. On the right side of the road.",
                    "B. On the left side of the road.",
                    "C. In front of the road.",
                    "D. Above the entrance of the tunnel."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_171_real.mp4"
    },
    {
        "time": "[0:04:10 - 0:04:30]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is on the left side of the road right now?",
                "time_stamp": "00:04:10",
                "answer": "D",
                "options": [
                    "A. A river.",
                    "B. A small building.",
                    "C. An open field.",
                    "D. Dense vegetation."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_171_real.mp4"
    },
    {
        "time": "[0:06:15 - 0:06:35]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is located on the left side of the path right now?",
                "time_stamp": "00:06:27",
                "answer": "D",
                "options": [
                    "A. A cornfield.",
                    "B. A tall building.",
                    "C. A dense forest.",
                    "D. A patch of grass."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_171_real.mp4"
    },
    {
        "time": "[0:08:20 - 0:08:40]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is located on the right side of the road right now?",
                "time_stamp": "00:08:34",
                "answer": "D",
                "options": [
                    "A. A field of corn.",
                    "B. A forest.",
                    "C. A river.",
                    "D. A field of wheat."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_171_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video begins with a wide-angle view of a football stadium crowded with spectators. The field is lush green, and there are multiple advertisements on the outskirts of the pitch. Players and a referee are seen, with one player in a yellow jersey and another in blue. The player in the yellow jersey is near the center. [0:00:04 - 0:00:05]: The focus shifts to a close-up of a player in a yellow jersey. The player appears focused and somewhat intense, with the background slightly blurred, emphasizing his face. [0:00:06 - 0:00:07]: Another scene shows the player wiping his face with his hand, possibly indicating concentration or stress. [0:00:07 - 0:00:09]: The next frame captures the goalkeeper from the back, wearing a bright green jersey with the name \"PICKFORD\" and the number one written on it. The background includes spectators and other players. [0:00:10]: Another close-up shows the player in the yellow jersey again, who has his eyes closed or looking downward, further emphasizing focus or perhaps a moment of contemplation. [0:00:11 - 0:00:14]: Two players in yellow jerseys, with \"L. MURIEL 14\" and \"number 15\" on their backs, are seen embracing each other in front of the goal post. A crowd is visible in the background. [0:00:15 - 0:00:18]: The focus moves to the field where a close-up of a player\u2019s feet wearing white cleats with ADIDAS branding can be seen. The player adjusts the position of a football, which has a distinctive red and black pattern. The white socks worn by the player are dirty, indicating the intensity of the match. [0:00:19 - 0:00:20]: The final frame features a player\u2019s lower body in white shorts and socks, standing on the field. The action seems to be gearing up for a critical moment like a kick or a move.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What name is written on the goalkeeper's bright green jersey?",
                "time_stamp": "00:00:14",
                "answer": "B",
                "options": [
                    "A. NEUER.",
                    "B. PICKFORD.",
                    "C. DE GEA.",
                    "D. COURTOIS."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Object Recognition",
                "question": "What brand is visible on the player's white cleats?",
                "time_stamp": "00:00:18",
                "answer": "C",
                "options": [
                    "A. Nike.",
                    "B. Puma.",
                    "C. Adidas.",
                    "D. Under Armour."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "football",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_5_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:24]: The video shows a penalty shootout in a soccer match. A player in a yellow jersey is preparing to take a penalty kick. He stands a few meters away from the ball, which is placed on the penalty spot. The goalkeeper, in a bright green jersey, is positioned in front of the goal, getting ready to defend the shot. The crowd in the stands behind the goal is waving flags and banners, and there is a visible advertisement board with \"Coca-Cola\" repeated on it. [0:02:25 - 0:02:29]: After taking the shot, the player in the yellow jersey runs forward a few steps and begins to celebrate. His facial expression shows determination and joy. He pumps his fist as he runs, clearly elated by the outcome of his kick. [0:02:30 - 0:02:33]: The video cuts back to a close-up of the player in the yellow jersey just as he strikes the ball with his right foot. The goalkeeper dives to his left, trying to reach the ball, but misses. The ball goes into the net, indicating a successful penalty. [0:02:34 - 0:02:35]: The goalkeeper is seen getting up from the ground after his failed attempt to save the penalty. His body language shows frustration. [0:02:36 - 0:02:39]: The view zooms out to show the entire field and the cheering crowd in the stands. The scoreboard at the bottom of the screen shows the current score: Colombia 2-1 England. The excitement in the stadium is palpable as the camera captures the celebratory atmosphere.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What is the outcome of the penalty kick just now?",
                "time_stamp": "00:02:27",
                "answer": "C",
                "options": [
                    "A. The goalkeeper saves the ball.",
                    "B. The ball hits the post.",
                    "C. The ball goes over the crossbar.",
                    "D. The ball goes into the net."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "football",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_5_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:44]: A penalty kick scene is depicted with a goalkeeper diving to his right, attempting to save a shot. The ball is headed towards the goal, and spectators in the background are intensely watching. Some stewards in neon vests and photographers are visible behind the goal, with a sponsorship banner behind them. [0:04:45 - 0:04:49]: The next sequence shifts focus away from the goal to show players in the playfield. A player in a yellow jersey appears to be walking away from the penalty area, as a group of players in red jerseys are standing together in anticipation. The LED display shows the match information with the score at 3-2, indicating a tense moment in the game. [0:04:50 - 0:04:53]: The scene shifts to the coaching or managerial area. A man in a suit, likely a coach, visibly expresses distress with his hand over his face while being consoled by others. This area also shows the live match updates with the ongoing score displayed at the bottom. [0:04:54 - 0:04:57]: The camera then focuses on a close-up of a player in a yellow jersey, possibly reflecting the emotional aftermath of the penalty kick. Another view captures the interaction between the referee and the player, where the referee's back is towards the camera. [0:04:58 - 0:04:59]: Finally, the focus shifts to a player in a bright green jersey, likely the goalkeeper, who appears to be processing the outcome, showing mixed expressions of frustration and composure. The player lowers his head briefly, indicating a moment of contemplation or disappointment.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What did the goalkeeper do during the penalty kick?",
                "time_stamp": "00:04:33",
                "answer": "B",
                "options": [
                    "A. Stood still.",
                    "B. Dived to his right.",
                    "C. Dived to his left.",
                    "D. Ran towards the ball."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "football",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_5_real.mp4"
    },
    {
        "time": "[0:07:00 - 0:08:00]",
        "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:05]: A goalkeeper dressed in a bright green kit with the number \"1\" on the back is poised to the left side of the goal line, preparing to dive as a player in a yellow kit takes a penalty kick. The goalkeeper dives to his right, fully stretched in an attempt to save the shot. The ball is seen mid-air to the right of the frame as the goalkeeper reaches out towards it. [0:07:06 - 0:07:09]: A close-up shot shows a man with a serious expression standing next to another individual. The background is blurred, revealing a filled stadium with spectators. Both individuals are focused on the ongoing game. [0:07:10 - 0:07:13]: The focus shifts to a player in a red kit with a look of concentration, possibly preparing for a turn to take a penalty kick. The background shows blurred spectators watching intently. [0:07:14 - 0:07:15]: The same player in the red kit is shown bending over, adjusting his position. An official in black stands close by, overseeing the situation. [0:07:16 - 0:07:17]: A goalkeeper in a teal kit with a determined look is seen standing in front of the goal, anticipating the next move. The net behind him and the crowd in the background are visible. [0:07:18 - 0:07:19]: The focus returns to the player in the red kit, capturing his focused expression as he prepares for his task at hand. The blurred background remains filled with spectators.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What number is on the back of the goalkeeper's kit who is preparing to dive for the penalty kick?",
                "time_stamp": "0:07:23",
                "answer": "D",
                "options": [
                    "A. 10.",
                    "B. 7.",
                    "C. 5.",
                    "D. 1."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "football",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_5_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:20]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the white road marker positioned relative to the cyclist right now?",
                "time_stamp": "00:00:06",
                "answer": "A",
                "options": [
                    "A. On the both sides of the cyclist.",
                    "B. In front of the cyclist.",
                    "C. To the left of the cyclist.",
                    "D. Behind the cyclist."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_178_real.mp4"
    },
    {
        "time": "[0:01:51 - 0:02:11]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "How are the road signs oriented right now?",
                "time_stamp": "00:01:55",
                "answer": "A",
                "options": [
                    "A. Indicating a left turn.",
                    "B. Pointing straight ahead.",
                    "C. Indicating a right turn.",
                    "D. Indicating a U-turn."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_178_real.mp4"
    },
    {
        "time": "[0:03:42 - 0:04:02]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is located at the far end of the left side of the road right now?",
                "time_stamp": "00:03:57",
                "answer": "A",
                "options": [
                    "A. A tall tree.",
                    "B. A hospital.",
                    "C. A park.",
                    "D. A bridge."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_178_real.mp4"
    },
    {
        "time": "[0:05:33 - 0:05:53]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the blue car located right now?",
                "time_stamp": "00:05:38",
                "answer": "A",
                "options": [
                    "A. On the right side of the cyclist.",
                    "B. On the left side of the cyclist.",
                    "C. Directly in front of the cyclist.",
                    "D. Behind the cyclist."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_178_real.mp4"
    },
    {
        "time": "[0:07:24 - 0:07:44]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the red car located right now?",
                "time_stamp": "00:07:38",
                "answer": "A",
                "options": [
                    "A. On the right side of the cyclist.",
                    "B. On the left side of the cyclist.",
                    "C. Directly in front of the cyclist.",
                    "D. Behind the cyclist."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_178_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a black screen. After a second, it transitions to a view of a character in Minecraft wearing purple armor. The backdrop includes a glowing portal on the left side and some chests and blocks on the right. [0:00:02 - 0:00:03]: The camera angle changes, showing the character from a slightly lower angle, with large text \"um\" appearing on the screen. The camera then zooms in on the character's face, showing its pixelated green eyes. [0:00:04 - 0:00:05]: The camera zooms out to show the character fully again in front of the portal. The background remains consistent with chests and blocks visible. [0:00:06]: The camera zooms back into the character's eyes. [0:00:07]: The camera offers an aerial view of a Minecraft landscape including mountains, water bodies, and blocks. [0:00:08 - 0:00:10]: The character in purple armor is shown flying with wings, as the camera follows its movements from a first-person perspective. The blocky Minecraft landscape passes below. [0:00:11 - 0:00:13]: The perspective transitions to a view from above, showing colorful block structures and vast expanses of water and trees. [0:00:14]: The character continues flying over the landscape, descending slightly. The view reveals more details of the terrain and water below. [0:00:15 - 0:00:16]: The camera shifts to an underwater scene, showing fish swimming around and aquatic plants. [0:00:17 - 0:00:18]: The perspective returns to the character flying over the landscape, with green and brown blocks along with a body of water visible below. [0:00:19 - 0:00:20]: The camera points upward towards the sky, showing a blue expanse with white clouds. The video ends with this upward perspective.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What action is the character in purple armor performing when the camera follows its movements from a first-person perspective?",
                "time_stamp": "00:00:10",
                "answer": "D",
                "options": [
                    "A. Mining blocks.",
                    "B. Building a structure.",
                    "C. Walking through a portal.",
                    "D. Flying with wings."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "minecraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_198_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:42]: The video starts with a first-person view of a character looking out from a white tunnel into a larger room filled with lava, floating yellow and black creatures, and particles falling from the ceiling. The lava appears thick and orange, and the creatures are blocky with flames around them. [0:04:43]: As the character moves backward, they turn away from the lava room into a darker hallway made of red and black brick textures, contrasting sharply with the white tunnel. The character's hand holds an item resembling a white block. [0:04:44 - 0:04:48]: The character places the white block in the wall and proceeds to create an opening. The brick texture changes to a mix of dark and lighter blocks, indicating a transition between areas. The dark bricks give way to a more open room with flowing lava columns and openings in the walls. [0:04:49 - 0:04:50]: The character navigates through the newly opened doorway into a broader area filled with red and black bricks and pillars, augmented by flowing lava columns on each side. [0:04:51 - 0:04:52]: The character wields a sword, approaching a yellow and black floating creature near the lava column. The area is dark, and the creature emits smoke particles, indicative of the hostile environment. [0:04:53]: The character moves closer to the yellow and black creature, observed with the sword still in hand while staunchly looking up at it against the dark brick background, lava casting an orange glow. [0:04:54]: The creature advances closer to the character, floating slightly upwards near the lava column with its roughly rectangular and fiery appearance. Its proximity indicates a brewing confrontation. [0:04:55]: The character swipes with the sword at the creature, resulting in a flying blue and white lightning particle effect marking the impact, showing that a strike on the creature took place. [0:04:56 - 0:04:57]: The creature emits a large burst of flames and smoke particles, indicating it has been hit and is being damaged. The dark and fiery background enhances the visibility of these effects. [0:04:58 - 0:05:00]: After a few seconds, the creature finally succumbs to the attack, disintegrating into smaller particles that fade away. The area returns to the previous state, with the character ready and looking towards another yellow and black creature across the room, near another lava column.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What item is the character holding in their hand right now?",
                "time_stamp": "00:04:41",
                "answer": "D",
                "options": [
                    "A. A sword.",
                    "B. A shield.",
                    "C. A torch.",
                    "D. A white block."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Action Recognition",
                "question": "What action does the character perform after turning away from the lava room?",
                "time_stamp": "00:04:44",
                "answer": "D",
                "options": [
                    "A. Fights a creature.",
                    "B. Collects a resource.",
                    "C. Lights a torch.",
                    "D. Places a white block in the wall."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "minecraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_198_real.mp4"
    },
    {
        "time": "[0:09:00 - 0:10:00]",
        "captions": "[0:09:20 - 0:09:40] [0:09:20 - 0:09:22]: The video starts in a mountainous terrain with green fields and patches of trees in the background. The terrain is composed of blocks and appears to be from the video game Minecraft. The player is flying over the landscape.  [0:09:23 - 0:09:27]: The player moves closer to the ground and a cave entrance. The cave is dark with visible waterfalls and lava flows inside. The environment becomes darker as they descend further into the cave. [0:09:28 - 0:09:33]: As the player continues flying deeper into the cave, text reading \"usefulness\" appears prominently on the screen. The cave is complex, with multiple layers, waterfalls, and lava streams lighting up various parts. [0:09:34]: The screen transitions to an image of a Minecraft character labeled \"farm #6 as minecraft steve.\" [0:09:36 - 0:09:39]: The scene changes to a twilight outdoor setting with a Minecraft Warden, a large and dark creature with distinctive white and teal markings, standing on a grassy field. The background shows a setting sun, casting an orange hue on the sky. The Warden appears to be standing still.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is prominently displayed on the screen as the player continues flying deeper into the cave?",
                "time_stamp": "00:09:33",
                "answer": "A",
                "options": [
                    "A. Usefulness.",
                    "B. Danger.",
                    "C. Exploration.",
                    "D. Achievement."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "minecraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_198_real.mp4"
    },
    {
        "time": "[0:14:00 - 0:15:00]",
        "captions": "[0:14:00 - 0:14:20] [0:14:00 - 0:14:01]: The scene starts with a first-person perspective view of a Minecraft player flying over a landscape using elytra wings. They appear to be at a significant height above the ground, moving forward with a clear view of the surrounding terrain. There's an eye-level scene of a white structure with wooden accents below and various green trees in the background. [0:14:01 - 0:14:05]: The camera continues to move forward, showing a hilly landscape with patches of trees and a distinct large mountain in the distance. The sky is predominantly clear with a few scattered clouds. The player's hotbar is visible at the bottom of the screen, indicating they have a variety of items available. [0:14:05 - 0:14:06]: As the player moves closer to a tall, snowy mountain, their altitude appears to slightly decrease. The Minecraft landscape continues to change, displaying different biomes including snowy areas and green hills. [0:14:06 - 0:14:09]: Cartoon graphics with colorful text overlay appear on the screen, partially obscuring the view. The background still shows the player flying over the landscape. The terrain below is largely green with patches of brown. [0:14:09 - 0:14:11]: The camera tilts downward to show a bird's-eye view of the ground below. The landscape consists of green blocks with scattered patches of brown, likely representing grass and dirt, and a few isolated trees. [0:14:11 - 0:14:13]: The view switches to a third-person perspective, showing the character equipped with purple-colored elytra wings flying above the landscape. The character maintains a steady altitude while heading towards a mountain. [0:14:13 - 0:14:16]: The player continues to fly forward, descending slightly as they approach the ground. The terrain below is diverse, featuring grassy areas and some elevated hills. [0:14:16 - 0:14:18]: The player increases altitude again, flying through a partly cloudy sky. The blocky clouds are white against a blue sky. The snowy mountain is visible to the left, and the player is heading away from it. [0:14:18 - 0:14:20]: The player flies at a high altitude over a large expanse of green terrain. The viewpoint shifts slightly, revealing a mix of biomes with snowy peaks in the distance and green hills and valleys below.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What happens as the player approaches the tall, snowy mountain?",
                "time_stamp": "00:14:16",
                "answer": "D",
                "options": [
                    "A. The player increases altitude significantly.",
                    "B. The player changes to a third-person perspective.",
                    "C. The player starts building a structure.",
                    "D. The player start to fly upwards."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "minecraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_198_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:05]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color is the vest the man is wearing right now?",
                "time_stamp": "00:00:03",
                "answer": "A",
                "options": [
                    "A. Green.",
                    "B. Orange.",
                    "C. Red.",
                    "D. Yellow."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_454_real.mp4"
    },
    {
        "time": "[0:02:10 - 0:02:15]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is mounted on the dashboard right now?",
                "time_stamp": "00:02:10",
                "answer": "A",
                "options": [
                    "A. A phone with GPS application.",
                    "B. A camera.",
                    "C. A radio.",
                    "D. A small fan."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_454_real.mp4"
    },
    {
        "time": "[0:04:20 - 0:04:25]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color is the dashboard right now?",
                "time_stamp": "00:04:23",
                "answer": "B",
                "options": [
                    "A. Blue.",
                    "B. Black.",
                    "C. Gray.",
                    "D. Brown."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_454_real.mp4"
    },
    {
        "time": "[0:06:30 - 0:06:35]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What type of vehicle is the vehicle ahead right now?",
                "time_stamp": "00:06:33",
                "answer": "C",
                "options": [
                    "A. Taxi.",
                    "B. Delivery van.",
                    "C. School bus.",
                    "D. Ambulance."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_454_real.mp4"
    },
    {
        "time": "[0:08:40 - 0:08:45]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is attached to the dashboard right now?",
                "time_stamp": "00:08:41",
                "answer": "C",
                "options": [
                    "A. A notebook.",
                    "B. A GPS device.",
                    "C. A mobile phone.",
                    "D. A calculator."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_454_real.mp4"
    },
    {
        "time": "0:00:00 - 0:00:20",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:08]: A person in a striped shirt and white shorts is running along a sidewalk in a city area, which features tall buildings, palm trees, and residential structures. The sidewalk is adjacent to a street with sparse traffic. To the left, there are green grassy areas, and on the right side, there are plants and shrubs in landscaped sections. The sky is clear and sunny. On the top left corner of the frames, there is a small screen showing a streaming overlay with chat messages. [0:00:08 - 0:00:13]: The runner continues along the sidewalk, moving past a sign on their right side and several trash cans. The background maintains the urban setting with visible buildings and parked cars along the street. The area is well-maintained with neatly trimmed grass and sidewalk. [0:00:13 - 0:00:18]: The person approaches a section of the sidewalk where it curves slightly, passing by more palm trees, a stone wall with plants, and a residential house on the right. A few cars are visible on the road, and a pedestrian appears in the distance walking towards the runner. The backdrop consistently features tall buildings and a clear sky. [0:00:18 - 0:00:19]: The runner nears the pedestrian, who appears to be walking in the opposite direction. The surroundings remain similar, with additional elements like more palm trees, plants, and buildings emphasizing the urban landscape. [0:00:19 - 0:00:20]: The perspective briefly changes, and a green filter overlays the screen, revealing what appears to be a map interface briefly before switching back to the running scene. The runner is now closer to the pedestrian, who is more clearly visible on the sidewalk.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What did the person wearing a striped shirt do just now?",
                "time_stamp": "00:00:20",
                "answer": "D",
                "options": [
                    "A. Grabbed a trash can.",
                    "B. Stopped to tie their shoelaces.",
                    "C. Engaged in a conversation with the pedestrian.",
                    "D. Ran closer to the pedestrian."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_272_real.mp4"
    },
    {
        "time": "0:05:20 - 0:05:40",
        "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:27]: The video appears to be a first-person perspective of someone using an inventory system in a video game. The screen shows an interface divided into several sections labeled \"Player,\" \"Ground,\" and \"Backpack,\" each containing grids for item storage. There's also a section at the bottom left labeled \"Personal Information.\" On the left side of the screen, a webcam feed displays an individual with headphones and a microphone, looking at the screen. They show various expressions during this time. [0:05:27 - 0:05:28]: The scene transitions from the inventory interface to an outdoor urban environment in a video game. It's a sunny day, with palm trees, buildings, and cars visible in the background. Two characters are standing near a palm tree. The character facing away from the viewer is bald and wearing a black shirt and blue jeans.  [0:05:28 - 0:05:38]: The camera shifts to provide a clearer view of the surroundings. The two characters continue their conversation while standing on the grass near a sidewalk. A fire hydrant is visible behind them, and other characters and vehicles occasionally pass by in the background. The taller character wears a striped shirt and shorts, facing another character holding a phone. Buildings and a traffic intersection become more evident in the background. [0:05:38 - 0:05:40]: The camera focuses more on the interaction between the two characters. The character with the phone appears to be showing something to the other character, gesturing with his hand. The background shows more details of the urban environment with street lamps, palms, and a modern building with red foliage trees. The character in the striped shirt listens attentively.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is the relationship between the fire hydrant and the two characters standing near the palm tree?",
                "time_stamp": "00:05:42",
                "answer": "C",
                "options": [
                    "A. The fire hydrant is in front of them.",
                    "B. The fire hydrant is to the right of them.",
                    "C. The fire hydrant is behind them.",
                    "D. The fire hydrant is to the left of them."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_272_real.mp4"
    },
    {
        "time": "0:08:00 - 0:08:20",
        "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:07]: A settings menu for a video game is displayed on the screen. It features various key bindings for vehicle controls, listed in a table format. To the left of the screen, a sub-menu highlights the \"Key Bindings\" option. In the top-left corner, a picture-in-picture (PiP) displays a person wearing a headset, likely the player, with a focused expression and hands possibly on a keyboard or controller. The right side features chat messages and viewer interactions, indicating a livestream or recorded gameplay session. [0:08:08 - 0:08:12]: The settings screen changes, presenting a broader view of categories such as \"Game,\" \"Info,\" \"Stats,\" and \"Settings.\" \"Key Bindings\" is still highlighted. The PiP shows the same individual as before, maintaining a consistent facial expression and posture. Chat messages continue to scroll on the right side. [0:08:13 - 0:08:14]: The game settings menu disappears, revealing gameplay with a small car at an intersection. The game\u2019s perspective is in third-person view. The PiP of the individual remains, showing them focused on the gameplay. [0:08:15 - 0:08:20]: The car is stationary at a red light with a clear view of a multi-lane road and buildings in the background. The traffic light turns green, and the car starts to move. The PiP remains consistent, and chat interactions continue on the side. The player\u2019s focus is on the road ahead, navigating through traffic lights and urban scenery with buildings and trees along the street.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the chat message section located on the screen right now?",
                "time_stamp": "00:08:20",
                "answer": "B",
                "options": [
                    "A. In the top-left corner.",
                    "B. On the right side.",
                    "C. At the bottom.",
                    "D. On the left side."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Action Recognition",
                "question": "What is the player doing right now?",
                "time_stamp": "00:08:15",
                "answer": "C",
                "options": [
                    "A. Adjusting game settings.",
                    "B. Watching a cutscene.",
                    "C. Driving a car in the game.",
                    "D. Selecting key bindings."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_272_real.mp4"
    },
    {
        "time": "0:09:40 - 0:09:50",
        "captions": "[0:09:40 - 0:09:50] [0:09:40 - 0:09:50]: The video shows a first-person perspective of a computer screen displaying a game settings menu. In the background, it appears to be a vehicle dashboard visible through a semi-transparent overlay. The menu lists various key bindings under the \"ACTION\" column, including indicators for turning on and off lights and toggling high beams. Each action is associated with a specific key listed under the \"PRIMARY\" column. There is a small webcam overlay in the top left corner showing a person, possibly the player, with some animated icons and text overlays on the right side of the screen. The person appears to be concentrating, possibly reviewing the game settings. Additional graphical elements include a small image of a dog's face near the bottom left of the menu. The background outside the game settings appears to be in shades of green and dark, indicating a night scene inside the game environment. The person in the webcam occasionally looks around, possibly interacting with the game or stream on another monitor.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the small image of a cat's face located right now?",
                "time_stamp": "00:09:50",
                "answer": "A",
                "options": [
                    "A. Top right of the menu.",
                    "B. Bottom left of the menu.",
                    "C. Center of the menu.",
                    "D. Top left of the menu."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_272_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:10]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following best summarizes the actions taken just now?",
                "time_stamp": "0:00:10",
                "answer": "D",
                "options": [
                    "A. The individual cleaned the kitchen floor using a mop and then organized the countertop.",
                    "B. The individual prepared ingredients, chopped vegetables, and placed them into a frying pan.",
                    "C. The individual unpacked a delivery box, organized the contents, and disposed of the packaging.",
                    "D. The individual washed their hands, applied soap, and rinsed thoroughly under running water."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_375_real.mp4"
    },
    {
        "time": "[0:00:49 - 0:00:59]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following best summarizes the actions taken just now?",
                "time_stamp": "0:00:59",
                "answer": "D",
                "options": [
                    "A. The individual washed potatoes, chopped them, and started cooking.",
                    "B. The individual peeled potatoes, rinsed them, and placed them in a container.",
                    "C. The individual sorted potatoes, discarded the bad ones, and organized the rest in a basket.",
                    "D. The individual emptied a sack of potatoes into a sink, discarded the sack, and prepared the workstation."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_375_real.mp4"
    },
    {
        "time": "[0:01:38 - 0:01:48]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following best summarizes the actions taken just now?",
                "time_stamp": "0:01:48",
                "answer": "D",
                "options": [
                    "A. The individual emptied a bag of potatoes into the sink, washed them under running water, and dried them with a towel.",
                    "B. The individual peeled the potatoes, rinsed them under running water, and placed them in a storage container.",
                    "C. The individual sorted the potatoes, removed any that were spoiled, and organized the rest into neat stacks.",
                    "D. The individual placed potatoes into the sink, washed them under running water, and sliced them using a potato slicer."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_375_real.mp4"
    },
    {
        "time": "[0:02:27 - 0:02:37]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following best summarizes the actions taken just now?",
                "time_stamp": "0:02:37",
                "answer": "D",
                "options": [
                    "A. The individual boiled potatoes, mashed them, and added seasoning.",
                    "B. The individual peeled potatoes, chopped them, and placed them into a baking dish.",
                    "C. The individual washed potato slices, rinsed them, and placed them into a pot of water.",
                    "D. The individual sliced potatoes, washed the slices in water, and prepared them for cooking."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_375_real.mp4"
    },
    {
        "time": "[0:03:16 - 0:03:26]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following best summarizes the actions taken just now?",
                "time_stamp": "0:03:26",
                "answer": "D",
                "options": [
                    "A. The individual sliced potatoes thinly, washed them gently under running water, and arranged them on a tray for drying.",
                    "B. The individual poured sliced potatoes into a water bath, agitated them using a strainer, and transferred them to a drying area.",
                    "C. The individual submerged the sliced potatoes in warm water, rinsed them with a colander, and then patted them dry with a cloth.",
                    "D. The individual placed potato slices into a container of water, mixed them vigorously for cleaning, and separated them using a divided compartment."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_375_real.mp4"
    },
    {
        "time": "0:00:00 - 0:00:20",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video begins with a first-person perspective view driving a light blue car on a wide, multi-lane urban street. The sky is slightly pink, indicating either dawn or dusk. There is a large billboard on the right side with the text \"Meteorite.\" Several palm trees line the streets, and buildings, including stores and restaurants, are visible in the background. The scene is generally well-lit with street lamps and vehicle headlights. [0:00:04 - 0:00:06]: The vehicle continues driving forward, approaching an intersection. The billboard for \"Meteorite\" becomes more prominent, while additional buildings and palm trees can be seen lining the sides of the road. Traffic lights are visible ahead. [0:00:07 - 0:00:10]: As the car makes a slight left turn at the intersection, a pedestrian crossing and a traffic signal are observed. There is also an inset webcam overlay on the bottom left corner showing a driver.  [0:00:11 - 0:00:17]: The car proceeds straight along a road flanked by more palm trees and streetlights. A series of smaller store signs and vehicles are visible in the background. The inset video of the driver remains in the bottom left corner of the screen. [0:00:18 - 0:00:20]: The vehicle continues to drive down the street, nearing another building complex with several signs marking different shops. The car is about to turn right into a parking lot. The buildings have various business signs with advertisements. [0:00:21 - 0:00:25]: The blue car enters a parking lot of a shopping center, with multiple storefronts lining the back. There are visible parking spaces and a sidewalk alongside the stores. [0:00:26 - 0:00:28]: The car moves further into the parking lot, approaching the storefronts which are more visible now. Various signs indicating different store names, such as \"click lovers\" and \"Food & Nature,\" are readable. [0:00:29 - 0:00:32]: The car pulls up and parks near the entrance of an electronics store named \"click lovers.\" The driver begins to disembark from the vehicle as the view focuses on the store entrance. [0:00:33 - 0:00:38]: A young man wearing a striped shirt exits the car and walks toward the entrance of the store. The camera follows his movement from behind. [0:00:39 - 0:00:43]: Approaching the entrance of the electronics store, the individual seems to be preparing to enter. Inside the store, some electronic items and shelves are visible. [0:00:44 - 0:00:46]: The person reaches the door, opens it, and steps inside the store. Once inside, the arrangement of products on shelves, a counter, and other customers becomes apparent. [0:00:47 - 0:00:51]: Inside the store, the camera captures a closer look at various electronic items displayed on shelves. The individual walks towards the checkout counter where a few other people are present. [0:00:52 - 0:00:55]: The individual walks up to the counter where another customer is being assisted by a store clerk. The shelves are filled with a variety of electronics, and the store is well-lit with a modern design. [0:00:56 - 0:00:59]: The person stands at the counter where another customer, dressed in white, and the store clerk are engaged in a transaction. More details of the store, including shelves stocked with electronics, are visible in the background. [0:01:00 - 0:01:03]: Another store clerk attends to the customer, and the person engages in browsing items on the counter. The interior of the store showcases neatly arranged products on the shelves and well-organized sections. [0:01:04 - 0:01:08]: The section culminates with the individual, along with other customers, interacting at the checkout counter with the store clerk. The professional layout and inventory are visible throughout the store.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What did the car do just now?",
                "time_stamp": "00:00:13",
                "answer": "C",
                "options": [
                    "A. It stopped at a traffic light.",
                    "B. It turned left at an intersection.",
                    "C. It parked near a shopping center.",
                    "D. It reversed into a parking space."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_277_real.mp4"
    },
    {
        "time": "0:02:40 - 0:03:00",
        "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:45]: The video features a first-person view from inside a car driving on a wide, multi-lane bridge during early evening or dusk. Buildings and streetlights can be seen along the sides of the bridge, and the sky is partly cloudy. There is a second view embedded in the top left corner of the video, showing a person in front of a computer monitor with a gaming setup, potentially live streaming the drive. [0:02:46 - 0:02:51]: The car continues to drive along the road, approaching a traffic light. Other cars are visible ahead, a few red, positioned in the lanes, driving in the same direction. [0:02:52 - 0:02:55]: The car moves further down the road, nearing an intersection and passing additional vehicles. Surrounding buildings begin to look more commercial, and the car keeps to the left lane approaching cars ahead. [0:02:56]: The car reaches the intersection, now next to a red car. Buildings and stores can be seen at the corners of the intersection. [0:02:57 - 0:02:58]: The light remains red at the intersection, and the car prepares to make a turn to the left, following the road. Streetlights already illuminate the area. [0:02:59 - 0:03:00]: The car completes the left turn at the intersection onto a new road. Visible in the distance is a bridge structure ahead, and the surroundings include more industrial buildings and utility poles.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the car doing right now?",
                "time_stamp": "00:02:44",
                "answer": "C",
                "options": [
                    "A. Preparing to make a right turn.",
                    "B. Driving on a multi-lane bridge.",
                    "C. Completing a right turn at the intersection.",
                    "D. Stopping at a red light."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_277_real.mp4"
    },
    {
        "time": "0:05:20 - 0:05:40",
        "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:29]: The video starts with a first-person perspective view of a computer screen displaying a game interface titled \"Trucking Progression\" in an area labeled BONVILLE. Various mission options are listed with details such as destinations, rewards, and expiration times. Each mission display features a similar image of a truck and a snowy landscape in the background. The interface is bordered by a blue frame and there is a chat window on the right side filled with messages and emotes. [0:05:30 - 0:05:34]: The scene transitions to an outdoor environment. The camera focuses on a small group of people. One individual, wearing a striped shirt, stands facing two others. The second person, in a blue hoodie, is holding and looking at a tablet or phone. The setting appears to be a poorly lit industrial area with some buildings and parked vehicles in the background. [0:05:35 - 0:05:37]: The view moves to display a map interface on the previously seen tablet or phone. The map shows a detailed layout of streets and buildings with the current location marked. The background of the map interface is turquoise, highlighting the area it represents. [0:05:38 - 0:05:40]: The perspective shifts back to the outdoor scene. The striped-shirt individual continues to stand with the others. They are gathered around a person wearing a uniform/suit who seems to be writing or reading something. The lighting of the scene remains dim, typical of an early morning or evening setting. The ground is littered with a few scattered objects. The overall atmosphere suggests the individuals might be discussing or preparing for an activity related to the game on the screen shown at the start.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the individual in the blue hoodie doing right now?",
                "time_stamp": "00:05:32",
                "answer": "C",
                "options": [
                    "A. Checking their reflection in a window.",
                    "B. Writing on a piece of paper.",
                    "C. Holding and looking at a tablet.",
                    "D. Adjusting their clothing."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_277_real.mp4"
    },
    {
        "time": "0:08:00 - 0:08:20",
        "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:05]: The video begins with a first-person perspective from what seems to be a video game interface. The screen shows a player driving a large vehicle down a dark street at night. The vehicle has a distinctive red light around its rear. To the left of the screen, a smaller window displays a live feed of a person, possibly the player, while to the right, various chat messages from other players or viewers appear in a vertical list. [0:08:06 - 0:08:09]: The vehicle continues to move forward along the street, approaching a few parked cars on the right side of the road. The surrounding area appears urban with sidewalks and illuminated buildings. The player window remains in view with the person focused on the screen, while the chat continues its activity. [0:08:10 - 0:08:14]: The vehicle decreases speed and seems to be getting ready to park along the sidewalk. The scene becomes clearer, showing more details of the buildings and street lamps. The sky is dark, indicating nighttime. The smaller live feed window and the chat are still visible. [0:08:15 - 0:08:18]: The vehicle comes to a stop next to a building entrance. The name \"grime\" is visible on the side of the vehicle, which is now identifiable as a van. The sidewalk and building entrance are well-lit, emphasizing the nighttime setting. [0:08:19 - 0:08:20]: The van remains stationary as the scene focuses on its placement in front of the building. The person in the small window appears attentive, perhaps planning their next action. The chat continues to scroll with messages. [0:08:21 - 0:08:23]: The player, seemingly the same person from the live feed, exits the driver\u2019s seat and steps out of the van. The camera angle shifts slightly to follow this movement. It's evident that the person in the live feed is controlling the on-screen actions. [0:08:24 - 0:08:26]: The perspective shows the person standing at the side of the van, looking around, indicating they might be assessing the surroundings. The urban environment remains consistent, with street lamps and buildings providing illumination. [0:08:27 - 0:08:30]: The player walks towards the rear of the van. Another character, controlled by a different player possibly, appears on screen and approaches the van from the sidewalk. The live feed and chat continue to display on the screen. [0:08:31 - 0:08:33]: Both characters stand at the rear of the van, where the user interface displays an option to open the back. The characters seem to be interacting with the van's cargo, preparing to unload or retrieve something. [0:08:34 - 0:08:36]: A detailed inventory screen appears, showing various items and storage options labeled Player, Cargo, Backpack, and Ground. The live feed remains visible, showing the person focused on managing the inventory. The chat continues with its active commentary. [0:08:37 - 0:08:45]: The inventory management continues with the player\u2019s interface indicating interactions with different items. The person in the live feed appears concentrated, and various comments and suggestions from the chat are visible. The cargo section highlights the interaction with items possibly stored in the van. [0:08:46 - 0:08:50]: The inventory screen retains focus as the player manages the items, indicating an involved and meticulous process. The live feed and chat maintain their activity, providing a sense of interaction and engagement from the community watching or participating in the game. [0:08:51 - 0:08:55]: The player seems to finalize their inventory management and prepares to exit the inventory screen. The chat activity remains significant, and the person in the live feed appears intent on their task. [0:08:56 - 0:08:59]: The inventory screen starts to close, reverting focus back to the characters standing at the rear of the van. The two characters look like they are ready to proceed, possibly with their next in-game action. [0:09:00]: The characters stand next to the now-closed van. The live feed shows the person expressing a reaction, likely in response to something in-game or from the chat. The chat continues its lively scroll on the right side.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What did the player do just now?",
                "time_stamp": "00:08:20",
                "answer": "C",
                "options": [
                    "A. Exiting the driver's seat.",
                    "B. Driving the vehicle.",
                    "C. Finalizing inventory management.",
                    "D. Interacting with another character."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_277_real.mp4"
    },
    {
        "time": "0:10:00 - 0:10:08",
        "captions": "[0:10:00 - 0:10:08] [0:10:00 - 0:10:02]: Two people are walking on a sidewalk next to a parked dark vehicle at night in a city. One person is wearing a striped shirt and shorts while the other is wearing a blue hoodie and dark pants, leading the way. The city lights illuminate the background, revealing tall buildings. [0:10:02 - 0:10:04]: The person in the striped shirt is turning towards the dark vehicle, while the person in the blue hoodie remains slightly ahead. There's a chat window overlay on the right side of the frame, showing various messages from viewers. [0:10:04 - 0:10:08]: Both individuals stop at the rear of the dark vehicle. The person in the striped shirt looks toward the person in the blue hoodie who is opening the rear door of the vehicle. The interior of the vehicle is not clearly visible, but a light appears to be on inside. The chat window overlay on the right side continues showing messages.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the person in the blue hoodie doing right now?",
                "time_stamp": "00:10:07",
                "answer": "D",
                "options": [
                    "A. Walking on the sidewalk.",
                    "B. Stopping at the rear of the vehicle.",
                    "C. Turning towards the dark vehicle.",
                    "D. Puting the goods into the trunk of the car."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "live_streams",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_277_real.mp4"
    },
    {
        "time": "0:00:00 - 0:00:20",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:06]: A close-up view of a hand holding four round keychains, each with a leopard print design. The keychains are glittery, with two being golden, and one pink. The text \"Caf\u00e9,\" \"Warm,\" and \"Joy\" is written on each keychain respectively. The keychains have small tassels attached to them, hanging from silver/metallic rings. The background is plain white, and there is a logo for \"DIY Craft Tutorials\" in the bottom right corner. [0:00:07 - 0:00:10]: The screen transitions to a dark background displaying the text \"Leopard Glitter Keychains\" in the center. [0:00:11]: The screen turns completely black. [0:00:12 - 0:00:16]: Instructions appear with the heading \"STEP I,\" followed by the step description: \"Upload Cut File to your cutting machine. I'm using a Cricut Joy.\" [0:00:17 - 0:00:18]: The same instruction screen slightly zoomed out, revealing the Cricut software interface with a leopard print keychain design on a grid layout.  [0:00:19 - 0:00:20]: The Cricut Joy software interface now features a leopard print keychain design along with various design options, including a grid pattern and additional design elements like circles and other shapes, suggesting the available tools and choices for customization.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is being shown on the screen right now?",
                "time_stamp": "00:00:05",
                "answer": "D",
                "options": [
                    "A. A dark background displaying \"Leopard Glitter Keychains.\".",
                    "B. Instructions for using a cutting machine.",
                    "C. The Cricut Joy software interface with design options.",
                    "D. A hand holding glittery leopard print keychains."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_65_real.mp4"
    },
    {
        "time": "0:01:40 - 0:02:00",
        "captions": "[0:01:40 - 0:02:00] [0:01:40 - 0:01:41]: From a first-person perspective, a pair of hands, wearing a maroon long-sleeve top, are seen loading a sheet of white material into a Cricut Joy machine. The machine has a turquoise top and white body, positioned on a dark gray, grid-patterned craft mat. [0:01:42 - 0:01:47]: The material is fed into the machine, and the hands are removed from the frame. The machine starts pulling the sheet in. As the sheet progresses, it remains centered within the machine's guides. [0:01:48 - 0:01:49]: The scene darkens slightly as the machine continues to pull in the material. The sheet turns black as it is fed further, indicating a possible transition or new type of material being used. [0:01:50 - 0:01:58]: The machine continues its operation undisturbed. There's a label on the craft mat that reads \"DIY Craft Tutorials\". The black material advances further, occupying almost the entire visible area fed through the machine. [0:01:59]: The scene transitions to a pair of hands holding scissors and cutting the now black sheet material horizontally. The area is well-lit with the grid pattern craft mat forming the background.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What action is being performed right now?",
                "time_stamp": "0:02:00",
                "answer": "D",
                "options": [
                    "A. Loading a sheet into the machine.",
                    "B. Pulling out a sheet from the machine.",
                    "C. Labeling the material.",
                    "D. Cutting the material vertically."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_65_real.mp4"
    },
    {
        "time": "0:03:20 - 0:03:40",
        "captions": "[0:03:20 - 0:03:40] [0:00:32 - 0:00:37]: The video begins with a close-up shot of a hand holding a round, thin, silver-colored stencil with small, irregularly shaped holes. The hand has a light skin tone and is positioned centrally in the frame. The hand is wearing a dark pink, ribbed long-sleeve sweater. The background consists of a grid-patterned mat, mostly grey with white lines creating small squares. [0:00:38 - 0:00:39]: The hand begins to place the round stencil down on the grid-patterned mat next to another identical stencil, which is already resting on the mat slightly to the left. To the top right of the hand, two square white pieces of paper with the stencil design printed on them are visible, placed adjacent to each other. [0:00:40 - 0:00:41]: As the hand continues to place the stencil down on the mat, it aligns the stencil to the left of the two white papers. The right hand moves towards another object situated on the mat, located further right beyond the frame. [0:00:42]: The right hand picks up another round stencil from the mat and brings it towards the center. [0:00:43 - 0:00:45]: The right hand aims to align this new stencil next to the first one, ensuring they are symmetrically placed. The left hand, still holding the stencil, makes slight adjustments. [0:00:46 - 0:00:47]: The right hand picks up a small white rectangular object, which appears to be a label with text printed on it. [0:00:48 - 0:00:49]: The right hand continues to hold the label while moving it closer to the two stencils on the mat. The left hand remains stationary, holding the stencil down. [0:00:50 - 0:00:51]: The label with text is now directly above the stencil held by the left hand. The right hand begins to apply the label onto the stencil, pressing down gently to ensure it sticks. [0:00:52 - 0:00:54]: Both hands work together to firmly apply the label onto the stencil. The right hand rubs the label to ensure it is securely adhered. [0:00:55 - 0:00:57]: The left hand steadies the stencil while the right hand continues to press down on the label. The stencils and white papers remain in their respective positions on the mat. [0:00:58 - 0:00:59]: The right hand continues to secure the label onto the stencil by applying consistent pressure. The left hand stabilizes both the stencil and the label. [0:01:00 - 0:01:02]: The label is now securely adhered to the stencil. The right hand begins to move away, leaving the label in place. [0:01:03]: The right hand moves out of the frame. The left hand, still holding the stencil, begins to adjust its position. [0:01:04 - 0:01:07]: The left hand rotates the stencil slightly to examine the label placement. The right hand returns to the frame momentarily, assisting with the adjustment. [0:01:08 - 0:01:09]: The left hand places the completed stencil back onto the mat beside the first stencil while the right hand picks up a small tool from the side. [0:01:10 - 0:01:11]: The right hand uses the small tool to lift the edge of the applied label, ensuring the adhesive bond is secure. [0:01:12 - 0:01:13]: The right hand carefully peels back a portion of the label from the stencil to check adhesion quality. [0:01:14 - 0:01:15]: After confirming the adhesion, both hands work together to smooth the label back onto the stencil, securing any lifted edges. [0:01:16 - 0:01:17]: The hands shift slightly, giving one final press to the label on the stencil to ensure it is fully attached and smooth. [0:01:18 - 0:01:19]: The right hand then moves away, and the left hand places the stencil onto the grid mat beside the other finished stencil. [0:01:20 - 0:01:21]: The frame changes to show the two stencils side by side on the mat, with the label neatly attached to each, and the white papers still at the top of the frame.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What are the hands doing right now?",
                "time_stamp": "0:03:13",
                "answer": "D",
                "options": [
                    "A. Drawing on a piece of paper.",
                    "B. Washing the stencil.",
                    "C. Lifting the stencil off the mat.",
                    "D. Applying a label to a stencil."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_65_real.mp4"
    },
    {
        "time": "0:05:00 - 0:05:20",
        "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:03]: A pair of gloved hands is seen holding a white object with a dotted pattern and black edges. The object is placed on a white textured cloth laid over a blue surface. There are two small cups, one filled with golden glitter and the other possibly with red glitter or similar colored substance. The left hand holds the white object, while the right hand uses a thin stick to apply a layer of golden glitter.  [0:05:04 - 0:05:17]: The right hand continues to spread the golden glitter evenly over the white object using the thin stick. The glitter is slowly covering the entirety of the surface within the black edges, ensuring it remains inside the boundary without spilling over. The process is meticulous, and the object begins to have a shiny, fully covered golden top.  [0:05:18 - 0:05:19]: The text overlay at the bottom reads, \"Resin domes - take it to the edge and it shouldn't go over,\" indicating guidance on the application technique being used.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the right hand doing right now?",
                "time_stamp": "00:05:17",
                "answer": "D",
                "options": [
                    "A. Applying a layer of red glitter.",
                    "B. Holding a pair of cups.",
                    "C. Removing glitter from the object.",
                    "D. Spreading golden glitter evenly over the white round object."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_65_real.mp4"
    },
    {
        "time": "0:06:40 - 0:07:00",
        "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:41]: Two hands, wearing a purple long-sleeved shirt, are centrally involved in handling a white piece of paper with a transparent adhesive on top. The background is a gray, grid-patterned mat with three round coasters placed horizontally at the upper edge of the frame. The coasters exhibit intricate patterns in green, brown, and multicolored designs respectively, from left to right. [0:06:42 - 0:06:43]: The left hand is pressing on the top part of the adhesive with the pointer finger while the right hand starts pulling the backing off of the adhesive from the bottom. [0:06:44 - 0:06:45]: The right hand continues to pull the adhesive backing upwards while the left pointer finger remains pressing the adhesive down, ensuring it sticks correctly. [0:06:46]: Both hands maintain pressure on the paper and the adhesive as more of the adhesive backing is peeled upwards. [0:06:47]: As the right hand progresses in peeling off the adhesive backing, the paper remains stationary, guided steadily by the left hand\u2019s pressure. [0:06:48 - 0:06:49]: The right hand now starts pulling the adhesive backing horizontally, nearing the end of the peeling process. [0:06:50 - 0:06:51]: The left hand lifts up the piece of paper, showing the freshly applied adhesive surface while the right hand completely removes the adhesive backing. [0:06:52]: The left hand inspects the adhesive application on the paper, holding onto the upper edge, while the right hand holds the removed backing. [0:06:53]: The right hand turns the adhesive backing sideways ensuring it's completely removed, while the left hand holds the now adhesive-applied paper piece and examines it. [0:06:54 - 0:06:57]: The hands work together to remove any excess backing still left, revealing a clean adhesive surface beneath. This clear, freshly adhesive-applied paper is then maneuvered, ready to be utilized for the crafting project. [0:06:58]: The left hand flips the adhesive-applied paper, examining the sticky side, and then both hands prepare to place it down. [0:06:59]: Holding the edges delicately, the left hand places the adhesive-applied paper against one edge of the transparent backing on the left coaster. [0:07:00]: The hands position the paper precisely over the adhesive on the left coaster, ensuring it's aligned well. [0:07:01]: Both hands begin pressing the adhesive-applied paper down on the coaster, ensuring it adheres smoothly. [0:07:02 - 0:07:03]: The left hand continues pressing down while the right hand adjusts the paper's position to ensure adhesion. [0:07:04]: The right hand then lifts the paper to inspect the adhesion on the coaster, revealing a clean application. [0:07:05]: Holding up the coaster now, the hands showcase a successful adhesive application on the round coaster. [0:07:06]: The hands maintain their hold on the coaster, showing the applied adhesive surface is smooth and well-fitted. [0:07:07]: The hands place the next coaster centrally, between the first and last coasters, preparing to apply the next adhesive-applied paper. [0:07:08 - 0:07:09]: The hands swiftly move to press the adhesive-applied paper on the central coaster, pressing down to ensure adhesion is firm and smooth. [0:07:10]: The right hand presses down firmly on the central coaster while the left hand holds the edges, securing the adhesive. [0:07:11]: Both hands then lift the coaster and examine, once more revealing a smooth, well-fitted adhesive application on the central coaster as the video comes to an end.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the hands doing right now?",
                "time_stamp": "0:06:56",
                "answer": "D",
                "options": [
                    "A. Pulling the adhesive backing upwards.",
                    "B. Adjusting the paper's position on the coaster.",
                    "C. Inspecting the adhesive surface on the paper.",
                    "D. Placing the adhesive-applied paper against the golden round object."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_65_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video depicts a wide stone pathway along a waterfront area. Lamp posts line the pathway, which is flanked by a metal railing on the left side. A woman in short denim shorts and a dark top walks ahead, while another person in a green dress walks slightly in front closer to the water. Several outdoor caf\u00e9 tables with white umbrellas are arranged next to a building on the right, which features old, multi-story structures with colorful facades.  [0:00:02 - 0:00:03]: The same two individuals continue walking forward. The person in the green dress carries a small black bag, and the woman in denim shorts walks beside another individual wearing a dark shirt and jeans. The waterfront area reveals more details with additional boats and a small floating dock on the left. [0:00:04 - 0:00:07]: The camera moves forward, showing more of the busy promenade. A few small vehicles, possibly taxis or rideshare services, are parked along the road. Passersby can be seen near the buildings and caf\u00e9s. The sky is partly cloudy with vibrant blue patches. [0:00:08 - 0:00:09]: The video continues to detail the same area, showing a bustling riverside walkway. A woman in a white dress and a man with tattoos are seen walking towards the camera. More people appear to be sitting and walking around the caf\u00e9 tables under large umbrellas. [0:00:10 - 0:00:12]: As the camera advances, the scene continues with views of residential buildings with balconies, lively with people walking and bunches of parked cars on the pathway. The video captures the continuous stream of people engaging in various activities along the waterfront. [0:00:13 - 0:00:14]: One of the cars now appears more prominently, a blue vehicle parked by the path. More people are seen closer to the camera, revealing the mixed bustle of the walkway with multiple vendors and caf\u00e9 attendees. [0:00:15 - 0:00:17]: Ongoing activity around the caf\u00e9 areas continues to be shown, with umbrellas casting shade on the tables. The camera approaches more closely to the blue vehicle. The people from earlier situate themselves nearer to various shop entrances on both sides of the pathway. [0:00:18 - 0:00:20]: The silver vehicle seen earlier has parked along the street now more visibly in front of the caf\u00e9 seating area. Additional people can be seen entering and exiting shops, while the central characters from earlier remain walking down the path. The camera continuously moves forward, capturing more of the street\u2019s length and activities.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the weather like in the video?",
                "time_stamp": "0:00:07",
                "answer": "D",
                "options": [
                    "A. Completely cloudy.",
                    "B. Rainy.",
                    "C. Overcast.",
                    "D. Partly cloudy."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_329_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:49]: The video takes place in a sunny outdoor plaza lined with tall, colorful buildings on the right side. These buildings have red and yellow facades with balconies on the upper floors. On the right, there are various cafes and restaurants with white umbrellas and outdoor seating where people are dining and enjoying the day. The ground is made of stone tiles, and the place is bustling with activity. Down the center of the plaza, various people are walking towards the camera, including individuals in shorts and t-shirts. On the left, there's a street where cars are parked, and some pedestrians are near vehicles. Additionally, there are street lamps and poles along the plaza's edge. [0:02:50 - 0:02:59]: A red phone booth is visible near the right side of the buildings. Multiple groups of people are walking toward and away from the camera. A person in a red shirt and white shorts is holding hands with someone in white. The left side of the frame shows the riverside with boats docked in the water and some people near a metal fence, capturing the lively and picturesque environment of the waterfront. The sky is mostly clear, showcasing a beautiful day.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is visible near the right side of the buildings right now?",
                "time_stamp": "0:02:57",
                "answer": "D",
                "options": [
                    "A. A blue mailbox.",
                    "B. A green bench.",
                    "C. A yellow taxi.",
                    "D. A red phone booth."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_329_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:21]: The video opens with a view of a cobblestone street lined with buildings on both sides. On the left, a tall building with an orange exterior is visible, while the right side features a beige building with several windows. In the foreground, a young man and woman walk past the camera, and in the background, a person wearing a blue outfit and a hat is walking up the street, carrying a shopping bag in one hand. The sky is bright blue with a few wispy clouds. [0:05:21 - 0:05:24]: As the camera moves forward, the young man and woman exit the frame, and more of the background becomes visible. The street ahead has some bollards and a traffic sign, and a red building with white accents can be seen in the distance. A few more pedestrians are moving along the street. [0:05:24 - 0:05:27]: The camera continues to move up the street. The man in the blue outfit and hat continues walking up the street, moving slightly to the right. Additional pedestrians, including a person in a red outfit, are present in the background near the red building.  [0:05:27 - 0:05:32]: The camera reaches the top of the street, approaching the intersection. The red building in the background becomes clearer, showcasing its detailed architecture. Pedestrians are seen crossing the street at the intersection, engaging in various activities. A prominent statue surrounded by a small green area also comes into view, situated in front of the red building.  [0:05:32 - 0:05:35]: More pedestrians walk in front of the statue and green area. The building to the left features classical architectural elements, including columns and a clock tower. On the right side of the frame, the blue-clad individual is seen walking along the sidewalk, now further ahead.  [0:05:35 - 0:05:36]: The camera slightly pans to capture the broader intersection view, displaying more pedestrian activity and traffic movements.  [0:05:36 - 0:05:38]: As the camera moves forward, the main focus remains on the intersection, the statue, and the red building. More detailed elements of the street and architecture are visible, including another structure with a crane on the right, indicating some construction activity.  [0:05:38 - 0:05:40]: The video closes with a clear view of the intersection, the statue, and the surrounding area. The red building remains prominent, with green trees on either side and further pedestrians visible in various parts of the frame. The cobblestone street and the historical architecture create a picturesque scene under the bright blue sky.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the person in the blue outfit carrying?",
                "time_stamp": "00:05:21",
                "answer": "D",
                "options": [
                    "A. A briefcase.",
                    "B. A backpack.",
                    "C. A suitcase.",
                    "D. A shopping bag."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_329_real.mp4"
    },
    {
        "time": "[0:08:00 - 0:09:00]",
        "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:02]: The scene begins with a street view showing a cobblestone road and a stone building on the left. There are three people standing near a traffic light. One person wears a white shirt and shorts, another in a black shirt and shorts, and the third person is walking away from the camera wearing a floral top and holding a bag. The background shows a green tree, street signs, and buildings in the distance. [0:08:03 - 0:08:05]: The person in the floral top keeps moving forward. The other two individuals remain near the traffic light, which is red. The camera pans slightly to the right, showing another tree and part of the green area across the street. [0:08:06 - 0:08:08]: The camera continues to track the person in the floral top, who reaches the edge of the frame as they walk along the street. A person in blue shorts and a white shirt walks towards the camera from the opposite direction on the left side of the frame. The stone building remains prominent on the left side of the frame. [0:08:09 - 0:08:11]: The camera focuses on the left side of the street, capturing the person in blue shorts walking past. The background shows street signs and parked cars along the right side, with a cobblestone sidewalk running parallel to the building. [0:08:12 - 0:08:14]: The person in blue shorts moves out of the frame. The sidewalk on the left side of the street becomes more evident as the camera captures more of the cobblestone. Parked cars remain on the opposite side of the street. [0:08:15 - 0:08:17]: The camera shifts higher up the street, showing more of the sidewalk and cobblestone road. Trees and buildings, including a red structure, appear on the right. A parked vehicle can be seen ahead on the left side of the street. [0:08:18 - 0:08:20]: As the video progresses, the camera continues to show the upper view of the street with the stone building on the left. The viewer can see two parked cars, and a clearer view of the road ahead, with more trees and buildings in the distance.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where was the historical stone building located in the frame right now?",
                "time_stamp": "00:08:09",
                "answer": "A",
                "options": [
                    "A. On the left side.",
                    "B. On the right side.",
                    "C. In the center.",
                    "D. In the background."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_329_real.mp4"
    },
    {
        "time": "[0:09:00 - 0:09:49]",
        "captions": "[0:09:40 - 0:09:49] [0:09:40 - 0:09:48]: The video features a first-person perspective as the viewer walks along a cobblestone sidewalk. To the right, there is a striking bright red building with a large outdoor area enclosed by a matching red fence. The building has signs indicating \"HARD CLUB\" in red letters on a black background. On the left side of the frame, parallel to the sidewalk, a narrow street runs uphill, lined with parked cars and historic buildings with balconies. The buildings on the left are painted in light colors with green railings and have old-fashioned street lamps attached. In the distance, a taller bell tower and other structures are visible. The sidewalk includes bollards, waste bins, a metal post, and a small staircase. The sky appears mostly clear with patches of light clouds.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is the name of the left side establishment indicated by the signs on the building?",
                "time_stamp": "00:09:49",
                "answer": "A",
                "options": [
                    "A. HARD CLUB.",
                    "B. HARD ROCK.",
                    "C. CLUB HARD.",
                    "D. ROCK CLUB."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_329_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins with a black background displaying the word \"And\" in white font. The text transitions smoothly to \"video\" against the same black background.  [0:00:02 - 0:00:05]: As the video continues, the next frames have a blurred background with blue and green shades, indicating an outdoor environment. The text on the screen reads, \"I've decided to build something that I've never seen built before.\" [0:00:05 - 0:00:06]: The background remains blurry, showing hints of greenery and a bright sky. The text changes to \"Well I say built but it's mostly...\" [0:00:07]: The scene shifts to a clear view of a character in a blocky, pixelated world, wearing blue armor and holding a purple and yellow item. The character stands on a wooden platform with wooden barrels on both sides. Behind the character, there is a hilly area with trees and a wooden structure. [0:00:08 - 0:00:09]: The view shifts slightly, with the character now standing closer to the edge of the wooden platform. The background includes a vast body of water with the sun near the horizon, indicating either sunset or sunrise. The hills and trees are still visible in the background. [0:00:10 - 0:00:11]: The perspective remains similar with minor adjustments in the character's position. The sun's position and the coloration of the sky suggest it is either early morning or late evening. [0:00:12]: The camera now focuses on the wooden structure's entrance, showing detailed wooden planks, a hanging lantern beside the door, and a barrel to the left. The character's hand is visible, holding what appears to be a yellow and white item. [0:00:13]: The camera pans to the right, capturing more of the scenic view. The sun is still near the horizon, casting a warm glow over the water and land. The lush greenery beside the water contrasts with the brown earth and blue water and sky. [0:00:14 - 0:00:15]: The scene transitions with \"Objectives:\" text appearing against the blurred background, followed by \"Get a bunch of TNT\" in red text.  [0:00:15 - 0:00:16]: The view returns to a clear scene of the water and sky. The sun's light remains prominent, and the surroundings maintain their vibrant colors with a mix of green, blue, and brown. [0:00:17 - 0:00:18]: The scene shifts indoors, showing a room with wooden walls and stone flooring. Various items like a brewing stand, a crafting table, and a brick structure are visible. [0:00:19]: The camera angle adjusts to reveal more of the interior. Additional elements such as chests, an anvil, and other assorted blocks can be seen in the background, indicating a well-equipped room. [0:00:20]: The video concludes with a close-up of an inventory screen in a chest. Various items, including weapons, armor, and resources, are neatly arranged. One item is highlighted as \"Golden Horse Armor.\"",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is one of the objectives mentioned in the video?",
                "time_stamp": "0:00:20",
                "answer": "B",
                "options": [
                    "A. Build a house.",
                    "B. Get a bunch of TNT.",
                    "C. Collect water.",
                    "D. Find a treasure."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "minecraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_197_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "captions": "[0:03:00 - 0:03:20] [0:03:01 - 0:03:02]: The video opens with a view of a grassy terrain featuring a large tree prominently in the center. The landscape includes patches of grass and upturned soil. A wooden crafting table lies to the right of the tree, while in the background, a body of water stretches out towards the horizon with distant hills covered with trees visible;  [0:03:02]: The camera is now directed upwards, capturing a large wooden disc-like structure suspended in the sky, with small flames observable in the center of the underside. A single tall pole appears to support the structure, extending down into the ground out of view. The word \"well\" is visible in a black and white outlined font at the bottom of the frame;  [0:03:03 - 0:03:10]: The initial angle changes, providing another view of a large tree with a wooden crafting table at its base. The background contains a body of water, and the camera then pans upwards, continuing to focus on the disc-shaped structure from various angles, emphasizing its broad circumference and the small amount of smoke emanating from its center. A stick-like pole remains as a support, and patches of green bamboo appear along the right edge of the frame; [0:03:10 - 0:03:11]: The camera zooms in towards the central part of the underside, showing an array of flames that give off a light, fiery glow. The fire's base extends towards a platform made out of different materials below the disc;   [0:03:12 - 0:03:15]: With a clear sky as a backdrop, the camera shifts to an upward-facing angle, capturing the top of the large tree and the long pole disappearing into its foliage. The bright, clear sky provides a stark contrast to the tree\u2019s dark green leaves. There are clouds scattered across the sky; [0:03:15 - 0:03:17]: The angle switches back to ground level, where the grassy terrain and various structures such as bamboos, a bright yellow bed, and the large disc-shaped structure suspended on the pole are in view. A small crafting table is once again present on a raised patch of land. The camera then zooms into a black and white inventory screen, showing various items neatly categorized; [0:03:17 - 0:03:20]: The camera shifts back to the ground level, displaying the green grassy fields spreading towards the horizon, a bed, and a few torches placed at intervals. The words \"but that should not be a problem\" appear at the bottom of the screen, creating a concluding statement.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What object is positioned to the left of the large tree?",
                "time_stamp": "0:03:00",
                "answer": "A",
                "options": [
                    "A. A wooden crafting table.",
                    "B. A stone statue.",
                    "C. A blue tent.",
                    "D. A metal bench."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Attribute Recognition",
                "question": "What color is the bed seen in the video?",
                "time_stamp": "0:03:32",
                "answer": "A",
                "options": [
                    "A. Yellow.",
                    "B. Red.",
                    "C. Blue.",
                    "D. Green."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "minecraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_197_real.mp4"
    },
    {
        "time": "[0:09:00 - 0:10:00]",
        "captions": "[0:09:00 - 0:09:20] [0:09:00]: The video begins with a view of a bright blue sky scattered with small, white clouds. The camera points towards a distant, elevated terrain, displaying a rugged, brown hill covered with sparse patches of green foliage. The foreground includes a HUD (Heads-Up Display) at the bottom, featuring health, tools, and inventory items such as a sword and pickaxe. [0:09:01]: The perspective shifts to the right, revealing more details of the landscape. The player stands adjacent to a tall, thick tree on the left, partially obscuring the view. The grass is yellowish-green and the terrain includes a cliff dropping to a lower level. [0:09:02 - 0:09:03]: As the player moves forward, a large cliff face becomes more prominent in the frame. The cliff face has a unique, square-hole formation resembling a pixelated face carved into the stone. The surrounding area consists of extensive brown and gray rocky surfaces. [0:09:04]: The camera angle adjusts slightly to provide a clearer view of the entire cliff. The rugged terrain below is complemented by a blue lake visible on the left side, bordered by rough, uneven ground. [0:09:05 - 0:09:06]: The scene remains focused on the mountainside, with the carved-out square-hole formation now entirely visible. The landscape around consists of mixed green and brown patches of rock and vegetation. [0:09:07 - 0:09:08]: The perspective changes subtly, keeping the focus on the detailed cliff and distant terrain. The top of the cliff shows bare stone with occasional green patches and a line of trees is visible at the horizon. [0:09:09 - 0:09:12]: The player\u2019s viewpoint slowly tilts downward, scanning the steep slope leading towards a large, calm lake. The water is deep blue and clear. The adjacent terrain features small patches of greenery among extensive rocky ground. [0:09:13]: The pace of the footage slows as the camera remains fixed on the detailed, jagged rocks leading down to the lake. The inventory bar at the bottom of the screen stays visible. [0:09:14]: The scene captures the wide expanse of the lake and surrouding landscape. Details of the rocky ground near the water\u2019s edge stand out, emphasizing the ruggedness of the environment. [0:09:15 - 0:09:16]: The perspective remains unchanged, allowing more time to observe the mixture of rocky and green surfaces throughout the scene. The player does not move, maintaining focus on the landscape. [0:09:17]: The camera tilts slightly to capture more of the rocky terrain in the foreground. The lake remains a central feature, its clear blue water contrasting with the rocky banks and cliffs around it. [0:09:18]: Without changing the overall scene, the focus slightly shifts more towards the rocky foreground, emphasizing the detailed texture of the rocks and small vegetation patches. [0:09:19]: The last frame of the video focuses on the same area, with the rugged cliffs and calm, expansive lake remaining the focal points. The HUD is still visible, indicating no change in the player\u2019s status or inventory items.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What inventory items are visible in the player's HUD?",
                "time_stamp": "0:09:19",
                "answer": "A",
                "options": [
                    "A. Sword and pickaxe.",
                    "B. Sword and shield.",
                    "C. Axe and bow.",
                    "D. Pickaxe and bow."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "minecraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_197_real.mp4"
    },
    {
        "time": "[0:11:00 - 0:11:48]",
        "captions": "[0:11:40 - 0:11:48] [0:11:39 - 0:11:40]: The video opens with a view of a gaming environment featuring a spacious room with wooden flooring and beams. A prominent crystal structure stands in the center emanating a blue light, anchored in a circular pool of water surrounded by a stone border. To the left, an ornate purple portal-like structure made of amethyst sits above stone stairs, contrasting with the sturdy wooden chests aligned against the wall on the right. [0:11:41]: The perspective shifts slightly to display more of the left side, including a character wearing bright diamond armor and wielding a sword. Behind the character, more details of the stone brick walls are visible, adding texture to the otherwise primarily wooden room. [0:11:42]: The viewpoint nudges to the right, showcasing the character holding a sword, now featuring an additional view of the trimmed wooden staircase leading downward and more of the aligned chests. [0:11:43 - 0:11:44]: The scene doesn't change much, but the position indicates slight movement, reinforcing the wooden and stone aesthetic, with minute adjustments highlighting the room's intricacies and arrangement of storage chests. [0:11:45 - 0:11:46]: Attention is drawn towards the center character, still in view, standing sturdily atop the stairs with a clearer view of the background, which includes additional walls and wooden textures. [0:11:47]: A \"Subscribe\" button animation overlays on the screen, symbolizing a typical call-to-action found in many online video recordings, below the character who remains the focus. [0:11:48]: The \"Subscribe\" button transitions to a clicked state with animation, indicating interaction, while the character maintains its pose, the room's details surrounding them with a blend of wood and stone. [0:11:49]: The \u201cSubscribe\u201d button changes to \"Subscribed,\" completing the animation, as the character stands stalwartly framed within the stone-and-wood detailed background, illustrating a completed interaction. [0:11:50]: The final frame highlights the character for one last second with no significant changes to the background or surrounding details, reinforcing the consistent structure and aesthetic of the scene as the video ends.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What action is depicted with the \"Subscribe\" button?",
                "time_stamp": "00:11:49",
                "answer": "C",
                "options": [
                    "A. It remains static.",
                    "B. It disappears.",
                    "C. It changes to \"Subscribed\".",
                    "D. It moves to the top of the screen."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "minecraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_197_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:05]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What type of car is featured in the video right now?",
                "time_stamp": "00:00:10",
                "answer": "D",
                "options": [
                    "A. Ferrari F8 Tributo.",
                    "B. Lamborghini Huracan.",
                    "C. BMW M4.",
                    "D. Porsche 911 GT3 RS."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_262_real.mp4"
    },
    {
        "time": "[0:01:25 - 0:01:30]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is the current gear level of the car?",
                "time_stamp": "00:01:28",
                "answer": "D",
                "options": [
                    "A. 1.",
                    "B. 2.",
                    "C. 3.",
                    "D. 4."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_262_real.mp4"
    },
    {
        "time": "[0:02:50 - 0:02:55]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "Right now, what is indicated by the green column?",
                "time_stamp": "00:02:52",
                "answer": "A",
                "options": [
                    "A. Braking.",
                    "B. Tire pressure.",
                    "C. Speed.",
                    "D. Throttle."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_262_real.mp4"
    },
    {
        "time": "[0:04:15 - 0:04:20]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Right now, what gear is the car using?",
                "time_stamp": "00:04:20",
                "answer": "D",
                "options": [
                    "A. 3.",
                    "B. 4.",
                    "C. 6.",
                    "D. 5."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_262_real.mp4"
    },
    {
        "time": "[0:05:40 - 0:05:45]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Right now, what is the current speed of the car?",
                "time_stamp": "00:05:40",
                "answer": "D",
                "options": [
                    "A. approx 226 km/h.",
                    "B. approx 189 km/h.",
                    "C. approx 190 km/h.",
                    "D. approx 249 km/h."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_262_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video starts with a black screen;  [0:00:02 - 0:00:03]: A residential street is visible on a clear, sunny day. A black car is parked on the right side of the road, facing away from the camera. To the left is a grassy area leading up to a house with a white exterior roof. Several houses and trees line the street;  [0:00:04 - 0:00:06]: The camera perspective advances along the sidewalk, showing additional details of the houses, trees, and greenery on the left side. The black car remains on the right side;  [0:00:07 - 0:00:08]: The camera continues to move along the street. A tall evergreen tree stands prominently in the middle. The sidewalk curves slightly as the camera moves forward;  [0:00:09 - 0:00:10]: The view of the residential area remains consistent. The street is slightly inclined, with more homes visible along the left side. A white car is parked further ahead on the opposite side of the road;  [0:00:11 - 0:00:13]: As the camera proceeds, more of the neighborhood comes into view, displaying a receding grassy area and a light blue house to the left. The white car approaches closer in the frame;  [0:00:14 - 0:00:16]: The camera angle progresses, showing a street sign on a green lamppost and more greenery. The white car is now quite near;  [0:00:17 - 0:00:19]: The view shows the white car in detail, along with more hedges and another house on the left. The grassy area extends towards more houses in the distant background. The sky is a clear blue;  [0:00:20]: The camera is now close to the house with vibrant green bushes on the left, while the white car remains parked in front of it, facing the camera direction.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What notable feature is seen in the middle of the street as the camera moves forward?",
                "time_stamp": "0:00:08",
                "answer": "A",
                "options": [
                    "A. A tall evergreen tree.",
                    "B. A parked truck.",
                    "C. A street vendor.",
                    "D. A bicycle lane."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_302_real.mp4"
    },
    {
        "time": "[0:01:00 - 0:02:00]",
        "captions": "[0:01:20 - 0:01:40] [0:01:20 - 0:01:21]: The camera captures a tall, dense, conical evergreen tree located in a lush garden. In the background, there is a two-story house with large windows and light-colored siding. The sky is clear and blue. [0:01:21 - 0:01:24]: The view changes as the camera moves forward, revealing more of the garden and house. Another evergreen tree comes into view, and the house's roof is more visible. [0:01:24 - 0:01:27]: The camera continues to move, passing by manicured shrubs and bushes with varying shades of green. The path appears to lead down and to the left side of the frame, indicating the cameraman is walking around the property. [0:01:27 - 0:01:29]: The camera approaches an area with a stone pathway flanked by garden beds containing various plants. To the left, houses alongside a road become visible. The sidewalk runs parallel to the street. [0:01:29 - 0:01:31]: The camera moves down a residential sidewalk shaded by large trees, with more greenery on either side and occasional garden beds. Several parked cars are visible along the street, and the road curves gently to the left. [0:01:31 - 0:01:36]: As the camera advances along the path, it captures foliage on the right and the sidewalk on the left. Trees lining the sidewalk provide a canopy of green above, and the sunlight filters through the leaves. [0:01:36 - 0:01:37]: A larger property with a blue house and a well-maintained garden comes into view. The blue house has multiple windows, a grey-fenced yard, and a large shaded area under a tree. [0:01:37 - 0:01:39]: The camera gets closer to the right-side property. The blue house and its detailed architecture dominate the frame, including its prominent windows and a side entrance. Green bushes obscure part of the scene, adding to the lush environment.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What does the camera capture after showing the manicured shrubs and bushes?",
                "time_stamp": "0:01:33",
                "answer": "A",
                "options": [
                    "A. A stone pathway flanked by garden beds.",
                    "B. A wooden bridge over a stream.",
                    "C. A large fountain in a central plaza.",
                    "D. An open field with wildflowers."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_302_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:42]: The scene is a sidewalk beside a grassy lawn, bordered by a hedge on the right and a row of parked cars on the left. Tall conifer trees are adjacent to the sidewalk, their dense foliage dominating the right side of the view. The sky is clear and blue. [0:02:43 - 0:02:45]: The sidewalk stretches forward, lined by the hedge and trees on the right, while the row of parked cars continues on the left. The dense hedge partially reveals a house and more greenery further along the path. [0:02:46 - 0:02:48]: As the camera moves forward, the hedge on the right begins to thin, revealing more of the house and additional plants behind it. The parked cars are still visible on the left side of the sidewalk. [0:02:49 - 0:02:51]: The sidewalk continues straight ahead with the thinning hedge on the right. More parts of the house and its landscaped garden become visible. Trees still border the path on the left, and the parked cars remain in view. [0:02:52 - 0:02:53]: Now past the dense part of the hedge, a more open view of the garden and houses on the right side is clear. The leafy branches of a tree hang above, partially shading the sidewalk. [0:02:54 - 0:02:56]: The camera captures a clear view of the house and part of its surrounding garden. The sidewalk is bordered by trimmed grass, progressing towards the back of the houses.  [0:02:57 - 0:02:59]: The view becomes more residential, with clear sights of multiple houses and their well-maintained gardens. The sky remains bright and cloudless, enhancing the vibrant greenery surrounding the houses.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What becomes visible past the dense part of the hedge now?",
                "time_stamp": "0:02:53",
                "answer": "C",
                "options": [
                    "A. A park.",
                    "B. A row of shops.",
                    "C. The garden and houses.",
                    "D. A busy street."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_302_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:02]: The video begins on a bright, sunny day with a clear blue sky. The scene is a quiet suburban area with trees and bushes offering some greenery. To the left, there is a paved road bordered by a gravel shoulder. The background includes mountains visible under a cloudless sky.  [0:04:02 - 0:04:05]: A white car is parked on the gravel shoulder of the road. The road curves slightly to the left. There are noticeable cracks and patches on the pavement. The residential area includes houses partially obscured by trees and bushes lining the road.  [0:04:05 - 0:04:08]: As the camera moves forward, the white car becomes more prominent, parked along the right side of the road. On the left, a house surrounded by tall trees and bushes is visible. The camera appears to be moving closer to the parked car, with shadows cast by the greenery and the trees alongside the road. [0:04:08 - 0:04:11]: The car parked has blue stripes on the hood. The driver\u2019s side of the car, along with the front windshield and license plate, are visible. Further down the road, another vehicle, possibly a van, is seen in the distance. The path along the road is lined with overgrown grass and shrubbery. [0:04:11 - 0:04:14]: Approaching the car, the camera view gets closer to the car\u2019s front. The road continues straight ahead, while the house on the left is partly visible, surrounded by more trees and shrubs. The parked car's side mirrors and part of the house's driveway appear. [0:04:14 - 0:04:17]: The camera now passes by the white car's front and proceeds forward along the sidewalk. Another vehicle can be seen more clearly up the road. Bushes and long grass are on the left side, and the overgrown pavement edge is visible. [0:04:17 - 0:04:20]: The camera continues along the sidewalk, moving past the white car. Tall trees, bushes, and grass line the way, and the vehicle in the distance becomes more prominent. The van parked further up the road looks more detailed as the perspective advances. The shadow of the trees falls across the pavement, with beams of sunlight shining through.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What distinguishes the car closest to the camera now?",
                "time_stamp": "00:04:07",
                "answer": "D",
                "options": [
                    "A. It has red stripes on the hood.",
                    "B. It has green stripes on the hood.",
                    "C. It has yellow stripes on the hood.",
                    "D. It has blue stripes on the hood."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Spatial Understanding",
                "question": "What is the position of the house relative to the white car?",
                "time_stamp": "00:04:13",
                "answer": "C",
                "options": [
                    "A. To the right, behind the car.",
                    "B. To the left, in front of the car.",
                    "C. To the left, beside the car.",
                    "D. To the right, beside the car."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_302_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins with a dark and chaotic background primarily composed of textures that appear to be multi-colored, with dominant hues of purple, black, and white. It implies a galaxy or space-like theme with irregular patterns.  [0:00:02 - 0:00:05]: As the video transitions, a bright and colorful logo appears prominently in the center of the frame. The logo has a yellow background with a red and blue arrow design pointing to the right. The text \"GRAVITY THROTTLE RACING\" is displayed in white uppercase letters. [0:00:06]: The scene shifts to a miniature model landscape featuring a desert terrain with a highway running diagonally from the bottom left corner to the top right. The backdrop consists of blue sky and mountainous terrain. A yellow and black model car is visible on the highway.  [0:00:07 - 0:00:08]: A model train appears on the left side of the frame, traveling on tracks parallel to the highway. The train is black with a lit front light and multiple cars following behind. [0:00:09 - 0:00:11]: The train continues to move past the viewer, revealing additional cars including red and white cargo containers. The scene maintains the desert and mountain backdrop. [0:00:12]: The train is now out of the frame, with the focus shifting back to the empty highway and railway tracks. The painted mountains and sky dominate the background.  [0:00:13]: The scene captures a bird's-eye view of a miniature airplane approaching a runway. The plane is white with a red stripe near the end of each wing. Below, various yellow and white model vehicles are featured near the start of the runway. [0:00:14 - 0:00:18]: The airplane is seen descending towards the runway, aligning itself with the landing path marked by painted lines. The runway extends forward into the desert terrain, framed by the railway tracks on the right side and a few scattered model buildings. [0:00:19]: The airplane touches down on the runway, continuing to move along the centerline. The final frame captures the airplane's tire marks on the runway with the surrounding desert terrain still in view.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What happens to the airplane as it aligns itself with the landing path?",
                "time_stamp": "0:00:20",
                "answer": "C",
                "options": [
                    "A. It takes off.",
                    "B. It crashes.",
                    "C. It lands on the runway.",
                    "D. It continues to fly in circles."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_496_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:07]: The video is set on a miniature snow-covered race track, featuring five toy cars ready to race. The cars are positioned at the starting line, which is directly in front of a gate with three lights. The cars in their respective lanes are as follows: a purple car in the first lane from the left, a green car in the second lane, a silver car with black stripes in the third lane, a red car in the fourth lane, and an orange car in the fifth lane. The track consists of five winding lanes with noticeable elevation changes. The environment resembles a mountainous snowy landscape, painted on the backdrop and the terrain. [0:04:08 - 0:04:09]: The race begins as the cars leave the starting line, speeding down the incline. The red car takes an early lead, followed by the silver and green cars. The purple car follows closely behind, with the orange car trailing. They navigate through a curved section of the track, maintaining their respective positions. [0:04:10 - 0:04:12]: The camera viewpoint shifts, showing the cars from behind as they continue on the snowy track. They navigate another set of curves and elevations as the scenery changes to a more mountainous backdrop, with snow-covered trees lining the track. The red car still leads, with a slight gap between it and the silver car. The other cars follow at close distances. [0:04:13 - 0:04:16]: The cars speed along the track with the red car maintaining the lead. The perspective shifts to various angles showing the winding path of the track carved into the snowy hillside. The scenery shows cliffs and snow-covered pines, adding to the sense of speed and motion. The silver car gains some ground on the red car, while the green car stays in third place, and the purple and orange cars race closely behind them. [0:04:17 - 0:04:19]: The track meanders through a more rugged part of the miniature landscape with brown rocky terrain and sparse vegetation. The red car still leads but is now followed very closely by the silver car, which almost catches up. The green car maintains its third position but starts to fall slightly behind the leading two. The purple and orange cars continue to follow through the winding path. [0:04:19]: The race progresses as the cars approach a tunnel near the end of the track. The red car enters first, with the silver car close behind. The green car is third, with a slight distance from the tunnel entrance. The purple and orange cars approach the tunnel last. The surroundings include varied terrain, from rocky hillsides to a train track course winding alongside the race track.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "Which car won first place in this competition?",
                "time_stamp": "0:04:35",
                "answer": "C",
                "options": [
                    "A. The blue car.",
                    "B. The dark purple car.",
                    "C. The green car.",
                    "D. The red car."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_496_real.mp4"
    },
    {
        "time": "[0:08:00 - 0:09:00]",
        "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: The video starts with a snowy landscape featuring small trees scattered around. There are winding roads with a gray car, red car, and purple car driving on them. The cars are moving from the right side of the frame towards the left. [0:08:01 - 0:08:02]: The camera angle shows a sharp turn in the road, with the three cars still visible. The snowy terrain has become more elevated. [0:08:02 - 0:08:03]: The camera focuses on a cliff edge with a red car leading on a curved road. The road is bordered by red and white checkered barriers. The background is snowy with green trees. [0:08:03 - 0:08:04]: The red car travels along a road that curves steeply downhill. The landscape shifts to include more rocky terrain and sparse vegetation. [0:08:04 - 0:08:05]: The red car continues down the winding road, now passing over a bridge. There's a drop below to another level of road and some green foliage. [0:08:05 - 0:08:06]: From this elevated perspective, the red car is proceeding along the road which curves tightly. Other cars can be seen ahead, navigating the same winding path. [0:08:06 - 0:08:07]: The red car is seen from a higher angle, descending along the curve with a purple car following closely behind. The terrain includes more greenery. [0:08:07 - 0:08:08]: The red car continues to descend, with train boxcars visible below next to the road. The road curves sharply alongside the hill. [0:08:08 - 0:08:09]: The red car approaches a tunnel that cuts through a rock formation. A train, including boxcars, is traveling next to the road. [0:08:09 - 0:08:10]: The perspective shifts to a desert landscape. The red car exits the tunnel and continues to descend the winding path. The scene includes scattered small trees. [0:08:10 - 0:08:11]: The red car travels on a road parallel to the moving train. There are various types of cargo cars on the train. The background remains a desert landscape. [0:08:11 - 0:08:12]: The red car drives on a two-lane marked road, alongside the train. The background shows a painted mountainous landscape. [0:08:12 - 0:08:13]: The red car continues to drive on the road which is now further along the railway. The desert terrain features green bushes and rocky outcrops. [0:08:13 - 0:08:14]: The road leads the red car past a sign indicating a turn, where several cars are seen parked near a hangar. An airplane is on the ground beside the hangar. [0:08:14 - 0:08:15]: The car passes by the hangar area and airplane, continuing on the road. The landscape features high rock formations in the background. [0:08:15 - 0:08:16]: The view shows the red car driving past another train. The road curves around a large rocky hill in the desert. [0:08:16 - 0:08:17]: The red car continues descending, passing by the rocky hill. More vehicles are seen travelling around the hill on winding roads. [0:08:17 - 0:08:18]: The camera focuses on a section of the rail track as the perspective zooms out. The train cars continue to be visible. [0:08:18 - 0:08:19]: The red car drives under a bridge structure, closely followed by a purple car. The road is still part of the desert landscape. [0:08:19 - 0:08:20]: The road continues straight, with the red car leading ahead of the purple car. Sparse bushes and small trees are seen on both sides of the road.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "Which car won first place in this competition?",
                "time_stamp": "0:08:23",
                "answer": "D",
                "options": [
                    "A. The blue car.",
                    "B. The dark purple car.",
                    "C. The green car.",
                    "D. The red car."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_496_real.mp4"
    },
    {
        "time": "[0:12:00 - 0:13:00]",
        "captions": "[0:12:00 - 0:12:20] [0:12:00 - 0:12:05]: The video begins with a view of a small, snowy racetrack from a first-person perspective. Four toy cars, a green SUV, a silver sports car, a red sports car, and a purple SUV, are positioned at the starting line. The racetrack has various lanes, each designated for one car. The starting lights are red; the green and white checkered starting banner is visible above the lanes. The scene is set against a blue background, with snow banks on each side of the racetrack and a mountainous background. [0:12:05 - 0:12:07]: The starting lights turn from red to green, signaling the start of the race. The toy cars begin to move off the starting line, with the purple SUV in the lead, followed closely by the red and silver sports cars, while the green SUV is slightly behind. The racetrack is winding and continues downward. [0:12:08 - 0:12:11]: The toy cars move along the racetrack, maintaining a relatively close distance. The paths are curving, with the purple SUV slightly ahead on a straight section. The snowy track is surrounded by snow banks, and the scene indicates more curves ahead. [0:12:12 - 0:12:13]: As the cars continue, they encounter more winding sections of the track. The purple SUV leads, followed by the red sports car, while the silver sports car and green SUV follow closely behind. Snow-covered hills and miniature evergreen trees are part of the background, creating a wintery landscape. [0:12:14 - 0:12:15]: The cars navigate a sharp curve; the purple SUV is ahead, negotiating the bend, followed by the red car closely behind. The silver and green cars follow. The background includes more snow-covered terrain with scattered trees. [0:12:16]: The race continues as the toy cars move into a section with a steep drop and a left bend in the track. The snowy terrain remains, but now there is more visible green vegetation. [0:12:17 - 0:12:19]: The cars navigate the downward and winding path, with the purple SUV still leading. The racetrack now winds around hills with sharp curves; green patches, suggesting a less snowy area, become more prominent. The red sports car tries to close the gap with the leader. The scene includes more diverse scenery with both snow and greenery.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "Which racing car won first place in this competition?",
                "time_stamp": "0:12:35",
                "answer": "B",
                "options": [
                    "A. The blue car.",
                    "B. The silver and red car.",
                    "C. The green car.",
                    "D. The red car."
                ],
                "required_ability": "episodic memory"
            },
            {
                "task_type": "Object Recognition",
                "question": "What is surrounding the racetrack right now?",
                "time_stamp": "0:12:03",
                "answer": "C",
                "options": [
                    "A. Sand dunes.",
                    "B. Tall grass.",
                    "C. Snow banks.",
                    "D. Rocky cliffs."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_496_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video begins with a black screen.   [0:00:01 - 0:00:03]: In an elegant room with classic wooden furniture and ornate decorations, a man in a white shirt and white pants stands near a closed white door. There are two lit candelabras on small tables, one on either side of the door, with a small dresser between them. The man seems to be positioned slightly to the right of the room, with another man in a dark suit standing to his left.   [0:00:04 - 0:00:06]: The first man, who has a distinctive handlebar mustache, looks forward before turning his head slightly downward. His hair is slightly tousled, and he seems to be in thought. The background includes a large framed painting of a person. The second man is partially visible, standing close to the first man.   [0:00:07]: The man in the white shirt starts to extend his arms forward as if preparing to put on a garment. The second man, dressed in dark clothing, offers an open black jacket.   [0:00:08 - 0:00:09]: The man in the white shirt reaches for the jacket, held by the second man. Their interaction suggests an exchange or assistance with dressing.   [0:00:10 - 0:00:13]: The man in the white shirt moves his hands through the sleeves of the jacket, beginning to put it on. The second man continues to assist, ensuring the jacket is correctly positioned. Both are focused on the task.   [0:00:14 - 0:00:16]: Close-up shots focus on the man in the white shirt adjusting his shirt and jacket. Details of the fabric and sleeve buttons are highlighted, as he ensures a proper fit.   [0:00:17 - 0:00:19]: The man completes the adjustments, straightening his attire and securing the final touches of his jacket. The camera captures the practiced movements and fabric texture as he smooths down the clothing.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the distinctive feature of the man's appearance?",
                "time_stamp": "0:00:05",
                "answer": "B",
                "options": [
                    "A. A scar on his cheek.",
                    "B. A handlebar mustache.",
                    "C. A monocle.",
                    "D. A bow tie."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "getting_dressed",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_153_real.mp4"
    },
    {
        "time": "[0:01:00 - 0:02:00]",
        "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:04]: There is a man with light brown hair wearing a dark suit, visible from a side profile. He stands in a room, slightly bent over, with a serious expression on his face. He is in front of a painting with a golden frame, and there are lit candles on a candelabrum on a piece of furniture below the painting. The room has light-colored walls, with a white door visible to the right. [0:01:05 - 0:01:06]: The man's hands and lower body are seen as he bends over, adjusting his dark trousers over highly polished black shoes. The floor appears to be covered with a patterned rug in tones of red and orange. [0:01:07 - 0:01:10]: The man continues to adjust his trousers and shoes, now sitting on a chair. His hands are positioned on his knees, and a part of his white shirt with a hint of a cuff is visible. [0:01:11 - 0:01:15]: The man stands and a second person, only partially visible, helps adjust the first man's shirt cuff. Both individuals appear to be dressed in formal attire. The second person's hands, also in dark sleeves, help to fasten the cuff link on the first man's sleeve. [0:01:16 - 0:01:19]: The first man is seen from a slightly wider view, now standing fully dressed in formal attire. A second man, younger with darker hair and a mustache, is also fully visible, dressed in a white shirt with suspenders. He stands in front of the candles and a piece of furniture with a red curtain to the left side of the frame. The first man turns and starts walking away to the left, while the second man stands still. The room's detailed decor, including the ornate furniture and the rug, are clearly noticeable.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What item is located below the painting with the golden frame?",
                "time_stamp": "00:01:04",
                "answer": "C",
                "options": [
                    "A. A bookshelf.",
                    "B. A table with a vase.",
                    "C. A candelabrum.",
                    "D. A mirror."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Action Recognition",
                "question": "What is the man doing while sitting on a chair?",
                "time_stamp": "00:01:10",
                "answer": "C",
                "options": [
                    "A. Reading a book.",
                    "B. Polishing his shoes.",
                    "C. Adjusting his trousers and shoes.",
                    "D. Writing a letter."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "getting_dressed",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_153_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:02]: In a well-furnished room with classical decor, a man is helping another man with his attire. The man getting assistance stands with his back to a door, while the helper, dressed in a dark coat, adjusts the collar of the man\u2019s white shirt and beige vest. [0:02:03 - 0:02:06]: The camera zooms in, showing a close-up view of the helper\u2019s hands adjusting the man's collar and tie. The man being assisted closes his eyes briefly, suggesting he is comfortable with the situation. [0:02:07 - 0:02:10]: After the closer view, the camera returns to a wider angle showing the helper continuing to make final adjustments to the man's collar. The room's details, like a red curtain, a sizable painting, and partly opened doors are visible again. [0:02:11 - 0:02:12]: The helper finishes adjusting the collar and bow tie. The man being helped has his eyes closed and appears composed and ready. [0:02:13 - 0:02:14]: The man looks down slightly as the attire adjustment is completed. He seems to be examining or feeling the fit of the tie and collar on his neck. [0:02:15 - 0:02:17]: The helper steps back slightly, taking a towel, possibly to dust off or final detailing. They remain in front of the elaborately decorated room, which includes items like candelabras and antique furniture. [0:02:18 - 0:02:20]: The man being helped lifts his arm to perhaps inspect his attire more closely or to make room for any last adjustments. The helper continues to assist with minor detailing as the man begins to move slightly, indicating completion.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color is the vest of the man being helped?",
                "time_stamp": "00:02:20",
                "answer": "B",
                "options": [
                    "A. White.",
                    "B. Beige.",
                    "C. Black.",
                    "D. Gray."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Action Recognition",
                "question": "What does the man being helped do after the helper finishes adjusting his collar and bow tie?",
                "time_stamp": "00:02:15",
                "answer": "D",
                "options": [
                    "A. Looks in the mirror.",
                    "B. Closes his eyes.",
                    "C. Lifts his arm.",
                    "D. Looks down slightly."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "getting_dressed",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_153_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:00:16",
                "answer": "D",
                "options": [
                    "A. 2.",
                    "B. 3.",
                    "C. 4.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            },
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:00:56",
                "answer": "A",
                "options": [
                    "A. 1.",
                    "B. 3.",
                    "C. 4.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_99_real.mp4"
    },
    {
        "time": "[0:01:00 - 0:02:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:01:26",
                "answer": "A",
                "options": [
                    "A. 2.",
                    "B. 3.",
                    "C. 4.",
                    "D. 1."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_99_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:03:26",
                "answer": "B",
                "options": [
                    "A. 3.",
                    "B. 4.",
                    "C. 6.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_99_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:04:41",
                "answer": "D",
                "options": [
                    "A. 2.",
                    "B. 8.",
                    "C. 3.",
                    "D. 7."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_99_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:05:24",
                "answer": "B",
                "options": [
                    "A. 2.",
                    "B. 8.",
                    "C. 3.",
                    "D. 7."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_99_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:03:12",
                "answer": "A",
                "options": [
                    "A. 1.",
                    "B. 3.",
                    "C. 4.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            },
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:03:28",
                "answer": "B",
                "options": [
                    "A. 1.",
                    "B. 2.",
                    "C. 4.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_72_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:04:22",
                "answer": "B",
                "options": [
                    "A. 1.",
                    "B. 3.",
                    "C. 4.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_72_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:05:18",
                "answer": "C",
                "options": [
                    "A. 1.",
                    "B. 3.",
                    "C. 4.",
                    "D. 5."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_72_real.mp4"
    },
    {
        "time": "[0:12:00 - 0:13:00\u3011",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:13:30",
                "answer": "D",
                "options": [
                    "A. 10.",
                    "B. 11.",
                    "C. 12.",
                    "D. 13."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_72_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:05]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is mounted on the left side of the helicopter cockpit right now?",
                "time_stamp": "00:00:04",
                "answer": "C",
                "options": [
                    "A. A phone.",
                    "B. A GPS device.",
                    "C. A tablet.",
                    "D. A camera."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_426_real.mp4"
    },
    {
        "time": "[0:02:09 - 0:02:14]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What type of store is the person in right now?",
                "time_stamp": "00:03:16",
                "answer": "D",
                "options": [
                    "A. A clothing store.",
                    "B. A pet store.",
                    "C. A grocery store.",
                    "D. A costco."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_426_real.mp4"
    },
    {
        "time": "[0:04:18 - 0:04:23]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What brand of chips is in the shopping cart right now?",
                "time_stamp": "00:04:22",
                "answer": "C",
                "options": [
                    "A. Lay's.",
                    "B. Pringles.",
                    "C. Miss Vickie's.",
                    "D. Doritos."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_426_real.mp4"
    },
    {
        "time": "[0:06:27 - 0:06:32]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What brand of paper towels is visible on the ground right now?",
                "time_stamp": "00:06:31",
                "answer": "B",
                "options": [
                    "A. Bounty.",
                    "B. Scott.",
                    "C. Viva.",
                    "D. Sparkle."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_426_real.mp4"
    },
    {
        "time": "[0:08:36 - 0:08:41]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Which brand of bottled water is visible on the passenger seat right now?",
                "time_stamp": "00:08:39",
                "answer": "C",
                "options": [
                    "A. Dasani.",
                    "B. Evian.",
                    "C. Ice Mountain.",
                    "D. Aquafina."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_426_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: An individual with black gloves maneuvering a flatbed cart piled with boxes. The setting is within a store, organized with metal shelves packed with neatly stacked product cartons. Some boxes are scattered around the floor. The person appears to be unloading boxes onto the top shelf. [0:00:02 - 0:00:05]: The individual picks up a carton with a green and white label from the flatbed cart and places it on the shelf. The shelves are labeled with price tags, and there are other dairy cartons on the lower and top shelves. [0:00:06 - 0:00:09]: The individual places another carton on the shelf. The first person perspective shows the individual is wearing a sleeve with a black jacket and a gray glove. The activity continues as the individual stocks shelves meticulously. [0:00:10 - 0:00:12]: The person pauses for a moment, possibly to arrange the boxes on the flatbed cart stacked with cartons. They place one more carton on a side shelf, ensuring proper stocking. [0:00:13 - 0:00:15]: More boxes are scattered in the store's aisle. The individual reaches for an empty shelf location and continues to place the cartons in their appropriate sections. [0:00:16 - 0:00:18]: The person starts to gather the empty cardboard boxes from the floor scattered around the cart. They consolidate these boxes, clearing up the surrounding space. [0:00:19 - 0:00:20]: Adjust the remaining boxes on the flatbed cart, organizing them to continue stocking shelves. The background context remains consistent, with orderly shelves packed with products.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What is the state of the shelves as the individual continues to stock them?",
                "time_stamp": "0:00:20",
                "answer": "D",
                "options": [
                    "A. Shelves are mostly empty.",
                    "B. The shelves are already full.",
                    "C. Shelves are cluttered and disorganized.",
                    "D. Shelves are orderly and packed with products."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "storage_manage",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_448_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:23]: The video showcases a first-person perspective of someone handling cardboard boxes in a storage or warehouse area. The boxes are medium-sized and uniformly brown with various labels. The individual is seen moving boxes from one location to another, stacking them, and arranging them neatly.  [0:02:24 - 0:02:28]: The person continues to organize the boxes, placing them onto a platform or trolley. Shelves filled with other items and more stacked boxes are visible in the background. The individual is wearing black gloves and dark clothing.  [0:02:29 - 0:02:33]: The person moves to a different section, still handling boxes. They focus on adjusting and straightening the boxes on the platform. The nearby shelves are stocked with various packaged products, possibly in a retail or stockroom environment. [0:02:34 - 0:02:37]: The individual shifts their attention to the shelves, placing some boxes from the platform onto a bottom shelf. There are price tags displayed on the shelves, suggesting a retail setting. The surrounding area is organized, but several boxes are still scattered on the floor. [0:02:38 - 0:02:39]: The person reaches for an item on the middle shelf and inspects it. The shelves are well-stocked, and labels are clearly visible on the products. The scene is brightly lit, enhancing the visibility of the items and surroundings.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What type of perspective is shown in the video?",
                "time_stamp": "0:02:23",
                "answer": "C",
                "options": [
                    "A. Third-person perspective.",
                    "B. Aerial perspective.",
                    "C. First-person perspective.",
                    "D. Side perspective."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "storage_manage",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_448_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:42]: A person is in a store, crouching near a shelf stocked with various cartons of milk. They are placing small white bottles of what appears to be supplements into a cardboard box. The cartons are white with different colored sections and labels. There are price tags on the shelf indicating prices around 20-24 krona. [0:04:43 - 0:04:45]: The person reaches for a carton of milk labeled \"Vanilj\" priced at 31.50 krona on the upper shelf. They are wearing black gloves. [0:04:46 - 0:04:50]: The person places the carton on the lower shelf and continues picking up white bottles to place them in the box. They repeat the process, reaching for items on the shelf and placing them into their box. [0:04:51 - 0:04:52]: The person continues this pattern, occasionally reaching for different brands of cartons and rearranging items on the shelves. [0:04:53 - 0:04:54]: The camera perspective moves back, showing more of the aisle. There are several cardboard boxes scattered on the floor and stacks of items against the wall and shelves. [0:04:55 - 0:04:56]: The person moves around, picking up one of the empty cardboard boxes from the floor near a red shelf and an orange pallet jack. [0:04:57 - 0:04:59]: The person is sorting through the scattered boxes on the floor, moving them from one pile to another, organizing items. The perspective consistently shows a view from above the person's shoulders.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color are the cartons of milk on the shelf?",
                "time_stamp": "00:04:51",
                "answer": "B",
                "options": [
                    "A. Yellow with different colored sections.",
                    "B. White with different colored sections.",
                    "C. Green with different colored sections.",
                    "D. Red with different colored sections."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "storage_manage",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_448_real.mp4"
    },
    {
        "time": "[0:07:00 - 0:08:00]",
        "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:01]: The video begins inside a storage or warehouse area. The first-person view shows a hand reaching towards a metallic cart. Nearby, there are stacked red crates and a large pile of flattened cardboard boxes. To the left, various products are organized on shelves. [0:07:02 - 0:07:03]: The perspective shifts slightly, facing down more towards the floor and the pile of cardboard. There is a stack of small boxed items placed on a platform along with an orange trolley. [0:07:04 - 0:07:05]: The camera focuses more on the platform with small boxes. The person is seen moving towards the boxes, preparing to move them. [0:07:06 - 0:07:07]: The person starts picking up the boxes from the platform and placing them in a metallic cart. The surroundings include shelves on the left and the pile of flattened cardboard on the right. [0:07:08 - 0:07:09]: The person continues reaching for more boxes, placing them in the cart. The number of boxes on the ground decreases as they are transferred to the cart. [0:07:10 - 0:07:11]: The focus shifts again. The view zooms out slightly, showing the person organizing and placing more boxes on the cart. The remaining boxes are getting fewer on the platform. [0:07:12 - 0:07:13]: The person starts moving the cart towards the green and white stacked boxes on the right side. The camera view moves past the shelves with various products. [0:07:14 - 0:07:15]: The action shifts to the person grabbing a large green box labeled \"Bregott\" and placing it onto the cart. The surrounding area shows more stacked items and shelves filled with different products. [0:07:16 - 0:07:17]: Larger green boxes are being arranged and stacked on the cart. The person positions these boxes carefully, and the camera view shows more of the packed shelves in the background. [0:07:18 - 0:07:19]: The person continues to maneuver the cart with the large green boxes, moving deeper into the storage area. The view shifts to a broader perspective of the room, with more items and equipment scattered around. [0:07:20]: The video ends as the person approaches another section of the warehouse with the cart fully loaded with green boxes and small boxed items. The surroundings include more storage shelves, tools, and stacked equipment.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color are the boxes labeled \"Bregott\"?",
                "time_stamp": "00:07:15",
                "answer": "C",
                "options": [
                    "A. Red.",
                    "B. Blue.",
                    "C. Green.",
                    "D. Orange."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Action Recognition",
                "question": "What action did the person perform repeatedly just now?",
                "time_stamp": "00:07:11",
                "answer": "C",
                "options": [
                    "A. Organizing products on shelves.",
                    "B. Flattening cardboard boxes.",
                    "C. Picking up and placing boxes in a cart.",
                    "D. Moving the cart towards the door."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "storage_manage",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_448_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:05]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "Right now, what is the weather like on the street?",
                "time_stamp": "00:00:22",
                "answer": "D",
                "options": [
                    "A. Sunny.",
                    "B. Snowy.",
                    "C. Clear.",
                    "D. Rainy."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_403_real.mp4"
    },
    {
        "time": "[0:02:35 - 0:02:40]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What type of vehicle is directly in front of the camera right now?",
                "time_stamp": "00:02:37",
                "answer": "A",
                "options": [
                    "A. Taxi.",
                    "B. Bus.",
                    "C. Bicycle.",
                    "D. Motorcycle."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_403_real.mp4"
    },
    {
        "time": "[0:05:10 - 0:05:15]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Which brand's store sign is seen on the right-hand side of the street right now?",
                "time_stamp": "00:05:12",
                "answer": "D",
                "options": [
                    "A. Nike.",
                    "B. Tesla.",
                    "C. Adidas.",
                    "D. UGG."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_403_real.mp4"
    },
    {
        "time": "[0:07:45 - 0:07:50]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the current condition of the pavement right now?",
                "time_stamp": "00:07:45",
                "answer": "D",
                "options": [
                    "A. Dry.",
                    "B. Sandy.",
                    "C. Snowy.",
                    "D. Wet."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_403_real.mp4"
    },
    {
        "time": "[0:10:20 - 0:10:25]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What makes of the yellow car is waiting at the red light right now?",
                "time_stamp": "00:10:08",
                "answer": "C",
                "options": [
                    "A. Honda.",
                    "B. Ford.",
                    "C. Toyota.",
                    "D. Nissan."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_403_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: I\u2019m tired of all of my items breaking on me.  [0:00:03 - 0:00:05]: So that\u2019s why I\u2019m gonna build the coolest villager breeder  [0:00:05 - 0:00:06]: you have ever seen.  [0:00:07 - 0:00:09]: Subscribe [0:00:12 - 0:00:13]: Hope you enjoy.  [0:00:13 - 0:00:16]: So for us to be able to get this build on the road  [0:00:16 - 0:00:17]: we\u2019re gonna need  [0:00:17 - 0:00:18]: a looting 3  [0:00:18 - 0:00:20]: enchanted sword.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Why does the speaker want to build the coolest villager breeder?",
                "time_stamp": "0:00:20",
                "answer": "A",
                "options": [
                    "A. He is tired of their items breaking.",
                    "B. He wants to impress their friends.",
                    "C. He need more resources.",
                    "D. He enjoys building new things."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "minecraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_190_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:00 - 0:04:20] [0:00:00 - 0:00:01]: A minecart with a chest is displayed on the screen, showing its contents in an organized grid format. Various items such as ingots, enchanted books, and a golden apple are visible in the chest. Below the chest interface is the player's inventory, filled with numerous items including weapons, blocks, and tools. [0:00:02 - 0:00:03]: The inventory view remains on the screen, with some items highlighted in the chest on the upper right, including several torches. The player\u2019s experience level is displayed as 19, and various tools and items are scattered in the inventory slots. [0:00:04 - 0:00:05]: The scene shifts perspective to a dark, enclosed underground area with cobblestone and wooden structures. There are rails on the ground, indicating a mineshaft setting. In front of the player, a post stands in the middle of the pathway. [0:00:06 - 0:00:07]: The environment remains dimly lit, with wooden beams supporting the structure. The player\u2019s hand holds a purple-colored weapon, possibly enchanted, with other icons such as hearts and food indicators at the bottom. [0:00:08 - 0:00:09]: Focus shifts to a nearby corner where a spider, hanging from the ceiling on its web, is visible. The player holds a piece of bread in their hand. [0:00:10 - 0:00:11]: The perspective changes as the player shifts their weapon to a diamond axe, still fixated on the upper portion of the wooden beam where the spider is positioned. A torch illuminates the scene nearby. [0:00:12 - 0:00:13]: The spider now lands on the ground close to the torch. The player's diamond axe remains in hand, ready for action. [0:00:14 - 0:00:15]: The player strikes the spider with the axe. The spider shows signs of damage with red heart animations appearing. [0:00:16 - 0:00:17]: After defeating the spider, focus shifts to a mob spawner surrounded by cobblestone and wooden structures. The spawner is topped with a lit torch, preventing further mobs from spawning. [0:00:18 - 0:00:19]: The player navigates through a narrow pathway between two wooden pillars. The pathway ahead is dimly lit and appears to be part of an interconnected tunnel system. [0:00:20 - 0:00:21]: A new scene displays a dark section of the tunnel system with water flowing down from the walls. The player's viewpoint shows the flowing water, which creates small pools on the ground. [0:00:22 - 0:00:23]: The player draws a diamond axe while facing the direction of the cascading water. The light in the tunnel remains limited. [0:00:24 - 0:00:25]: The player switches from the diamond axe to a diamond pickaxe, preparing to possibly mine resources within the dimly lit, water-filled tunnel section. [0:00:26 - 0:00:27]: The view moves towards another section of the mineshaft where a minecart with a chest sits on rails. Webs are visible above, indicating the lingering remnants of spiders. [0:00:28 - 0:00:29]: The player opens the minecart's chest, revealing its contents. The chest contains items such as rails, redstone, and a name tag. [0:00:30]: The player comments with an audible \u201coh!\u201d [0:00:31 - 0:00:32]: Continues viewing the chest's items, appreciating the findings with an exclamation, \u201cYes! Perfect!\u201d [0:00:33 - 0:00:34]: The chest's contents are examined more closely; rails, redstone, torches, and a name tag are visible. [0:00:35 - 0:00:36]: The player continues to look through the chest, focusing on the various items available for retrieval. [0:00:37 - 0:00:39]: The player expresses gratitude, saying \u201cthank you very much,\u201d as He proceed to collect the desired items from the chest.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What does the player do immediately after the spider lands on the ground?",
                "time_stamp": "0:04:10",
                "answer": "C",
                "options": [
                    "A. Runs away.",
                    "B. Places a torch.",
                    "C. Strikes the spider with an axe.",
                    "D. Switches to a sword."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "minecraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_190_real.mp4"
    },
    {
        "time": "[0:08:00 - 0:09:00]",
        "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:02]: The video begins with a top-down view of a village in a green landscape. The village consists of several small buildings with brown rooftops, surrounded by green grass and paths made of dirt blocks. Adjacent to the village is a body of water with a few docks extending into it. The player's inventory bar is visible at the bottom of the screen, showing tools and items such as a diamond sword, pickaxe, and other supplies. [0:08:03 - 0:08:04]: The perspective shifts, and the village begins to recede from view as the camera turns to the right. The scene reveals a larger landscape with forests and plains expanding into the distance. The same black structure is now shown extending outwards, indicating the player is positioned on a higher platform of some structure, possibly flying or standing on an elevated area. [0:08:05 - 0:08:09]: Turning further, a red balloon-like structure appears in the distance, floating above the landscape. The balloon has white trimmings and a basket hanging beneath it, secured to the ground by a long rope. The landscape beneath it features forests, a river, and some mountains further away. The inventory bar shows the player holding a block of gray concrete, indicating a potential building activity. [0:08:10 - 0:08:14]: As the player's view shifts left, the red balloon appears more central in the frame. The surrounding terrain shows mixed biomes with both forested and open grassy areas. The player seems to be moving towards the edge of the structure, possibly aligning for constructing something. [0:08:15 - 0:08:16]: The perspective zooms in on the balloon, highlighting its detailed design with a basket below and ropes anchoring it. The balloon's texture appears made from red wool blocks, and finer details are crafted meticulously. [0:08:17 - 0:08:19]: The camera retreats again, offering a broader overhead view of the landscape. The balloon remains in sight, but the focus shifts back to the structural platform the player is standing on. The black and gray hues of the platform contrast sharply with the greenery below. The player\u2019s inventory shows another gray concrete block, indicating continued construction as more blocks are added while navigating the platform.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color are the rooftops of the village buildings?",
                "time_stamp": "00:08:02",
                "answer": "D",
                "options": [
                    "A. Red.",
                    "B. Green.",
                    "C. Gray.",
                    "D. Brown."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "minecraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_190_real.mp4"
    },
    {
        "time": "[0:12:00 - 0:13:00]",
        "captions": "[0:12:00 - 0:12:20] [0:12:00 - 0:12:01]: The video begins with a view of a large, pyramid-shaped structure. The pyramid is constructed from black and white blocks and is set against a clear blue sky. To the left, a red hot air balloon with intricate designs is floating in the sky. There are small patches of green land and water visible on the lower left side of the screen. [0:12:01 - 0:12:02]: The perspective shifts slightly, moving closer towards the pyramid. The hot air balloon remains to the left, but now a bit more obscured due to the change in angle. [0:12:02 - 0:12:03]: Transitioning further, the pyramid now takes up more of the frame. A small figure in blue appears on the side of the pyramid, ascending the structure. [0:12:03 - 0:12:04]: More of the pyramid is visible now, with the blue-clad figure moving further up. Several orange figures appear near the base of the pyramid to the left, indicating some movement or possible action. [0:12:04 - 0:12:05]: The figures near the base of the pyramid seem to gather, and smoke or a gray cloud is faintly visible on the left side, suggesting some form of activity or disturbance. [0:12:05 - 0:12:06]: The camera continues to move, primarily showing the pyramid and a few figures along its levels. The blue figure has climbed higher, nearing the peak. [0:12:06 - 0:12:07]: The pyramid's intricate stepped design is clear, with the blue figure continuing its ascent. The surroundings, including the left patches of green land and water, are partially visible. [0:12:07 - 0:12:08]: The focus remains on the pyramid as the blue figure navigates along one of its white block paths. The structure's symmetry and design details are prominently displayed. [0:12:08 - 0:12:09]: The pyramid's peak is more centered in the frame, with the blue figure almost reaching the top. Vegetation and landscape details become slightly clearer on the perimeters of the screen. [0:12:09 - 0:12:10]: The camera angle elevates, showing the blue figure near the summit of the pyramid on a white block path. [0:12:10 - 0:12:11]: The blue figure reaches the top of the pyramid. The detailed stepped structure and the vast platform around the pyramid are clearly visible. [0:12:11 - 0:12:12]: The camera starts to move around the pyramid, displaying its meticulously designed steps and symmetry. The blue figure stands near the peak, while the hot air balloon and surrounding landscapes are distant. [0:12:12 - 0:12:13]: The camera angle shifts further, continuing to circle around the pyramid. The blue figure remains at the top, and the sun casts shadows emphasizing the pyramid's geometric shapes. [0:12:13 - 0:12:14]: The pyramid's height and its detailed structure are highlighted as the camera moves. The blue figure is seen slightly descending, while the surrounding regions are partially visible. [0:12:14 - 0:12:15]: The pyramid's extensive stepped design is emphasized, with the camera angle showcasing its height from various perspectives.  [0:12:15 - 0:12:16]: The view shifts to show more of the pyramid\u2019s surroundings, with additional greenery visible on the perimeter. The blue figure appears smaller as it remains near the top. [0:12:16 - 0:12:17]: The structure's intricate pattern is prominently displayed as the camera continues to move around it. More landscape details such as water and grass patches become visible. [0:12:17 - 0:12:18]: The blue figure moves slightly lower on the pyramid steps. The camera angle broadens, giving a comprehensive view of the entire structure and some of the surrounding environment. [0:12:18 - 0:12:19]: The pyramid remains the focal point, with the blue figure still in view but lower on the steps. The surrounding landscape, including patches of land and water, is more distinctly visible.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is floating in the sky to the left of the pyramid right now?",
                "time_stamp": "00:12:01",
                "answer": "A",
                "options": [
                    "A. A red hot air balloon.",
                    "B. A blue kite.",
                    "C. A white bird.",
                    "D. A black drone."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "minecraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_190_real.mp4"
    },
    {
        "time": "[0:15:00 - 0:15:52]",
        "captions": "[0:15:40 - 0:15:52] [0:15:40 - 0:15:43]: The video begins with a first-person perspective, looking at a large, circular structure suspended in the sky. The structure is supported by a narrow, vertical column that extends down to the landscape below. The landscape consists of grassy plains and a few scattered trees. The sky is mostly clear with a few clouds. [0:15:44 - 0:15:47]: The perspective shifts to a third-person view showing a character standing on a wooden platform. The character is wearing blue armor and is positioned towards the center of the frame. In the background, the large, circular structure remains prominent in the sky. [0:15:48 - 0:15:51]: The character continues to stand on the platform, slightly adjusted in position. A red \u201cSubscribe\u201d button with a white YouTube logo appears at the bottom of the frame. The landscape in the background includes rivers and forests. [0:15:52 - 0:15:55]: The \u201cSubscribe\u201d button changes to include a clicking animation, indicating the action of subscribing. The character on the platform remains fixed in position with the circular structure still visible in the sky. [0:15:56 - 0:15:57]: The animation of the \u201cSubscribe\u201d button concludes, and it changes to a \u201cSubscribed\u201d button with a checkmark. The character on the platform appears unchanged, still wearing blue armor. [0:15:58 - 0:15:59]: The video briefly shows the third-person perspective of the character on the platform, with the \"Subscribed\" button still displayed. The background features the wide landscape of rivers, forests, and the large, floating structure.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What does the red button at the bottom of the frame display initially?",
                "time_stamp": "0:15:51",
                "answer": "A",
                "options": [
                    "A. A \u201cSubscribe\u201d button with a white YouTube logo.",
                    "B. A \u201cLike\u201d button with a white thumbs-up icon.",
                    "C. A \u201cShare\u201d button with a white arrow icon.",
                    "D. A \u201cFollow\u201d button with a white star icon."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "minecraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_190_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video opens with a first-person perspective looking up at a tall residential building. There is a red sign in the foreground, slightly to the right, adorned with several advertisements. The building fa\u00e7ade displays numerous windows and air conditioning units. A yellow logo and the words \"HONG KONG CITYWALK\" are prominently featured over the red sign.  [0:00:04 - 0:00:07]: The camera tilts slightly downward, still gazing upwards at the building. Overlaid text appears, displaying \"2023 / 7 / 7\", \"Hong Kong\", \"Tsim Sha Tsui\", and \"7:00pm\". The angle shifts to include a busy street scene with vehicles and pedestrians. [0:00:05 - 0:00:07]: A red double-decker bus moves through the frame from left to right. The background features multiple commercial buildings, including a large screen displaying advertisements, and the entrance to a transportation hub is visible on the right. [0:00:08 - 0:00:09]: The camera continues descending, focusing closer on the street level. Now, the entrance to the Tsim Sha Tsui MTR station is prominently seen on the right, with people walking past it. [0:00:10 - 0:00:12]: There's a clear view of the MTR station entrance, with the station name lit up in red and white. The camera perspective shows various pedestrians walking in and out of the station, alongside a few walking on the cobblestone pedestrian path. [0:00:13 - 0:00:16]: The scene captures more street-level detail. The pathway is occupied by various pedestrians walking in both directions. Shops and commercial establishments line the pathway, displaying products in their windows. [0:00:17 - 0:00:19]: The camera continues to focus on the bustling pedestrian walkway. More people walk by, with some heading toward the MTR station entrance and others moving further along the path. The video concludes with an extended view of the vibrant street scene, capturing the ongoing hustle and bustle of the city area.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What details are displayed in the overlaid text just now?",
                "time_stamp": "0:00:13",
                "answer": "D",
                "options": [
                    "A. \"2023 / 6 / 7\", \"Hong Kong\", \"Tsim Sha Tsui\", and \"8:00pm\".",
                    "B. \"2022 / 7 / 7\", \"Hong Kong\", \"Central\", and \"7:00pm\".",
                    "C. \"2023 / 7 / 7\", \"Hong Kong\", \"Tsim Sha Tsui\", and \"6:00pm\".",
                    "D. \"2023 / 7 / 7\", \"Hong Kong\", \"Tsim Sha Tsui\", and \"7:00pm\"."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_332_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:45]: The video shows a display in a shopping mall, centered on a large structure designed to look like a bridge made of brick with an archway. On the bridge are large cat figures in a variety of colors (black, orange, white, and others). The surrounding area features various brands and storefronts with colorful and bright lighting.  [0:02:46 - 0:02:48]: The camera pans to the right, revealing a large, round, yellow structure and a white figure that resembles a cloud. There are people walking around, some looking at their phones, and others appearing to admire the display.  [0:02:49 - 0:02:55]: As the camera continues to move, more of the yellow structure becomes visible, which is now seen to be a large fruit or vegetable with a green stem-like element. There is a woman wearing a mask and taking a photo of the display. The scene shows various other decorative elements. [0:02:56 - 0:02:59]: Moving further to the right, the camera captures more detailed views of the decorations, including a large white cat figure sitting among pink flowers. The yellow structure is part of an exhibit, with branding and additional decor in the background. The scene shows more people interacting with the display.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What colors are the largest cat figures on the bridge right now?",
                "time_stamp": "0:03:00",
                "answer": "D",
                "options": [
                    "A. Orange.",
                    "B. Black.",
                    "C. grey.",
                    "D. white."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Event Understanding",
                "question": "What is the woman wearing a mask doing right now?",
                "time_stamp": "0:02:55",
                "answer": "D",
                "options": [
                    "A. Talking on the phone.",
                    "B. Walking around.",
                    "C. Looking at her phone.",
                    "D. Taking a photo."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_332_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:29]: The video shows a busy urban street with tall buildings lined on either side. The buildings are mainly rectangular with lots of windows and colorful signage, illuminated against a dusky sky. Numerous pedestrians are walking across a yellow zebra crossing divided by intermittent dashed yellow lines. Some pedestrians are carrying shopping bags, while others appear to be having conversations. On the left side of the street, multiple storefronts display bright signs\u2014some predominantly yellow, some pink, and others with varying colors. There is significant vehicular traffic, including a bus approaching from the background, and several stationary vehicles visible. [0:05:30 - 0:05:39]: The street continues to get busier as more people cross the road, and vehicles move closer. A red bus and several cars are visible, some of them moving. Pedestrians on the right side of the street near the sidewalk are seen walking, standing, and interacting. One man in an orange shirt stands near the foreground, apparently speaking on his phone. Billboards with advertisements in bright colors, including yellow and red, capture attention. The building lights create a lively urban night setting with various shops and signs illuminating the area.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color is the zebra crossing that pedestrians are walking across right now?",
                "time_stamp": "0:05:29",
                "answer": "D",
                "options": [
                    "A. White.",
                    "B. Blue.",
                    "C. Red.",
                    "D. Yellow."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_332_real.mp4"
    },
    {
        "time": "[0:08:00 - 0:09:00]",
        "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:05]: The video starts with a view of a bustling city sidewalk. On the left, there is a building with bright white lights above its entrance area. Several people are walking in the same direction as the camera, while others are moving in the opposite direction. A large yellow and red bus is parked on the right side, with potted plants filled with colorful flowers placed between the bus and the sidewalk. The sky is overcast, hinting at the late evening or early twilight. [0:08:06 - 0:08:09]: The camera continues moving forward along the sidewalk. A man in a light gray shirt and light blue jeans walks toward the camera, while several other pedestrians, including a group of three people dressed in casual attire, are observed walking in the opposite direction. A stall selling various items is visible on the left side. [0:08:10 - 0:08:13]: The camera moves past the stall selling various items, which is located on the left side of the sidewalk. Ahead, more people are observed walking in clusters, some heading towards the camera, while others move away. The large building on the left has a facade made of white and gray materials with garage-like doors at regular intervals. [0:08:14 - 0:08:17]: The sidewalk remains crowded with pedestrians, and the camera maintains a steady pace forward. The street to the right appears busy with vehicles, and the potted plants filled with colorful flowers continue to line the edge of the sidewalk. The bus from the earlier scene is still visible, stationary, creating a constant backdrop in this urban landscape. [0:08:18 - 0:08:20]: The video concludes with the camera continuing down the same sidewalk, nearing the edge of the bus. More pedestrians are walking around, including some children. The background features tall buildings with lit signage and visible green-leaved palm trees, maintaining the bustling city atmosphere.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What is the overall atmosphere of the scene in the video?",
                "time_stamp": "0:08:20",
                "answer": "D",
                "options": [
                    "A. Calm and serene.",
                    "B. Quiet and empty.",
                    "C. Tense and chaotic.",
                    "D. Bustling and busy."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_332_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: A person's hand holds a paintbrush and begins to paint a sketched portrait on a white canvas. The brush applies dark brown paint to the upper left area of the head, suggesting the start of painting hair. [0:00:02 - 0:00:06]: The painting progresses with more dark brown paint added to the left side of the head, covering more hair area as the hand continues to brush downward. [0:00:07 - 0:00:09]: The hand moves the brush to the right side of the head and begins to apply dark brown paint near the right ear of the sketched portrait.  [0:00:10 - 0:00:12]: The brush continues to add dark brown paint around the right ear and then moves downward to fill in more of the hair. [0:00:13 - 0:00:15]: More paint is added to the upper and right side of the head, further defining the hair and giving it more volume and shape. [0:00:16 - 0:00:17]: The hand holding the paintbrush moves back to the left side of the head and starts adding brown paint to fill in more of the hair, with the color becoming richer and more defined. [0:00:18 - 0:00:19]: The brush continues to blend and perfect the hair, ensuring even coverage and more detailed texture. The hair on the portrait starts to take a more complete shape, emphasizing the overall look of the sketched portrait.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Which side of the head does the person paint first?",
                "time_stamp": "0:00:09",
                "answer": "B",
                "options": [
                    "A. The top.",
                    "B. The left side.",
                    "C. The right side.",
                    "D. The back."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_129_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:25]: The video begins with a close-up view of a painting in progress, depicting a person with short brown hair, a slightly distressed expression, and a white shirt with a bit of black being added. The painter, using a paintbrush, focuses on adding darker shades to the collar area, creating a shadow effect to enhance the depth of the shirt. [0:02:26 - 0:02:29]: The painter continues to work on the figure\u2019s collar, reinforcing the area with bold black brushstrokes. The background remains a plain white canvas, directing focus on the subject\u2019s face and upper body. [0:02:30 - 0:02:32]: The focus shifts to the clothing of the subject. The painter starts to add red to the figure's upper garment, beginning to define what looks like a jacket. The colors are vibrant, with red dominating the garment. [0:02:33 - 0:02:36]: The detail in the face stays consistent as the artist transitions to adding more highlights and depth to the hair. The brush moves swiftly, accentuating lighting and shadows. [0:02:37 - 0:02:39]: The artist finalizes the painting details. The jacket\u2019s shades and folds are defined, enhancing the three-dimensionality of the portrait. The background remains untouched, ensuring the subject stands out prominently.",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following best summarizes the painting process that was just shown in the video?",
                "time_stamp": "0:02:39",
                "answer": "B",
                "options": [
                    "A. Adding final touches to a landscape.",
                    "B. Detailing a person with short brown hair and a red coat.",
                    "C. Painting an abstract art piece.",
                    "D. Working on a cityscape with vibrant colors."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_129_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:42]: The video shows a detailed painting of a person wearing a red jacket. The painting has a realistic quality, focusing on the upper body and face of the subject. The background is white, and around the canvas are objects partially visible, like a plant with green leaves on the left side and colorful abstract shapes on the wall. [0:04:43 - 0:04:52]: As the video progresses, a hand holding a thin paintbrush appears in the frame, refining details on the face, particularly around the eyes and mouth area. The hand moves the brush precisely, enhancing the realistic texture and depth of the artwork. [0:04:53 - 0:04:55]: The paintbrush shifts to other parts of the face, adjusting the shading on the left eyebrow and the lower chin. The artist\u2019s hand shows controlled, deliberate strokes as they work. [0:04:56 - 0:04:59]: The final touches are applied to the facial features, emphasizing expressions. The overall video focuses on the process of refining the painting, showcasing the artist\u2019s precision and attention to detail.",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why is the artist applying touches to the facial features frequently?",
                "time_stamp": "00:04:59",
                "answer": "B",
                "options": [
                    "A. To correct mistakes.",
                    "B. To emphasize expressions.",
                    "C. To add abstract elements.",
                    "D. To change the background color."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_129_real.mp4"
    },
    {
        "time": "[0:07:00 - 0:08:00]",
        "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:20]: The video starts with a focus on a detailed painting of a person, capturing a realistic expression. The painting has vibrant colors, with the person wearing a red jacket. The background is mainly white with some subtle color gradients making the portrait more striking. The scene is set in a studio, with some plants visible on the left side and art supplies, such as a computer or tablet, on a table beneath the painting. The artist's hand starts to appear around [0:07:03], holding a paintbrush and making precise strokes on the canvas. The hand reaches towards the top part of the painting, around the forehead, [0:07:10], adding details to the hair. The artist continues to work on fine details in the hair and other areas like the ear and background, adding subtle strokes and adjustments till the end of this segment. The video captures the careful and deliberate motions of the artist as they enhance the already detailed painting.",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where are the plants located in relation to the painting in the studio?",
                "time_stamp": "00:07:20",
                "answer": "C",
                "options": [
                    "A. To the right side of the painting.",
                    "B. On a table beneath the painting.",
                    "C. On the left side of the painting.",
                    "D. Behind the artist."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following best summarizes the content that was just shown in the video?",
                "time_stamp": "00:07:20",
                "answer": "B",
                "options": [
                    "A. The artist is setting up the studio before starting to paint.",
                    "B. The artist is making precise adjustments to an already detailed portrait.",
                    "C. The artist is selecting colors and preparing brushes for a new painting.",
                    "D. The artist is cleaning up the studio after completing a painting."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "painting",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_129_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:30]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why does the older woman appear annoyed or angry?",
                "time_stamp": "00:00:26",
                "answer": "B",
                "options": [
                    "A. Because the character shouted loudly.",
                    "B. Because the character disturbed her.",
                    "C. Because the character knocked on the door.",
                    "D. Because the character broke something."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_249_real.mp4"
    },
    {
        "time": "[0:02:15 - 0:02:45]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why is Mr. Bean under the bed now?",
                "time_stamp": "0:02:25",
                "answer": "A",
                "options": [
                    "A. Because he wanted to snatch back the teddy bear doll that was taken by the cat.",
                    "B. Because he dropped something valuable and is trying to retrieve it.",
                    "C. Because he is hiding from someone who just entered the room.",
                    "D. Because he thought he saw something strange under the bed."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_249_real.mp4"
    },
    {
        "time": "[0:04:30 - 0:05:00]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why did the character exit the building and approach a green car?",
                "time_stamp": "0:06:02",
                "answer": "C",
                "options": [
                    "A. Because he forgot something important in the car.",
                    "B. Because he needs to pick up a friend from the nearby station.",
                    "C. Because he wants to drive to the pet hospital to treat his teddy bear doll.",
                    "D. Because he plans to run an errand across town."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_249_real.mp4"
    },
    {
        "time": "[0:06:45 - 0:07:15]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why Mr. Bean would be very panicked now.",
                "time_stamp": "0:08:38",
                "answer": "C",
                "options": [
                    "A. Because he realized he lost his wallet.",
                    "B. Because he accidentally locked himself out of his house.",
                    "C. Because he found that his teddy bear doll was not in the box.",
                    "D. Because he noticed that his car keys were missing."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_249_real.mp4"
    },
    {
        "time": "[0:09:00 - 0:09:30]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why did the character react with noticeable concern after stopping the car?",
                "time_stamp": "00:10:17",
                "answer": "C",
                "options": [
                    "A. Because they remember they left something important behind.",
                    "B. Because they have car trouble and need to check the engine.",
                    "C. Because he thought the one that was flattened was the old lady's cat.",
                    "D. Because they see a strange figure in the distance."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_249_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:20]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is located on the right side of the road right now?",
                "time_stamp": "00:00:15",
                "answer": "A",
                "options": [
                    "A. A river.",
                    "B. A forest.",
                    "C. A field.",
                    "D. A building."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_185_real.mp4"
    },
    {
        "time": "[0:02:02 - 0:02:22]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is located on the left side of the path right now?",
                "time_stamp": "00:02:22",
                "answer": "A",
                "options": [
                    "A. A line of trees.",
                    "B. A lake.",
                    "C. A canal.",
                    "D. A field."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_185_real.mp4"
    },
    {
        "time": "[0:04:04 - 0:04:24]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is on the right side of the path right now?",
                "time_stamp": "00:04:10",
                "answer": "A",
                "options": [
                    "A. A river.",
                    "B. A grassy field.",
                    "C. A dense forest.",
                    "D. A parking lot."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_185_real.mp4"
    },
    {
        "time": "[0:06:06 - 0:06:26]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "Where is the canal located right now?",
                "time_stamp": "00:06:26",
                "answer": "A",
                "options": [
                    "A. On the right side of the path.",
                    "B. On the left side of the path.",
                    "C. Directly behind the cyclist.",
                    "D. Directly ahead of the cyclist."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_185_real.mp4"
    },
    {
        "time": "[0:08:08 - 0:08:28]",
        "questions": [
            {
                "task_type": "Spatial Understanding",
                "question": "What is on the right side of the path right now?",
                "time_stamp": "00:08:18",
                "answer": "A",
                "options": [
                    "A. A river.",
                    "B. A forest.",
                    "C. A mountain.",
                    "D. A road."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cycling",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_185_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: A race track curves from the left foreground to the right background. The track is black, with a smooth, shiny surface. On the left side, there is a chain-link fence with colorful signage behind it. There are miniature cars and objects near the fence. The backdrop is a scenic mural with a blue sky, white clouds, green hills, and trees. The perspective indicates a downhill slope. [0:00:02 - 0:00:05]: The initial scene with the race track transitions to a black screen displaying a distorted logo with the text \"NEXT GEN DIECAST\" and \"The Next Generation of Diecast Racing.\" The text and logo appear glitchy and colorful. [0:00:06]: The logo and text stabilize, remaining clear and centered on a black background with the same message, \"NEXT GEN DIECAST\" and \"The Next Generation of Diecast Racing.\" [0:00:07 - 0:00:08]: Back to a race track scene, showing a close-up view of three diecast cars lined up from left to right: a red car, a silver car, and a green car with \u201cN2O COLA\u201d written on it. The track's surface has lane dividers and is surrounded by artificial grass. There are Thunder Bunny Racing signs and other signage around. [0:00:09 - 0:00:13]: Text \u201cNEXT-GEN PISTON CUP\u201d and \u201cRace 1 - Final\u201d overlays the scene with the three cars. The cars remain stationary, lined up at the starting line. The background shows parts of the chain-link fence and the area behind it. [0:00:14 - 0:00:20]: The scene continues focusing on the starting line of the race. The text \"Race 1 - Final\" remains superimposed over it. The green car in the center lane slightly moves forward, suggesting the beginning of the race. The surrounding signs and track layout stay the same.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What is written on the green car in the race track scene?",
                "time_stamp": "0:00:08",
                "answer": "A",
                "options": [
                    "A. N2O COLA.",
                    "B. Speedy Cola.",
                    "C. Racing Fuel.",
                    "D. Nitro Power."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Clips Summarize",
                "question": "What main event is depicted in the captions?",
                "time_stamp": "0:00:20",
                "answer": "B",
                "options": [
                    "A. A diecast car race victory lap.",
                    "B. The beginning of a car model race.",
                    "C. The construction of a race track.",
                    "D. The final lap of a race."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_484_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:23]: Four toy race cars, red, blue, gray, and another color, are on a black race track at various positions. The track has a fenced pit stop area on the right, where four small toy trucks are parked in front of a garage. The setting includes a background with green hills and trees under a bright sky; [0:05:24 - 0:05:29]: The cars drive past the pit stop area and are now on a different section of the race track, which curves around a hillside. There are signs in the background indicating sponsors or racing teams. The landscape is lush with greenery and a painted sky with clouds; [0:05:30 - 0:05:32]: The cars continue on the curving track. The camera view shifts to focus on a part of the race track that has a green grassy area in the middle of a curve. There are more toy trucks and cars visible off the track, along with racing signs; [0:05:33 - 0:05:34]: The cars are still racing, moving steadily along the track while the background scenery maintains a consistent look of green hills and painted sky; [0:05:35 - 0:05:39]: The first blue car moves further ahead from the other cars on the track past the pit stop area, while the other cars follow behind. The pit stop area scenery remains unchanged.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What happens to the green-purple car car when the racing car sprints?",
                "time_stamp": "00:05:42",
                "answer": "D",
                "options": [
                    "A. It falls behind other cars.",
                    "B. It crashes into the pit stop area.",
                    "C. It leaves the race track.",
                    "D. It moves further ahead from the other cars."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_484_real.mp4"
    },
    {
        "time": "[0:08:00 - 0:09:00]",
        "captions": "[0:08:00 - 0:08:20] [0:08:00]: The video starts with a racing track in view. A silver racing car with purple accents speeds by on a black track surface. The track is elevated and has a fence on the right side. In the background, there are green hills and a white building resembling a pit stop or garage area with several toy-like vehicles. [0:08:01 - 0:08:10]: The track times for the race are displayed on screen. The background remains consistent, showing the racing track and the green hills in the backdrop. The track times board lists the top five racers: #39 Michael Rotor with a time of 8.103 seconds, #94 Jay with a time of 8.267 seconds, #35 Ronald with a time of 8.348 seconds, #28 Tim Treadless with a time of 8.417 seconds, and #123 Jonas Carvers with a time of 8.485 seconds. The display stays on screen while the camera perspective remains the same. [0:08:11 - 0:08:19]: The standings for Race 1 - Final are shown. The racers are listed with their corresponding points: #68 H.J. Hollis is in the first place with 13 points, #39 Michael Rotor is in the second place with 12 points, and #123 Jonas Carvers is also in the second place with 12 points (noted by *1 indicating an asterisk for some condition), and #57 Junyi is in the third place with 6 points. The background remains consistent with the previous shots, showing the end of the racing track and the garage area.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "Who is in first place in Race 1 - Final standings?",
                "time_stamp": "0:08:19",
                "answer": "C",
                "options": [
                    "A. Michael Rotor.",
                    "B. Jay.",
                    "C. H.J. Hollis.",
                    "D. Jonas Carvers."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_484_real.mp4"
    },
    {
        "time": "[0:09:00 - 0:09:45]",
        "captions": "[0:09:40 - 0:09:45] [0:09:40 - 0:09:42]: A racetrack with a black surface is visible, bordered by a fence on the right side and green grass on the left. Multiple toy racecars of different colors, including green, blue, and purple, are moving along the track. In the background, there's a white garage with an open front displaying various miniature vehicles such as forklifts, and a grassy hill. [0:09:42 - 0:09:44]: The blue and red racecars continue to move forward while the purple racecar exits the frame. A gray racecar appears from the background, heading towards the foreground. The landscape remains the same, with a glimpse of trees and more greenery visible in the distance. [0:09:44 - 0:09:45]: The red racecar continues to move closer to the foreground, while the gray racecar follows behind. The background elements, including the white garage and the miniature vehicles, remain unchanged. The fence on the right side of the track is consistently visible.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What is the sequence of the racecars when the racecars sprints?",
                "time_stamp": "0:09:45",
                "answer": "A",
                "options": [
                    "A. Red racecar followed by blue racecar.",
                    "B. Blue racecar followed by green racecar.",
                    "C. Purple racecar followed by blue racecar.",
                    "D. Gray racecar followed by red racecar."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_484_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:05]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What type of sign is displayed on the pole right now?",
                "time_stamp": "00:00:04",
                "answer": "A",
                "options": [
                    "A. Bus zone sign.",
                    "B. Speed limit sign.",
                    "C. No parking sign.",
                    "D. Pedestrian crossing sign."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_450_real.mp4"
    },
    {
        "time": "[0:01:52 - 0:01:57]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color is the car visible in the distance on the right side of the road?",
                "time_stamp": "00:01:54",
                "answer": "A",
                "options": [
                    "A. White.",
                    "B. Red.",
                    "C. Blue.",
                    "D. Black."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_450_real.mp4"
    },
    {
        "time": "[0:03:44 - 0:03:49]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color is the sign visible on the left side of the road right now?",
                "time_stamp": "00:03:47",
                "answer": "A",
                "options": [
                    "A. Blue.",
                    "B. Red.",
                    "C. Yellow.",
                    "D. Green."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_450_real.mp4"
    },
    {
        "time": "[0:05:36 - 0:05:41]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color are the parking signs visible on the right side of the road right now?",
                "time_stamp": "00:03:47",
                "answer": "A",
                "options": [
                    "A. Blue.",
                    "B. Red.",
                    "C. Yellow.",
                    "D. Green."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_450_real.mp4"
    },
    {
        "time": "[0:07:28 - 0:07:33]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What type car is the orange car visible right now?",
                "time_stamp": "00:07:29",
                "answer": "A",
                "options": [
                    "A. SUV.",
                    "B. Van.",
                    "C. Bus.",
                    "D. Truck."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_450_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:30]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why is Mr. Bean scolded by the older woman?",
                "time_stamp": "00:00:27",
                "answer": "A",
                "options": [
                    "A. Because he was making too much noise.",
                    "B. Because he was late.",
                    "C. Because he was shining a flashlight at the teddy bear.",
                    "D. Because he was not paying attention."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_234_real.mp4"
    },
    {
        "time": "[0:02:02 - 0:02:32]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why does the character place the cheese on a plate?",
                "time_stamp": "0:02:35",
                "answer": "B",
                "options": [
                    "A. Because he plans to make a sandwich with the cheese.",
                    "B. Because his refrigerator is empty except for a piece of vegetable.",
                    "C. Because he thinks the cheese will taste better at room temperature.",
                    "D. Because the character organizes the fridge."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_234_real.mp4"
    },
    {
        "time": "[0:04:04 - 0:04:34]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why does the man successfully retrieve the bottle from the top shelf?",
                "time_stamp": "00:04:24",
                "answer": "A",
                "options": [
                    "A. Because he uses his shopping cart for support.",
                    "B. Because he asks for assistance from a store employee.",
                    "C. Because he finds a step stool to reach the bottle.",
                    "D. Because he uses a long tool to knock the bottle down."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_234_real.mp4"
    },
    {
        "time": "[0:06:06 - 0:06:36]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why Mr. Bean is very shocked.",
                "time_stamp": "00:06:40",
                "answer": "D",
                "options": [
                    "A. Because his own pie just fell to the floor.",
                    "B. Because the opponent did a magic trick with the pie.",
                    "C. Because the pie suddenly exploded when the opponent touched it.",
                    "D. Because the opponent finished the pie in one go."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_234_real.mp4"
    },
    {
        "time": "[0:08:08 - 0:08:38]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why does the Mr. Bean look tired or exhausted?",
                "time_stamp": "0:08:36",
                "answer": "B",
                "options": [
                    "A. Because he had to run around the supermarket to find items.",
                    "B. Because he lost the competition in the supermarket.",
                    "C. Because he stayed up all night preparing for the competition.",
                    "D. Because he had to carry heavy bags out of the supermarket."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_234_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:09]: A man with blonde hair and a serious expression stands in front of a stainless steel refrigerator, wearing a navy blue T-shirt. The background consists of white subway tiles on the wall. He begins speaking to the camera with his hands clasped in front of him and occasionally gestures while talking. A window is visible to his right side, showing an outdoor scene with greenery. [0:00:10 - 0:00:15]: The man continues to speak, gesturing with his hands to emphasize his points. His expression occasionally changes to a smile. [0:00:16]: The scene changes to a graphic with the number 10 and an image of a spoon, all displayed on a blue background. [0:00:17 - 0:00:19]: A new scene shows the man from the previous frames dressed in a white chef's coat, arms crossed, standing in front of the text \"RAMSAY in 10\" with a similar blue background.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the man standing in front of in the initial video?",
                "time_stamp": "00:00:20",
                "answer": "D",
                "options": [
                    "A. A wooden cabinet.",
                    "B. A glass door.",
                    "C. A blackboard.",
                    "D. A stainless steel refrigerator."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_41_real.mp4"
    },
    {
        "time": "[0:01:00 - 0:02:00]",
        "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:00]: An arm is moving across a white bowl, partially blocking the view. The bowl contains a white substance, possibly a batter or sauce, situated on a kitchen counter. There are other kitchen items situated near the bowl, but their details are unclear due to the arm's obstruction. [0:01:01 - 0:01:02]: A woman with long brown hair, wearing a black sleeveless top, is holding a jar in her right hand while her left hand is near a white bowl on the counter. There is a loaf of bread, a carton of eggs, a carton of milk, and various kitchen utensils spread out on the marble countertop. Behind the woman is a stainless steel refrigerator, a tiled white wall with a window showing greenery outside, and a cabinet with potted plants. [0:01:02 - 0:01:03]: The woman opens the jar she is holding and looks down at the countertop. She is positioned slightly to the left of the refrigerator doors. A black frying pan is seen next to the bowl on top of a stove. [0:01:03 - 0:01:04]: A close-up view of the white bowl displays two cracked eggs with yolks visible, resting in the white substance. The background reveals parts of the stove and a dark object, possibly a piece of cloth, next to the bowl. [0:01:04 - 0:01:04]: The woman's hand, with neatly manicured nails, is holding a jar of seasoning above the bowl. The seasoning appears to be about to be sprinkled onto the eggs and white substance in the bowl. [0:01:05 - 0:01:06]: The woman begins to scoop some seasoning from the jar with a metal spoon. The ridged surface of the countertop and part of the stove can be seen in the background.  [0:01:06 - 0:01:07]: The woman\u2019s hands are shown holding the jar and the spoon, preparing to sprinkle the seasoning onto the eggs. The white bowl remains stationary on the counter. [0:01:07 - 0:01:08]: Close-up of the woman\u2019s hands sprinkling the seasoning directly into the bowl over the eggs. The black stove and its components are visible in the background. [0:01:08 - 0:01:09]: A close-up view of the bowl reveals the sprinkled seasoning on top of the egg yolks and whites. The substance mixture surrounds the eggs in the bowl. [0:01:09 - 0:01:10]: The eggs and seasoning in the bowl are clearly shown. The detailed texture of the stove surface next to the countertop is visible. [0:01:11 - 0:01:12]: The woman turns to her left, reaching towards an unseen object. The countertop still displays a loaf of bread, egg carton, and other items, while the refrigerator and tiled wall remain in the background. [0:01:12 - 0:01:13]: The woman is seen holding a lemon in her right hand as she looks down towards the countertop. Her left hand reaches towards the items on the marble counter. [0:01:13 - 0:01:14]: The woman begins to zest the lemon over the bowl with a zester held in her left hand. Her focus is directed towards the bowl. [0:01:14 - 0:01:15]: The woman continues zesting the lemon over the bowl, and the yellow zest starts to fall onto the egg mixture. The surrounding kitchen items maintain their positions on the counter. [0:01:15 - 0:01:16]: Close-up view of the lemon being zested, with yellow lemon zest visible over the egg mixture in the bowl. The setup of the kitchen counter and the stove surface remains the same. [0:01:16 - 0:01:17]: More lemon zest falls into the bowl as the woman continues to zest the lemon. The lemon zest starts to accumulate on top of the egg mixture. [0:01:17 - 0:01:18]: The woman continues to zest the lemon while looking intently at the bowl. Her left hand holds the zester firmly, and she is focused on grating more zest. [0:01:18 - 0:01:19]: The woman briefly pauses, still holding the lemon and zester. She turns her head to the left as if to glance at something off-camera. The kitchen counter, tools, and stove are in their original positions.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the woman holding in her left hand?",
                "time_stamp": "0:01:04",
                "answer": "D",
                "options": [
                    "A. A loaf of bread.",
                    "B. A carton of milk.",
                    "C. A frying pan.",
                    "D. A jar."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Action Recognition",
                "question": "What does the woman do after opening the jar?",
                "time_stamp": "0:01:03",
                "answer": "A",
                "options": [
                    "A. Pour the seasoning inside it into the white bowl with eggs.",
                    "B. Looks out the window.",
                    "C. Moves to the refrigerator.",
                    "D. Picks up a loaf of bread."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_41_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:00 - 0:02:20] [0:02:00]: A woman with long dark hair, wearing a sleeveless black top, stands in a kitchen holding a white shaker bottle with a black top. The background features stainless steel appliances and white tiles on the walls. [0:02:01 - 0:02:05]: The woman is in front of a marble kitchen counter full of ingredients: a large white bowl, a smaller bowl, a loaf of bread, an egg carton, some butter, a brown plate, and various containers. She is reaching for a box of cereal. [0:02:05 - 0:02:07]: She starts opening the cereal box, holding it with both hands, and situating it in front of her. Various ingredients remained spread across the counter. [0:02:07 - 0:02:08]: She opens the cereal box and prepares to pour its contents. Next to the ingredients on the counter are a large white bowl, butter, eggs, spices, and other containers. [0:02:09 - 0:02:14]: She pours the cereal from the box into a clear shaker bottle held by her left hand. The camera captures her movements in front of the stainless steel fridge and kitchen cabinets.  [0:02:15 - 0:02:19]: The woman continues pouring cereal into the shaker bottle until it is filled. Various kitchen items like a knife, a green lid, spices, and butter are on the counter next to her.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the woman doing right now?",
                "time_stamp": "0:02:00",
                "answer": "A",
                "options": [
                    "A. Pour the cereal from the box into a transparent glass of water.",
                    "B. Pour the sugar from the jar into a cup of hot tea.",
                    "C. Pour the pasta from the bag into a pot of boiling water.",
                    "D. Pour the flour from the container into a bowl of milk."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_41_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:01]: A close-up of a hand holding a slice of bread, dipping it into a bowl filled with a yellow custard-like mixture. The bread is slightly submerged, and the hand is positioned on the right side of the bowl, guiding the bread in with careful precision. [0:04:01 - 0:04:02]: The hand continues to dip the slice of bread into the mixture, covering it thoroughly. The fingers are slightly spread to maintain a firm grip on the bread, and the bowl is white with a ribbed texture on the outside. [0:04:02 - 0:04:03]: The camera zooms out to reveal a woman standing in a modern, well-lit kitchen. She is placing the bowl with the dipped bread on the counter while focusing on the task at hand. There are two slices of bread, a carton of eggs, and a mixing bowl on the countertop. [0:04:03 - 0:04:04]: The woman continues to focus on preparing the bread, now using a spatula to ensure the bread is evenly coated with the mixture. The stainless steel refrigerator and the white subway tile backsplash are visible in the background.  [0:04:04 - 0:04:05]: The woman shifts her attention to the bowl, lifting the soaked bread with the spatula. The bread is visibly soaked in the yellow mixture, and she is careful to let any excess drip back into the bowl before moving it. [0:04:05 - 0:04:06]: She holds the bread over the bowl, allowing any extra liquid to drip off before moving it to the hot surface. Her movements are precise and deliberate, ensuring minimal mess. [0:04:06 - 0:04:07]: As she continues holding the bread, she maintains a steady grip with her left hand, slightly tilting the bowl to prevent spills. The kitchen is bright, with natural light illuminating the space through a large window. [0:04:07 - 0:04:08]: She places the soaked bread back into the bowl briefly, possibly to adjust or soak the other side evenly. The other kitchen items remain in their original positions on the counter. [0:04:08 - 0:04:09]: She gently shakes the bread to ensure it is thoroughly coated, preparing to move it to the cooking surface. The woman\u2019s focus remains on her task, showing a methodical approach to her cooking. [0:04:09 - 0:04:10]: The bread is then carefully transferred to another container or surface, outside the current frame, likely for cooking. Her hands are steady, and the countertop remains clean and organized. [0:04:10 - 0:04:11]: Another close-up shows the bread being dipped into a different mixture, potentially for another coating. The texture of the mixture appears consistent and smooth. [0:04:11 - 0:04:12]: The bread receives another thorough coating, with the hands ensuring all sides are covered uniformly. The white bowl used earlier is still present and filled with the mixture. [0:04:12 - 0:04:13]: The camera zooms out to reveal the woman back at the counter, focusing on preparing the next piece of bread. The kitchen\u2019s modern appliances and decor, including the potted plant by the window, are visible in the background. [0:04:13 - 0:04:14]: She sets down the coated bread, possibly on a cooking surface, and prepares to reach for another slice. Her attention to detail is evident as she maintains the workspace. [0:04:14 - 0:04:15]: The woman momentarily pauses, perhaps to adjust an item or check the progress of the cooking bread. The kitchen remains brightly lit, showcasing its clean and modern design. [0:04:15 - 0:04:16]: She reaches towards the left side of the counter, picking up another kitchen tool or ingredient. The focus remains on her consistent and deliberate actions. [0:04:16 - 0:04:17]: She prepares another slice of bread for dipping, ensuring it's evenly sliced and ready for the mixture. The countertop reflects the bright lighting of the kitchen. [0:04:17 - 0:04:18]: The bread is being sliced carefully, with a knife held in her right hand. She continues to maintain a methodical approach to the preparation. [0:04:18 - 0:04:19]: A close-up of the bread being dipped into the mixture again shows careful attention to even coating. The yellow mixture appears thick and smooth.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What does the woman use to ensure the bread is evenly coated with the mixture?",
                "time_stamp": "0:04:04",
                "answer": "D",
                "options": [
                    "A. A spoon.",
                    "B. Her fingers.",
                    "C. A fork.",
                    "D. A spatula."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_41_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:30]",
        "questions": [
            {
                "task_type": "Prospective Reasoning",
                "question": "What mathematical concept will likely be demonstrated next?",
                "time_stamp": "00:00:09",
                "answer": "D",
                "options": [
                    "A. Subtraction.",
                    "B. Addition.",
                    "C. Multiplication.",
                    "D. Division."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "math tutorials",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_229_real.mp4"
    },
    {
        "time": "[0:02:42 - 0:03:12]",
        "questions": [
            {
                "task_type": "Prospective Reasoning",
                "question": "What is the presenter likely to do next?",
                "time_stamp": "00:03:12",
                "answer": "D",
                "options": [
                    "A. Add 48 to 52.",
                    "B. Multiply 48 by 52.",
                    "C. Divide 48 by 52.",
                    "D. Subtract 48 from 52."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "math tutorials",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_229_real.mp4"
    },
    {
        "time": "[0:05:24 - 0:05:54]",
        "questions": [
            {
                "task_type": "Prospective Reasoning",
                "question": "What might the speaker discuss next?",
                "time_stamp": "00:05:38",
                "answer": "A",
                "options": [
                    "A. The steps to perform the division.",
                    "B. The final quotient of the division.",
                    "C. The relevance of the remainder.",
                    "D. How to multiply the dividend."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "math tutorials",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_229_real.mp4"
    },
    {
        "time": "[0:08:06 - 0:08:36]",
        "questions": [
            {
                "task_type": "Prospective Reasoning",
                "question": "What might the speaker do next?",
                "time_stamp": "00:08:44",
                "answer": "A",
                "options": [
                    "A. Subtract 440 from 528.",
                    "B. Divide 528 by 55.",
                    "C. Subtract 88 from 528.",
                    "D. Add 5 to 88."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "math tutorials",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_229_real.mp4"
    },
    {
        "time": "[0:10:48 - 0:11:18]",
        "questions": [
            {
                "task_type": "Prospective Reasoning",
                "question": "What might the speaker explain next?",
                "time_stamp": "00:11:30",
                "answer": "A",
                "options": [
                    "A. How to address the situation where the remainder is less than the divisor.",
                    "B. How to check the division result.",
                    "C. How to proceed with the next division step.",
                    "D. How to interpret the decimal expansion."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "math tutorials",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_229_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:10]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the barista's actions just now?",
                "time_stamp": "00:00:14",
                "answer": "D",
                "options": [
                    "A. The barista cleaned the counter, sanitized cups, and organized coffee beans on the shelves.",
                    "B. The barista wiped down the espresso machine, added a new milk container, and prepared a cappuccino.",
                    "C. The barista organized the espresso machine, ground fresh coffee beans, and created a detailed latte art design.",
                    "D. The barista cleaned the milk frothing equipment, and brought the two cups of prepared espresso to the customers.."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_358_real.mp4"
    },
    {
        "time": "[0:01:42 - 0:01:52]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the bartender's actions just now?",
                "time_stamp": "00:01:55",
                "answer": "D",
                "options": [
                    "A. The bartender filled a pitcher with beer and served it to customers at the bar.",
                    "B. The bartender cleaned the taps, filled a pitcher with hot water, and steeped tea bags in it.",
                    "C. The bartender prepared a fruit smoothie by blending fresh fruits and ice, then pouring it into a glass to serve.",
                    "D. The bartender steamed milk using a frothing wand, cleaned the wand, and combined the steamed milk with an espresso shot."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_358_real.mp4"
    },
    {
        "time": "[0:03:24 - 0:03:34]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions taken by the individual just now?",
                "time_stamp": "00:03:33",
                "answer": "D",
                "options": [
                    "A. The individual prepared coffee beans by weighing them, grinding them, and adjusting the grind size.",
                    "B. The individual brewed a coffee by immersing the beans in boiling water and stirring frequently.",
                    "C. The individual cleaned the coffee machine, steamed milk, and poured it into a cup to serve a cappuccino.",
                    "D. The individual prepared a portafilter with ground coffee, weighed it, and used a spoon to adjust the coffee amount."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_358_real.mp4"
    },
    {
        "time": "[0:05:06 - 0:05:16]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions taken by the barista just now?",
                "time_stamp": "00:05:16",
                "answer": "D",
                "options": [
                    "A. The barista steamed milk, weighed it, and poured it into a coffee cup.",
                    "B. The barista cleaned the coffee machine, weighed coffee grounds, and placed them into the portafilter.",
                    "C. The barista poured milk into a glass, placed it on a scale, and then threw the milk container away.",
                    "D. The barista poured milk into a measuring cup, weighed it, and then returned the container to the fridge."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_358_real.mp4"
    },
    {
        "time": "[0:06:48 - 0:06:58]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the barista's actions just now?",
                "time_stamp": "00:06:55",
                "answer": "D",
                "options": [
                    "A. The barista poured milk into a pitcher, steamed it, and then mixed it into a hot cup of coffee.",
                    "B. The barista filled a pitcher with milk, adjusted the coffee machine, rinsed the pitcher, and cleaned the counter area.",
                    "C. The barista filled a pitcher with milk, cleaned the pitcher, and prepared an iced coffee.",
                    "D. The barista poured milk into a glass, and steamed the milk in a pitcher."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_358_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:05]",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the driver currently doing with his hands?",
                "time_stamp": "00:00:06",
                "answer": "C",
                "options": [
                    "A. Honking the horn.",
                    "B. Shifting gears.",
                    "C. Holding the steering wheel.",
                    "D. Adjusting the dashboard."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_267_real.mp4"
    },
    {
        "time": "[0:02:04 - 0:02:09]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What logo is visible right now?",
                "time_stamp": "00:02:05",
                "answer": "C",
                "options": [
                    "A. JKS-Racing-Club.",
                    "B. JKS-Sports-Car.",
                    "C. JKS-Classic-Cars.",
                    "D. JKS-Rally-Sport."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_267_real.mp4"
    },
    {
        "time": "[0:04:08 - 0:04:13]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is placed in front of the co-driver?",
                "time_stamp": "00:04:10",
                "answer": "C",
                "options": [
                    "A. A map.",
                    "B. A joystick.",
                    "C. A notebook.",
                    "D. A bottle of water."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_267_real.mp4"
    },
    {
        "time": "[0:06:12 - 0:06:17]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color are the driver's gloves right now?",
                "time_stamp": "00:06:14",
                "answer": "B",
                "options": [
                    "A. Blue.",
                    "B. Red.",
                    "C. Yellow.",
                    "D. Black."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_267_real.mp4"
    },
    {
        "time": "[0:08:16 - 0:08:21]",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What brand is visible on the steering wheel right now?",
                "time_stamp": "00:08:18",
                "answer": "B",
                "options": [
                    "A. JKS.",
                    "B. OMP.",
                    "C. Freem.",
                    "D. Classic-Cars."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_267_real.mp4"
    },
    {
        "time": "0:00:00 - 0:00:20",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:04]: On top of a black surface, there are two transparent containers laid out horizontally, filled with various tiny colorful flower petals, organized by type in separate compartments. Below these containers, three hexagonal keychains are placed neatly in a row. These keychains have names 'Emma', 'Lory', and 'Susan' written on them in elegant cursive with embedded flowers of different shapes and colors. Scattered around on the table are smaller flower petals in various vibrant colors including pink, blue, red, and yellow, along with green leaf-shaped pieces;  [0:00:05]: The screen goes black;  [0:00:06 - 0:00:09]: The text \"Flower Keychains\" appears displayed steadily against a solid black background;  [0:00:10]: In a well-lit workspace with a gridded cutting mat, four plain white hexagonal shapes are arranged in a line. A pair of hands, wearing black gloves, holds two containers, labeled \"PUDUO Epoxy Resin Part A\" and \"PUDUO Epoxy Resin Hardener Part B,\" respectively. In the center, a small, transparent plastic cup is placed;  [0:00:11 - 0:00:16]: The person shows the two bottles of Epoxy Resin to the camera, clearly labeled as part A and part B. Details such as \"Crystal Clear,\" \"Self Leveling,\" and \"Low Odor\" can be seen on the labels of both containers. The hands hold the bottles steadily over the workspace, giving a clear view of the instructions and details;  [0:00:17 - 0:00:19]: The hands begin to pour the contents of the \"Epoxy Resin Part A\" into the transparent plastic cup, positioning it next to the hexagonal shapes prepared for the crafting process in the workspace. The area remains well-organized with the tools neatly positioned for the subsequent steps.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the person doing right now?",
                "time_stamp": "00:00:20",
                "answer": "A",
                "options": [
                    "A. Pouring Epoxy Resin Part A into a plastic cup.",
                    "B. Mixing flower petals in a container.",
                    "C. Writing names on the keychains.",
                    "D. Organizing the hexagonal shapes."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_63_real.mp4"
    },
    {
        "time": "0:01:40 - 0:02:00",
        "captions": "[0:01:40 - 0:02:00] [0:01:41 - 0:01:49]: A pair of hands wearing black gloves is seen from a first-person perspective. The hands are working on a blue cutting mat. They hold a transparent plastic box containing various small, colorful items, likely decorative pieces. There is a jar with a mixing stick to the left. Some small floral decorations are positioned on the right, while three white hexagon-shaped tags are laid out horizontally across the top. [0:01:50 - 0:01:58]: The jar with the mixing stick, the transparent box with decorative items, and the floral decorations remain in the same positions. The hands continue to manipulate the small items using tweezers, carefully selecting and moving pieces. [0:01:59]: Attention shifts to a small clear container being held above the hexagon-shaped tags. This container appears to be related to the crafting process.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is currently placed under the hexagonal label?",
                "time_stamp": "00:01:59",
                "answer": "A",
                "options": [
                    "A. A small clear container.",
                    "B. A transparent plastic box with colorful items.",
                    "C. A mixing stick.",
                    "D. Tweezers."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_63_real.mp4"
    },
    {
        "time": "0:03:20 - 0:03:40",
        "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:21]: In the initial frame, there is a detailed close-up of a workspace, with a blue cutting mat covering the surface. On the mat, there is a small clear plastic cup containing liquid being held in a black gloved hand positioned on the left side of the frame. Directly in front of the cup, a white piece of material with an irregular, slightly hexagonal shape is placed. To the right, another gloved hand uses a wooden stick to manipulate some material on the hexagon-shaped piece. Nearby, small decorative items such as miniature flowers and leaves are scattered around. [0:03:22 - 0:03:23]: The gloved hand on the right continues to adjust the small decorative elements on the hexagon-shaped piece using the wooden stick. The hand holding the clear cup remains steady, ensuring that the liquid is ready to be poured if necessary. [0:03:24 - 0:03:26]: The positions of the hand and the decorative elements stay largely consistent. The liquid in the clear cup remains unused, as the focus stays on arranging the small decorations on the hexagon-shaped piece. The background consists of several compartments containing various colorful items and tools. [0:03:27]: The clear cup with the liquid is set down on the blue cutting mat to the left of the workspace, while the other hand continues to use the wooden stick to adjust the decorations on the hexagon-shaped piece. [0:03:28]: The right gloved hand now holds a tool with a pink handle near the hexagon-shaped piece, while the left still holds the clear cup. The decorative elements on the hexagon are being carefully placed. [0:03:29]: The left hand places another hexagon-shaped piece on the workspace, preparing it for the next steps. The right hand does not manipulate any decorations at this moment. [0:03:30 - 0:03:32]: The right hand uses a pen-like tool to make precise adjustments to the newly placed hexagon-shaped piece, ensuring everything is in the correct position. [0:03:33]: The right hand reaches down to the workspace. The left hand is seen picking up a tool with a pink handle while the clear cup with the liquid remains on the blue cutting mat.  [0:03:34 - 0:03:35]: The left-hand lifts the clear cup with the liquid again, and the right hand uses the wooden stick to stir or align contents within the cup. [0:03:36]: The scene remains focused on the left hand holding the clear cup slightly tilted, and the right hand continuing to manipulate the contents, preparing to add them to the hexagon-shaped piece. [0:03:37 - 0:03:38]: Switching tools, the right hand holds a different tool, possibly for precision work. Meanwhile, the left hand continues to hold the clear cup, ready to assist with the next task. [0:03:39]: The scene shifts slightly as the tool held by the right hand is now closer to the hexagon-shaped piece, indicating new decorative elements or details being added. The workspace in the background remains cluttered with various tools and materials.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the left gloved hand doing right now?",
                "time_stamp": "00:03:25",
                "answer": "A",
                "options": [
                    "A. Holding a clear cup with liquid.",
                    "B. Using a wooden stick to manipulate decorations.",
                    "C. Placing another hexagon-shaped piece on the workspace.",
                    "D. Making precise adjustments with a pen-like tool."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_63_real.mp4"
    },
    {
        "time": "0:05:00 - 0:05:20",
        "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:06]: A hand wearing a black glove is holding a transparent cup containing a clear liquid on the left side of the frame. The hand is positioned above a blue grid cutting mat, which covers the entire background of the scene. On the mat, there are four hexagonal pieces with floral decorations aligned horizontally near the top. The first and second pieces from the left contain pink and red flowers, while the third contains green foliage, and the fourth is empty. To the right, a few loose pieces of pink flowers and green leaves are scattered. A paintbrush with a pink handle is placed close to the right edge of the mat. The hand holding the cup is seen pouring the liquid into the first and second hexagonal pieces containing pink flowers.  [0:05:07]: The gloved hand points to an empty hexagonal piece on the mat with an index finger while the other hand holds the transparent cup to the left. A wooden stick is held in the right hand, positioned above the mat. [0:05:08]: The left hand still holds the cup, while the right hand, now closer to the mat, holds a white tool with a pink handle. The tool is used to scrape something from the surface of the hexagonal piece. [0:05:09 - 0:05:17]: Both hands are engaged in the activity. The left hand continues to hold the transparent cup, while the right hand uses a wooden stick to work with the empty hexagonal piece on the mat. The stick and cup move in coordinated motions, suggesting that the liquid is being poured into the hexagonal piece. Occasionally, the right hand adjusts the placement of small floral and foliage decorations inside the hexagonal piece to arrange them properly. [0:05:18 - 0:05:19]: The hand continues to precisely place the clear liquid into the hexagonal piece using a wooden stick. The other hand is steady, supporting the movements by holding the cup firmly. Small floral and foliage decorations are carefully positioned within the hexagon.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the gloved hand doing right now?",
                "time_stamp": "00:05:10",
                "answer": "A",
                "options": [
                    "A. Pouring liquid into a hexagonal piece.",
                    "B. Scraping the surface of the hexagonal piece.",
                    "C. Adjusting floral and foliage decorations.",
                    "D. Holding the paintbrush with a pink handle."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_63_real.mp4"
    },
    {
        "time": "0:06:40 - 0:07:00",
        "captions": "[0:06:40 - 0:07:00] [0:06:51 - 0:06:54]: A pair of hands, with a ring on one of the fingers, is operating on a hexagonal-shaped translucent piece with pink and green markings. This piece is placed in the center of the frame, which is set against a grey grid-patterned work surface. Surrounding this central piece on the top are three more hexagon pieces and three rectangular white pieces, all evenly spaced. [0:06:54 - 0:06:58]: The hands start to apply pressure on the green marking of the hexagonal-shaped piece, making sure the piece stays secure and in position. You can also notice that the person's sleeves are grey, which complements the grey grid-patterned background. [0:06:58 - 0:07:01]: The hands slightly adjust the hexagonal piece, ensuring it is precisely placed. The surrounding pieces remain in their positions, untouched. The background maintains its grey grid pattern throughout this segment.  [0:07:01 - 0:07:03]: A new tool is introduced into the frame. A small purple device is held in the right hand while still pressing the hexagonal piece with the left. The purple device appears to be aimed at the hexagonal piece, demonstrating careful handling and positioning. [0:07:03 - 0:07:05]: The purple tool is used to press something onto the hexagonal piece. The other elements\u2014the three hexagonal pieces and the three rectangular pieces\u2014stay in their original positions, undisturbed. [0:07:05 - 0:07:08]: The pressure is applied consistently with the purple tool on the hexagonal piece in the center, ensuring precise contact. The background and position of surrounding pieces remain unchanged. [0:07:08 - 0:07:11]: The hands move the purple tool away, setting it aside from the working area. The untreated pieces (three hexagonal and three rectangular white pieces) stay in place on the grey grid-patterned surface. [0:07:11 - 0:07:13]: The left hand now maneuvers the hexagonal piece again, ensuring it's properly adjusted and secure after the use of the purple tool. The grid-patterned surface continues to serve as the background. [0:07:13 - 0:07:16]: The hands then pick up another tool, possibly tweezers or a pick, and hold it near the hexagonal piece, starting to engage in another step of the process. The rest of the pieces stay immobile. [0:07:16 - 0:07:19]: Attention shifts slightly as the tool in the right hand works on the hexagonal piece, focusing on a specific part of the piece. The detail work implies delicacy and precision. [0:07:19 - 0:07:22]: The hands pause for a fraction of a second, seemingly verifying the progress on the hexagonal piece. The other craft pieces and the grey background remain constant",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What is the person doing right now?",
                "time_stamp": "00:07:22",
                "answer": "A",
                "options": [
                    "A. Attaching the name to the transparent hexagonal object.",
                    "B. Adjusting the hexagonal piece with their left hand.",
                    "C. Applying pressure on the green marking with their left hand.",
                    "D. Using a purple tool to press onto the hexagonal piece."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "handcraft",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_63_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:10]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the sequence of actions taken just now?",
                "time_stamp": "0:00:10",
                "answer": "A",
                "options": [
                    "A. The person unlocks the door, opens it to let in light, and mentions the opening.",
                    "B. The person prepares a dish of pasta, garnishes it, and places it on a serving tray.",
                    "C. The person sets tables, arranges seats, and places cutlery in a coffee shop.",
                    "D. The person cleans a window, arranges flowers, and adjusts a sign."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_410_real.mp4"
    },
    {
        "time": "[0:03:49 - 0:03:59]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions performed just now?",
                "time_stamp": "0:03:59",
                "answer": "A",
                "options": [
                    "A. The individual measured and poured milk into a container, stored the milk, and then started steaming the milk.",
                    "B. The individual prepared a sandwich, added vegetables, and wrapped it for serving.",
                    "C. The individual prepared a salad by chopping and mixing various vegetables.",
                    "D. The individual polished dishes, arranged them on a tray, and placed them on a shelf."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_410_real.mp4"
    },
    {
        "time": "[0:07:38 - 0:07:48]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the sequence of actions taken just now?",
                "time_stamp": "0:07:54",
                "answer": "A",
                "options": [
                    "A. The person pours steamed milk into a cup of espresso, creating a latte, and places it on the counter.",
                    "B. The person fills a pitcher with water, hands it to a customer, and wipes the counter.",
                    "C. The person makes a cup of tea, adds sugar, and serves it to a customer.",
                    "D. The person brews a pot of coffee, fills a mug, and hands it to a coworker."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_410_real.mp4"
    },
    {
        "time": "[0:11:27 - 0:11:37]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the sequence of actions taken just now?",
                "time_stamp": "0:11:37",
                "answer": "A",
                "options": [
                    "A. The person steamed milk, poured it into a cup with espresso.",
                    "B. The person took a cup, added sugar, and stirred it with a spoon.",
                    "C. The person brewed a fresh cup of coffee, added milk, and handed it to the customer.",
                    "D. The person prepared tea, added honey, and placed it on the counter."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_410_real.mp4"
    },
    {
        "time": "[0:15:16 - 0:15:26]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions taken just now?",
                "time_stamp": "0:15:26",
                "answer": "A",
                "options": [
                    "A. The person weighed coffee beans.",
                    "B. The person prepared vegetables, cooked them, and served them on a plate.",
                    "C. The person measured milk, steamed it, and poured it into a cup.",
                    "D. The person brewed tea, added honey, and handed the cup to the customer."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_410_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:00:16",
                "answer": "D",
                "options": [
                    "A. 2.",
                    "B. 3.",
                    "C. 4.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_90_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:03:44",
                "answer": "B",
                "options": [
                    "A. 3.",
                    "B. 5.",
                    "C. 6.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_90_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:04:14",
                "answer": "D",
                "options": [
                    "A. 2.",
                    "B. 4.",
                    "C. 3.",
                    "D. 6."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_90_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:05:27",
                "answer": "B",
                "options": [
                    "A. 2.",
                    "B. 8.",
                    "C. 4.",
                    "D. 6."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_90_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:06:24",
                "answer": "C",
                "options": [
                    "A. 4.",
                    "B. 8.",
                    "C. 10.",
                    "D. 7."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_90_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:10]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions just taken?",
                "time_stamp": "00:00:10",
                "answer": "C",
                "options": [
                    "A. The person brewed a pot of tea, added lemon, and served it.",
                    "B. The person prepared a glass of milk by measuring its weight and pouring it into a jug.",
                    "C. The person measured ground coffee beans, and gathered different types of milk alternatives.",
                    "D. The person set up the kitchen for baking by arranging flour, sugar, and eggs."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_355_real.mp4"
    },
    {
        "time": "[0:02:03 - 0:02:13]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions just taken?",
                "time_stamp": "00:02:13",
                "answer": "D",
                "options": [
                    "A. The person filled a cup with hot water, added a tea bag, and let it steep.",
                    "B. The person prepared a cup for a latte, steamed oat milk, and gathered different types of syrups.",
                    "C. The person filled a pitcher with water, added ice, and prepared to brew coffee.",
                    "D. The person steamed oat milk, and prepared to make a coffee."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_355_real.mp4"
    },
    {
        "time": "[0:04:06 - 0:04:16]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions just taken?",
                "time_stamp": "00:04:16",
                "answer": "B",
                "options": [
                    "A. The individual brewed a pot of tea and added a slice of lemon.",
                    "B. The individual cleaned a pitcher, filled it with milk, and steamed it.",
                    "C. The individual washed a jug, filled it with water, and placed it in a fridge.",
                    "D. The individual rinsed a coffee cup, added hot water, and wiped the counter."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_355_real.mp4"
    },
    {
        "time": "[0:06:09 - 0:06:19]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions just taken?",
                "time_stamp": "00:06:19",
                "answer": "B",
                "options": [
                    "A. The person prepared a glass of water and added a slice of lemon.",
                    "B. The person cleaned a kitchen surface, wiped it down, and dried it with a cloth.",
                    "C. The person used a spray bottle to sanitize the counter, then dried it with a towel.",
                    "D. The person wet a cloth, cleaned the surface with it, then dried it with paper towels."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_355_real.mp4"
    },
    {
        "time": "[0:08:12 - 0:08:22]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions just taken?",
                "time_stamp": "00:08:20",
                "answer": "B",
                "options": [
                    "A. The individual brewed a cup of coffee and set it on the counter.",
                    "B. The individual cleaned coffee grounds by a little broom and disposed of the waste in a bin.",
                    "C. The individual refilled the coffee grinder with fresh coffee beans.",
                    "D. The individual washed and dried a coffee cup before using it for a new drink."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_355_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:10]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions taken just now?",
                "time_stamp": "00:00:8",
                "answer": "C",
                "options": [
                    "A. The individual poured milk into a cup, stirred it, and then took it to the customer.",
                    "B. The individual brewed a pot of tea, poured it into a thermal flask, and brought it to a table.",
                    "C. The individual finished preparing a latte, placed it alongside a black coffee.",
                    "D. The individual selected pastries from a display case, packed them in a box, and handed them to a customer."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_364_real.mp4"
    },
    {
        "time": "[0:02:54 - 0:03:04]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions taken just now?",
                "time_stamp": "00:03:04",
                "answer": "B",
                "options": [
                    "A. The individual checked order status on a tablet, cleaned a countertop area, and served a sandwich.",
                    "B. The individual prepare matcha, carried the matcha to a sink, and then checked order details on a tablet.",
                    "C. The individual brewed coffee, poured it into a cup, and then prepared a latte with milk.",
                    "D. The individual selected utensils, eco-certified cleaning solutions, and organized a storage area."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_364_real.mp4"
    },
    {
        "time": "[0:05:48 - 0:05:58]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions taken just now?",
                "time_stamp": "00:05:58",
                "answer": "B",
                "options": [
                    "A. The individual selected tea leaves, boiled water, and brewed a fresh pot of tea.",
                    "B. The individual poured milk into a metal jug, steamed the milk, and prepared it for use.",
                    "C. The individual prepared an espresso shot, added steamed milk, and created latte art.",
                    "D. The individual grabbed utensils, washed them, and organized them on a drying rack."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_364_real.mp4"
    },
    {
        "time": "[0:08:42 - 0:08:52]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions taken just now?",
                "time_stamp": "00:08:51",
                "answer": "B",
                "options": [
                    "A. The individual filled a cup with hot water, steeped a tea bag, and served the tea to a customer.",
                    "B. The individual poured milk into a container, steamed it, and cleaned the workstation with a cloth.",
                    "C. The individual brewed a pot of coffee, poured it into multiple cups, and served it to customers.",
                    "D. The individual mixed various ingredients into a jug, prepared a smoothie, and cleaned the blender."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_364_real.mp4"
    },
    {
        "time": "[0:11:36 - 0:11:46]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions taken just now?",
                "time_stamp": "00:11:46",
                "answer": "B",
                "options": [
                    "A. The individual poured hot water into a cup, steeped the tea, and served it.",
                    "B. The individual weighed matcha powder.",
                    "C. The individual mixed chocolate powder into a glass and added hot milk.",
                    "D. The individual cleaned the workspace with a cloth and disposed of the trash."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_364_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03, 0:00:05 - 0:00:07, 0:00:09, 0:00:12, 0:00:15]: In a brightly lit room with tall windows revealing a green, leafy exterior, a nurse in a white and blue uniform with a red cross on her apron is seen making a bed. She is diligently straightening the bedspread and adjusting the pillows. The surroundings include vintage iron-framed beds, ceramic bowls, and mugs on a table covered with a patterned cloth. The nurse moves with focused intent, showing careful attention to ensuring the bed is neatly made. A Union Jack flag hangs on the wall beside her. [0:00:04]: The scene abruptly shifts to a close-up of muddy boots on wooden planks, indicating a rough, wet environment. [0:00:08]: In a dark, stormy night, branches and leaves fly around, carried by a strong wind amidst ominous surroundings. Debris and dirt are being violently stirred up. [0:00:10 - 0:00:11]: A soldier in a helmet and dark uniform is seen in a chaotic trench environment. There is intense smoke and explosions around him, suggesting a battlefield. He appears to be in distress, moving hurriedly through the trench, with one hand holding onto his helmet, indicating the dire situation. [0:00:13 - 0:00:14]: The perspective changes to looking through a tattered net, revealing only glimpses of what lies beyond it. The view is obstructed but it suggests a hidden or protective stance, trying to see while staying concealed. [0:00:16 - 0:00:19]: The final frames show a desolate, war-torn landscape. A soldier lies motionless on the ground amidst the dirt and rubble. The scene is gloomy, shrouded in dust and despair, with the figure remaining prone until the screen fades to black.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is hanging on the wall beside the nurse?",
                "time_stamp": "0:00:14",
                "answer": "C",
                "options": [
                    "A. A clock.",
                    "B. A portrait.",
                    "C. A Union Jack flag.",
                    "D. A medical certificate."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Action Recognition",
                "question": "What is the nurse doing in the room?",
                "time_stamp": "0:00:20",
                "answer": "C",
                "options": [
                    "A. Preparing a meal.",
                    "B. Writing notes.",
                    "C. Making a bed.",
                    "D. Cleaning the floor."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "getting_dressed",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_162_real.mp4"
    },
    {
        "time": "[0:01:00 - 0:02:00]",
        "captions": "[0:01:00 - 0:01:20] [0:01:00]: A small dark blue cloth bag is lying on a textured white surface, likely a bedspread. The bag has a white label with writing on it. [0:01:01 - 0:01:02]: Three individuals are in a room with light-colored walls and large windows showing a green landscape outside. Two women dressed as nurses are attending to a person sitting on a chair. The setting appears to be a historical or medical environment. Both nurses, identifiable by their uniforms, are kneeling in front of the seated person, who is dressed in white. [0:01:03 - 0:01:09]: Close-up view of the two nurses assisting the seated person by putting on a pair of green socks. The action is focused on the hands and feet of the individuals, with detailed wooden flooring visible in the background. [0:01:10 - 0:01:19]: The nurses complete putting on the socks and continue to interact with the seated person. The room features metal-framed beds with white linens, a wooden floor, and a British flag hanging on the wall to the right. Various medical utensils can be seen next to one of the beds. One nurse is adjusting the seated individual's socks, while the other begins to stand up. The seated person looks on with a neutral expression while the nurses continue their duties. The room remains brightly lit by the natural light streaming in through the large windows.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color are the socks being put on the seated person?",
                "time_stamp": "00:01:12",
                "answer": "D",
                "options": [
                    "A. Red.",
                    "B. Blue.",
                    "C. Yellow.",
                    "D. Green."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "getting_dressed",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_162_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:02]: In a well-lit room, a man wearing a white shirt and red tie is sitting on a chair. Two nurses in white uniforms with red crosses on their chests are assisting him, adjusting his attire. The room has large windows in the background, offering a view of greenery outside. Two neatly made beds are placed parallel to the windows, with a British flag draped on the wall to the right. Various medical supplies are placed on a bedside table. [0:02:03 - 0:02:07]: One of the nurses begins to prepare an item, likely a medical gown, which is laid on one of the beds. She picks up the gown and holds it out, possibly to assist the man in wearing it. The other nurse continues to be attentive to the man, ensuring he is comfortable. [0:02:08 - 0:02:11]: Both nurses now focus on helping the man put on the gown. They carefully lift and adjust it, ensuring it is properly positioned. The man remains seated, slightly adjusting his arms to facilitate the process. The three maintain a cooperative interaction, with a clear division of tasks. [0:02:12 - 0:02:14]: The nurses continue to carefully assist the man in putting on the gown. They attentively handle the gown, making sure it is correctly placed on him. The man slightly leans forward to help them complete the task. [0:02:15 - 0:02:16]: All three are now bending slightly near the floor, likely adjusting the lower part of the gown or dealing with an accessory. The cooperation among them remains evident as they smoothly proceed with their tasks. [0:02:17 - 0:02:19]: As they finalize the adjustments, both nurses and the man continue their coordinated efforts, with the man showing minor movements to assist. The room remains orderly, with medical supplies on hand and an air of calm efficiency in the interactions.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color is the man's tie?",
                "time_stamp": "0:02:04",
                "answer": "D",
                "options": [
                    "A. Blue.",
                    "B. Green.",
                    "C. Yellow.",
                    "D. Red."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "getting_dressed",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_162_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:05]: The video features a room with large windows and wooden floors. There are two metal-framed beds on the right side, each covered with white linens. Against the far right wall, a British flag is draped. In the foreground, a man in white shirt, navy blue pants, and red suspenders is being assisted by two women dressed in white and gray uniforms with white head coverings. The man appears to be preparing to dress or finishing dressing. One of the women holds a navy blue jacket. [0:03:06 - 0:03:11]: The man continues to put on the navy blue jacket, with both women assisting him. One of the women helps him put his arms through the sleeves while the other adjusts the jacket at the shoulders.  [0:03:12 - 0:03:15]: Once the jacket is on, the women make final adjustments to ensure it fits correctly. They stand close to the man, ensuring that the jacket is properly positioned. [0:03:16 - 0:03:19]: The camera angle shifts to show a close-up of the man's hands and feet. He is now seated, tying the laces of his black boots. The women assist him in securing the laces, ensuring they are tight and properly fastened.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What are the women doing as the man puts on the navy blue jacket?",
                "time_stamp": "0:03:11",
                "answer": "B",
                "options": [
                    "A. They are leaving the room.",
                    "B. They are helping him put on the jacket and adjusting it.",
                    "C. They are folding the linens.",
                    "D. They are standing by the window."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "getting_dressed",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_162_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:01 - 0:00:02]: The video starts by showing a multicolored toy train made of interlocking building blocks. The train is predominantly orange, green, yellow, blue, and red. It consists of a rectangular body with windows formed by orange and yellow blocks, and blue and green blocks at the top. It has red wheels and is placed on a multicolored track made of blocks in blue, pink, orange, green, and yellow. The word \"Train\" appears on the top left in a stylish yellow font;  [0:00:03 - 0:00:05]: The focus then shifts to the right side of the track. The train starts to move, guided by someone\u2019s hand, which appears from the top of the frame. The hand is pushing the train forward along the track;  [0:00:06 - 0:00:09]: The hand continues to move the train along the track. The train is being moved slowly, and the track remains consistent with its multicolored pattern. The hand maintains a steady grip on the train's green block on top as it progresses forward;  [0:00:10 - 0:00:12]: The hand still guides the train, maintaining the same pace. The blocks of the train and the track are clearly defined, with the red wheels rolling over the colorful track. The background remains a plain light-colored wall;  [0:00:13 - 0:00:17]: The hand is no longer visible. The train is now stationary, positioned slightly further along the track than in previous frames. The visible parts of the track include sections in orange, green, yellow, and blue blocks in an orderly pattern;  [0:00:18 - 0:00:19]: The view changes slightly to the back of the train, showing a slightly angled perspective. The train still sits firmly on the multicolored track, with the solid, light-colored wall in the background.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What action is the hand performing on the train?",
                "time_stamp": "0:00:12",
                "answer": "D",
                "options": [
                    "A. Lifting the train.",
                    "B. Assembling the train.",
                    "C. Painting the train.",
                    "D. Push the train to the right."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "block_building",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_201_real.mp4"
    },
    {
        "time": "[0:01:00 - 0:02:00]",
        "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:03]: A hand is seen placing a yellow Lego brick on a flat, light-colored surface. In front of the hand is a row of yellow LEGO bricks aligned horizontally. The background wall is a light tan color. [0:01:04 - 0:01:09]: Another hand appears holding a green Lego brick and places it on top of the yellow row, towards the left end. The process is repeated as more green bricks are added on top of the first few yellow bricks. [0:01:10 - 0:01:16]: The individual continues stacking the green Lego bricks vertically, creating a small green column attached to the horizontal yellow base.  [0:01:17 - 0:01:20]: Another color is introduced as the hand places a blue Lego brick adjacent to the green column, continuing to build upwards. The blue Lego is positioned on the yellow base, next to the green Lego pieces.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What color Lego brick is placed first on the surface?",
                "time_stamp": "0:01:03",
                "answer": "D",
                "options": [
                    "A. Blue.",
                    "B. Green.",
                    "C. Red.",
                    "D. Yellow."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "block_building",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_201_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:04]: A hand is seen constructing a structure with colorful interlocking toy bricks on a flat, white surface. Initially, the person places orange and green bricks on top of a yellow base. The structure already appears partially built, featuring a combination of orange, green, and blue bricks. The backdrop is a plain, light-colored wall. [0:02:05 - 0:02:10]: The hand continues to add more green bricks to the structure, placing them very carefully on top of the existing formation. The movements are deliberate, and the green bricks are aligned to create a rooftop-like appearance. [0:02:11 - 0:02:14]: Following the roof completion, the hand starts aligning more yellow bricks next to the existing yellow base, extending the structure horizontally. The construction seems to be following a linear pattern. [0:02:15 - 0:02:19]: The hand continues to add additional yellow bricks, systematically expanding the structure further to the right. The placement of bricks is precise, and the hand moves out of the frame after each addition, making the sequence smooth and continuous.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What was the sequence of actions performed by the hand just now?",
                "time_stamp": "00:02:29",
                "answer": "B",
                "options": [
                    "A. Placing orange and green bricks, adding blue bricks, extending the yellow base.",
                    "B. Adding green bricks for the roof, extending the yellow base, expanding the structure to the right.",
                    "C. Starting with blue bricks, adding a rooftop, then dismantling it.",
                    "D. Completing the structure and then painting it."
                ],
                "required_ability": "episodic memory"
            },
            {
                "task_type": "Counting",
                "question": "How many colors of bricks have been used so far?",
                "time_stamp": "00:02:19",
                "answer": "C",
                "options": [
                    "A. Two.",
                    "B. Three.",
                    "C. Four.",
                    "D. Five."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "block_building",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_201_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:09]: A colorful structure made of interlocking plastic bricks is observed on a flat surface, against a light-colored wall. It comprises primarily of orange, yellow, green, blue, and less frequently, red bricks. The bricks are assembled in an organized manner, creating a structure with alternating color patterns and a consistent horizontal layout. At the initial timestamp, a hand, emerging from the right side of the frame, is seen moving towards the structure and begins adjusting bricks on the right end. The hand moves deliberately, placing and removing the blue and green bricks while maintaining the structure's integrity. [0:03:10 - 0:03:14]: The hand moves to the left side of the structure and rotates it, revealing the underside. This side shows that the bottom layer consists of yellow bricks arranged to create a stable base. The underside\u2019s layout exposes the interconnected design of the bricks, with hollow sections visible. The hand then places the structure back on its original side, maintaining its overall form. [0:03:15 - 0:03:17]: The hand takes additional green bricks and places them onto the structure\u2019s right end, incrementally adding height and creating layered stacks. The movement is precise, positioning the bricks to ensure they align with the existing design. [0:03:18 - 0:03:19]: The hand picks up two red circular pieces, resembling wheels, and attaches them to the top surface of the right end of the structure, providing a hint of mobility or embellishing the model. The hand's actions conclude with the pieces being securely placed on the structure.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What type of pieces did the hand attach to the top surface of the structure just now?",
                "time_stamp": "00:03:25",
                "answer": "D",
                "options": [
                    "A. Square pieces.",
                    "B. Triangular pieces.",
                    "C. Rectangular pieces.",
                    "D. Circular pieces resembling wheels."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "block_building",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_201_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:00:18",
                "answer": "D",
                "options": [
                    "A. 2.",
                    "B. 3.",
                    "C. 4.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_75_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:03:23",
                "answer": "B",
                "options": [
                    "A. 2.",
                    "B. 3.",
                    "C. 4.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_75_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:04:31",
                "answer": "B",
                "options": [
                    "A. 4.",
                    "B. 5.",
                    "C. 3.",
                    "D. 2."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_75_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:05:18",
                "answer": "D",
                "options": [
                    "A. 1.",
                    "B. 3.",
                    "C. 2.",
                    "D. 5."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_75_real.mp4"
    },
    {
        "time": "[0:06:00 - 0:07:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:06:37",
                "answer": "D",
                "options": [
                    "A. 1.",
                    "B. 3.",
                    "C. 5.",
                    "D. 7."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_75_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:00:16",
                "answer": "D",
                "options": [
                    "A. 2.",
                    "B. 3.",
                    "C. 4.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_93_real.mp4"
    },
    {
        "time": "[0:01:00 - 0:02:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:01:14",
                "answer": "D",
                "options": [
                    "A. 2.",
                    "B. 3.",
                    "C. 4.",
                    "D. 1."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_93_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:03:26",
                "answer": "B",
                "options": [
                    "A. 3.",
                    "B. 4.",
                    "C. 6.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_93_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:04:07",
                "answer": "D",
                "options": [
                    "A. 2.",
                    "B. 4.",
                    "C. 3.",
                    "D. 5."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_93_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:05:31",
                "answer": "B",
                "options": [
                    "A. 2.",
                    "B. 7.",
                    "C. 8.",
                    "D. 6."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_93_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:10]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions of the individual just now?",
                "time_stamp": "00:00:12",
                "answer": "D",
                "options": [
                    "A. The individual is cleaning trays, organizing workspaces, and baking bread in the oven.",
                    "B. The individual is taking an order, chopping vegetables, and making a quick salad.",
                    "C. The individual is preparing a dessert by mixing ingredients and placing them in the refrigerator.",
                    "D. The individual is putting on gloves, picking up a hot dog bun, and assembling a hot dog."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_350_real.mp4"
    },
    {
        "time": "[0:02:20 - 0:02:30]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions of the individual just now?",
                "time_stamp": "00:02:31",
                "answer": "D",
                "options": [
                    "A. The individual is taking an order, grilling a hamburger patty, and placing it in a bun.",
                    "B. The individual is preparing a sandwich by slicing bread, adding fillings, and wrapping it up.",
                    "C. The individual is filling a tray with fries, adding salt, and serving it to a customer.",
                    "D. The individual is putting on gloves, picking up a hot dog bun, adding the wiener, and preparing toppings."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_350_real.mp4"
    },
    {
        "time": "[0:04:40 - 0:04:50]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions of the individual just now?",
                "time_stamp": "00:04:50",
                "answer": "D",
                "options": [
                    "A. The individual is assembling pastries, adding icing, and placing them on a cooling rack.",
                    "B. The individual is grilling a hot dog, applying condiments, and wrapping it in foil.",
                    "C. The individual is chopping vegetables, tossing them into a bowl, and serving a salad.",
                    "D. The individual is assembling an order by packaging items into a paper bag and preparing it for delivery."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_350_real.mp4"
    },
    {
        "time": "[0:07:00 - 0:07:10]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions of the individual just now?",
                "time_stamp": "00:07:39",
                "answer": "D",
                "options": [
                    "A. The individual is arranging condiments on a counter and placing food items in the refrigerator.",
                    "B. The individual is taking orders from customers and showing them to their seats.",
                    "C. The individual is pouring beverages into cups and setting them on a tray for delivery.",
                    "D. The individual is putting on gloves, picking up a hot dog bun, adding the wiener, and preparing toppings."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_350_real.mp4"
    },
    {
        "time": "[0:09:20 - 0:09:30]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "Which of the following options best summarizes the actions of the individual just now?",
                "time_stamp": "00:09:31",
                "answer": "D",
                "options": [
                    "A. The individual is grilling a hamburger patty, adding cheese and condiments, and wrapping it up in paper.",
                    "B. The individual is filling a drink cup with a beverage, adding ice, and sealing it with a lid.",
                    "C. The individual is assembling a vegetarian wrap with fresh vegetables, dressing, and a tortilla.",
                    "D. The individual is preparing a hot dog, topping it with grilled onions and meat sauce, and wrapping it in wrapping paper."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_350_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: In a promenade area, numerous people are gathered around a statue of a woman holding a lit globe in her raised left hand. The statue is positioned on a pedestal that is illuminated by yellow lights. Many of the people appear to be tourists, conversing, taking photos, and mingling. The background features a waterfront view with a skyline of tall buildings, including a recognizable skyscraper, with some areas appearing to be under cloudy skies. [0:00:04 - 0:00:06]: The crowd continues around the statue, with a man wearing a mask walking past. Several people are seen holding cameras or mobile phones, taking photos, and capturing the scenic view. [0:00:07 - 0:00:11]: As the video progresses, it focuses more on the statue, with many people still gathered around it, taking pictures and observing. The waterfront and the cityscape with a variety of building lights in the background remain visible and become more prominent. [0:00:12 - 0:00:17]: The focus zooms in closer to the statue, highlighting the illuminated pedestal. The crowd's activity around the statue continues, with people looking out towards the waterfront, capturing photos, and admiring the view. [0:00:18 - 0:00:20]: The scene showcases the vibrant colors of the illuminated buildings and boats at the waterfront. The crowd is still densely packed around the statue, with some people pointing toward the distant skyline, engaging in conversations, and taking in the scenic view around them.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the woman in the statue holding in her left hand?",
                "time_stamp": "0:00:19",
                "answer": "D",
                "options": [
                    "A. A book.",
                    "B. A sword.",
                    "C. A shield.",
                    "D. A lit globe."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Action Recognition",
                "question": "What are many people in the crowd doing around the statue?",
                "time_stamp": "0:00:12",
                "answer": "D",
                "options": [
                    "A. Dancing.",
                    "B. Eating.",
                    "C. Singing.",
                    "D. Taking photos."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_334_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:59]: The video captures a broad plaza with a distinctive dome-shaped structure situated prominently to the left center of the frame. The dome illuminates with a soft yellowish light near its base, enhancing its visibility against the backdrop of an impending evening sky. Multiple tall buildings, adorned with various lighting arrangements, frame the background. The skyscrapers display different architectural designs and light up with both cooler blue tones and warmer white lights. Two major buildings have visible logos, one on the far left and another on the far right. In the foreground, the ground consists of a mosaic of light-colored tiles arranged in a pattern. A few individuals populate the scene, engaging in casual activities. Some are walking across the plaza, while others are standing near the dome, seemingly absorbed in viewing or conversing. In addition to the dome, a low, rectangular beige structure is positioned slightly off-center to the right. The plaza itself is open and spacious, emphasizing the scale of both the dome and the surrounding buildings. To the far left, part of a modern canopy structure with vertical beams is visible, adding a contrast to the more rounded forms of the dome. There is also a small lamp post with light reflections on the ground near one of the walking individuals. The ambient light gradually dims, indicating the transition from dusk to evening, with the artificial lighting becoming more pronounced as time progresses.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What kind of light enhances the visibility of the dome?",
                "time_stamp": "00:03:00",
                "answer": "D",
                "options": [
                    "A. A bright white light.",
                    "B. A cooler blue light.",
                    "C. A flashing red light.",
                    "D. A soft yellowish light."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_334_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:25]: The video shows a first-person perspective of a covered walkway with bright overhead lights. The path is paved with small, square, stone-like tiles and has a tree planted near the left side. On the left side, there is a large sign for HKMOA with some art displayed. The path extends forward, with pedestrians visible at a distance. [0:05:26]: The perspective moves slightly forward, the large HKMOA poster remains visible to the left. There are more signs and posters visible further down the path, and more pedestrians become apparent. [0:05:27]: Moving further along the path, more of the right side becomes visible, showing a clear pool of water and some steps leading down to it. [0:05:28 - 0:05:30]: Continuing forward, the same elements persist with more clarity on the right side. The tiled walkway continues and more pedestrians can be seen under the covered area. [0:05:31 - 0:05:35]: The movement continues with the covered path extending straight ahead. The left side shows more signs and seating areas. The right side consistently shows the pool of water, and the walkway still has its small square tiles. [0:05:36 - 0:05:38]: The forward motion persists; the structure and elements stay the same. On the left side, there is now a display booth or information stand. Pedestrians walk down the path in both directions. [0:05:39]: The perspective continues forward with the covered walkway ahead. The structure of the area remains consistent with lights overhead and pedestrians walking along the tiled path. The area maintains a balanced, symmetric appearance with objects and people aligning along the path.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What largest text is visible on the left side of the walkway right now?",
                "time_stamp": "00:05:27",
                "answer": "A",
                "options": [
                    "A. HKMOA.",
                    "B. City Park.",
                    "C. Art Museum.",
                    "D. Train Station."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_334_real.mp4"
    },
    {
        "time": "[0:09:00 - 0:09:59]",
        "captions": "[0:09:40 - 0:09:59] [0:09:40 - 0:09:59]: The video showcases a vibrant cityscape at night, taken from a first-person perspective along a waterfront. The skyline features an array of brightly-lit skyscrapers with various colors such as blue, red, and green. Reflections of these lights shimmer on the calm water, creating a dazzling effect. The horizon also includes a prominent tower illuminated in yellow, which stands taller than the surrounding buildings. To the left of the frame, there is a sturdy concrete barrier, curving slightly, adding depth to the composition. In the background, further left, a mountain range is partially visible under the night sky. An overhanging tree branch appears in the upper part of the frame, providing a natural contrast to the urban environment. Several boats with colorful lights, including a distinctive purple one, are moving across the water, enhancing the lively atmosphere of the scene.",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the color of the tallest tower's illumination?",
                "time_stamp": "00:09:59",
                "answer": "D",
                "options": [
                    "A. Blue.",
                    "B. Green.",
                    "C. Red.",
                    "D. Yellow."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "city_walk",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_334_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video begins with a view of a football field, showing green grass with a bit of wear, and a white line running horizontally across the frame. Part of a person\u2019s leg in black shorts and cleats is visible on the right side. [0:00:01 - 0:00:02]: The perspective changes to show several individuals on the football field. There are about three players in blue jerseys, black shorts, and two in darker clothing. They appear to be running and engaging in a game. The background includes a fence, trees, and some structures. [0:00:03 - 0:00:04]: A red and white football is prominently featured. Initially, it is closer to the camera, then it is seen lying on the grass further away as the player seems to move towards it. [0:00:05 - 0:00:08]: The player interacts with the ball, possibly dribbling or controlling it. The player's hand can be seen pointing or gesturing. Other players in blue jerseys are visible in the distance. They seem to be part of the same play, running and positioning themselves. [0:00:09 - 0:00:12]: More interaction with the ball is shown. The player appears to be advancing it downfield. Other players, including one in blue and another in red, are seen running ahead, possibly towards the goal area. The player's leg is captured kicking the ball. [0:00:13 - 0:00:18]: The scene shifts to an individual wearing a white face mask, black shirt, and backpack, standing near a pile of wooden poles. A description at the bottom of the video mentions he is at Bangkok FC's training center and trying to film their training in a first-person view. [0:00:19]: The video concludes with a blurred image of a logo or emblem, mostly red and white.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the color of the football prominently featured in the video?",
                "time_stamp": "0:00:04",
                "answer": "D",
                "options": [
                    "A. Blue and white.",
                    "B. Black and white.",
                    "C. Green and white.",
                    "D. Red and white."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "football",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_265_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:01]: The video starts with a first-person perspective view following a white line on a grassy soccer field. On the left, there are buildings and a goalpost. [0:03:01 - 0:03:11]: The view pans across the field, displaying more players in blue shirts scattered around the field. Some players are seen closer to the goalposts, possibly indicating a practice session or game being played. [0:03:11 - 0:03:12]: Suddenly, a soccer ball rolls into view from the bottom of the screen, approaching the camera. The field remains in the background. [0:03:12 - 0:03:14]: The focus moves closer to the ground, with a view of the soccer ball, part of a player's leg in blue cleats, and the lower part of the player's body. The player seems to be preparing to kick the ball. [0:03:14 - 0:03:16]: The perspective shifts again to follow the ball as it is kicked. A goalpost and several players in blue can be seen in the background. [0:03:16 - 0:03:20]: The first-person view now returns to a wider angle, showing more of the field and players. The player who kicked or interacted with the ball is now focusing on teammates further down the field. Toward the end, the text \"Good pass Aim!\" and \"Good job!\" are displayed, indicating verbal encouragement likely from the player operating the camera.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What text appeared just now?",
                "time_stamp": "00:03:20",
                "answer": "D",
                "options": [
                    "A. \"Nice shot!\".",
                    "B. \"Great goal!\".",
                    "C. \"Well played!\".",
                    "D. \"Good pass Aim!\" and \"Good job!\"."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "football",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_265_real.mp4"
    },
    {
        "time": "[0:06:00 - 0:07:00]",
        "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:02]: The video begins on a green soccer field with the goalpost in the background. The viewer's point of view includes an individual running towards a ball on the grass. A blue structure and red banners with advertisements for Toyota and others are visible on the left. The weather appears overcast. [0:06:03 - 0:06:05]: The camera moves closer to the ball, which is now more centrally positioned in the frame. One can see the individual's arm and leg, indicating they are approaching the ball to kick it. [0:06:06 - 0:06:08]: As the individual reaches the ball, they appear to be preparing to kick it. The goalposts and other players in blue jerseys in the background become more apparent. The field markings are also more visible. [0:06:09 - 0:06:12]: After the ball is kicked, it rolls away, and another player in a blue jersey is visible running towards it. Additional players in the background are seen playing or positioning themselves on the field.  [0:06:13 - 0:06:16]: The camera follows the ball, which is now in the possession of another player in a blue shirt. This individual is heading towards the opposing team's side. The background shows a fence, a goal in the distance, and some trees. [0:06:17 - 0:06:20]: The viewer captures an interaction where a player in a blue shirt passes the ball, which rolls on the ground, and another player in a dark shirt attempts to intercept it. The video's focus remains on the ball and the footwork of the players involved.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What advertisement is visible right now?",
                "time_stamp": "00:06:02",
                "answer": "D",
                "options": [
                    "A. Honda and Chang.",
                    "B. Nissan.",
                    "C. Ford.",
                    "D. Toyota and Chang."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "football",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_265_real.mp4"
    },
    {
        "time": "[0:09:00 - 0:10:00]",
        "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:01]: The video begins with a downward perspective focused on a white and red soccer ball on a green grassy field. A player\u2019s blue shoe is slightly visible at the bottom of the frame, suggesting they are in motion. [0:09:02 - 0:09:03]: The view shifts to the soccer field where several players are seen in dynamic positions, one preparing to kick the ball towards the goal, and another standing near the goalposts in the background. The sky is overcast and the background shows green trees and goalposts. [0:09:04 - 0:09:05]: The video focuses on the forward movement of players on the field, with one player in a red shirt closest to the goal and another in blue nearby. The field lines and playing area become more evident. [0:09:06 - 0:09:07]: A player in a red shirt stands ready in front of the goal as the ball approaches, positioned centrally in the goal area with another player in blue closing in. [0:09:08]: The goalkeeper, dressed in red, dives to the ground attempting to save the ball, while two players in blue watch closely from nearby. [0:09:09]: Multiple players are seen congregating in front of the goal post, with some dressed in red and blue, indicating a close engagement during the play. [0:09:10]: The scene shifts to the player\u2019s perspective holding a water bottle, indicating a moment of rest or pause in gameplay. The sky remains overcast in the background. [0:09:11]: The player lowers the water bottle, revealing the soccer field extending ahead with distant players and spectators visible around the perimeter. [0:09:12]: A broad view of the field shows it partially with other players dispersed around, indicating some distance from the active play zone. [0:09:13]: The player's hand holding the water bottle returns into view, with the scene showing similar overcast skies and vast green field. [0:09:14]: The focus shifts again to a broader field view with buildings and field boundaries visible, and players continuing in the background. [0:09:15 - 0:09:16]: Attention pivots towards a broader area with the scene capturing a player in blue with a command \u201cTiger!\u201d being shouted, possibly indicating a nickname or instruction. [0:09:17 - 0:09:19]: The video concludes with two players in blue closer to a distant goal and a defensive player, maintaining the engagement level and field details consistent throughout. The call out \u201cTiger!\u201d is reiterated.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the player wearing the camera holding right now?",
                "time_stamp": "00:09:10",
                "answer": "D",
                "options": [
                    "A. A soccer ball.",
                    "B. A whistle.",
                    "C. A flag.",
                    "D. A water bottle."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Action Recognition",
                "question": "What action does the goalkeeper in red perform as the ball approaches the goal?",
                "time_stamp": "00:09:08",
                "answer": "A",
                "options": [
                    "A. Dives to the ground.",
                    "B. Kicks the ball away.",
                    "C. Jumps to block the ball.",
                    "D. Stands still."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "football",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_265_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:00:18",
                "answer": "C",
                "options": [
                    "A. 2.",
                    "B. 3.",
                    "C. 0.",
                    "D. 4."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_79_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:03:05",
                "answer": "C",
                "options": [
                    "A. 3.",
                    "B. 4.",
                    "C. 2.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_79_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:04:02",
                "answer": "C",
                "options": [
                    "A. 5.",
                    "B. 6.",
                    "C. 3.",
                    "D. 4."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_79_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:05:01",
                "answer": "C",
                "options": [
                    "A. 2.",
                    "B. 5.",
                    "C. 3.",
                    "D. 4."
                ],
                "required_ability": "episodic memory"
            },
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:05:33",
                "answer": "C",
                "options": [
                    "A. 2.",
                    "B. 3.",
                    "C. 4.",
                    "D. 5."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_79_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:10]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "What is the best summary of the actions taken just now?",
                "time_stamp": "0:00:12",
                "answer": "C",
                "options": [
                    "A. A worker picked up a bag, added condiments, and handed it to another worker.",
                    "B. A worker grabbed a bag, placed a cheeseburger inside, and passed it to a colleague.",
                    "C. A worker retrieved a bag, added fries and a cheeseburger, checked the order slip, and moved to the fryer area.",
                    "D. A worker took a bag, filled it with a drink, and proceeded to the front counter."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_345_real.mp4"
    },
    {
        "time": "[0:01:38 - 0:01:48]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "What is the best summary of the actions taken just now?",
                "time_stamp": "0:01:48",
                "answer": "C",
                "options": [
                    "A. A worker placed fries into a fryer, prepared drinks, and gave them to a colleague.",
                    "B. A worker assembled a hamburger, wrapped it, and handed it to a customer.",
                    "C. A worker collected fries in a bag, placed an order slip on it, and moved it to the counter.",
                    "D. A worker took a customer's order, processed the payment, and handed over a receipt."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_345_real.mp4"
    },
    {
        "time": "[0:03:16 - 0:03:26]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "What is the best summary of the actions taken just now?",
                "time_stamp": "0:03:26",
                "answer": "C",
                "options": [
                    "A. A worker filled a bag with fries, attached an order slip, delivered the bag to the customer, and returned to their station.",
                    "B. A worker packed a bag with a cheeseburger, sealed it, printed an order slip, and handed it to another worker.",
                    "C. A worker grabbed a bag, placed the order receipt with it, and headed to the pick-up counter to deliver the order.",
                    "D. A worker assembled a sandwich, confirmed the customer's name, and moved to the serving counter."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_345_real.mp4"
    },
    {
        "time": "[0:04:54 - 0:05:04]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "What is the best summary of the actions taken just now?",
                "time_stamp": "0:05:04",
                "answer": "C",
                "options": [
                    "A. A worker reviewed a customer's order, cleaned the workstation, and prepared it for the next task.",
                    "B. A worker assembled a new order from scratch, wrapped each item, and handed it to the kitchen staff.",
                    "C. A worker retrieved a filled order bag, attached the order slip to the bag, and delivered it to the front counter.",
                    "D. A worker took a customer's order, processed the payment, and handed the receipt to a colleague."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_345_real.mp4"
    },
    {
        "time": "[0:06:32 - 0:06:42]",
        "questions": [
            {
                "task_type": "Clips Summarize",
                "question": "What is the best summary of the actions taken just now?",
                "time_stamp": "0:06:42",
                "answer": "C",
                "options": [
                    "A. A worker checked an order ticket, filled a bag with food items, and placed the bag on the counter.",
                    "B. A worker retrieved a receipt, confirmed the order with the customer, and then handed the order to another worker.",
                    "C. A worker attached a ticket to a bag, moved to the front counter, and handed the bag to a customer.",
                    "D. A worker processed a payment, handed the change to a customer, and returned to the kitchen."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "preparation_of_meals",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_345_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:04]: The video begins with a first-person perspective view of an illustration featuring a white clock face with silver knife and fork hands, set at 10 o'clock. The number \"10\" is prominently displayed to the left of the clock on a gray background.  [0:00:01 - 0:00:04]: The scene transitions to a man standing against a background that reads \"RAMSAY in 10.\" He is wearing a navy blue shirt. [0:00:05]: A transition screen features a dark purple and light blue geometric shape overlaying the previous frame. [0:00:05 - 0:00:06]: The scene shifts to a bright kitchen with white brick walls and colorful shelves. Various cooking utensils and ingredients are arranged neatly on the shelves. The word \"COOK\" in bright red letters is displayed on the top shelf. The man, now wearing a green shirt, stands in the center of the frame, explaining something. [0:00:07 - 0:00:08]: The man continues his explanation, making expressive gestures with his hands. He appears to be very engaged in his demonstration. [0:00:09 - 0:00:10]: The man turns to his right, speaking towards an off-screen audience or object. The background reveals more kitchen equipment including an oven and some jars. [0:00:11 - 0:00:13]: He faces forward again, continuing his explanation while occasionally gesturing. The kitchen setting remains the same with the word \"HOT\" appearing on the bottom left of the frame. [0:00:14]: The man looks directly into the camera, emphasizing his point. [0:00:15 - 0:00:16]: He claps his hands together, continuing to talk, showing enthusiasm in his expressions. [0:00:17 - 0:00:18]: He gestures sideways, looking animated and engaged. The angle provides a closer view of the jars and appliances in the kitchen. [0:00:19]: The video ends with the man standing calmly, finishing his explanation, with the word \"COOK\" still visibly prominent in the background.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What word appears on the bottom left of the frame while the man continues his explanation?",
                "time_stamp": "00:00:13",
                "answer": "C",
                "options": [
                    "A. COOK.",
                    "B. RAMSAY.",
                    "C. HOT.",
                    "D. READY."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_31_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:01]: A man wearing a green t-shirt stands in a kitchen with white brick walls and open wooden shelves. On the countertop in front of him, there are various items, including a glass bowl of beaten eggs, a bowl of raw chicken pieces, and various ingredients and utensils. He is holding a piece of chicken over the bowl while sprinkling what seems to be seasoning or flour onto it. [0:02:01 - 0:02:02]: The man continues to prepare the chicken by placing it into the glass bowl. Behind him, there are several shelves with neatly arranged dishes, and on the left, there is a red \"COOK\" sign on the top shelf. [0:02:02 - 0:02:03]: The man is focused on the task, which involves handling the chicken pieces. In the background, various utensils and jars are visible on the shelves, while a gas stove and several frying pans are noticeable on the countertop. [0:02:03 - 0:02:04]: The man continues working with the chicken, ensuring it is coated evenly. At this point, he picks up another piece of chicken. Various kitchen tools, like a whisk and some bottles, are scattered around the working area. [0:02:04 - 0:02:05]: Closer view of the man's hands as he handles the raw chicken fillet, about to dip it into the beaten egg mixture. Both bowls are transparent, making the contents visible. There is some cilantro placed on the cutting board. [0:02:05 - 0:02:06]: The man carefully places the chicken piece into the beaten egg mixture. The workspace is organized, with cutting boards, bowls, and other ingredients aligned in a practical manner. [0:02:06 - 0:02:07]: The man dips the chicken piece into the beaten egg mixture, ensuring it is well-coated. The kitchen appears orderly, with knives hanging on a magnetic strip on the wall behind him. [0:02:07 - 0:02:12]: He continues this process, turning towards the counter to his right, where a bowl with more chicken pieces is placed. The background includes a sink with a faucet and a neatly organized shelf with bowls and plates. Green and brown bottles and a lush bunch of herbs are also present on the counter. [0:02:12 - 0:02:13]: The man focuses on coating the chicken pieces, putting them into the egg mixture one at a time. He appears to be explaining something, possibly providing cooking tips as he works. [0:02:13 - 0:02:14]: An overhead shot shows the man's hand coating the chicken in flour. Surrounding the workspace are various bowls filled with ingredients like chopped nuts, herbs, and seasoning, suggesting they are part of the recipe. [0:02:14 - 0:02:15]: The man proceeds to coat the chicken pieces, which are now in the egg mixture, with flour from another bowl. The countertop remains so neatly arranged with cooking ingredients and equipment, ensuring efficiency. [0:02:15 - 0:02:16]: The man shakes off the excess flour from a piece of chicken that has been dipped in the egg mixture and then the flour, preparing it for cooking. He is very meticulous in his method, emphasizing thorough coating. [0:02:16 - 0:02:17]: Returning to the bowl with the coated chicken pieces, the man continues to handle the ingredients attentively. The organized kitchen setup indicates a well-prepared cooking session. [0:02:17 - 0:02:18]: The man looks focused as he coats another piece of chicken. Various ingredients are laid out systematically to ensure each step of the cooking process is streamlined. The backdrop includes a shelved arrangement with knives, plates, and other kitchen essentials. [0:02:18 - 0:02:19]: The man continues to work diligently, coating the chicken pieces. His actions suggest a level of expertise and comfort in the kitchen. The setting remains consistent with various cooking ingredients and tools visible.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the man wearing in the kitchen?",
                "time_stamp": "00:02:40",
                "answer": "C",
                "options": [
                    "A. A blue apron.",
                    "B. A red t-shirt.",
                    "C. A green t-shirt.",
                    "D. A white shirt."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Action Recognition",
                "question": "What is the man doing with the chicken right now?",
                "time_stamp": "00:02:00",
                "answer": "D",
                "options": [
                    "A. Frying it.",
                    "B. Cutting it.",
                    "C. Washing it.",
                    "D. Coat the chicken with flour."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_31_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:02]: A person is seen standing in a kitchen, dipping their hand into a bowl of beaten eggs on a wooden cutting board. The bowl is transparent, and another bowl containing breadcrumbs is positioned to the left of the egg bowl. In the background, a stove with a pot is present on the left side, and there are blue and wooden cabinets on either side of the countertop. A bunch of green herbs is placed near the edge of the cutting board. [0:04:03 - 0:04:03]: The person starts transferring the egg-coated item into the bowl of breadcrumbs. The individual is wearing a green short-sleeve shirt. [0:04:04 - 0:04:06]: In a wider shot, the same person continues handling the food while standing in a white-tiled kitchen with wooden shelves laden with plates, bowls, and various utensils. The countertop also holds additional items, including bottles of oil and a dish with green vegetables. [0:04:06 - 0:04:09]: The person stands in the same position, adding seasoning or another ingredient to the bowl of breadcrumbs and continuing to coat the item. [0:04:09 - 0:04:10]: The perspective shifts to an overhead view showing the two bowls on the cutting board. The person uses both hands to cover the food item in the breadcrumbs mixture. Around the board are bowls containing spices and condiments. [0:04:11 - 0:04:13]: The person continues to firmly press the breadcrumbs onto the food item, ensuring it is well-coated. [0:04:14 - 0:04:16]: The scene returns to the front view of the person standing in the kitchen, adding more breadcrumbs to the bowl. Utensils and ingredients are arranged neatly on the countertop around them. [0:04:17 - 0:04:19]: The person starts cleaning up, using a cloth to wipe their hands, and the bowls with ingredients are seen on the wooden cutting board. The kitchen's layout includes modern appliances, with more utensils and ingredients arranged nearby.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the person dipping their hand into right now?",
                "time_stamp": "00:04:02",
                "answer": "C",
                "options": [
                    "A. A bowl of flour.",
                    "B. A bowl of breadcrumbs.",
                    "C. A bowl of beaten eggs.",
                    "D. A bowl of green herbs."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_31_real.mp4"
    },
    {
        "time": "[0:06:00 - 0:07:00]",
        "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:01]: A person in a green t-shirt stands in a kitchen with white brick walls and shelves holding various kitchen items like utensils, jars, and bowls. He holds a white cloth in one hand while looking down towards the countertop, which has a chopping board and several ingredients on it. [0:06:01 - 0:06:02]: The person in the green t-shirt extends his arm to the left towards a bottle of olive oil while looking at the countertop. The shelves in the background display additional kitchen items such as dishes and cooking pots. [0:06:02 - 0:06:03]: The person moves a green vegetable, possibly a cucumber, on the chopping board with his right hand while maintaining his gaze downwards. The background remains consistent, showing the organized shelves. [0:06:03 - 0:06:05]: Positioned behind the kitchen counter, the person continues preparing the ingredients on the chopping board. He maintains focus, occasionally glancing up towards the background where various kitchen tools and dishes are neatly arranged on the shelves. [0:06:05 - 0:06:06]: While still standing near the kitchen counter, the person turns slightly to his right and reaches towards the upper shelf, appearing to grab a jar off a shelf. The countertop holds an assortment of green vegetables and bowls. [0:06:06 - 0:06:07]: Returning his attention to the chopping board, the person appears to hold an object, possibly a piece of vegetable in his right hand. He shifts his position slightly to the left while glancing down. [0:06:07 - 0:06:08]: The perspective shifts to a top-down view, focusing on the kitchen counter where different items, including sliced vegetables, a cucumber on a chopping board, and a frying pan, are visible. The person\u2019s arm reaches for the cucumber. [0:06:08 - 0:06:09]: A close-up view captures the person slicing a cucumber on the chopping board. He holds the cucumber steadily while his other hand manages the knife. Several kitchen items, including olive oil and a bunch of parsley, are within reach on the countertop. [0:06:09]: The person is seen slicing cucumbers with a knife on the chopping board, with several kitchen items like olive oil and parsley positioned nearby on the countertop. [0:06:10 - 0:06:11]: The person shifts his focus while slicing, appearing to concentrate on the task. Surrounding kitchen items provide a backdrop to the chopping activity as he works precisely with the knife. [0:06:11 - 0:06:12]: With continued focus on the chopping board, the person cuts the cucumber into smaller pieces. His careful movements ensure the task is performed efficiently amidst the neatly organized kitchen setup. [0:06:12 - 0:06:13]: The person completes slicing the cucumber pieces, placing them aside on the chopping board. The organized kitchen backdrop remains consistent, displaying various utensils and ingredients neatly arranged on the shelves and countertop. [0:06:13 - 0:06:14]: Standing upright again, the person holds the cucumber, turning slightly towards the left. He engages momentarily with the task at hand against the background setting of the organized kitchen shelves filled with dishes and jars. [0:06:14 - 0:06:15]: Shifting towards the right side of the countertop, the person repositions some items while continuing to hold a piece of cucumber. His gaze moves briefly towards the countertop setup and the neatly arrayed kitchen items behind him. [0:06:15 - 0:06:16]: The person appears to inspect and remove the cucumber's peel. His focus remains on the vegetable, and the kitchen's backdrop continues to feature organized kitchenware on the shelves. [0:06:16 - 0:06:17]: Paying close attention, the person uses a peeler to remove the cucumber's skin. The kitchen background continues to showcase various cooking items and utensils well-organized on the shelves. [0:06:17 - 0:06:18]: Concentrated on peeling the cucumber, the person efficiently manages the task while positioned behind the counter. The well-maintained and neatly arranged kitchen backdrop remains visible. [0:06:18 - 0:06:19]: Consistent attention is given to the cucumber as the person completes the peeling task. The kitchen's background remains organized, exhibiting various utensils and ingredients neatly placed on the shelves and countertop.",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many pieces of fried chicken are there in the frying pan right now?",
                "time_stamp": "00:06:12",
                "answer": "B",
                "options": [
                    "A. 3.",
                    "B. 4.",
                    "C. 7.",
                    "D. 9."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_31_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with the text \"UNDER CONSTRUCTION TOURNAMENT\" displayed in the center of a black screen with white text.  [0:00:02]: The screen turns completely black. [0:00:03 - 0:00:05]: The frame transitions to a new title screen that reads \"GRAVITY THROTTLE RACING.\" The background features a yellow triangle with a red arrow pointing to the right and a blue border. The background of this title screen is a mix of dark colors with subtle lighting effects. [0:00:06 - 0:00:10]: The next scenes provide a view of what appears to be a miniature race track in four different quadrants labeled \"Drone1,\" \"Runway3,\" \"Drift Turn,\" and \"Drone2.\" Each quadrant displays sections of the race track from various angles, showcasing different parts of the track layout. These sections primarily use white, gray, and some black colors, giving an impression of a detailed and complex design.  [0:00:11 - 0:00:15]: The view changes to a new set of four quadrants labeled \"Train1,\" \"Scrambler,\" \"Train2,\" and \"Big-U.\" These sections show different parts of another miniature landscape with painted backgrounds depicting mountains and sky, along with sections of the track winding through the terrain. The tracks are primarily grey, and the backgrounds are colorful with shades of brown, green, and blue. [0:00:16 - 0:00:19]: The camera shifts to a first-person point of view, showing a track layout close up with detailed scenery. A small, inset video in the upper left corner shows a person speaking, accompanied by red and blue text that introduces \"Bayreuth, Germany\" and \" 'Munch' Cromartie.\" The track consists of winding paths, railings, and miniature vehicles. As the camera navigates through the track, it details more specific sections and elements, including other miniature trains and cars moving around the set landscape.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is depicted in the quadrants labeled \"Train1,\" \"Scrambler,\" \"Train2,\" and \"Big-U\"?",
                "time_stamp": "0:00:15",
                "answer": "B",
                "options": [
                    "A. Various miniature race tracks.",
                    "B. Four different sections of the race track.",
                    "C. A detailed map of a large city.",
                    "D. A diagram of an amusement park."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_497_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:02]: The scene shows a model racetrack with multiple toy cars racing along a winding, elevated section of the track. Two cars, one blue and one white with red accents, are seen closely competing on a sharp turn. The track itself is detailed with railings and has multiple levels including a higher track section with train tracks. [0:04:03 - 0:04:05]: The cars continue to race along the track, with a background featuring a painted scenery. Additional toy trains are visible on a parallel track, one of which is a larger black locomotive. The racing cars encounter straight and curved segments of the track, competing for position. [0:04:06 - 0:04:08]: The scene shifts slightly, showing the cars from a higher vantage point. The leading blue car is closely followed by the white car with red accents. The track has a mix of straight and curve sections, with the toy trains still visible on the adjacent tracks. [0:04:09 - 0:04:11]: Another angle shows the cars racing in a straight line. They are closely grouped together, with the white car now making a move to overtake the leader. Trains continue to move along the track in the background. [0:04:12 - 0:04:14]: The cars are seen taking a sharp turn. The white car with red accents has taken the lead, while the blue car follows closely. Another white car is nearby, and the vibrant track details and background appear consistent with previous frames. [0:04:15 - 0:04:20]: The scene transitions to a scoreboard titled \"FINAL RACE\" from Gravity Throttle Racing showing scores for different rounds. The scores for each round are visible, with cars of different colors and designs represented, indicating their performance in various rounds. The background has a peaceful scenery of mountains and fields, while the scoreboard provides a detailed summary of the race results.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What does the scoreboard titled \"FINAL RACE\" show?",
                "time_stamp": "0:04:21",
                "answer": "B",
                "options": [
                    "A. The number of laps completed.",
                    "B. The cars' performance in round 1 and round 2.",
                    "C. The fastest lap time.",
                    "D. The starting positions of the cars."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_497_real.mp4"
    },
    {
        "time": "[0:08:00 - 0:09:00]",
        "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:05]: The video shows a first-person angle of three toy cars racing on a miniature sandy racetrack. One black car, one white car with black and red patterns, and one red car with a white roof are traveling down a slope with grooves creating three separate lanes in the sandy terrain. The background is primarily blue, with the track extending to the right and uphill. [0:08:06 - 0:08:09]: The cars continue up a cardboard ramp, transitioning from the sandy track to a smooth, winding track with curves and elevated sections. The track is lined with low walls, and the background features a painted scenery of mountains and greenery. The cars navigate the turns, showing controlled, smooth movement, emphasizing the track\u2019s design. [0:08:10 - 0:08:16]: As they race further, the white car with the red roof leads into a wider, more complex section of the track, which includes a train set with miniature rail cars and engines. The train tracks run parallel to the race track, and the detailed background features a hilly landscape with green and red hues. The white car with red patterns continues to lead, maintaining its pace and trajectory, followed closely by the red car and the black car. [0:08:17 - 0:08:20]: Approaching the final section of the track, the cars reach a designated area resembling an airport runway. White markings and small model airplanes are visible on the track. The white car with red patterns is still in the lead, navigating a runway turn followed by the black car, while the red car trails behind. The surrounding landscape continues to be detailed with mountains and sky painted in the background.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "Which car is leading right now?",
                "time_stamp": "0:08:29",
                "answer": "C",
                "options": [
                    "A. Black car.",
                    "B. Red car.",
                    "C. White car with red roof.",
                    "D. Green car."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_497_real.mp4"
    },
    {
        "time": "[0:11:00 - 0:12:00]",
        "captions": "[0:12:00 - 0:12:20] [0:12:00 - 0:12:08]: In a set-up resembling a model racetrack, a small white toy car is seen navigating a curved, elevated track. The track is surrounded by white sculpted terrain and a blue wall. The car moves onward, making its way down the curve of the track. The area appears to be finely detailed with careful attention to the curves and edges of the track. [0:12:09 - 0:12:10]: The toy car continues along the track. The perspective of the shot remains largely the same, showcasing the intricate details of the track's structure. [0:12:11 - 0:12:15]: The scene shifts to a wide shot of a road in front of a mural backdrop displaying a mountainous landscape with vibrant colors, including reds, greens, and browns. The road is marked with white lines, and a toy car, now blue and different from the previous one, travels the straight path, captured from a higher viewpoint. [0:12:16]: The blue toy car continues on the marked road, and more of the mural landscape becomes visible, featuring green hills blending into rugged mountains. [0:12:17 - 0:12:18]: The track changes orientation, and another section of the mural with towering cliffs is visible. The blue toy car moves forward, and the terrain in the foreground includes large, nuanced sculptures, giving the appearance of a rugged, arid environment. [0:12:19]: The scene reveals a larger portion of the model track setup with various cars on different track levels. The landscape mural in the background remains, adding depth and context to the scene. Multiple cars are seen navigating the complex track system.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "Which car won first place in this competition?",
                "time_stamp": "0:11:40",
                "answer": "C",
                "options": [
                    "A. The blue car.",
                    "B. The dark purple car.",
                    "C. The blue and black car.",
                    "D. The red car."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_497_real.mp4"
    },
    {
        "time": "[0:12:00 - 0:13:00]",
        "captions": "[0:12:00 - 0:12:20] [0:12:00 - 0:12:08]: In a set-up resembling a model racetrack, a small white toy car is seen navigating a curved, elevated track. The track is surrounded by white sculpted terrain and a blue wall. The car moves onward, making its way down the curve of the track. The area appears to be finely detailed with careful attention to the curves and edges of the track. [0:12:09 - 0:12:10]: The toy car continues along the track. The perspective of the shot remains largely the same, showcasing the intricate details of the track's structure. [0:12:11 - 0:12:15]: The scene shifts to a wide shot of a road in front of a mural backdrop displaying a mountainous landscape with vibrant colors, including reds, greens, and browns. The road is marked with white lines, and a toy car, now blue and different from the previous one, travels the straight path, captured from a higher viewpoint. [0:12:16]: The blue toy car continues on the marked road, and more of the mural landscape becomes visible, featuring green hills blending into rugged mountains. [0:12:17 - 0:12:18]: The track changes orientation, and another section of the mural with towering cliffs is visible. The blue toy car moves forward, and the terrain in the foreground includes large, nuanced sculptures, giving the appearance of a rugged, arid environment. [0:12:19]: The scene reveals a larger portion of the model track setup with various cars on different track levels. The landscape mural in the background remains, adding depth and context to the scene. Multiple cars are seen navigating the complex track system.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What does the scoreboard titled \"FINAL RACE\" show?",
                "time_stamp": "0:12:38",
                "answer": "A",
                "options": [
                    "A. The cars' performance from round 1 to round 7.",
                    "B. The cars' performance in round 1 and round 2.",
                    "C. The fastest lap time.",
                    "D. The starting positions of the cars."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "rc_model_competition",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_497_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video starts with an animation showing a large number \"10\" prominently in the center. The number is white and has a silver knife and fork aligned as clock hands, forming an analogy to a clock. The background is a gradient of blue colors;  [0:00:01 - 0:00:05]: The scene transitions to a man in a dark blue shirt, smiling broadly. Behind him, the text \"RAMSAY in 10\" is displayed. The text is in blue and white colors, matching the color scheme of the previous frame. He remains smiling consistently, and the background remains a gradient of blue; [0:00:05 - 0:00:10]: The scene changes to a kitchen setting. The man now wears a green shirt and stands in front of a white brick wall with shelves filled with kitchen items. The shelves have colored glass bottles, bowls, and various kitchen utensils arranged neatly. The word \"COOK\" appears in large red letters on the top left shelf. Several knives are mounted below the shelves. He appears to be talking, gesturing with both his hands. The background includes a few green bottles and some cookbooks. The countertop is wooden, and utensils are arranged meticulously behind him; [0:00:11]: A close-up side view of the man shows him animatedly speaking with hand gestures. The kitchen is partially visible in the background, showcasing a modern kitchen setup with blue cabinets and built-in ovens; [0:00:12 - 0:00:19]: The focus returns to a frontal view of the man standing in the kitchen. He continues to talk, gesturing occasionally. His facial expressions become more serious and intent, indicating he is explaining something important. Throughout these frames, there are slight variations in his hand movements and facial expressions, but the background details of the kitchen remain constant. The video frames depict a continuous scene from a perspective in a kitchen setting, featuring a man engaging with the audience in a detailed explanation or conversation related to cooking or kitchen tasks.\n[0:00:20 - 0:00:40] [0:00:20 - 0:00:21]: The video begins with a scene in a modern kitchen. In the center stands a person wearing a teal polo shirt and gesturing with their hands. The background features white brick walls with shelves that hold various kitchen items, including cups, bowls, and decorative items. On the main counter, to the left, large red letters spell out the word \"COOK,\" while below on the counter, smaller white letters spell \"HOT.\" Various kitchen tools like knives and bottles are also visible. [0:00:21 - 0:00:22]: The person begins to turn to their right, seeming to prepare for the next step. The countertop continues to show an array of bowls, jars, and utensils next to a stove on the left side. [0:00:22 - 0:00:23]: The person, now facing the camera again, stands in front of a wooden cutting board that holds several pieces of raw chicken. With hands slightly above the board, they start pointing at the chicken, ready to demonstrate something. [0:00:23 - 0:00:24]: A close-up of the cutting board shows the person's hands emphasizing particular parts of the chicken. In the foreground are various ingredients, including a red tin, some metal bowls, a white ceramic bowl, and a bottle with a black top. [0:00:24 - 0:00:25]: The camera shifts slightly to the right, showing a better view of the assortment of ingredients on the countertop. Items include spices, a glass bowl for mixing, jars, and a wooden chopping block. The person is less visible now, but their hand movements continue above the cutting board. [0:00:25 - 0:00:26]: A similar scene shows the person's hands hovering over the chicken pieces on the cutting board. The focus remains on the ingredients, highlighting the prepared workspace with various bowls, jars, and cooking tools strategically placed around. [0:00:26 - 0:00:27]: The person continues to use dynamic hand movements, possibly explaining the next steps in the cooking process. The view emphasizes the organized kitchen setup, including the clearly labeled wooden board, Roos S. [0:00:27 - 0:00:28]: The camera captures the individual performing another hand gesture, possibly related to seasoning the chicken. The surrounding ingredients remain consistent, indicating preparedness for the cooking activity. [0:00:28 - 0:00:31]: The camera zooms closer to the cutting board as the person sprinkles a substance onto the chicken. The scene displays a range of fresh ingredients, including lemons, avocados, and herbs, indicating a planned, comprehensive recipe. [0:00:31 - 0:00:32]: The angle shifts slightly again, focusing on the person who is now looking down at the cutting board with an intent gaze, possibly considering the next step. The well-organized kitchen remains a prominent backdrop. [0:00:32 - 0:00:33]: The person is once again facing the camera and gesturing with both hands over the cutting board. Beside them, a large glass mixing bowl and a collection of spices and oils underline the preparatory phase of the cooking session. [0:00:33 - 0:00:34]: The scene shows the person leaning slightly forward, reaching out to touch or rearrange something on the cutting board. The broad workspace continues to show a range of ingredients, suggesting an elaborate dish in progress. [0:00:34 - 0:00:35]: The individual gestures towards a series of spice containers lined up on the counter. The side view emphasizes the length of the workspace and the meticulous arrangement of kitchen tools and ingredients. [0:00:35 - 0:00:36]: The person is seen picking up a container, possibly to add a seasoning to the dish. The assortment of items on the counter, including small bowls, a bottle of oil, fresh fruits, and spices, strongly complements the culinary theme. [0:00:36 - 0:00:37]: Camera focuses on the person's hand as they pick up a pinch of spice from a small container. The organized kitchen counter, displaying lemons, a mix of sauces, and additional cooking utensils, serves as a backdrop to the activity. [0:00:37 - 0:00:39]: The camera angle changes to an overhead view, showing the person\u2019s hand sprinkling the spice onto the chicken from above. The workspace showcases a detailed arrangement of ingredients, including a bowl of cheese, lemons, and other cooking essentials. [0:00:39]: The person smiles at the camera, holding an ingredient in the middle of the preparation process. The display of fresh ingredients and kitchen tools around him underscores a highly organized and methodical\n[0:00:40 - 0:01:00] [0:00:40 - 0:00:43]: The video opens with a first-person perspective of a kitchen. There is a man, wearing a dark green polo shirt, standing behind a countertop. The countertop is filled with various cooking ingredients and equipment. On the left side, there is a cutting board with pieces of what looks like prepared chicken. The background features white brick walls with shelves containing various kitchen items including plates, jars, and a brown clay pot. The man is seen reaching out to pick something up from the countertop. [0:00:44]: The focus shifts to a close-up of the ingredients on the countertop. There is a bowl of white sauce in the center. Surrounding it, there are two avocados, a lemon, a lime, an assortment of herbs, and several small bowls containing different spices and seasonings. The prepared chicken can still be seen on the cutting board in the background. [0:00:45 - 0:00:49]: Another close-up shot from above captures more of the countertop. In addition to the previously mentioned items, there is a bottle of what appears to be olive oil next to the avocados, a jar of honey, and a small glass container with a red lid containing more spices. The camera then moves back slightly, showing the man\u2019s arm reaching down to interact with the ingredients. [0:00:50 - 0:00:51]: The focus returns to the man as he speaks, holding a small item between his thumb and index finger. The background remains the same, with the kitchen's blue cabinets and white brick walls. [0:00:52 - 0:00:55]: The man steps back slightly, continuing to speak. His gestures suggest he is explaining something. On the countertop, the cooking items are still clearly visible. Behind him, kitchen appliances and shelves filled with various utensils and containers are seen. [0:00:56 - 0:00:57]: The man looks down at the countertop, raising one finger as though emphasizing a point. The background maintains the same setting as before, with the shelves and kitchen items still in view. [0:00:58 - 0:00:59]: The focus shifts once again to a close-up of the man as he reaches for a small bowl with sauce and places it in a larger mixing bowl on the counter. His facial expression is concentrated, and he appears to be explaining the next step in his process. The chicken pieces on the cutting board remain in frame, along with other nearby ingredients.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is displayed prominently in the center at the very beginning of the video?",
                "time_stamp": "0:00:20",
                "answer": "D",
                "options": [
                    "A. A clock with hands.",
                    "B. A knife and fork.",
                    "C. A kitchen setting.",
                    "D. A large number \"10\"."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_14_real.mp4"
    },
    {
        "time": "[0:01:00 - 0:02:00]",
        "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:05]: A man stands in a kitchen with a white-tiled backsplash and shelves filled with dishes and ingredients. He wears a dark green polo shirt and begins preparing a recipe. He reaches for a glass mixing bowl on the wooden countertop, near a raw, spatchcocked chicken displayed on a cutting board. [0:01:05 - 0:01:07]: The man adds seasoning to the bowl, starting with ground spices from a small jar. He sprinkles the contents evenly into the bowl while wooden cabinets and additional kitchen utensils are visible in the background. [0:01:07 - 0:01:15]: The man continues seasoning by adding various ingredients, including spices from a thin, red, rectangular can. He meticulously sprinkles these spices, adding layers of flavor to the mixture in the bowl, which is positioned next to the raw chicken on the cutting board. Meanwhile, the kitchen environment, including a stovetop and an assortment of nearby bowls and utensils, remains visible. [0:01:15 - 0:01:18]: Positioned at an angle that captures more of the kitchen\u2019s layout, the man moves swiftly as he seasons the mixture with salt from a grinder. The spacious kitchen is warm and well-lit, with a mixture of modern and rustic elements, such as potted plants near large windows and industrial lights overhead. [0:01:18 - 0:01:20]: The video ends with a close-up of the seasoned mixture in the glass bowl next to the raw chicken, highlighting the combination of spices added so far.\n[0:01:20 - 0:01:40] [0:01:20 - 0:01:24]: A person wearing a dark green polo shirt is seen in what appears to be a kitchen. The kitchen has white brick walls with two dark blue cabinets, one of which is open. There are various items on wooden shelves, including glassware, bottles, cookware, and a red \"Cook\" sign. In front of the person on the countertop, several items, including a large cutting board, a glass mixing bowl, an assortment of fresh vegetables, and other cooking ingredients, can be seen. The person is in the process of slicing food on a cutting board.  [0:01:25 - 0:01:29]: The person can be seen mixing food in the glass bowl. The mixing bowl is placed on the cutting board, and the person uses both hands to knead or mix the food thoroughly. At this point, the person appears to be focusing on ensuring the ingredients in the bowl are well combined. Given the hands' movement, the content in the bowl gets mixed vigorously. [0:01:30 - 0:01:34]: The perspective shifts to a different location in the kitchen, showing a closer view of the person repetitively kneading or mixing the food in the bowl. The surrounding kitchen layout continues to show the same elements, such as the white backsplash, wooden shelves filled with kitchen utensils and ingredients, and various small kitchen appliances. The person's movements are deliberate and consistent as they continue to mix the food. Various bowls, bottles, and utensils on the countertop add to the kitchen's busy and well-used appearance.  [0:01:55 - 0:01:58]: A top-down view shows the person's hands actively mixing food in the bowl placed on the cutting board. The cutting board is stained with food, suggesting ongoing prep work. The surrounding countertop features various small bowls and jars containing ingredients like spices and liquids. The person's hands are engaged in repetitive motions, ensuring the mixture inside the bowl is being worked on thoroughly. This perspective provides a clear and detailed view of the cooking process, highlighting the textures and movements of the ingredients being mixed. [0:01:39]: Returning to ground level, the person is still focused on the glass bowl, their hands moving rhythmically as they continue to mix the contents inside the bowl. The countertop around them, filled with various kitchen supplies, remains visible, depicting an active cooking session. The combination of motion and preparation tools suggests that the person is meticulously working on a culinary creation, continuously blending the ingredients.\n[0:01:40 - 0:02:00] [0:01:40 - 0:01:42]: In a kitchen, a man is standing at a counter, focused on a bowl containing a mixture. Wearing a dark short-sleeved polo shirt, he positions his hands inside the bowl, working with the contents. The bowl is placed on a wooden chopping board on the counter, which also holds various ingredients and utensils, including bowls, a knife, and bottles. Behind him, a brick wall lined with shelves holds cooking equipment and other items. [0:01:43 - 0:01:47]: As the man continues mixing the contents in the bowl, he adjusts the mixture and uses one hand to manipulate what appears to be a piece of meat or a similar ingredient. His expressions change slightly, showing concentration and engagement with the task at hand. The kitchen background, with its modern appliances and neatly arranged items on the shelves, remains consistent. [0:01:48 - 0:01:50]: He looks up momentarily from his task, turning his head to the side as if responding to something. Afterward, he directs his attention back to the bowl and continues his preparatory activities.  [0:01:51 - 0:01:54]: The man shifts focus, wiping his hands with a cloth while stepping sideways, moving from the counter with the chopping board to a stove area. He places the bowl with the mixture aside on another counter. [0:01:55 - 0:01:57]: Next to the stove, he carefully pours oil onto a grill pan, preparing the cooking surface. The oil is in a small bottle with a spout, allowing controlled pouring. [0:01:58 - 0:01:59]: Continuing the task, he pours more oil, ensuring it spreads evenly on the grill. The camera angle provides a clear view of the grill pan and the surrounding stove area with other utensils and ingredients within reach.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What item is the man holding while adding seasoning to the bowl?",
                "time_stamp": "0:01:14",
                "answer": "D",
                "options": [
                    "A. A small jar.",
                    "B. A wooden spoon.",
                    "C. A bottle of oil.",
                    "D. A thin, red, rectangular can."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_14_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:00 - 0:02:20] [0:00:00 - 0:00:03]: In a kitchen with white brick walls and shelves filled with dishes and utensils, a person wearing a dark green polo shirt stands behind a counter. At center-right, the person mixes ingredients in a clear glass bowl, positioned atop a wooden cutting board. Nearby, there are stacked bowls, a small jar, a bottle of oil, a lemon, and other cooking essentials;  [0:00:04 - 0:00:06]: The camera focuses on the person's hands working in the glass bowl, blending and lifting chicken pieces with a reddish seasoning. With a teal-colored cabinet in the background, the person's actions are meticulously captured as they handle and prepare the chicken;  [0:02:07 - 0:02:08]: The person transfers the seasoned chicken to a hot grill pan on the stove, carefully placing it on the heated surface. The kitchen setup includes a saucepan on an adjacent burner, emphasizing the cooking process;  [0:02:09 - 0:02:14]: An overhead shot shows the chicken sizzling on the grill. The person\u2019s hands move the glass bowl out of the way while sprinkling additional seasoning onto the meat. The action captures the essence of methodically cooking and seasoning the chicken for optimal flavor;  [0:02:15 - 0:02:18]: Back to a side angle, the individual continues basting the chicken while holding the glass bowl. Turning briefly and gesturing with one hand, they engage with an unseen audience, indicating possible instructions or explanations regarding the cooking technique;  [0:02:19]: Another angle portrays the person passionately explaining something, hands gesturing above the countertop. The kitchen backdrop with its organized elements and the ongoing cooking process remain visible, enhancing the relatability and realism of the scene.\n[0:02:20 - 0:02:40] [0:02:20 - 0:02:21]: A person is standing in front of a kitchen counter, facing the stove. The back of their head shows short, light brown hair. They are wearing a dark green polo shirt. The kitchen has a modern design with teal cabinets, white brick walls, and wooden shelves holding various kitchen items. On the counter, there are bowls and a griddle with pieces of seasoned chicken. [0:02:21 - 0:02:24]: The person continues facing the counter, apparently preparing something on the work surface. The griddle on the stove with seasoned chicken is visible. An open saucepan and other kitchen utensils can be seen in the background. [0:02:24 - 0:02:31]: The camera shifts focus to the griddle where the pieces of seasoned chicken are cooking. The chicken pieces appear to be in a frying stage, with a slight hint of smoke or steam rising from the griddle. [0:02:32 - 0:02:34]: The person now turns towards the camera, moving slightly to the left. The kitchen background remains consistent with shelves, dishes, and utensils. The framed word \"HOT\" is visible on the counter's edge beside other cooking items. [0:02:34 - 0:02:38]: Facing the camera, the person seems to be explaining something, using hand gestures to emphasize points. The stovetop and griddle with the chicken remain constant in the background, with the modern kitchen d\u00e9cor as the backdrop. [0:02:38 - 0:02:39]: The person continues to talk, tilting their head slightly and making more hand gestures. They pick up a pan from the stovetop, holding it up beside them as if showing something related to the cooking process. Vegetation visible through a window or a glass door adds a touch of greenery to the scene.\n[0:02:40 - 0:03:00] [0:02:40 - 0:02:41]: In a modern kitchen with white subway tile backsplash and stainless-steel appliances, a man stands at a countertop, speaking animatedly while holding his hands together in front of him. To his right is a stove with a large pan on it, and various utensils are scattered on the counter to his left. [0:02:41 - 0:02:42]: The man looks down and keeps his hands together in front of him. Behind him, shelves holding various kitchen items like dishes and containers are visible, and a double oven is built into the wall to his right. [0:02:42 - 0:02:43]: The man lifts a lid off the pan on the stove, revealing several pieces of food cooking. He uses his left hand to hold the lid and his right hand to adjust the food. [0:02:43 - 0:02:44]: The man points his right index finger up, possibly indicating a point, while still holding the lid in his left hand. He looks to his right, focusing intently. [0:02:44 - 0:02:45]: He reaches over with his right hand to adjust something off-camera while continuing to cook the food on the stove. His body is slightly turned to the right. [0:02:45 - 0:02:46]: The man now holds a pair of red tongs in his right hand, examining them closely while looking at the food on the stove. He appears focused on what he is doing. [0:02:46 - 0:02:47]: Using the tongs, the man starts to turn over the pieces of food on the stove. His left hand is placed on the counter for support, and the food sizzles on the grill pan. [0:02:47 - 0:02:48]: He continues to turn the food with the tongs, ensuring each piece is evenly cooked. The camera angle shows a close-up of his actions. [0:02:48 - 0:02:49]: The man meticulously adjusts the pieces of food on the grill pan, making sure they are placed correctly. The food starts to develop a charred, crispy surface. [0:02:49 - 0:02:50]: From an overhead perspective, the man uses the tongs to turn a piece of food on the grill pan, ensuring it cooks evenly. The adjacent skillet remains on the stove, unused at the moment. [0:02:50 - 0:02:51]: He adjusts another piece of food with the tongs, focusing intently on his task. The overhead view provides a clear view of his precise movements. [0:02:51 - 0:02:52]: The man checks the positioning of the food once more, using the tongs to move them around on the grill pan. The overhead perspective highlights his efficient method of cooking. [0:02:52 - 0:02:53]: He pauses momentarily, holding the tongs in his right hand over the food, preparing for the next step. The food continues to sizzle on the grill pan. [0:02:53 - 0:02:54]: The man turns back to face the counter with the stove and skillets, focusing once again on the cooking process. His left hand rests near the edge of the counter. [0:02:54 - 0:02:55]: Using the red tongs, he carefully picks up one piece of food from the grill pan, inspecting it closely. His focus remains on the food to ensure it's cooked properly. [0:02:55 - 0:02:56]: He continues holding the food piece with the tongs, looking at its underside. His left hand is still positioned close to the stove, ready to adjust anything if needed. [0:02:56 - 0:02:57]: The man raises the piece of food from the grill pan, possibly to check its doneness. His expression shows concentration as he evaluates the food. [0:02:57 - 0:02:58]: Returning to a close-up, the food piece is turned over by the red tongs, revealing a well-cooked, charred outer layer. The man ensures it is cooked to perfection. [0:02:58 - 0:02:59]: He continues to turn and adjust another piece of food on the grill pan using the tongs, while smoke from the hot grill pan rises into the air. [0:02:59]: The detailed close-up reveals the man\u2019s steady hand turning the food on the grill pan to ensure even charring and cooking on all sides.",
        "questions": [
            {
                "task_type": "Event Understanding",
                "question": "What does the person do immediately after transferring the seasoned chicken to the grill pan?",
                "time_stamp": "0:02:12",
                "answer": "B",
                "options": [
                    "A. Adds oil to the pan.",
                    "B. Sprinkles additional seasoning onto the meat.",
                    "C. Places the glass bowl in the sink.",
                    "D. Turns up the heat on the stove."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_14_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:01]: A person wearing a dark green shirt tears a bunch of small green herbs and places them into a plastic blender cup, which is already loaded with a white liquid and other green herbs. The blender cup is sitting on a wooden cutting board on a kitchen counter with a stove visible in the background. [0:04:01 - 0:04:02]: The person continues to tear more herbs and drop them into the blender cup. [0:04:02 - 0:04:05]: The camera angle shifts to show more of the kitchen, which features a white brick backsplash and shelves with various kitchen items. The person continues to tear herbs and place them in the blender cup.  [0:04:05 - 0:04:06]: The person plucks several small leaves from a stem and adds them to the blender cup, continuing the same motion. [0:04:06 - 0:04:07]: The perspective changes to an overhead shot directly above the cutting board. The person\u2019s hands tear herbs and drop them into the blender cup from above, emphasizing the focus on the hands. [0:04:07 - 0:04:13]: The overhead view continues as the person tears and adds more green herbs into the blender cup, working systematically. A lid for the blender cup and other kitchen items like small bowls containing spices and salt are visible around the cutting board. [0:04:13 - 0:04:14]: The person places the last few leaves into the blender cup and begins to prepare the area for the next steps. [0:04:14 - 0:04:15]: The person reaches for a container near the cutting board, adding what seems to be more seasoning ingredients into the blender cup. [0:04:15 - 0:04:16]: The perspective returns to a wider shot of the person in the green shirt, who closes the blender cup and positions it on the counter. [0:04:16 - 0:04:17]: The person adjusts the positioning of the blender cup and reaches for a small bottle from a nearby group of condiments on the counter. [0:04:17 - 0:04:18]: The person picks up an avocado and starts to cut it cautiously using a knife, preparing it for addition to the blender cup. [0:04:18 - 0:04:19]: The person continues to handle the avocado with a knife, carving out the flesh as part of the preparation process.\n[0:04:20 - 0:04:40] [0:04:20 - 0:04:22]: A man in a green polo shirt is holding an avocado in his left hand and using a small knife to cut into it with his right hand. The kitchen backdrop includes a white tile backsplash, wooden shelves with various dishes, and a counter with kitchen tools. [0:04:23 - 0:04:24]: The man has finished cutting the avocado and begins to twist the two halves apart, still holding the knife in his right hand. The kitchen counter in front of him has a cutting board and a blending container. [0:04:25 - 0:04:28]: The man has successfully separated the two halves of the avocado. He holds up one half to show the inside. The background continues to show the wooden cabinets and kitchen counter with various tools and ingredients. [0:04:29 - 0:04:30]: The man uses a spoon to scoop out the avocado flesh, holding it over the blending container as seen in the overhead shot from above the cutting board. Ingredients like herbs are already in the blending container. [0:04:31 - 0:04:32]: The man shows the scooped-out avocado half to the camera, pointing to it with the spoon. The background reveals more of the kitchen setup, including a blue cabinet and white brick wall. [0:04:33 - 0:04:34]: The man starts placing the scooped avocado pieces into the blending container, which contains other ingredients. There is a stove in the background with a pot on it, and the counter has several small bowls. [0:04:35 - 0:04:36]: The man reaches for a bottle and begins to open it. The counter in the background has various kitchen equipment and ingredients. [0:04:37 - 0:04:38]: The man holds the open bottle and pours the liquid into the blending container, which now has avocado and other ingredients in it. The kitchen background continues to show various items on the shelves and counter. [0:04:39]: The man continues pouring the liquid, focusing on the blending container. The kitchen setup in the background includes a stove, countertops, and various utensils and ingredients neatly arranged.\n[0:04:40 - 0:05:00] [0:04:40 - 0:04:40]: A person is seen standing at a kitchen counter wearing a green shirt. On the wooden cutting board in front of them, there is a clear plastic blender cup containing ingredients such as green herbs, a slice of lemon, and a white liquid. They are holding a bottle with a golden cap, pouring a dark liquid into the blender cup. [0:04:41 - 0:04:41]: The camera now focuses on the person who places the blender cup on the counter. The background showcases a white tiled wall with wooden shelves holding plates and green bottles, and various kitchen utensils hanging on the wall. [0:04:42 - 0:04:42]: The person screws on the lid of the blender cup. Their right hand firmly holds the cup while their left hand twists the lid. The action takes place in front of the same kitchen setup with neatly arranged shelves and hanging utensils. [0:04:43 - 0:04:43]: The person continues tightening the lid by turning it several times. Their focus remains on ensuring the lid is secure. The white tiled background and organized kitchen tools and items remain visible. [0:04:44 - 0:04:44]: After securing the lid, the person lifts the blender cup slightly, appearing ready to attach it to a blender base. The camera angle gives more visibility to the bottles and jars placed on the kitchen counter. [0:04:45 - 0:04:46]: The person moves the blender cup towards a blender base. The camera captures a better view of their arm movement and the counter setting, which includes containers, utensils, and the wooden cutting board. [0:04:47 - 0:04:49]: The person places the blender cup onto the base, pressing down firmly with both hands to ensure it is properly attached. Visible on the counter are a bottle with a golden cap and other kitchen tools, indicating preparation for blending. [0:04:50 - 0:04:52]: The blender is turned on, and the person holds it down to keep it steady. The ingredients inside the cup start to blend, gradually changing consistency. The background remains consistent with the previous frames, showcasing the organized kitchen setup. [0:04:53 - 0:04:54]: The person stops the blender and detaches the blender cup from the base. There is a noticeable change in the texture of the contents, looking smoother. The person is still standing at the kitchen counter with the array of ingredients and tools around them. [0:04:55 - 0:04:55]: The person gives the blended mixture a good shake to ensure everything is well mixed. They seem content with the texture as they closely examine the contents of the cup. [0:04:56 - 0:04:57]: The person pours the contents from the blender cup into a separate container, with the camera focusing on the action. The kitchen counter remains cluttered with various tools and ingredients. [0:04:58 - 0:04:59]: The person places the now empty blender cup back onto the counter. The scene concludes with them organizing the blender base and the cup, ensuring everything is tidy on the kitchen counter. The background continues to show the well-organized kitchen setup.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What does the person do with the avocado after cutting it?",
                "time_stamp": "0:04:34",
                "answer": "C",
                "options": [
                    "A. Adds it whole to the blender.",
                    "B. Mashes it with a spoon.",
                    "C. Scoops out the flesh and places it into the blending container.",
                    "D. Peels off the skin and discards it."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_14_real.mp4"
    },
    {
        "time": "[0:10:00 - 0:10:47]",
        "captions": "[0:10:00 - 0:10:20] [0:00:00 - 0:00:03]: A kitchen setting is visible, with white brick walls and shelves stocked with dishes, glasses, and cooking utensils. A man stands near a gas stove, holding a skillet with red tongs on the right side and a kitchen towel in his left hand. He tilts the skillet towards a plate on the right side of the counter, where a cooked dish is placed. [0:00:03 - 0:00:04]: He places the food from the skillet onto the plate on the right and aligns the skillet back onto the stove. [0:00:04 - 0:00:06]: Still holding the skillet with the tongs and the towel, he turns to face forward and glances at the camera, next to the stove. [0:00:06 - 0:00:07]: The man puts the skillet down on the stove and starts adjusting it. [0:00:07 - 0:00:08]: He continues to look towards the camera while holding a kitchen towel in his left hand. [0:00:08 - 0:00:10]: He begins talking, making a few hand gestures, and appears to give instructions. His facial expression is animated. [0:00:10 - 0:00:12]: He leans slightly forward, continuing to talk, with the blue cabinets and modern kitchen appliances visible in the background. [0:00:12 - 0:00:13]: He stands straight again, holding a kitchen towel with both hands, and appears to be listening intently. [0:00:13 - 0:00:14]: He keeps talking, glancing slightly to the right. The words \"COOK\" are displayed in large red letters on the shelf above the counter. [0:00:15 - 0:00:16]: A close-up of the cooked dish on a wooden cutting board is visible, showcasing grilled meat, roasted vegetables, and a fresh salad with avocado and dressing. [0:00:16 - 0:00:17]: The camera remains focused on the dish, providing a detailed view of the charred, juicy meat and the vibrant colors of the salad. [0:00:17 - 0:00:18]: The camera then zooms in further on the grilled meat, emphasizing its texture and the marks from grilling. [0:00:18 - 0:00:19]: The camera shifts to a top-down view of the cutting board, displaying the entire dish, including the meat, salad, grilled vegetables, and a small bowl of creamy dressing.\n[0:10:20 - 0:10:40] [0:10:20 - 0:10:21]: On a wooden cutting board, there is a beautifully presented dish. The left side of the board features grilled, seasoned chicken with hints of char marks. It is accompanied by a vibrant salad composed of lettuce, red cabbage, slices of avocado, chunks of feta cheese, scattered croutons, and grilled rounds of zucchini.  [0:10:22]: The dish appears from a different angle, with more visibility of the small white bowl filled with a green, creamy dressing located at the top right corner of the wooden board. [0:10:23]: The view transitions to a kitchen setting where a man is speaking, gesturing with his hands. He is wearing a green shirt and standing in front of a background that includes shelves stocked with jars, plates, and kitchen utensils. [0:10:24 - 0:10:26]: The man continues to speak, moving his hands expressively. Behind him is a blue cabinet and shelves filled with various kitchen items, including jars with grains and spices. The background also contains a wooden cutting board and a white-brick wall, adding a rustic charm to the setting. [0:10:27 - 0:10:28]: The man appears animated as he talks, his facial expressions suggest engagement. The background remains the same, with shelves holding neatly arranged crockery and kitchen essentials. [0:10:29 - 0:10:30]: The man remains the focal point, speaking and gesturing. The word \"COOK\" in red letters is prominently displayed on a shelf in the background, alongside green bottles and white dishes. [0:10:31 - 0:10:33]: The scene shifts slightly, showing the man in mid-conversation, appearing enthusiastic. The blue kitchen cabinet and various jars on shelves in the background are still visible, maintaining continuity in the kitchen setting. [0:10:34 - 0:10:36]: Returning to a centered shot, the man continues to talk, now with less exaggerated gestures. The kitchen background remains consistent, showcasing an organized array of kitchenware. [0:10:37 - 0:10:39]: The man concludes his speech, standing calmly with his hands clasped in front of him. The neat and rustic kitchen setting with the white brick wall and tidy shelves filled with jars and plates remains visible in the backdrop.\n[0:10:40 - 0:10:47] [0:10:40 - 0:10:41]: The scene begins with clear visibility of a kitchen. A man, wearing a green t-shirt, stands centrally in front of a white-tiled wall with shelves holding various items like dishes, glasses, and green bottles. On the upper right corner of the screen shows the timestamp \"10:40\". The word \"COOK\" in red letters is displayed prominently on a shelf behind him. He has light brown hair and is looking directly at the camera, with both hands clasped in front of him, slightly lower than chest level. The countertop behind him has a cutting board, and several knives are mounted on the wall to his left.  [0:10:42 - 0:10:45]: The perspective shifts to focus on a wooden surface with a book titled \"GORDON RAMSAY RAMSAY IN 10\" placed atop it. The wooden surface appears to be the same countertop from the previous frames. The subtitle of the book reads, \"Delicious Recipes Made in a Flash\". The book cover features multiple images, including pictures of prepared dishes and some glimpses of the man seen earlier, likely in different settings or poses. The background seems to be consistent with a textured, concrete-like floor. The timestamp in each consecutive frame is visible in the top left corner, showing \"10:42\", \"10:43\", \"10:44\", and \"10:45\".",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What are some of the components of the salad on the cutting board?",
                "time_stamp": "0:10:21",
                "answer": "D",
                "options": [
                    "A. Spinach, red cabbage, tomato, feta cheese, and olives.",
                    "B. Lettuce, red cabbage, tomato, feta cheese, and croutons.",
                    "C. Spinach, red cabbage, avocado, feta cheese, and croutons.",
                    "D. Lettuce, red cabbage, avocado, feta cheese, and croutons, chicken."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_14_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:00:18",
                "answer": "C",
                "options": [
                    "A. 2.",
                    "B. 3.",
                    "C. 0.",
                    "D. 4."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_77_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:03:28",
                "answer": "C",
                "options": [
                    "A. 2.",
                    "B. 4.",
                    "C. 6.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_77_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:04:31",
                "answer": "C",
                "options": [
                    "A. 8.",
                    "B. 7.",
                    "C. 9.",
                    "D. 10."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_77_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:05:18",
                "answer": "C",
                "options": [
                    "A. 10.",
                    "B. 7.",
                    "C. 9.",
                    "D. 8."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_77_real.mp4"
    },
    {
        "time": "[0:07:00 - 0:08:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:07:07",
                "answer": "C",
                "options": [
                    "A. 13.",
                    "B. 10.",
                    "C. 12.",
                    "D. 11."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_77_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:01]: A man in a dark shirt stands in front of a kitchen counter with a marble top. He holds up a smartphone with his right hand. On the counter behind him are various kitchen appliances, including a mixer, toaster, and instant pot. The cabinets have glass doors showing plates and glassware inside; [0:03:02]: The man has started to walk away from the counter and heads towards the open door of a refrigerator, which stands adjacent to the marble countertop. A faucet is visible on the island in the foreground, and steam is emanating from two pots on the stove; [0:03:02 - 0:03:03]: The scene shifts to a new room with light wooden flooring. A person is seated on the floor in front of a fireplace and beside a small child who is playing. Toys and miniature furniture are scattered around; [0:03:04]: The same individual, who appears to be a woman, smiles and looks down at the child. They are both situated near a white cabinet housing various small toys and objects; [0:03:05]: A close-up of a small child who is playing with the person sitting next to them. The child is reaching for something near their feet; [0:03:06]: The child is now sitting on the floor close to the person in the floral dress. There is a wooden miniature kitchen playset behind them; [0:03:07]: A small white dog comes into view and is lying on the floor beside the person in the floral dress; [0:03:08]: A close-up of the small white dog staring at the camera. It is sitting on the wooden floor, partially leaning against the person in the floral dress; [0:03:09]: The camera begins to shift quickly, causing a blurred, disoriented image. A white wall and a part of the previous room are faintly visible as the camera moves; [0:03:10]: The scene changes back to the wooden floored hallway as the small white dog scurries down the corridor towards another room. The walls are white, and there is a wooden trim at the bottom; [0:03:11]: The camera angle shifts upwards, showing a dining area with windows in the background. There are large pendant lights hanging from the ceiling, providing illumination to the space; [0:03:12]: The direction shifts again, a rapid movement through a hallway towards another part of the kitchen. Some elements of the previously seen kitchen come back into view, including dark cabinets and a light-colored countertop; [0:03:13]: The scene is back in the kitchen. The man in a dark shirt is once again in the view, standing next to the stovetop. Pots are boiling, with steam rising from them; [0:03:14]: The man continues to prepare food as he stands before the stovetop. The countertop shows different ingredients and utensils for the cooking process; [0:03:15]: The camera angle shifts slightly to the right. The man begins handling meat and potatoes that are placed on the counter. A chopping board and a kitchen knife are close by; [0:03:16]: A close-up of the man, now facing the camera, possibly taking instructions from his phone, as he looks at it intently. Various cooking ingredients lie neatly arranged on the counter beside him; [0:03:17]: The man appears to focus on his phone, typing or reading from it while standing in the kitchen. Some utensils and ingredients are clearly visible in front of him; [0:03:18]: The scene catches the man in mid-motion, as he possibly transitions from interacting with his phone back to focusing on the cooking tasks at hand; [0:03:19]: The man energetically raises his arms, displaying enthusiasm, while continuing to work in the kitchen. His posture suggests he is deeply engaged in his cooking activity.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the man holding in his right hand?",
                "time_stamp": "0:03:16",
                "answer": "C",
                "options": [
                    "A. A pen.",
                    "B. A spatula.",
                    "C. An olive oil bottle.",
                    "D. A remote."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_35_real.mp4"
    },
    {
        "time": "[0:06:00 - 0:07:00]",
        "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:01]: A person wearing a dark shirt and a towel draped over their shoulder is standing in a kitchen. They are holding a knife and a piece of garlic over a wooden cutting board. To the right, a bowl filled with peas can be seen, and a plate with various vegetables is in the lower right corner. The countertop is made of marble.  [0:06:01 - 0:06:02]: The person adjusts their grip on the knife, preparing to cut the garlic. The garlic cloves are positioned on the wooden cutting board, and the man is standing with their right hand ready to slice. [0:06:02 - 0:06:03]: The person starts to chop the garlic while keeping their left hand steady on the board. A portion of the garlic has been sliced, and the left hand hovers just above the pieces. [0:06:03 - 0:06:04]: The person continues to chop the garlic into smaller pieces with precision. The left hand has moved away slightly to give space for the knife to work. [0:06:04 - 0:06:05]: After chopping the garlic, the person motions over to the right of the cutting board, preparing to move the chopped garlic. The bowl of peas remains in the background. [0:06:05 - 0:06:06]: The person now moves the chopped garlic to the side with the flat side of the knife. More detailed parts of the kitchen can be seen in the background, including drawers and utensils. [0:06:06 - 0:06:07]: A close-up view of the person holding the freshly chopped garlic. They spread the chopped pieces with the flat part of the knife to further mince them on the wooden cutting board. [0:06:07 - 0:06:08]: The remaining garlic clove lies on the cutting board, with minced pieces around. The person then lifts the mince garlic pieces using the knife. [0:06:08 - 0:06:09]: Moving over to the stove, the person holds the minced garlic on the knife near a hot skillet with steaks sizzling on it. The person appears ready to add the garlic to the pan. [0:06:09 - 0:06:10]: The perspective shifts to a close-up view of the skillet containing multiple steaks, slightly seared and cooking on a gas burner. The red utensils and sizzling steaks make this the focal point. [0:06:10 - 0:06:11]: The camera retreats slightly and shows the cutting board now empty, except for some residual garlic left behind. The pan with the steaks is still evident in the capture, with a broader look at the kitchen setup. [0:06:11 - 0:06:12]: The person shifts focus back to the pan with steaks, which can be seen cooking actively on a gas stove. The person prepares to flip or stir the steaks. [0:06:12 - 0:06:13]: The camera zooms out to show the person attending to the cooking steaks. The kitchen background reveals more details, including spice containers and cooking appliances on countertops. [0:06:13 - 0:06:14]: The person, holding a towel, stands next to the counter with cooking ingredients arranged orderly. The person continues to focus on the hot skillet with steaks. [0:06:14 - 0:06:15]: After briefly attending to the steaks, the person moves back to the cutting board, readying to continue food preparation with the various ingredients. [0:06:15 - 0:06:16]: With the towel now off to the side, the person stands near the wooden cutting board. Various kitchen utensils, a blender, and food containers are visible behind the person. [0:06:16 - 0:06:17]: The person begins arranging the trimmed garlic cloves on the cutting board. The kitchen equipment, such as an immersion blender and containers, are also in sight. [0:06:17 - 0:06:18]: The camera angle changes, revealing more of the kitchen, including two toasters, spices, and other preparations. The person adjusts their focus back to slicing vegetables on the cutting board. [0:06:18 - 0:06:19]: The person's attention remains on the cutting board, with more intense chopping activity in progress. The arranged onions and potatoes are prepared for further chopping.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the person holding in his hand right now?",
                "time_stamp": "0:06:00",
                "answer": "B",
                "options": [
                    "A. A spoon and a piece of ginger.",
                    "B. A knife and a towel.",
                    "C. A fork and a piece of onion.",
                    "D. A spatula and a piece of pepper."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Action Recognition",
                "question": "What action is the person performing right now?",
                "time_stamp": "0:06:09",
                "answer": "C",
                "options": [
                    "A. Stirring a pot.",
                    "B. Slicing an onion.",
                    "C. Chopping garlic.",
                    "D. Peeling potatoes."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_35_real.mp4"
    },
    {
        "time": "[0:09:00 - 0:10:00]",
        "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:01]: The scene shows a close-up shot of a frying pan on a stovetop. The pan contains several pieces of meat, likely chops, searing with a few small vegetables around them. The handle of the pan is red, and to the side is another pan partially visible with other food cooking in it. The person's hand holding a cloth is situated above the frying pan. [0:09:01 - 0:09:13]: The camera pulls back to reveal a broader kitchen setting. The person cooking is wearing a dark shirt and holding a pair of tongs in their right hand and a cloth in their left. The stovetop has multiple burners with different pans cooking various ingredients. The environment is well-organized, with kitchen utensils and gadgets arranged on the countertop behind. A pot with a glass lid is also visible, with green vegetables being cooked inside. [0:09:13 - 0:09:15]: The camera captures a woman in a floral dress holding a child while standing beside the stove. The woman is observing the cooking process, and the child is also watching intently. The background shows a clean and neatly arranged kitchen, with white tiles on the backsplash and dark-colored cabinets. [0:09:15 - 0:09:17]: The scene continues with the person cooking making an adjustment to the pan with the tongs. They are standing to the right of the stove, wearing a cloth over their shoulder. The woman is still holding the child and watching the person cook.  [0:09:17 - 0:09:18]: The cook reaches into the second pan to adjust the vegetables cooking there. The woman and child remain watching. The kitchen is busy and filled with different cooking activities. [0:09:18]: Finally, the person cooking adds a piece of butter to the center of the pan with the chops and vegetables, ensuring proper seasoning as the cooking continues.",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What color is the handle of the frying pan that contains meat inside?",
                "time_stamp": "00:09:01",
                "answer": "C",
                "options": [
                    "A. Black.",
                    "B. Silver.",
                    "C. Red.",
                    "D. Blue."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_35_real.mp4"
    },
    {
        "time": "[0:12:00 - 0:13:00]",
        "captions": "[0:12:00 - 0:12:20] [0:12:00 - 0:12:01]: The video begins with a first-person perspective focusing on a black pot placed on a gas stove, with blue flames visible beneath it. A person is holding a copper colander in one hand and a kitchen towel in the other.  [0:12:01 - 0:12:02]: In the next moment, the person begins to pour green vegetables from the colander into the black pot on the stove, with steam rising from the hot contents.  [0:12:02 - 0:12:03]: The colander is almost empty now, and the green vegetables are mostly inside the pot, with just a few remaining in the colander.  [0:12:03 - 0:12:04]: The colander is fully emptied into the pot, and the person's hand moves away.  [0:12:04 - 0:12:05]: The person places the empty copper colander into the sink and steps away, leaving the pot on the flaming stove with green vegetables inside. [0:12:05 - 0:12:06]: The scene shifts to show a kitchen counter with various cooking items and ingredients, including a cutting board, knife, and a pan with cooked food.  [0:12:06 - 0:12:07]: The person moves toward the stove area, reaching for a small item on the counter, as the cooking pot and ingredients are in the foreground.  [0:12:07 - 0:12:08]: The person shifts slightly to the right side of the kitchen, reaching into a dish with some grated yellow substance.  [0:12:08 - 0:12:09]: The person adds the grated yellow substance into the pot with the green vegetables, holding a container and using a utensil.  [0:12:09 - 0:12:10]: The person begins mixing the contents of the pot with a spoon, while another person in the background holds a phone, possibly recording or viewing the process. [0:12:10 - 0:12:11]: The camera angle briefly focuses on the stove burner, with the visible blue flames indicating that the heat is still on. [0:12:11 - 0:12:12]: The person shifts back to the stove, pot in hand, places it on the counter, and reaches for a lid. [0:12:12 - 0:12:13]: The person uses a spoon to stir the contents of the pot, with the rest of the kitchen counter visible, including colorful ingredients and kitchen tools.  [0:12:13 - 0:12:14]: The focus shifts to a plate on the counter as the person moves items and prepares other ingredients, including placing dishes and containers. [0:12:14 - 0:12:15]: The person opens a tin of seasoning or sauce, holding it over the pot containing the green vegetables. [0:12:15 - 0:12:16]: The person spoon feeds some of the seasoning from the tin into the pot. [0:12:16 - 0:12:17]: The person continues to add more seasoning into the pot, carefully measuring each portion. [0:12:17 - 0:12:18]: The person begins to stir the ingredients in the pot, ensuring the seasoning is well mixed with the green vegetables. [0:12:18 - 0:12:19]: The person reaches for an item on the counter, possibly to add more ingredients into the pot, with other kitchen items visibly organized around the workspace.",
        "questions": [
            {
                "task_type": "Action Recognition",
                "question": "What does the person do after adding yogurt into the pot?",
                "time_stamp": "0:12:09",
                "answer": "B",
                "options": [
                    "A. Turns off the stove.",
                    "B. Begins mixing the contents of the pot.",
                    "C. Places a lid on the pot.",
                    "D. Adds more green vegetables."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_35_real.mp4"
    },
    {
        "time": "[0:01:00 - 0:02:00]",
        "captions": "[0:01:00 - 0:01:20] \n[0:01:20 - 0:01:40] [0:01:20 - 0:01:24]: The scene begins with a woman standing in a modern kitchen. She's facing the camera wearing a white sleeveless top with black polka dots. In front of her on a light wooden cutting board, there's an array of ingredients including a zucchini, some chopped garlic, and a large white bowl. She holds a small glass jar of a red spice and a spoon in her hands, poised above the bowl. The kitchen background features stainless steel appliances and white subway tiles. [0:01:25 - 0:01:28]: The woman continues to spoon the red spice into the bowl, her hands moving steadily. The bowl contains a mixture of green and red ingredients. She holds the spice jar above the bowl, her left hand controlling the spoon. [0:01:29 - 0:01:30]: The camera zooms out slightly, showing the woman talking and making a gesture with her right hand while still holding the spoon and jar in her left hand. She appears to be explaining something. [0:01:31 - 0:01:32]: The woman continues speaking, gesturing with both hands for emphasis. Her expression seems engaged as she communicates with the viewer. [0:01:33 - 0:01:34]: The shot remains steady as the woman continues her explanation. She occasionally looks down at the ingredients on the cutting board. [0:01:35 - 0:01:36]: The scene continues with a similar setup, as the woman makes more gestures and movements while speaking. Her facial expressions remain animated. [0:01:37]: The woman glances to the side briefly while still continuing her explanation. Her right hand is now extended towards the side as she speaks. [0:01:38 - 0:01:39]: The camera cuts to a close-up of the bowl with the ingredients, showing her hands moving towards a green cup. Other utensils and appliances in the background become slightly blurred.\n[0:01:40 - 0:02:00] ",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the woman wearing?",
                "time_stamp": "0:01:00",
                "answer": "D",
                "options": [
                    "A. A red dress with white dots.",
                    "B. A blue blouse with yellow stripes.",
                    "C. A green apron with floral patterns.",
                    "D. A white sleeveless top with black polka dots."
                ],
                "required_ability": "working memory"
            },
            {
                "task_type": "Action Recognition",
                "question": "What action is the woman performing with the spice jar?",
                "time_stamp": "0:01:24",
                "answer": "C",
                "options": [
                    "A. She is shaking the spice jar over the ingredients.",
                    "B. She is pouring the spice directly into the bowl.",
                    "C. She is spooning the spice into the bowl.",
                    "D. She is closing the lid of the spice jar."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_24_real.mp4"
    },
    {
        "time": "[0:02:00 - 0:03:00]",
        "captions": "[0:02:00 - 0:02:20] \n[0:02:20 - 0:02:40] \n[0:02:40 - 0:03:00] [0:02:40 - 0:02:42]: Hands are pressing a raw hamburger patty flat on a black griddle. The countertop and stove setup indicates a kitchen environment. The patty is maintaining a roughly circular shape, with small droplets of moisture visible on its surface. [0:02:43 - 0:02:47]: In a modern kitchen with clean, white tiled walls and a large stainless steel refrigerator in the background, a woman wearing a white, polka-dot sleeveless top is preparing ingredients on a countertop. Various objects such as a wooden pepper grinder, a bottle of oil, a green mug, and a white bowl are arranged on the counter. [0:02:48 - 0:02:53]: The woman continues to prepare food, speaking actively as she gestures with her hands. There is a sense of movement and engagement in her actions as she organizes the ingredients and utensils around her. [0:02:54 - 0:02:57]: The camera shifts back to the hamburger patty cooking on the black griddle. The patty remains stationary as it continues to cook, with text appearing on the screen reading, \"RECIPE IS IN THE DESCRIPTION BELOW.\" [0:02:58 - 0:02:59]: The scene returns to the woman in the kitchen, pointing and conversing as she reaches for a lemon on the countertop among the various cooking items.",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "What text appeared on the just now?",
                "time_stamp": "00:02:57",
                "answer": "D",
                "options": [
                    "A. \"Cooking Time: 10 minutes\".",
                    "B. \"Season to Taste\".",
                    "C. \"Ingredients: Beef, Salt, Pepper\".",
                    "D. \"Recipe is in the description below\"."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_24_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:02]: The video begins with a close-up of hands squeezing a lemon over a white bowl. The person wears a ring on one finger, and there is a bit of pulp visible in the bowl as the juice drips down. The person wears a white, polka-dotted top, and a spoon rests on the left side of the bowl on the wooden countertop. [0:04:03 - 0:04:04]: The scene shifts to show a wider view of the kitchen. The person, a woman with a ponytail, stands behind a kitchen island with various items, including a large bowl, lemons, and an olive oil bottle. The kitchen has stainless steel appliances, white subway tiles, and green plants around the window. [0:04:05 - 0:04:06]: The woman starts to reach for something on the counter. A closer shot shows her adjusting the bowl on the wooden board before grabbing a bottle of olive oil. [0:04:07 - 0:04:08]: She pours olive oil into the white bowl containing the lemon and other ingredients. The light reflects off the bottle as the golden liquid flows, showing her concentrated expression as she performs the action. [0:04:09 - 0:04:10]: The shot switches to a medium close-up of her vigorously mixing the ingredients in the bowl with a fork or similar utensil. She looks focused as she blends everything. [0:04:11 - 0:04:12]: A close-up image shows the mixture inside the bowl as it is being stirred. The green contents, likely avocado or a similar ingredient, are becoming well-mixed as the fork moves around. [0:04:13 - 0:04:14]: The camera returns to a medium shot of the woman standing behind the counter, continuing to mix the ingredients in the bowl. Various items such as lemons, a pepper mill, and a small green cup are visible on the counter. [0:04:15]: She steps to the right side of the frame to grab a green bottle of olive oil before moving toward the stove area. [0:04:16 - 0:04:17]: Over at the stove, she begins to drizzle olive oil onto a pan that contains a patty or similar item. The kitchen remains consistent in appearance, exuding a clean and modern feel with white cabinetry and a large window in the background. [0:04:18 - 0:04:19]: The final close-up shows the patty on the grill now glistening with olive oil. The surface of the patty appears slightly rough, likely indicating that it is a homemade preparation. The video captures the detail of the patty being cooked thoroughly.\n[0:04:20 - 0:04:40] \n[0:04:40 - 0:05:00] ",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What is the woman holding right now?",
                "time_stamp": "0:04:06",
                "answer": "D",
                "options": [
                    "A. A spoon.",
                    "B. A white bowl.",
                    "C. A pepper mill.",
                    "D. A green bottle of olive oil."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_24_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "captions": "[0:05:00 - 0:05:20] \n[0:05:20 - 0:05:40] [0:05:20 - 0:05:23]: In a bright and modern kitchen, a person wearing a white, polka-dotted sleeveless top and dark pants is tending to food on a large countertop. On the counter, there are bottles of olive oil, pepper grinder, a small green ceramic bowl, and a plate with lettuce. They use a spatula to flip a burger patty on the griddle while a hamburger bun is also warming nearby. [0:05:24]: The focus shifts to a close-up of the griddle where the person lifts the cooked burger patty with a spatula. The hamburger bun is toasted brown on the griddle. [0:05:25]: The scene transitions to a cutting board on the counter, which has slices of tomato, pickles, and lettuce prepared for assembly. The image is slightly out of focus. [0:05:26 - 0:05:28]: The person places a piece of lettuce on the bottom half of the hamburger bun on the cutting board and then places the burger patty on top of the lettuce using a black spatula. Slices of tomato are nearby. [0:05:29 - 0:05:30]: The person focuses on assembling the burger. They are carefully positioning the lettuce and burger patty on the bottom half of the bun. The kitchen setup in the background includes marble countertops, a refrigerator, and kitchen cabinets. [0:05:31 - 0:05:33]: The next few frames show the person continuing to assemble the burger, adjusting the ingredients to ensure they are well placed. Their hair is tied back, allowing a clear view of their focused expression. [0:05:34 - 0:05:35]: The camera briefly zooms in on the assembling process. The top half of the bun is about to be placed on the burger, which now has layers of lettuce and a patty. [0:05:36 - 0:05:37]: The viewpoint momentarily moves back to show an olive oil bottle in the foreground. The forming of the burger continues as the person reaches for ingredients.  [0:05:38 - 0:05:39]: Finally, the person retrieves the fully assembled burger and starts to step away from the counter, holding the burger with some satisfaction. The kitchen counter remains filled with various ingredients and cooking utensils, signifying an active cooking environment.\n[0:05:40 - 0:06:00] ",
        "questions": [
            {
                "task_type": "Text-Rich Understanding",
                "question": "How much time does this person have left now??",
                "time_stamp": "0:05:49",
                "answer": "D",
                "options": [
                    "A. 10s.",
                    "B. More than ten seconds.",
                    "C. 20s.",
                    "D. Less than ten seconds."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "cooking",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_24_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:05]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the shape of the large structure on the left side of the road right now?",
                "time_stamp": "00:00:04",
                "answer": "C",
                "options": [
                    "A. Cube.",
                    "B. Pyramid.",
                    "C. Sphere.",
                    "D. Cone."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_397_real.mp4"
    },
    {
        "time": "[0:02:12 - 0:02:17]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What color are the traffic lights right now?",
                "time_stamp": "00:02:16",
                "answer": "C",
                "options": [
                    "A. Red.",
                    "B. Yellow.",
                    "C. Green.",
                    "D. Blue."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_397_real.mp4"
    },
    {
        "time": "[0:04:24 - 0:04:29]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the weather like right now?",
                "time_stamp": "00:04:27",
                "answer": "C",
                "options": [
                    "A. Sunny.",
                    "B. Snowy.",
                    "C. Foggy and Rainy.",
                    "D. Clear."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_397_real.mp4"
    },
    {
        "time": "[0:06:36 - 0:06:41]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What type of vehicle is the yellow car right now?",
                "time_stamp": "00:06:38",
                "answer": "A",
                "options": [
                    "A. Taxi.",
                    "B. School bus.",
                    "C. Firetruck.",
                    "D. Police car."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_397_real.mp4"
    },
    {
        "time": "[0:08:48 - 0:08:53]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is the width of the road right now?",
                "time_stamp": "00:08:50",
                "answer": "C",
                "options": [
                    "A. Narrow.",
                    "B. Tiny.",
                    "C. Wide.",
                    "D. Very narrow."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "driving",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_397_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:05]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "Which type of vehicle is visible inside the hangar right now?",
                "time_stamp": "00:00:03",
                "answer": "C",
                "options": [
                    "A. Truck.",
                    "B. Boat.",
                    "C. Aircraft.",
                    "D. Motorcycle."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_423_real.mp4"
    },
    {
        "time": "[0:02:53 - 0:02:58]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "Which device is currently placed on the right side of the cockpit?",
                "time_stamp": "00:02:56",
                "answer": "A",
                "options": [
                    "A. A GPS device.",
                    "B. A radio transmitter.",
                    "C. An altimeter.",
                    "D. A fuel gauge."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_423_real.mp4"
    },
    {
        "time": "[0:05:46 - 0:05:51]",
        "questions": [
            {
                "task_type": "Attribute Recognition",
                "question": "What is right now the terrain mainly covered with, as seen from the flying POV?",
                "time_stamp": "00:05:49",
                "answer": "C",
                "options": [
                    "A. Desert.",
                    "B. Urban buildings.",
                    "C. Forest.",
                    "D. Water bodies."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_423_real.mp4"
    },
    {
        "time": "[0:08:39 - 0:08:44]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "What instrument is the pilot using right now to navigate?",
                "time_stamp": "00:08:42",
                "answer": "C",
                "options": [
                    "A. Altimeter.",
                    "B. Compass.",
                    "C. GPS tablet.",
                    "D. Map."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_423_real.mp4"
    },
    {
        "time": "[0:11:32 - 0:11:37]",
        "questions": [
            {
                "task_type": "Object Recognition",
                "question": "Which device is placed next to the pilot's seat right now?",
                "time_stamp": "00:11:36",
                "answer": "B",
                "options": [
                    "A. A GPS device.",
                    "B. A camera.",
                    "C. An oxygen mask.",
                    "D. A water bottle."
                ],
                "required_ability": "working memory"
            }
        ],
        "video_categories": "flying_pov",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_423_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:01:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:00:18",
                "answer": "A",
                "options": [
                    "A. 0.",
                    "B. 2.",
                    "C. 3.",
                    "D. 4."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_82_real.mp4"
    },
    {
        "time": "[0:03:00 - 0:04:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:03:07",
                "answer": "A",
                "options": [
                    "A. 5.",
                    "B. 6.",
                    "C. 4.",
                    "D. 0."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_82_real.mp4"
    },
    {
        "time": "[0:04:00 - 0:05:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:04:09",
                "answer": "A",
                "options": [
                    "A. 6.",
                    "B. 2.",
                    "C. 3.",
                    "D. 4."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_82_real.mp4"
    },
    {
        "time": "[0:05:00 - 0:06:00]",
        "questions": [
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:05:01",
                "answer": "A",
                "options": [
                    "A. 7.",
                    "B. 2.",
                    "C. 6.",
                    "D. 8."
                ],
                "required_ability": "episodic memory"
            },
            {
                "task_type": "Counting",
                "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
                "time_stamp": "00:05:33",
                "answer": "A",
                "options": [
                    "A. 9.",
                    "B. 5.",
                    "C. 8.",
                    "D. 6."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "introduction_to_film",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_82_real.mp4"
    },
    {
        "time": "[0:00:00 - 0:00:30]",
        "questions": [
            {
                "task_type": "Causal Reasoning",
                "question": "Why Mr. Bean was previously flipping through boxes?",
                "time_stamp": "00:02:06",
                "answer": "B",
                "options": [
                    "A. Because he was looking for his lost keys.",
                    "B. Because he is looking for his green little hat.",
                    "C. Because he was organizing old photographs.",
                    "D. Because he wanted to find his shopping list."
                ],
                "required_ability": "episodic memory"
            }
        ],
        "video_categories": "kids cartoon",
        "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_241_real.mp4"
    }
]