[
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual taken just now?",
        "time_stamp": "00:00:31",
        "answer": "C",
        "options": [
          "A. The individual organized serving trays and sanitized the preparation area.",
          "B. The individual retrieved produce from the refrigerator and began chopping vegetables.",
          "C. The individual replenished bread inventory by placing new buns onto the shelves.",
          "D. The individual checked inventory levels and noted items lacking in stock."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_348_real.mp4"
  },
  {
    "time": "[0:03:17 - 0:03:27]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual taken just now?",
        "time_stamp": "00:03:26",
        "answer": "A",
        "options": [
          "A. The individual prepared a burger by retrieving buns, and placing them on the package.",
          "B. The individual organized utensils by sanitizing them and placing them back in their designated areas.",
          "C. The individual cleaned the preparation area and refilled condiment dispensers.",
          "D. The individual baked a batch of fresh buns and arranged them in an orderly manner."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_348_real.mp4"
  },
  {
    "time": "[0:06:34 - 0:06:44]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual taken just now?",
        "time_stamp": "00:06:43",
        "answer": "B",
        "options": [
          "A. The individual arranged fresh vegetables on plates and prepared salads.",
          "B. The individual topped burger buns with ketchup, onions.",
          "C. The individual arranged freshly cut fruits into serving containers.",
          "D. The individual prepared sandwiches by arranging cheese and lettuce on bread slices."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_348_real.mp4"
  },
  {
    "time": "[0:09:51 - 0:10:01]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual taken just now?",
        "time_stamp": "00:10:07",
        "answer": "B",
        "options": [
          "A. The individual toasted the buns and added ketchup and mustard.",
          "B. The individual placed two buns in boxes, added cheese slices, and added condiments.",
          "C. The individual grilled burger patties and placed them on the buns with lettuce and pickles.",
          "D. The individual wrapped sandwiches with paper and handed them to the customer."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_348_real.mp4"
  },
  {
    "time": "[0:13:08 - 0:13:18]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual taken just now?",
        "time_stamp": "00:13:22",
        "answer": "C",
        "options": [
          "A. The individual selected burger buns, added patties, and placed them on a griddle.",
          "B. The individual retrieved burger buns, added lettuce and sauces, and placed them on a tray.",
          "C. The individual took burger buns, added lettuce and condiments, and arranged them on a preparation area.",
          "D. The individual cleaned the preparation area, organized serving trays, and put away ingredients."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_348_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What can be seen in the cockpit's center screen right now?",
        "time_stamp": "00:00:06",
        "answer": "B",
        "options": [
          "A. A compass.",
          "B. A map.",
          "C. An altitude meter.",
          "D. A weather radar."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_416_real.mp4"
  },
  {
    "time": "[0:02:23 - 0:02:43]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What colors are the pilot's handheld controls right now?",
        "time_stamp": "00:02:37",
        "answer": "A",
        "options": [
          "A. Black.",
          "B. Red and green.",
          "C. Yellow and blue.",
          "D. Black and orange."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_416_real.mp4"
  },
  {
    "time": "[0:04:46 - 0:05:06]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What are the visible weather conditions right now?",
        "time_stamp": "00:05:01",
        "answer": "A",
        "options": [
          "A. Clear sky with scattered clouds.",
          "B. Overcast sky with rain.",
          "C. Foggy with low visibility.",
          "D. Thunderstorms with lightning."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_416_real.mp4"
  },
  {
    "time": "[0:07:09 - 0:07:29]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What are colors of the curved flight path right now?",
        "time_stamp": "00:07:15",
        "answer": "D",
        "options": [
          "A. Yellow and blue.",
          "B. Blue and white.",
          "C. Green and ograne.",
          "D. Red and blue."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_416_real.mp4"
  },
  {
    "time": "[0:09:32 - 0:09:52]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is located to the bottom-left of the cockpit's central control panel right now?",
        "time_stamp": "00:09:33",
        "answer": "A",
        "options": [
          "A. A blue handle.",
          "B. A red button.",
          "C. An altitude meter.",
          "D. A compass."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_416_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:00:11",
        "answer": "D",
        "options": [
          "A. The area of different shapes.",
          "B. How to measure the diameter.",
          "C. The concept of length in 2-dimensional shapes.",
          "D. How to calculate perimeter."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_221_real.mp4"
  },
  {
    "time": "[0:01:32 - 0:02:02]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What will the speaker most likely explain next?",
        "time_stamp": "00:02:13",
        "answer": "A",
        "options": [
          "A. How to calculate the perimeter of the shape.",
          "B. How to convert meters into centimeters.",
          "C. The differences between perimeter and area.",
          "D. How to measure the perimeter accurately."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_221_real.mp4"
  },
  {
    "time": "[0:03:04 - 0:03:34]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:03:33",
        "answer": "B",
        "options": [
          "A. How to calculate the area of the rectangle.",
          "B. How to find the perimeter of a rectangle.",
          "C. How to label the units.",
          "D. How to measure with a ruler."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_221_real.mp4"
  },
  {
    "time": "[0:04:36 - 0:05:06]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:05:06",
        "answer": "A",
        "options": [
          "A. How to calculate the perimeter of any regular polygon.",
          "B. The significance of repeated addition.",
          "C. The properties of regular polygons.",
          "D. The importance of perimeter in geometry."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_221_real.mp4"
  },
  {
    "time": "[0:06:08 - 0:06:38]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker ask the students to do next?",
        "time_stamp": "00:06:32",
        "answer": "B",
        "options": [
          "A. Calculate the area of the shape.",
          "B. Determine the perimeter of the shape.",
          "C. Measure each side length.",
          "D. Convert the measurements to centimeters."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_221_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the primary color of the control panel shown right now?",
        "time_stamp": "00:00:09",
        "answer": "A",
        "options": [
          "A. Black.",
          "B. White.",
          "C. Blue.",
          "D. Grey."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_417_real.mp4"
  },
  {
    "time": "[0:02:36 - 0:02:56]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the primary color of the mountains visible right now?",
        "time_stamp": "00:02:53",
        "answer": "A",
        "options": [
          "A. White.",
          "B. Green.",
          "C. Brown.",
          "D. Red."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_417_real.mp4"
  },
  {
    "time": "[0:05:12 - 0:05:32]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the main object visible outside the aircraft right now?",
        "time_stamp": "00:05:27",
        "answer": "A",
        "options": [
          "A. A mountain.",
          "B. An ocean.",
          "C. A forest.",
          "D. A desert."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_417_real.mp4"
  },
  {
    "time": "[0:07:48 - 0:08:08]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the predominant weather condition visible right now?",
        "time_stamp": "00:07:28",
        "answer": "A",
        "options": [
          "A. Sunny.",
          "B. Cloudy.",
          "C. Foggy.",
          "D. Rainy."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_417_real.mp4"
  },
  {
    "time": "[0:10:24 - 0:10:44]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the body of water visible right now?",
        "time_stamp": "00:10:44",
        "answer": "A",
        "options": [
          "A. Green.",
          "B. Blue.",
          "C. Brown.",
          "D. Red."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_417_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:09]: The video begins with a close-up view of a sketched portrait of a female face on a canvas. The drawing is in thin, light pencil lines, showing detailed facial features such as eyes, nose, lips, and hair. A paintbrush appears in the frame, held by a hand, starting to add details to the drawing. The text \"SQUARESPACE\" with the Squarespace logo appears prominently over the canvas. [0:00:10 - 0:00:12]: The paintbrush continues to add details to the sketched face, filling in the eyes with black paint to create the outline and pupils. The brush focuses on one eye and gradually moves to the other. [0:00:13 - 0:00:17]: The brush begins to add color to the eyes, starting with a blue hue on the left eye. The brush strokes are precise, filling the iris with a vibrant blue color while making sure not to paint over the details of the pupil. [0:00:18 - 0:00:20]: The paintbrush continues adding blue color to the right eye, completing the eye coloring process. Both eyes now have a blue hue, and the facial features of the sketched portrait are more pronounced with the added paint. The detailed work of the brush is evident as the eyes appear more lifelike.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What colo was added to the eyes of the sketched portrait?",
        "time_stamp": "00:00:20",
        "answer": "C",
        "options": [
          "A. Green.",
          "B. Brown.",
          "C. Blue.",
          "D. Hazel."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_127_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:40 - 0:04:00] [0:03:40 - 0:03:43]: The video starts with a close-up view of a portrait painting in progress, focusing on the subject's face. The face is incomplete, with only the eyes, nose, and mouth fully detailed. The eyes have blue irises and heavy, dramatic makeup with green and dark shading around them. The lips are painted red. The surrounding skin areas are sketched but not filled in yet. [0:03:43 - 0:03:49]: A paintbrush enters the frame from the right and begins adding details to the area around the nose. The brush moves with precision, adding subtle changes to the shading and definition of the nose area. The background remains consistent, with the rest of the face's outline visible but uncolored. [0:03:50 - 0:03:53]: The video continues with the paintbrush carefully working on the portrait. Text saying \"2. RED DOT MANIA\" briefly appears across the screen, seemingly indicating a segment of a series or a step in the painting process. [0:03:54 - 0:03:56]: The paintbrush now focuses on adding detail to the skin around the nose and eye area. The surrounding outlines of the hair and other facial features remain unpainted and white. [0:03:57 - 0:03:59]: The final part of the video shows the artist continuing to refine the details around the eye and nose with the paintbrush. The brush adds layers and texture, enhancing the portrait's realism. The background and unfinished parts of the painting remain the same, with the focus firmly on the nose and eye area.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action is the paintbrush performing right now?",
        "time_stamp": "00:03:49",
        "answer": "C",
        "options": [
          "A. Adding details to the hair.",
          "B. Coloring the lips.",
          "C. Adding details to the nose.",
          "D. Shading the background."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "Which parts of the face remain unpainted right now?",
        "time_stamp": "00:04:00",
        "answer": "B",
        "options": [
          "A. Eyes and lips.",
          "B. Hair and surrounding skin areas.",
          "C. Nose and lips.",
          "D. Eyes and nose."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_127_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:20 - 0:07:40] [0:07:20 - 0:07:40]: The video starts with a close-up view of a detailed painting of a woman's face. The artwork features vivid red hair, intense blue eyeshadow, and red lips. Throughout the video, a paintbrush is visible, moving across different areas of the painting, indicating ongoing adjustments or additions. At different moments, the paintbrush is seen near the eyes, the forehead, and the nose areas, providing subtle refinements to the portrait. The background remains neutral, ensuring the focus stays on the painting and the artist's actions. The woman's face in the painting exhibits a serious and slightly intense expression, with strong brushstroke details highlighting the contours and shadows of the face.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the primary focus of the video?",
        "time_stamp": "0:07:40",
        "answer": "C",
        "options": [
          "A. The artist mixing paints.",
          "B. The artist adjusting the background.",
          "C. The detailed painting of a woman's face.",
          "D. The artist framing the artwork."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_127_real.mp4"
  },
  {
    "time": "[0:11:00 - 0:12:00]",
    "captions": "[0:11:00 - 0:11:20] [0:11:00 - 0:11:05]: The video presents a close-up view of a canvas on an easel displaying a painting of a woman's face with long red hair and blue eyes. The background of the canvas is yellow. A green plant can be seen in the background on the left side of the frame. The painting shows vivid brush strokes that detail the woman's facial features, especially her striking blue eyes and red lips. [0:11:05 - 0:11:09]: The painter uses a fine brush to add details to the painting, focusing on different areas. The brush is actively moved across the woman's face in several strokes, enhancing the eyes, hair, and facial contours. [0:11:09 - 0:11:12]: The painter continues refining the areas around the eyes and cheeks, using swift and deliberate strokes. The brush points and dabs gently around the areas, indicating detailing and blending of colors. [0:11:12 - 0:11:15]: More attention is paid to the lower face, including the lips and chin. The painter adds subtle touches to enhance the dimensionality of the painting. A steady hand guides the brush to ensure precise and controlled application of paint. [0:11:15 - 0:11:17]: The painter revisits the upper part of the painting, working on the forehead and hair. The brush strokes are carefully applied to blend colors and add texture to the red hair, highlighting strands and adding depth. [0:11:17 - 0:11:20]: The final touches focus slightly on the lower face and shoulder area. The painter's hand holds the brush with care, adding the last strokes that bring out the clarity and final details of the portrait. The changes are subtle but provide a polished finish.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What does the painter focus on during the last strokes?",
        "time_stamp": "0:11:20",
        "answer": "B",
        "options": [
          "A. The eyes and hair.",
          "B. The lower face.",
          "C. The background and plant.",
          "D. The canvas texture."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_127_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:01]: In a kitchen, a person wearing a grey shirt stands in front of a counter with cooking elements. The person concentrates on cooking, with a frying pan on the stove containing ingredients.  [0:02:01 - 0:02:02]: The person adjusts items on the counter, appears to be preparing something beside a frying pan. [0:02:02 - 0:02:03]: He handles a cut of meat, preparing to place it in the frying pan. [0:02:03 - 0:02:04]: He lowers the cut of meat into the frying pan, initiating the cooking process. [0:02:04 - 0:02:05]: He peers into the frying pan, monitoring the cooking meat. [0:02:05 - 0:02:06]: His hand hovers over the pan, possibly adjusting the seasoning or the heat. [0:02:06 - 0:02:07]: He makes broader gestures, likely indicating the next steps or explaining a process. [0:02:07 - 0:02:08]: His hands lower closer to the pan, focusing again on cooking. [0:02:08 - 0:02:10]: A close-up shot of the meat sizzling in the frying pan, starting to cook. [0:02:10 - 0:02:11]: The person turns back towards the stove, potentially preparing another ingredient. [0:02:11 - 0:02:12]: He organizes items on the counter, setting the scene for further cooking steps. [0:02:12 - 0:02:13]: He moves toward the stove with a chopping board containing additional ingredients. [0:02:13 - 0:02:14]: The view shifts to an overhead shot, displaying the addition of more meat portions into the frying pan. [0:02:14 - 0:02:15]: He scrapes the chopping board, ensuring no remnants are left behind as he adds the meat to the pan. [0:02:15 - 0:02:16]: He turns to place the chopping board down, organizing the workspace. [0:02:16 - 0:02:17]: The person gestures towards the frying pan with emphasis, potentially explaining the cooking process. [0:02:17 - 0:02:18]: He appears to highlight important points or tips related to cooking. [0:02:18 - 0:02:19]: The person reaches for an ingredient on the counter, likely to enhance the dish. [0:02:19]: A close-up displays the person adding seasoning to the meat in the frying pan, focusing on enhancing the flavor.\n[0:02:20 - 0:02:40] \n[0:02:40 - 0:03:00] ",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action is the person taking with the meat?",
        "time_stamp": "0:02:04",
        "answer": "B",
        "options": [
          "A. He seasons the meat.",
          "B. He lowers the meat into the frying pan.",
          "C. He cuts the meat into pieces.",
          "D. He removes the meat from the frying pan."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_17_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:03]: The video begins with a close-up view of a pair of hands working on a wooden cutting board. Several halved fruits, including red and yellow plums, are positioned on the board. A person is seen squeezing a yellow fruit, and juice is dripping onto the cutting board. The board sits atop a kitchen counter with a gas stove and other kitchen tools nearby. [0:04:04 - 0:04:09]: The scene transitions to a wider view, revealing a kitchen with white brick walls and shelves filled with kitchenware like plates, bowls, and jars. A person in a grey shirt is seen on the right side of the screen, arranging the halved fruits on the cutting board. They move around the kitchen island, adjusting pans and utensils in preparation for cooking. [0:04:10 - 0:04:12]: The person picks up a dark brown small bowl containing some seasoning and moves toward a stove, where two pieces of meat are searing in a frying pan. The camera changes to a top-down view showing two pieces of meat sizzling in a pan next to another pan with something brown being stirred. [0:04:13 - 0:04:19]: The camera angle changes back to a wider kitchen view, showing the person sprinkling the seasoning onto the meat in the pan. They continue to cook and stir the contents, causing some steam to rise from the pan. The video captures the robust kitchen environment, with various kitchen tools and ingredients visible on the countertop, evoking a bustling cooking session.\n[0:04:20 - 0:04:40] \n[0:04:40 - 0:05:00] ",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type of fruit is being squeezed right now?",
        "time_stamp": "00:04:03",
        "answer": "C",
        "options": [
          "A. Blueberry.",
          "B. Orange.",
          "C. Yellow Plum.",
          "D. Apple."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_17_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:01]: A man, seen in profile, is wearing a grey shirt and is talking, his right hand raised with the index finger extended. Behind him, a blue wall with shelves containing jars and other kitchen items is visible. [0:06:01 - 0:06:02]: The man continues speaking, his facial expression showing engagement. The background remains the same with kitchen shelves and a blue wall. [0:06:02 - 0:06:03]: The man is now using a pair of red kitchen tongs to cook food in a pan on the stove. There are two visible pans on the stove, and the counter has various utensils and a plate. [0:06:03 - 0:06:04]: He continues cooking, now gesturing with his left hand while holding the red tongs in the right hand. Kitchen shelves with jars are still in the background. [0:06:04 - 0:06:05]: The man stands more upright, speaking and gesturing with his left hand. The counter and kitchen utensils remain the same. [0:06:05 - 0:06:06]: His left hand gestures while he speaks; the right hand rests on his hip. Behind him, the shelves with jars and kitchen items are still visible. [0:06:06 - 0:06:07]: He turns slightly to his right, pointing at something off-camera. The kitchen countertop and shelves in the background remain unchanged. [0:06:07 - 0:06:08]: A close-up shot shows his hand moving food in the pan with a metal spatula. On the stove, two pans are visible, one with pieces of food being cooked, and the other with whole vegetables. [0:06:08 - 0:06:09]: The focus remains on the cooking process; he continues to stir food in the second pan. Various kitchen utensils and spices are displayed on the counter. [0:06:09 - 0:06:10]: From an overhead view, the contents of both pans are clearly visible. The left pan contains two pieces of food, while the right pan has multiple small round vegetables. [0:06:10 - 0:06:11]: The man bends slightly forward, continuing his tasks with an expression of engagement while holding a kitchen cloth. The background shelves filled with dishes are visible. [0:06:11 - 0:06:12]: Standing upright again, the man looks to the right, holding the kitchen cloth at his side. Various kitchen items on shelves remain visible in the background. [0:06:12 - 0:06:13]: The man moves towards the stove, appearing to prepare something. Kitchen utensils and a plate are on the counter in the foreground. [0:06:13 - 0:06:14]: The man faces the stove, focused on cooking. The background shows a blue wall with kitchen shelves and a window with plants outside. [0:06:14 - 0:06:15]: The man uses a spoon to stir food in the pan while talking. The countertop near the stove has various kitchen utensils. [0:06:15 - 0:06:16]: A close-up of the man’s hands stirring food in a pan with a spoon, two yellow and red vegetables visible in the pan. Various kitchen items are on the counter. [0:06:16 - 0:06:17]: He continues to stir the food in the pan, adjusting the items with the spoon. The kitchen counter and utensils remain in the background. [0:06:17 - 0:06:18]: The overhead view shows the contents of both pans again, with the man’s hand adjusting the vegetables in the right pan. Items on the counter are still visible. [0:06:18 - 0:06:19]: The overhead shot shows him stirring the food in the pan on the right, ensuring even cooking. The kitchen layout remains consistent, with items neatly arranged. [0:06:19 - 0:06:20]: He continues to adjust the food in the pan, ensuring they are well-cooked. The general setup of the kitchen, with utensils and kitchen items organized, remains in view.\n[0:06:20 - 0:06:40] \n[0:06:40 - 0:07:00] ",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the man using to cook food in the pan?",
        "time_stamp": "0:06:03",
        "answer": "C",
        "options": [
          "A. A wooden spoon.",
          "B. A metal spatula.",
          "C. Red kitchen tongs.",
          "D. A fork."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_17_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: A kitchen counter is equipped with various cooking items. A white plate with cooked food rests on the top left. A bowl of vegetables sits nearby, alongside a few small dishes, each containing various spices. An empty frying pan is positioned on the stove in the center. On the right, sausage slices simmer in a pan. A hand appears on the left, placing a bunch of leafy greens onto a wooden cutting board.    [0:08:01 - 0:08:03]: The scene primarily focuses on the kitchen counter from an aerial perspective. Apart from the earlier-actionated items, a person’s arm is seen moving a fresh piece of leafy greens onto the cutting board. [0:08:03 - 0:08:04]: The countertop scene remains consistent, though more attention is directed towards the leafy greens on the wooden cutting board. The small bowl of butter and spices remain intact. [0:08:04 - 0:08:05]: The leafy greens stay centralized on the cutting board while the surrounding bowls of spices remain undisturbed. [0:08:05 - 0:08:06]: The camera shifts focus to a man standing before a row of kitchen utensils and shelves packed with plates, bottles, and kitchenware. He is wearing a grey T-shirt and has a short haircut. The man is talking and gesturing, holding a utensil in his right hand, facing slightly to the side. [0:08:06 - 0:08:07]: Still focused on the person, the man redirects his attention to the cookware around him. [0:08:07 - 0:08:09]: The downward camera angle shows the leafy green on the cutting board as hands wrap around the greens, preparing for cutting among the assortment of small bowls holding ingredients. [0:08:09 - 0:08:10]: The camera zooms in on the vibrant green leafy vegetable on the cutting board as a hand holds it firmly. [0:08:10 - 0:08:11]: A closer angle shows a leafy green vegetable being sliced. [0:08:11 - 0:08:12]: The hand holds a knife mid-cut through the leafy green, clearly displayed in the center of the board. [0:08:12]: A close-up depicts hands slicing through leafy greens on the cutting board. The knife is freshly chopping the greens into smaller pieces. [0:08:12 - 0:08:13]: The man is standing by the kitchen counter, focused on an activity before him. Kitchen shelves laden with items form the background while he chops vegetables. [0:08:13 - 0:08:14]: The man is continuing to chop vegetables on the board. [0:08:14 - 0:08:15]: A focused expression on the man's face as he looks down at the vegetables he's cutting. [0:08:15 - 0:08:16]: The man continues to slice the vegetables attentively, standing at the counter. [0:08:16 - 0:08:17]: The vegetables on the cutting board are now in smaller pieces, with the man's hand still holding the knife and slicing. [0:08:17 - 0:08:18]: The man remains focused on preparing the vegetables on the counter, with his hand chopping more finely. [0:08:18 - 0:08:19]: He picks up a stainless steel utensil to stir the contents of the pot on the stove in front of him. The cutting board is now cluttered with finely chopped greens.\n[0:08:20 - 0:08:40] \n[0:08:40 - 0:09:00] ",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is simmering in a pan on the right side of the kitchen counter right now?",
        "time_stamp": "00:08:01",
        "answer": "C",
        "options": [
          "A. Cooked food.",
          "B. Vegetables.",
          "C. Sausage slices.",
          "D. Leafy greens."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_17_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:00 - 0:09:20] \n[0:09:20 - 0:09:40] \n[0:09:40 - 0:10:00] ",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is this person doing now right now?",
        "time_stamp": "0:09:12",
        "answer": "B",
        "options": [
          "A. Dice the green onions finely.",
          "B. Chop the cilantro finely.",
          "C. Climbing ropes and carabiners.",
          "D. First aid kits and maps."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_17_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:05]: The video begins with a first-person perspective of a city street. The foreground shows a row of parked cars on the left side of the road and a bright red metal fence to the right. The pavement is made of light, neatly arranged cobblestones. The building on the right features a large mural with geometric patterns in different shades of blue, red, and brown, extending from the top to the sidewalk. In the background, multiple multi-story buildings line both sides of the street, including some with ornate architectural details. A tall street lamp is positioned near the center of the frame, providing additional vertical interest. The sky is clear and blue, enhancing the overall brightness. [0:00:06 - 0:00:10]: As the video progresses, the view continues down the street, with the perspective moving slightly forward. Signposts indicating parking information appear more prominently on the left side, and various parked cars are visible, including a white car in the forefront. The street remains relatively empty of pedestrians. The geometric mural on the right building becomes slightly more detailed as the camera moves closer, and a shadow of the streetlamp falls onto the sidewalk. [0:00:11 - 0:00:15]: Moving further down the street, a blue parking meter becomes visible on the left side of the frame, adjacent to the parked cars. The architectural details of the buildings on both sides of the street become more defined, with visible balconies, windows, and facades. The mural on the right wall continues to dominate the visual scene on that side, and the bright red fence runs parallel to the building's facade, complementing the sophisticated patterns of the mural. [0:00:16 - 0:00:20]: In the final stretch of the video, the perspective continues forward, revealing more of the street ahead. The shadow of the streetlamp elongates across the cobblestone pavement. At the end of the red metal fence, the sidewalk extends, leading to more cars parked on both sides of the street. A few more architectural details of the distant buildings become discernible, including additional street signs and a white van parked near the curb further down the road. The street remains calm and orderly, reflecting a tranquil urban environment under a clear blue sky.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What object is visible on the left side of the frame next to the parked cars?",
        "time_stamp": "0:00:15",
        "answer": "B",
        "options": [
          "A. A red mailbox.",
          "B. A blue parking meter.",
          "C. A green trash can.",
          "D. A yellow bike rack."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_330_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:43]: The video begins with a view of a plaza featuring two buildings with distinct architectural styles. One building has red doors and red-framed windows with small balconies, while the other is decorated with blue tiles and intricate stone carvings. The sky is clear and blue, and pedestrians are walking in the plaza. [0:02:44 - 0:02:47]: A woman in a yellow top and blue skirt continues walking past the camera on the left, moving towards the center of the plaza. The ornate building's stone carvings become more prominent as the camera shifts slightly. [0:02:48 - 0:02:50]: The woman exits the frame, and the focus shifts to the right side where an outdoor café setup with wooden tables and chairs appears. Further ahead, more pedestrians can be seen walking down the street. [0:02:51 - 0:02:55]: The ornate stone building with large arched windows and intricate carvings becomes the main focus. The café area remains visible on the right, and the street continues to extend forward, showing more buildings with colorful facades. [0:02:56 - 0:02:59]: The camera tilts upward, revealing more of the detailed carvings on the stone building, including statues and decorative elements. The sky remains clear, and the buildings on the left feature balconies with black iron railings and red-framed windows. The video ends focusing on this architectural detail.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What distinct feature is highlighted on the ornate stone building right now?",
        "time_stamp": "00:03:00",
        "answer": "B",
        "options": [
          "A. Blue doors.",
          "B. Stone carvings.",
          "C. Red-framed windows.",
          "D. Iron railings."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_330_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:32]: The video depicts a scene on a bustling pedestrian street on a sunny day. Both sides of the street are lined with multi-story buildings featuring various architectural details. The facades exhibit a mix of neutral tones and occasional colorful accents. Some buildings have balconies with ornate railings and shuttered windows at upper levels. On the ground level, there are shops with large display windows and awnings, some of which are red. Several pedestrians walk along the evenly-paved street, some wearing casual clothing such as t-shirts and jeans, while others wear more formal or summertime outfits. In the foreground, a group of four people, two men and two women, walk in the same direction, their backs to the camera. Of the group, one man in a bright pink shirt and blue jeans is slightly ahead, and another, wearing a blue backpack, is to his right. The women, one with a black skirt and another with black pants, walk to the left. Further down the street, more people can be seen, some walking individually and others in small groups. A few people are standing and conversing by shop entrances. The street is well-lit with sunlight, casting crisp shadows, and the sky is clear and blue above. [0:05:33 - 0:05:39]: As the video progresses, more pedestrians enter the frame, including a woman in black pants walking closer to the position of the camera. A woman carrying a bag emerges from one of the shops on the right, while another woman in shorts and sunglasses appears from the left. Three people, conversing animatedly, walk together on the right side of the street. The general flow of people heading in both directions continues, maintaining the lively atmosphere of the area. Overhead, a bird soars through the clear sky, adding a dynamic element to the scene. Tables and chairs outside a café on the right come into view, suggesting places to sit and relax. The overall impression is one of a vibrant, active street in a city or town with a mix of local and tourists enjoying the day.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What color is the shirt of the man who is slightly ahead in the group of four people?",
        "time_stamp": "00:05:32",
        "answer": "C",
        "options": [
          "A. Green.",
          "B. Blue.",
          "C. White.",
          "D. Red."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_330_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:09]: In a lively urban setting with bustling activity, the scene unfolds on a cobblestone pedestrian street. The right side is dominated by a historic, grand building with intricate architectural details such as arched windows and a clock tower. Adjacent to this building, colorful graffiti and street art adorn construction barriers, adding a contrasting modern touch. On the left side, a row of old buildings with balconies line the street. Several outdoor cafés with red and brown umbrellas provide seating for customers, many of whom are engaged in conversations. People walk along the street, some wearing masks, others enjoying the sunny weather. Among the pedestrians, a group of tourists is seen walking and taking in the surroundings. One man in a black shirt and another in pink are noticeable, along with a couple strolling hand in hand. A street lamp is visible in the background near the central architectural landmark. [0:08:09 - 0:08:12]: The camera continues to move forward, showing more details of the lively atmosphere. More café tables are occupied, with people facing the street or each other, engaging in casual conversation. The background reveals another historical building further up the street, and the top of a tower is visible in the distance. Pedestrians continue to walk in both directions, passing by the cafés and the construction barriers. [0:08:12 - 0:08:16]: As the motion persists, the camera captures additional details of the street and its occupants. There is a slight increase in the number of people walking and sitting at the cafés. The scene's rich architectural details become more apparent, showcasing the city's historic charm. The right side reveals more of the construction barrier, filled with graffiti and street art, standing in contrast to the old buildings. [0:08:16 - 0:08:19]: The scene involves a brief glance at a glass structure, possibly a modern bus stop or an information kiosk. People pass by it, and some are seen waiting inside or nearby. The street continues to be lively and crowded with locals and tourists alike, all enjoying the pleasant weather and the vibrant urban atmosphere. The surrounding historic architecture and the various activities on the street paint a vivid picture of daily life in this city. [0:08:19 - 0:08:20]: The camera momentarily focuses on a specific individual wearing a light-colored dress, walking past the modern glass structure. This quick moment highlights the mix of traditional and contemporary elements coexisting in the urban landscape. The historic buildings with intricate designs create a picturesque backdrop against the modern street life bustling with pedestrians, cafés, and street art.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What type of umbrellas are seen at the outdoor cafés right now?",
        "time_stamp": "0:08:05",
        "answer": "B",
        "options": [
          "A. Blue and white.",
          "B. Red and brown.",
          "C. Green and yellow.",
          "D. Black and white."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Event Understanding",
        "question": "What does the the glass structure likely depict right now?",
        "time_stamp": "0:08:19",
        "answer": "B",
        "options": [
          "A. A historic monument.",
          "B. A modern bus stop.",
          "C. A construction site.",
          "D. A marketplace."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_330_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions just now?",
        "time_stamp": "00:00:10",
        "answer": "D",
        "options": [
          "A. The individual cleans the espresso machine and places the portafilter in position.",
          "B. The individual grinds coffee beans, tamps the ground coffee, and brews an espresso shot.",
          "C. The individual serves a coffee beverage, cleans up the work area, and restocks coffee supplies.",
          "D. The individual steams the milk using the steam wand, and prepares it for a coffee beverage."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_369_real.mp4"
  },
  {
    "time": "[0:01:50 - 0:02:00]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions just now?",
        "time_stamp": "00:02:00",
        "answer": "D",
        "options": [
          "A. The individual prepares a hot beverage by steaming milk and selecting a paper cup.",
          "B. The individual clears the espresso machine area by wiping it clean, rearranging cups, and replacing a filter.",
          "C. The individual operates the deep fryer to prepare a food item, setting the timer and monitoring the cooking process.",
          "D. The individual brews an espresso shot by grinding beans, tamping them, and inserting the portafilter into the machine."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_369_real.mp4"
  },
  {
    "time": "[0:03:40 - 0:03:50]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions just now?",
        "time_stamp": "00:03:50",
        "answer": "D",
        "options": [
          "A. The individual brews an espresso shot, mixes it with hot water to create an Americano, and places it on the counter.",
          "B. The individual dismantles the espresso machine, cleans each part thoroughly, and reassembles it.",
          "C. The individual heats milk on the stovetop, pours it into a blender, and prepares a smoothie.",
          "D. The individual operates the espresso machine, froths milk using the steam wand."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_369_real.mp4"
  },
  {
    "time": "[0:05:30 - 0:05:40]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions just now?",
        "time_stamp": "00:05:40",
        "answer": "D",
        "options": [
          "A. The individual inserts the portafilter into the espresso machine and begins making an espresso shot.",
          "B. The individual cleans the coffee grinder and refills it with new coffee beans.",
          "C. The individual inspects the coffee machine, runs a cleaning cycle, and restocks the supply tray.",
          "D. The individual measures ground coffee, fills the portafilter, and use spoon to adjust it."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_369_real.mp4"
  },
  {
    "time": "[0:07:20 - 0:07:30]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions just now?",
        "time_stamp": "00:07:30",
        "answer": "D",
        "options": [
          "A. The individual rinses out a used cup and places it in the dishwasher.",
          "B. The individual refills the coffee machine with water and prepares it for brewing.",
          "C. The individual measures out coffee grounds and places them into the portafilter.",
          "D. The individual fills a glass with ice cubes from a container."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_369_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: In a brightly lit storage or stockroom, a gloved hand is seen reaching towards a stack of yogurt containers on a metal rack. The containers are in white trays with blue and white packaging. In the background, there is a glimpse of a steel door and other stacked items. [0:00:01 - 0:00:03]: The hand starts to grab a tray of yogurt from the stack. The background shows more shelves and products. [0:00:03 - 0:00:06]: The tray is now being positioned in front of a refrigerator shelf stocked with various milk products. The gloved hand adjusts the placement of the tray. [0:00:06 - 0:00:09]: The person is placing the tray on the shelf and positioning the yogurt containers. The shelf has many similar containers already stacked. [0:00:09 - 0:00:10]: The person’s hands are seen moving the containers around, ensuring they are properly aligned and organized on the shelf. [0:00:10 - 0:00:12]: The empty tray is now visible as the containers have been shelved. The hands continue to make final adjustments. [0:00:12]: A blur indicates a quick movement, possibly the person turning or moving away from the shelf. The background shows more shelves with various products. [0:00:13]: The person is carrying another tray of blue-lidded yogurt containers. The background shows another person and more stock items. [0:00:13 - 0:00:14]: The person approaches the shelf again with the new tray of yogurt. Nearby milk cartons are visible. [0:00:14 - 0:00:17]: The hand places the new tray onto the shelf and starts distributing the containers. The shelf gets more crowded with the additional products. [0:00:17 - 0:00:19]: The containers are being adjusted to fit properly. Some containers are taken from the tray and placed on the shelf. [0:00:19 - 0:00:20]: The final adjustments are made, and the shelf looks organized with yogurt containers neatly placed. The person then retracts their hands, completing the stocking process.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the gloved hand reaching towards at the beginning of the video?",
        "time_stamp": "0:00:01",
        "answer": "A",
        "options": [
          "A. Yogurt containers.",
          "B. Milk cartons.",
          "C. Juice bottles.",
          "D. Cheese blocks."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the packaging of the yogurt containers?",
        "time_stamp": "0:00:10",
        "answer": "B",
        "options": [
          "A. Red and white.",
          "B. Blue and white.",
          "C. Green and white.",
          "D. Yellow and white."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_443_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:25]: The scene opens in a store, focusing on a refrigerated section stocked with multiples of the same product, a blue and white container labeled \"crème fraîche.\" A hand wearing a black glove picks up one of the containers from the upper shelf and places it into a cardboard tray with circular cutouts designed to hold them in place. The hand continues to pick up and arrange the containers systematically. [0:02:26 - 0:02:27]: The person holding the cardboard tray moves away from the refrigerated section, walking through the aisle, which is stocked with various other products on the metal shelves.  [0:02:28 - 0:02:32]: The scene transitions to a back room or storage area where the hand stacks several empty cardboard trays onto a larger stack. Nearby, there are boxes labeled \"BRAVO,\" and the surroundings suggest a stocking or inventory area with carts and additional supplies. [0:02:33 - 0:02:38]: The individual picks up a case of new crème fraîche containers from a shelf and places it on a cart with other boxes. The cart is densely packed and sits in an area filled with shelves carrying various inventory items. The hand moves carefully to ensure the items are securely placed on the cart. [0:02:39]: The video briefly shows a final shot of the crème fraîche containers, indicating the task's repetitive nature and the prominence of this particular product in the scene.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What does the individual do after picking up the yogurt container?",
        "time_stamp": "0:02:25",
        "answer": "D",
        "options": [
          "A. Opens it.",
          "B. Discards it.",
          "C. Hands it to someone else.",
          "D. pick it up on the shelf."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_443_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:42]: The video begins with a person wearing black gloves picking up a yellow carton box labeled \"Bravo\" from a shelf in a warehouse-like storage room. The box has orange text reading \"Apelsin Juice.\" [0:04:42 - 0:04:44]: The camera then turns to show a wider view of the room with shelves filled with boxes and products on the left side and directly ahead. The floor is gray, and there are industrial carts loaded with items at the far end of the room. [0:04:44 - 0:04:45]: The perspective shifts back towards the yellow \"Bravo\" boxes, with the person holding two boxes and placing them on a cart filled with additional \"Bravo\" boxes. [0:04:45 - 0:04:47]: The individual moves another box labeled \"Bravo\" from the shelf and places it on the cart, which is stacked with more boxes. [0:04:47 - 0:04:48]: The view pans to show another part of the storage area, revealing more shelves filled with various items. The perspective focuses on a box being held open. [0:04:48 - 0:04:50]: The camera shows more details of the storage area, including stacked green boxes and additional items on metal carts. The person then picks up a large cardboard box. [0:04:50 - 0:04:51]: The individual closely examines and holds the large carton box. [0:04:51 - 0:04:53]: They continue handling the cardboard box, tearing it open to reveal more boxes with \"Bravo\" written on them. [0:04:53 - 0:04:55]: The person then lifts one of the opened boxes, showing several \"Bravo\" juice cartons inside. The view focuses on the yellow cartons stacked neatly in the box. [0:04:55 - 0:04:57]: The perspective shifts to show the person organizing the contents, ensuring the \"Bravo\" juice cartons are correctly arranged within the box. [0:04:57 - 0:04:59]: The view then shows the individual picking up a single \"Bravo\" carton from the box. The hand and gloves are clearly visible as they lift the carton. [0:04:59 - 0:05:00]: To end, the person is seen moving towards the original shelf with boxes still being held, and more product boxes can be seen on the shelves in the background.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What does the individual do after picking up the large cardboard box?",
        "time_stamp": "0:05:00",
        "answer": "D",
        "options": [
          "A. Places it back on the shelf.",
          "B. Moves it to another room.",
          "C. Examines it and sets it aside.",
          "D. Take out the orange juice inside and put it on the shelf."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_443_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:01]: The video begins with a view of several shelves in a store. The shelves display various boxed items, likely juices, in different colors - predominantly green and red packaging. The boxes are neatly arranged in rows. [0:07:02 - 0:07:03]: The camera moves closer, and a gloved hand appears, reaching for one of the red boxes. The hand grabs a box, partially obscuring the view. [0:07:04 - 0:07:05]: The hand places the box back on the shelf, but slightly further to the left. The camera angle shifts slightly to the left as well. [0:07:06]: The gloved hand reappears, pointing towards another red box on the right. The surrounding shelves remain stocked with green and red boxes. [0:07:07 - 0:07:11]: The camera briefly focuses back from the shelves, showing more of the gloved hand and black sleeve. The hand reaches for another box, placing it back on the shelf. The camera starts to move away from the shelf. [0:07:12 - 0:07:15]: The scene changes to another part of the store with a focus on a large cardboard box. The person appears to be breaking down the box or inspecting it. The background includes shelves and some carts with various products. [0:07:16 - 0:07:17]: The camera's focus moves downward, showing the person continuing to work on the cardboard boxes, which are now on the floor, possibly flattening them. The gloved hands are visible, holding and manipulating the boxes. [0:07:18]: The view shifts to a close-up of a dark, plain surface. It's not immediately clear what the surface is, but it obscures most of the frame. [0:07:19 - 0:07:20]: The final frames show the camera focusing on a different set of shelves stocked with various beverages, including some green and yellow bottled drinks. The shelves are organized, and there are many product options visible.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the scene focusing on in the store right now?",
        "time_stamp": "0:07:19",
        "answer": "B",
        "options": [
          "A. The checkout counter.",
          "B. A different set of shelves with beverages.",
          "C. The entrance of the store.",
          "D. A display of snacks."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_443_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the barista taken just now?",
        "time_stamp": "00:00:10",
        "answer": "B",
        "options": [
          "A. The barista prepared a cappuccino, stirred it, and served it to a customer.",
          "B. The barista steamed milk, prepared two lattes, and began making a new latte.",
          "C. The barista brewed coffee, added sugar and cream, and handed it to a customer.",
          "D. The barista made a latte, added cocoa powder, and served it to a customer."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_362_real.mp4"
  },
  {
    "time": "[0:01:56 - 0:02:06]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:02:18",
        "answer": "A",
        "options": [
          "A. The worker prepared an espresso by measuring coffee grounds, tamping them, and placing them in the machine.",
          "B. The worker prepared a cup of tea by measuring loose leaves, placing them in a pot, and adding hot water.",
          "C. The worker cleaned the coffee machine and sorted cups for the next order.",
          "D. The worker brewed a fresh pot of coffee by measuring water and coffee grounds, then starting the machine."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_362_real.mp4"
  },
  {
    "time": "[0:03:52 - 0:04:02]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:03:58",
        "answer": "B",
        "options": [
          "A. The individual cleaned the coffee machine and placed a cup on the scale.",
          "B. The individual measured cocoa powder into a pitcher.",
          "C. The individual selected a cup, poured coffee grounds into it, and added hot water.",
          "D. The individual brewed fresh coffee by placing a new filter and coffee grounds into the machine."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_362_real.mp4"
  },
  {
    "time": "[0:05:48 - 0:05:58]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:06:02",
        "answer": "C",
        "options": [
          "A. The individual placed the milk jug under the steamer, wiped the machine, and steamed the milk.",
          "B. The individual brewed a fresh cup of coffee, cleaned the counter, and served the drink.",
          "C. The individual cleaned the work area, steamed milk, and prepared a milk-based beverage.",
          "D. The individual restocked supplies, brewed coffee, and served it to a customer."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_362_real.mp4"
  },
  {
    "time": "[0:07:44 - 0:07:54]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:07:47",
        "answer": "B",
        "options": [
          "A. The individual brewed a fresh cup of tea, added sugar, and served it to a customer.",
          "B. The individual steamed milk, prepared a latte, and added a lid to the cup.",
          "C. The individual prepared an espresso shot, cleaned the coffee machine, and discarded used grounds.",
          "D. The individual restocked the cups, cleaned the counter, and replaced the lids."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_362_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:16",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      },
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:37",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_91_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:16",
        "answer": "B",
        "options": [
          "A. 3.",
          "B. 5.",
          "C. 6.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_91_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:14",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 4.",
          "C. 3.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_91_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:13",
        "answer": "B",
        "options": [
          "A. 2.",
          "B. 9.",
          "C. 8.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_91_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video begins with a close-up of a stylus arm of a turntable, showcasing a dark metal tone and a small screw on its surface. It is oriented horizontally, casting shadows on a dark vinyl record spinning beneath it. The background displays an out-of-focus setting which seems to include additional devices or components. [0:00:04 - 0:00:05]: The scene transitions to a wooden desk with various small objects on it like rings and a red book. The drawer of the desk is partly open, revealing a colorful assortment of fabrics or clothing inside. [0:00:06 - 0:00:10]: The hand is seen opening the drawer further and taking out a light pink fabric item with light blue accents. It is then unfolded, revealing it to be a piece of clothing with straps, possibly a garment or accessory. [0:00:11 - 0:00:12]: The hand places the light pink item back onto the desk, adjusting position to reach for something else from the drawer. The desk's top remains the same with rings, a red book, and other decorative objects. [0:00:13 - 0:00:16]: A new fabric piece, colorful with white and floral patterns, is retrieved from the drawer. The item is inspected by hands as it spreads out to reveal its design, once again likely a piece of clothing. [0:00:17 - 0:00:19]: The focus shifts to a woman with light hair sitting in front of a mirror, seen from a shoulder-height perspective. She adjusts her hair and facial expression subtly changes as she looks into the mirror, then glances sideways. The background depicts a warmly lit room and blurred details of furniture or decorative items.\n[0:00:40 - 0:01:00] [0:00:40 - 0:00:44]: The video shows a well-lit bedroom with posters on the wall, a bed with patterned bedsheets, a wooden desk with drawers, and a chair. A person, dressed in a light blue bra and partially covered in a floral skirt, stands next to the bed and is putting on a black dress with red and white vertical stripes running down the middle. Her hair is tied back in a ponytail, and she is in the action of lifting the dress up over her head. [0:00:45 - 0:00:52]: The person is now standing upright, adjusting the dress across her torso and smoothing it out. She continues to look down, checking the fit of the dress, and straightens it around her waist. The dress is now fully on, and she appears to be ensuring everything is in place. [0:00:53 - 0:00:55]: The focus shifts briefly to a close-up of the bedside table, showing some items placed on it, including an orange object resembling a book or a planner. [0:00:56 - 0:00:59]: The person moves away from the bedside table and walks towards a small wooden stool. She bends down, appearing to pick something up from the floor, then rises slightly while still looking downwards towards the stool, continuing with her task.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What are the patterns on the second fabric piece retrieved from the drawer?",
        "time_stamp": "0:00:16",
        "answer": "C",
        "options": [
          "A. Polka dots.",
          "B. Stripes.",
          "C. Floral patterns.",
          "D. Geometric shapes."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_158_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:20 - 0:01:40] [0:01:20 - 0:01:24]: A person wearing checkered tights is standing on a colorful patterned rug, next to a pair of red shoes. The shoes have a low heel and a strap. The person lifts one foot and begins to position it into one of the shoes. They then proceed to guide their foot into the shoe with their hands, ensuring it fits properly. [0:01:25 - 0:01:30]: Continuing the process, the person secures the strap of the shoe around their foot. The person then reaches for the second shoe, and repeats the process, carefully fitting their other foot into the red shoe and adjusting the strap to secure it. [0:01:31 - 0:01:36]: Both shoes are now on the person's feet. They adjust the straps, making sure the fit is snug and secure. The patterned rug below features horizontal stripes in a variety of colors, adding a vibrant background to the scene. [0:01:37 - 0:01:39]: The camera then cuts to a close-up of the person’s face. The person has light-colored hair tied back and is wearing makeup. They appear focused, perhaps adjusting their appearance or ensuring everything is in place. In the background, an interior space with various objects is visible.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person putting on their feet?",
        "time_stamp": "0:01:30",
        "answer": "B",
        "options": [
          "A. Red boots.",
          "B. Red shoes with a low heel and a strap.",
          "C. Black sneakers.",
          "D. Sandals."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What pattern is on the stocking the person is wearing?",
        "time_stamp": "0:01:24",
        "answer": "C",
        "options": [
          "A. Stripes.",
          "B. Polka dots.",
          "C. Checkered.",
          "D. Floral."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "What is featured in the background of the scene right now?",
        "time_stamp": "0:01:36",
        "answer": "B",
        "options": [
          "A. A plain white wall.",
          "B. A patterned rug with horizontal stripes in various colors.",
          "C. A window with curtains.",
          "D. A bookshelf."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What is the focus of the person when the camera cuts to a close-up of their face?",
        "time_stamp": "0:01:40",
        "answer": "C",
        "options": [
          "A. They are smiling and talking.",
          "B. They are eating.",
          "C. They appear focused, possibly adjusting their appearance.",
          "D. They are looking at the camera and waving."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_158_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:16",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_98_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:01:26",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 1."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_98_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:18",
        "answer": "C",
        "options": [
          "A. 3.",
          "B. 4.",
          "C. 6.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_98_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:41",
        "answer": "B",
        "options": [
          "A. 2.",
          "B. 8.",
          "C. 3.",
          "D. 7."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_98_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:24",
        "answer": "B",
        "options": [
          "A. 2.",
          "B. 8.",
          "C. 3.",
          "D. 7."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_98_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:58",
        "answer": "A",
        "options": [
          "A. 1.",
          "B. 2.",
          "C. 3.",
          "D. 4."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_83_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:07",
        "answer": "A",
        "options": [
          "A. 6.",
          "B. 5.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_83_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:02",
        "answer": "A",
        "options": [
          "A. 7.",
          "B. 2.",
          "C. 3.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_83_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:01",
        "answer": "A",
        "options": [
          "A. 8.",
          "B. 2.",
          "C. 7.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      },
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:33",
        "answer": "A",
        "options": [
          "A. 9.",
          "B. 5.",
          "C. 8.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_83_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What are the green cones on the road being used for right now?",
        "time_stamp": "00:00:03",
        "answer": "A",
        "options": [
          "A. Signaling road construction.",
          "B. Indicating a lane merge.",
          "C. Marking a pedestrian walkway.",
          "D. Reserving parking spaces."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_394_real.mp4"
  },
  {
    "time": "[0:01:59 - 0:02:04]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the predominant color of the buildings' facades right now?",
        "time_stamp": "00:02:02",
        "answer": "A",
        "options": [
          "A. Red and brown.",
          "B. Blue and gray.",
          "C. Green and yellow.",
          "D. Pink and orange."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_394_real.mp4"
  },
  {
    "time": "[0:03:58 - 0:04:03]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the main color of the awnings above the shop windows right now?",
        "time_stamp": "00:04:00",
        "answer": "B",
        "options": [
          "A. Blue.",
          "B. Red.",
          "C. Green.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_394_real.mp4"
  },
  {
    "time": "[0:05:57 - 0:06:02]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the food cart on the right side of the road selling right now?",
        "time_stamp": "00:06:00",
        "answer": "A",
        "options": [
          "A. Hot dogs and burgers.",
          "B. Ice cream.",
          "C. coffe.",
          "D. marshmallow."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_394_real.mp4"
  },
  {
    "time": "[0:07:56 - 0:08:01]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color are the buses parked on the street right now?",
        "time_stamp": "00:08:00",
        "answer": "D",
        "options": [
          "A. Red and white.",
          "B. Yellow and grey.",
          "C. Green and grey.",
          "D. Blue and white."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_394_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video begins from a first-person perspective in an urban environment on a wet, rainy day. The viewer is positioned on a sidewalk composed of both dark and light tiles, running alongside a street. The sidewalk tiles vary in size and shape, with the larger, darker ones adjacent to the road and smaller, lighter ones closer to the walls on the left. A manhole cover embedded in the street and a series of concrete blocks are visible. [0:00:03 - 0:00:06]: The scene progresses forward, revealing more of the street and sidewalk. On the left, greenery and smaller plants line the sidewalk, adding a touch of color in contrast to the drab, rainy surroundings. The street itself has a clear, straight white line indicating traffic direction. [0:00:06 - 0:00:10]: Moving further, buildings with various facades appear on both sides of the street. The sidewalks are lined with bushes, and some low concrete barriers are visible on the left, which are plants of assorted green bushes that punctuate the mostly grey scene. The manhole cover appears more prominent in the middle of the street, emphasizing the wet, shiny road surface. [0:00:10 - 0:00:14]: The urban setting has buildings of different heights and styles, including some with metallic roller shutters. The viewer continuously moves down the street, nearing residential buildings. There's a noticeable incline in pavement closer to the buildings' edges, redirecting the water flow from the rain. [0:00:14 - 0:00:18]: The path showcases more residential buildings with windows, doors, and small front gardens. Some of the buildings have overhangs and balconies, which partially shield parts of the sidewalk from the rain. A series of parked vehicles, covered with white and grey tarpaulins, punctuate the scene. [0:00:18 - 0:00:20]: The video continues to advance down the wet street, capturing a more detailed view of the surrounding environment. Electrical poles and cables intersect overhead, and more details of the residential and commercial buildings become apparent. The continued presence of greenery and covered vehicles adds to the overall feel of a quiet, rainy day in the city.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What movement is depicted from the first-person perspective in the video?",
        "time_stamp": "0:00:18",
        "answer": "C",
        "options": [
          "A. Moving backward on the street.",
          "B. Moving sideways on the sidewalk.",
          "C. Progressing forward down the street.",
          "D. Standing still."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_307_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:20 - 0:04:40] [0:04:20 - 0:04:27]: The video depicts a narrow urban street on a rainy day, with wet asphalt reflecting buildings and surroundings. On the left side of the scene, there is a sidewalk bordered by greenery and various plants. The sidewalk is adjacent to a tall grey building that gradually becomes beige with a visible air conditioner unit. Moving towards the background, a smaller white car is parked straight ahead under a green and yellow building structure, while colorful posters adorn a pole near the sidewalk. On the right side, a reddish-brown brick building is visible with small plants and bushes along the edge.  [0:04:28 - 0:04:33]: As the video progresses, the viewer moves forward along the street, approaching the parked car. A wooden fence appears on the left side, adjacent to the sidewalk. The street slightly curves to the right, leading toward the parked white car under a metallic awning against a yellow wall. The road remains wet, with clear reflections of the surrounding structures. [0:04:34 - 0:04:37]: The viewer draws nearer to the parked white car, which now takes up a larger portion of the frame. The scene shows more fence detail, and a metal barrier with gates to a driveway becomes visible. On the right, large windows of a modern building reflect the damp street.  [0:04:38 - 0:04:39]: The video concludes with the viewer closely approaching the white car and the adjacent car beside it. The reflection on the road continues to be prominent, highlighting the rainy conditions. The building on the right side has large glass windows, and the overall scene depicts a calm, wet, urban environment.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What was on the left side of the street now?",
        "time_stamp": "00:04:32",
        "answer": "C",
        "options": [
          "A. A series of tall trees.",
          "B. A brick building with large windows.",
          "C. A wooden fence.",
          "D. A row of parked cars."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_307_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:40 - 0:09:00] [0:08:40 - 0:08:50]: The scene takes place on a city sidewalk during a rainy day, where the wet ground reflects light. Two people holding umbrellas walk side by side on the red brick sidewalk; the person on the right wears a dark jacket and white shoes, and the person on the left wears a green jacket and dark pants. To their left, a building with signage and a red vending machine is visible, while ahead of them, cars move along the wet street, and a green P parking sign is lit on the building across the street. Various buildings line the street, including multi-story residential and commercial structures. [0:08:51 - 0:08:59]: As the video progresses, they continue walking down the sidewalk with the same orientation. A green garbage truck appears on the street, and the pair approaches a crosswalk at an intersection. The crosswalk's white stripes contrast with the wet black asphalt. They navigate around a pole with street signs, pass some parked bicycles and scooters near the buildings, and proceed to the other side along the crosswalk, crossing the path of a small white truck. The environment remains consistently urban with buildings and parked vehicles on both sides.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the jacket worn by the person on the left?",
        "time_stamp": "00:09:04",
        "answer": "C",
        "options": [
          "A. Blue.",
          "B. Red.",
          "C. Green.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "What is to the left of the two people walking on the sidewalk now?",
        "time_stamp": "00:08:43",
        "answer": "B",
        "options": [
          "A. A green P parking sign.",
          "B. A red vending machine and building signage.",
          "C. Parked bicycles and scooters.",
          "D. A garbage truck."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_307_real.mp4"
  },
  {
    "time": "[0:13:00 - 0:14:00]",
    "captions": "[0:13:00 - 0:13:20] [0:13:00 - 0:13:03]: The video begins on a rainy street, captured from a first-person perspective. A clear umbrella is visible, displaying raindrops and the umbrella's ribs. On the left-hand side, there is a restaurant with a brightly lit signboard and a display showing various food dishes. A few people, some with umbrellas, can be seen walking along the sidewalk. [0:13:04 - 0:13:06]: The perspective shifts slightly to the right, revealing more of the sidewalk as it moves forward. The restaurant signage continues to be visible, while additional pedestrians, some holding black umbrellas, walk towards the walker. Further ahead, a man with a light jacket walks in the same direction as the camera. [0:13:07 - 0:13:09]: The walker continues down the sidewalk, which has a clear red awning stretching over it, providing cover from the rain. The awning stretches to the end of the frame with various shops lining the left side. The sidewalk has several trash bins positioned on the right edge, closer to the street where cars pass by amidst the rain. [0:13:10 - 0:13:13]: As the walker progresses, more shops are seen on the left, selling books and magazines. Stacks of books are neatly organized on tables, with some stands displaying open pages. The path ahead appears clear, with a few pedestrians further in the distance. [0:13:14 - 0:13:19]: The walker moves closer to a shop displaying more books and possibly art. The sidewalk, still covered by the red awning, is wet from the rain. The street on the right shows a steady stream of traffic in the rain. The person holding the umbrella continues to walk straight ahead, moving past additional stores and their displays.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What was visible on the left-hand side of the street just now?",
        "time_stamp": "00:13:06",
        "answer": "B",
        "options": [
          "A. A coffee shop.",
          "B. A brightly lit signboard of a restaurant.",
          "C. A bookstore.",
          "D. A clothing store."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_307_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What device is the person currently using to capture the video?",
        "time_stamp": "00:00:01",
        "answer": "A",
        "options": [
          "A. Head-mounted camera.",
          "B. Smartphone.",
          "C. DSLR camera.",
          "D. Camcorder."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_421_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:02:20]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type of vehicle is visible in the parking lot right now?",
        "time_stamp": "00:02:03",
        "answer": "A",
        "options": [
          "A. Car.",
          "B. Heavy truck.",
          "C. Bus.",
          "D. Helicopter."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_421_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:04:20]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type of aircraft is visible right now?",
        "time_stamp": "00:04:10",
        "answer": "A",
        "options": [
          "A. Helicopter.",
          "B. Airplane.",
          "C. Drone.",
          "D. Glider."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_421_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:06:20]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person grabbing from the food basket right now?",
        "time_stamp": "00:06:00",
        "answer": "A",
        "options": [
          "A. Chicken strips.",
          "B. French fries.",
          "C. Onion rings.",
          "D. Mozzarella sticks."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_421_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:08:20]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What navigational tool is visible on the screen right now?",
        "time_stamp": "00:08:09",
        "answer": "A",
        "options": [
          "A. GPS device.",
          "B. Compass.",
          "C. Altimeter.",
          "D. Airspeed indicator."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_421_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:09]: Two hands are holding a decorative paper wreath. The wreath is composed of alternating dark red and cream-colored flowers with green leaves. They are symmetrically placed in a circular pattern. Each flower has a golden bead at its center. The background features a grey grid pattern, giving a precise and organized appearance. The hands slowly rotate the wreath to showcase its entirety. [0:00:10]: The screen turns black. [0:00:11 - 0:00:13]: The text “DIY Paper Christmas Wreath” appears centered on a black background. [0:00:14]: The screen turns black. [0:00:15 - 0:00:19]: The text “Cut files available in: - SVG / DXF format for die cutting machines - PDF format for print and hand cut” is displayed on a black screen.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are the hands doing right now?",
        "time_stamp": "00:00:08",
        "answer": "B",
        "options": [
          "A. Arranging flowers on a table.",
          "B. Rotating a decorative paper wreath.",
          "C. Painting a wreath.",
          "D. Holding a black screen."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_58_real.mp4"
  },
  {
    "time": "0:02:00 - 0:02:20",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:07]: The video begins with a top-down view of a Cricut cutting machine in action. The machine has a light gray and mint green body with a control knob on the right side. The cutting mechanism, labeled \"Cricut Cut Smart 2,\" is situated at the center of the machine. In the cutting area, a green cutting mat holds a piece of light beige paper in place. The machine's blade is positioned over the paper, as it starts cutting intricate lines that appear to form a flower pattern. The cutting head moves horizontally from left to right, and then vertically, making precise incisions on the paper. The work area beneath the machine shows a gridded pattern in dark gray and white, providing a textured background. [0:02:08 - 0:02:15]: As the cutting machine continues its operation, the flower pattern becomes more visible with each cut. The cutting head moves primarily from the left side to the middle and slightly toward the right side of the machine, maintaining precision and steadiness. The blade makes fine incisions, adding details to the flower petals on the beige paper. The background with the gridded pattern consistently remains visible, providing context to the workspace setup. Towards the end of this segment, some parts of the flower pattern are fully cut out. [0:02:16 - 0:02:19]: The view now shifts to a new segment where a different material, a brown cardboard-like paper, is positioned on the green cutting mat inside the Cricut machine. The cutting machine's head starts at the center again, and the blade is poised to begin a new cut. The material is flat and evenly laid out beneath the blade. The control knob on the right side of the machine and other mechanical parts remain unchanged in this view.  [0:02:20]: The cutting head initiates a new cutting sequence on the brown cardboard. A text overlay at the bottom of the video appears, stating, \"I had to split this card because the '12x12' size was out of stock.\" The cutting machine’s blade continues its precise movement, working on the brown material with the same accuracy as it did with the beige paper earlier.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the machine doing right now?",
        "time_stamp": "0:02:18",
        "answer": "B",
        "options": [
          "A. Pausing the cutting process.",
          "B. Initiating a new cutting sequence.",
          "C. Moving the cutting mat.",
          "D. Changing the blade."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_58_real.mp4"
  },
  {
    "time": "0:04:00 - 0:04:20",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:01]: Hands, clothed in a maroon-colored knitted sweater, are manipulating paper petals to form a flower. On the checkered cutting mat in the background, multiple completed paper flowers are lying around; cream-colored flowers on the left and red-colored ones on the right.  [0:04:01 - 0:04:02]: One hand holds the cream-colored paper petals together while the other hand adds another petal to the assembly. The petals are arranged precisely and fit together to form a flower. [0:04:02 - 0:04:03]: Now, both hands are seen carefully bringing more petals together to complete the flower. The cream-colored flower in focus has six petals. [0:04:03]: The hands take a brief pause as if inspecting the flower, ensuring the petals are aligned correctly. The background remains the same, with a grid mat and completed flowers at the top left and right corners. [0:04:04 - 0:04:06]: The hands resume the process of assembling paper petals to form more flowers. Checking occasionally to ensure everything is coming together nicely. [0:04:06 - 0:04:09]: The hands continue their work, now moving to what seems to be a finished part of the flower assembly and adjusting the petals to ensure symmetry and alignment. [0:04:09 - 0:04:11]: A finished cream-colored flower is placed on the mat beside other completed flowers. The hands begin gathering another set of petals for a new flower assembly. [0:04:11 - 0:04:13]: With careful and precise movements, the hands continue the process of flower assembly. The repetitive motion of joining petals is observed. [0:04:13 - 0:04:16]: Once the cream-colored flower is complete, the hands move aside and pick up a red-colored set of petals next. The attention is shifted towards the assembly of a red flower. [0:04:16 - 0:04:19]: Using a tool, likely a pen, the hands begin outlining or scoring the red paper petals to give them a lifelike curve or shape. More red petals are carefully laid out on the mat, prepared for the same treatment. [0:04:19 - 0:04:20]: The tool is used to make precise lines on the petals. The background remains consistent with the earlier setup of flowers and cutting mat. [0:04:20 - 0:04:21]: Each petal is meticulously scored or outlined to add dimension. The hands demonstrate a repetitive, skilled motion in the flower-making process. [0:04:21 - 0:04:24]: The process of scoring or outlining continues; the red petals' edges become more pronounced. The grid background and scattered flowers remain unchanged, providing a consistent workspace. [0:04:24 - 0:04:25]: The transition from one petal to another is smooth; every petal receives the same detailed attention with the tool. The hands are very steady, ensuring each flower petal is perfectly shaped. [0:04:25 - 0:04:28]: The petals are organized in groups for a more systematic flower formation. The hands appear to follow a methodical approach, ensuring each petal is scored or outlined perfectly before proceeding to assemble the flower. [0:04:28 - 0:04:29]: Despite the repetitiveness, each petal looks unique yet uniform. The hands skillfully manage multiple tasks - handling petals, using the tool, and organizing the workspace. [0:04:29 - 0:04:31]: The work is delicate and seems very involved, with hands constantly engaged in positioning, outlining, or pondering over the next steps in this crafting process.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are the hands doing right now?",
        "time_stamp": "0:04:20",
        "answer": "B",
        "options": [
          "A. Assembling cream-colored paper petals together.",
          "B. Adding dimension by scoring the red petals.",
          "C. Sorting finished cream-colored flowers.",
          "D. Disposing of unwanted petals."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_58_real.mp4"
  },
  {
    "time": "0:06:00 - 0:06:20",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:02]: The scene shows a person working on a craft project involving paper flowers. The background is a checkered grey cutting mat, and on it lies four assembled red paper flowers in two rows at the top part. The person’s hands are seen in the lower part of the frame, arranging a red and green paper flower that has already been partially assembled. The person appears to be placing the red petals onto the green base, adding depth to the flower. They are wearing a purple long-sleeve sweater. [0:06:02 - 0:06:06]: The person continues to press the red petals onto the green base carefully. The focus remains on the flower being assembled, which is taking shape as layers are added. The right hand is slightly above the flower, with fingers positioned to adjust the petals, while the left hand holds the flower in place. [0:06:06 - 0:06:07]: The right hand reaches for one of the fully assembled red paper flowers from the top row. The remaining completed flowers stay in place. [0:06:07 - 0:06:08]: The right hand brings the taken red flower closer to the crafting area. Meanwhile, the left hand continues to stay near the red and green flower. [0:06:08 - 0:06:10]: Both hands move the red paper flower closer to the partially assembled red and green flower. The left hand appears to be positioning the flower, while the right hand adjusts the red petals. [0:06:10 - 0:06:12]: The person’s hands position the red flower on top of the partially assembled one, layering it on the green base. The right hand ensures that the petals align correctly. [0:06:12 - 0:06:14]: Adjustment of the petals continues with both hands ensuring the newly place flower is secure and aligned as desired. [0:06:14 - 0:06:16]: The right hand completes the adjustments, moving slightly away from the finished work to give a final look at the positioning of all petals. [0:06:16 - 0:06:18]: The person’s hands move the newly completed red and green flower slightly to present it properly and ensure all petals are well adjusted. Three fully assembled red flowers remain visible at the top. [0:06:18 - 0:06:20]: After completing adjustments, the person’s hands move the finished flower into a line with the other red flowers at the top of the frame. The background remains the same with its checkered grey pattern. The hands retrieve another green base to start the next assembly.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are the person's hands doing right now?",
        "time_stamp": "00:06:02",
        "answer": "B",
        "options": [
          "A. Cutting paper into flower shapes.",
          "B. Placing a finished flower with others.",
          "C. Assembling a new set of flower petals.",
          "D. Drawing a design on the cutting mat."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_58_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:02]: The video begins with a first-person view of a hand moving towards a partially completed wreath placed on a checkered grid background. The wreath is constructed from a green ring base and decorated with a combination of five red and white artificial poinsettia flowers with green leaves. The red flowers are positioned at the top left and bottom right, and the white flowers are at the top center and bottom left. The hand is positioned on the left side of the wreath, preparing to adjust it. A logo “DIY CRAFT TUTORIALS” is visible in the bottom right corner;  [0:08:02 - 0:08:08]: The person’s hand continues to adjust the flowers on the wreath, adding a red poinsettia at the lower right position, slightly above the already existing red flower at the bottom right. Another hand enters the frame from below, holding another red poinsettia;  [0:08:09 - 0:08:13]: With both hands, the person positions the new red poinsettia at the lower left part of the wreath, ensuring equal spacing. Simultaneously, the person continues to adjust the previously positioned white poinsettia to achieve a symmetrical and aesthetically arranged pattern;  [0:08:14 - 0:08:17]: The hands rearrange the flowers along the bottom of the wreath to perfect their positions, with slight adjustments made to the surrounding green leaves. Small movements ensure the wreath looks balanced and the colors are evenly distributed;  [0:08:18 - 0:08:19]: With the final adjustments made, the person's hands move away slightly as the video shows a fully decorated wreath featuring a symmetrical alternation of red and white poinsettias around the entire green ring base. The hands are then positioned at the bottom corners of the frame.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are the person's hands doing right now?",
        "time_stamp": "00:08:22",
        "answer": "B",
        "options": [
          "A. Spraying water on the flowers.",
          "B. Rearranging flowers and leaves on the wreath.",
          "C. Cutting the flowers with scissors.",
          "D. Painting the flowers on the wreath."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_58_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:16",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      },
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:56",
        "answer": "D",
        "options": [
          "A. 1.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_102_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:01:26",
        "answer": "A",
        "options": [
          "A. 0.",
          "B. 3.",
          "C. 4.",
          "D. 1."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_102_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:26",
        "answer": "D",
        "options": [
          "A. 3.",
          "B. 4.",
          "C. 6.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_102_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:41",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 8.",
          "C. 3.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_102_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:04",
        "answer": "B",
        "options": [
          "A. 2.",
          "B. 0.",
          "C. 3.",
          "D. 7."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_102_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is mounted on the lamp post right now?",
        "time_stamp": "00:00:05",
        "answer": "C",
        "options": [
          "A. A camera.",
          "B. A streetlight.",
          "C. An American flag.",
          "D. A traffic sign."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_380_real.mp4"
  },
  {
    "time": "[0:02:07 - 0:02:12]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the predominant color of the banners on the street right now?",
        "time_stamp": "00:02:08",
        "answer": "A",
        "options": [
          "A. Purple.",
          "B. Red.",
          "C. Blue.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_380_real.mp4"
  },
  {
    "time": "[0:04:14 - 0:04:19]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which street is indicated on the green sign right now?",
        "time_stamp": "00:04:16",
        "answer": "A",
        "options": [
          "A. E 40 St.",
          "B. E 34 St.",
          "C. W 42 St.",
          "D. Broadway."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_380_real.mp4"
  },
  {
    "time": "[0:06:21 - 0:06:26]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is positioned next to the road right now?",
        "time_stamp": "00:06:24",
        "answer": "C",
        "options": [
          "A. Parking meters.",
          "B. Trash bins.",
          "C. Orange traffic barrels.",
          "D. Benches."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_380_real.mp4"
  },
  {
    "time": "[0:08:28 - 0:08:33]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What sign is visible on the left side of the street right now?",
        "time_stamp": "00:08:28",
        "answer": "A",
        "options": [
          "A. One Way.",
          "B. Speed Limit.",
          "C. No Parking.",
          "D. Yield."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_380_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What object is visible on the left side of the cockpit?",
        "time_stamp": "00:00:09",
        "answer": "A",
        "options": [
          "A. A blue handler.",
          "B. A red first aid kit.",
          "C. A yellow flashlight.",
          "D. A green map."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_419_real.mp4"
  },
  {
    "time": "[0:02:25 - 0:02:45]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the dominant color of the pants the person is wearing right now?",
        "time_stamp": "00:02:38",
        "answer": "A",
        "options": [
          "A. Beige.",
          "B. Blue.",
          "C. Black.",
          "D. Gray."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_419_real.mp4"
  },
  {
    "time": "[0:04:50 - 0:05:10]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Which way is the plane turning now?",
        "time_stamp": "00:05:12",
        "answer": "C",
        "options": [
          "A. Left.",
          "B. Right.",
          "C. Up.",
          "D. Down."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_419_real.mp4"
  },
  {
    "time": "[0:07:15 - 0:07:35]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What object is visible in the lower-left corner of the cockpit right now?",
        "time_stamp": "00:07:33",
        "answer": "A",
        "options": [
          "A. A blue handler.",
          "B. A red first aid kit.",
          "C. A yellow flashlight.",
          "D. A green map."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_419_real.mp4"
  },
  {
    "time": "[0:09:40 - 0:10:00]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the current lighting condition as seen right now?",
        "time_stamp": "00:09:54",
        "answer": "A",
        "options": [
          "A. Bright and sunny.",
          "B. Overcast.",
          "C. Dusk.",
          "D. Night."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_419_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:16",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_87_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:04",
        "answer": "A",
        "options": [
          "A. 4.",
          "B. 5.",
          "C. 6.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_87_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:11",
        "answer": "B",
        "options": [
          "A. 2.",
          "B. 4.",
          "C. 3.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_87_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:01",
        "answer": "C",
        "options": [
          "A. 2.",
          "B. 5.",
          "C. 4.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_87_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:06:19",
        "answer": "D",
        "options": [
          "A. 4.",
          "B. 8.",
          "C. 6.",
          "D. 5."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_87_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Where does this letter on the ground come from?",
        "time_stamp": "00:00:39",
        "answer": "D",
        "options": [
          "A. It was delivered by the postman earlier.",
          "B. It fell out of someone's pocket as they walked by.",
          "C. It was dropped by a courier who was in a hurry.",
          "D. Mr. Bean just stuffed it in."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_240_real.mp4"
  },
  {
    "time": "[0:02:09 - 0:02:39]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does Mr. Bean's sitting figure react with confusion or surprise?",
        "time_stamp": "00:02:29",
        "answer": "A",
        "options": [
          "A. Because Mr. Bean interacts with the TV remote.",
          "B. Because Mr. Bean gets distracted by a phone call.",
          "C. Because the TV suddenly turns off.",
          "D. Because Mr. Bean falls asleep while watching TV."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_240_real.mp4"
  },
  {
    "time": "[0:04:18 - 0:04:48]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the wall have green patterns painted on it?",
        "time_stamp": "00:04:37",
        "answer": "C",
        "options": [
          "A. Because the wall was decorated with a stencil and green spray paint.",
          "B. Because the man accidentally spilled green paint while working on a project.",
          "C. Because the man applied green paint using a teddy bear attached to a stick.",
          "D. Because the wall was originally green, and someone tried to cover it up but failed."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_240_real.mp4"
  },
  {
    "time": "[0:06:27 - 0:06:57]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the person with multiple cameras leave the room?",
        "time_stamp": "00:06:34",
        "answer": "C",
        "options": [
          "A. Because the person with cameras needed to recharge their equipment.",
          "B. Because the person with cameras had an urgent phone call.",
          "C. Because he has finished taking the photos.",
          "D. Because the person with cameras spotted something interesting in the other room."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_240_real.mp4"
  },
  {
    "time": "[0:08:36 - 0:09:06]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why is the queenly figure angry right now?",
        "time_stamp": "00:08:35",
        "answer": "C",
        "options": [
          "A. Because someone accidentally spilled tea on her dress.",
          "B. Because her crown was knocked off by a gust of wind.",
          "C. Because the Mr.bean pushing the broom past her face.",
          "D. Because Mr. Bean interrupted her speech with loud noises."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_240_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:13]: The video begins with a close-up of a painting. The painting features the portrait of a young woman with blonde hair against a dark blue background with scattered white dots resembling stars. Pink clouds are visible on the right side of the painting. The portrait appears well-lit, emphasizing the woman's soft facial features and the gentle blending of colors in her hair, which ranges from light blonde to hints of darker shades. There is a bright white circle in the background, which seems to represent the moon, situated to the left of the woman's head. The video is filmed from a steady, first-person perspective, focusing on the upper part of the painting. [0:00:14]: The scene shifts to a different view showing a blank white canvas or paper. The surface is oriented horizontally, and the edges of the paper are visible. The background now appears to be a light-toned surface, perhaps a table or desk. The lighting is even, without casting heavy shadows, indicating a well-lit area. [0:00:15 - 0:00:16]: An object, which looks like a piece of gray fabric or paper, enters the frame from the right side and is placed over the white canvas. The first-person perspective is maintained, and the object is moved into position quickly, covering a portion of the canvas. [0:00:17 - 0:00:18]: A black-and-white printed photograph of the same woman depicted in the painting appears, placed on top of the gray fabric or paper. The portrait in the photograph shows the woman in a similar pose, with her head angled slightly upward. The fabric or paper underneath is partially visible around the edges of the photograph. [0:00:19 - 0:00:20]: A hand holding a blue pen starts to trace or draw over the photograph, focusing on the contours and details of the woman's face. The other hand holds the photograph steady, indicating that the person is using the printed image as a reference or template for their artwork. The first-person perspective captures the entire process from above.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What happens right after the scene shifts to a different view showing a blank white canvas?",
        "time_stamp": "0:00:16",
        "answer": "A",
        "options": [
          "A. A piece of gray fabric is placed over the canvas.",
          "B. The canvas is painted with light colors.",
          "C. A new portrait appears on the canvas.",
          "D. The video zooms out to show the entire table."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_125_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:06]: A hand is actively painting a portrait of a woman with light-colored hair on a white canvas. The brush adds depth and detail to the hair on the left side of the woman’s head. In the background, a yellow circle is situated in the upper left, resembling a sun, while cloud outlines stretch across the canvas. [0:03:07 - 0:03:12]: The focus remains on the left side of the woman’s hair and face. The hand continues to add layers of paint. The brush moves carefully around the ear and cheek areas, enhancing the texture and shading. [0:03:13 - 0:03:15]: The brush moves towards the lower part of the woman’s face, concentrating on the chin and neck regions. The hair and facial features are becoming more defined, adding to the portrait’s realism. [0:03:16 - 0:03:19]: The artist's hand shifts attention to the top of the head, making sure to add highlights and shadows to give the hair volume and a sense of light source. The yellow circle and cloud patterns in the background remain constant throughout the video.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the circle in the background of the painting?",
        "time_stamp": "00:03:19",
        "answer": "D",
        "options": [
          "A. Red.",
          "B. Blue.",
          "C. Green.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_125_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:06]: A hand holding a paintbrush is seen painting a detailed portrait of a woman with blonde hair. The background is a mixture of dark blue at the top and pink hues at the bottom. The paintbrush is applying light, intricate strokes to the subject's hair, blending in various shades of white and blonde to create texture and depth in the hair. [0:06:07 - 0:06:14]: The focus shifts to the lower part of the painting, where the hand continues to paint pink and purple hues below the subject's hair. The brush adds layers of color, suggesting a background or additional elements within the painting. The strokes are broader, and the colors are more blended, creating a soft and vibrant transition. [0:06:15 - 0:06:19]: The brush returns to work on an area near the woman's hair where the dark blue background meets the pink hues. The brush adjusts and refines the transition between these sections, adding subtle details and blending to smooth the contrast between the colors. The hand appears steady, providing detailed and delicate brushwork to enhance the overall composition.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the hand doing just now?",
        "time_stamp": "00:06:06",
        "answer": "D",
        "options": [
          "A. Painting a landscape.",
          "B. Blending the background colors.",
          "C. Painting the subject's face.",
          "D. Adding details to the hair."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      },
      {
        "task_type": "Event Understanding",
        "question": "What is the hand doing when it shifts focus to the lower part of the painting?",
        "time_stamp": "00:06:27",
        "answer": "B",
        "options": [
          "A. Adding texture to the hair.",
          "B. Painting broad strokes with pink and purple hues.",
          "C. Detailing the woman's face.",
          "D. Refining the transition between background colors."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_125_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:05]: A hand is holding a small metallic object, possibly a screw, and pointing it towards a piece of dark blue paper or canvas. There is a white circular shape on the canvas, surrounded by small white dots, likely representing stars. The hand moves slightly, positioning the object closer to the canvas. [0:09:06 - 0:09:19]: The perspective shifts to reveal a detailed painting of a woman with blonde hair on the same dark blue background. The painting includes a full moon and clouds in the background. The woman is depicted in a three-quarter profile, looking towards the right side of the frame. As the video progresses, the painting stays mostly static, highlighting the intricate details and colors of the artwork, including the realistic textures of the woman's hair and the soft gradients in the background. [0:09:20]: The static view of the painting continues, emphasizing the vivid colors and precise details, particularly in the shading and highlights on the woman's face and hair, as well as the blending of colors in the clouds.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the painting shown in the video?",
        "time_stamp": "00:09:20",
        "answer": "A",
        "options": [
          "A. A detailed painting of a woman with blonde hair on a dark blue background with a full moon and clouds.",
          "B. A landscape painting with mountains and rivers.",
          "C. An abstract painting with vibrant colors and shapes.",
          "D. A close-up portrait of an elderly man."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_125_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video begins with a view of an indoor track where multiple remote-controlled (RC) cars are racing. The background includes a banner on the left with the word \"FALKEN,\" and there is a spectator kneeling on the floor. A tool bag is visible in the background. The cars, featuring various designs and colors, navigate a curved section of the track, surrounded by barriers, with the primary focus on a red car and a white car. [0:00:04 - 0:00:07]: The camera angle shifts slightly to follow the RC cars as they continue racing. The red car leads, followed closely by other cars with different paint jobs. The track is outlined with small barriers and some scattered leaves or debris. There is an additional green RC car visible in the background near a small white and yellow structure that might be a pit area or control station. [0:00:08 - 0:00:11]: The cars navigate another bend with some gaining speed. A variety of obstacles and barriers, including red and white striped barriers, segment the track. A person is seen standing near the track's edge, and several more spectators are visible in the background. The area around the track includes a setup of tables, tools, and equipment likely used by participants. [0:00:12 - 0:00:17]: Two of the RC cars, one black and one red, are seen with considerable speed and maneuverability as they continue through the course. The video shows more detail of the surroundings, including stacked barriers and a detailed view of the indoor setting. A person is walking along the track, actively involving in either monitoring or possibly retrieving a car. [0:00:18 - 0:00:20]: The final frames show an overview of the track with multiple RC cars racing and several participants and spectators engaged in the background. The indoor space appears spacious with various setups, indicative of a competition or showcase for RC car enthusiasts.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "Which RC car leads the race initially?",
        "time_stamp": "0:00:02",
        "answer": "C",
        "options": [
          "A. White car.",
          "B. Green car.",
          "C. Red car.",
          "D. Black car."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_482_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:20 - 0:01:40] [0:01:20 - 0:01:23]: A white remote-controlled car with black accents and a spoiler is navigating a marked indoor track. There are red and white striped barriers on the track. The background shows structures such as an orange chair and a metal ladder; [0:01:24 - 0:01:27]: The RC car continues to move around the track, approaching some scattered red obstacles that resemble bricks or another type of small debris. The car maintains steady speed and follows the curve of the track; [0:01:28 - 0:01:32]: The white RC car drives through the red obstacles, pushing them aside as it passes. The vehicle appears to be designed for off-road or complex tracks, giving it the ability to traverse the uneven surface; [0:01:33 - 0:01:38]: After passing through the debris, the car continues on a clear path that is bordered by more red and white barriers. The track loops around multiple obstacles, showcasing the agility and control of the RC car; [0:01:39 - 0:01:40]: As the car navigates through the track, other RC cars and various objects like ramps and barriers are visible. The background includes a large banner advertising tires and other racing-related items, indicating the setting is likely a controlled environment such as a competition or exhibition.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What abnormal event just happened?",
        "time_stamp": "0:01:32",
        "answer": "A",
        "options": [
          "A. A white racing car crashed into the obstacle beside the track.",
          "B. A blue racing car lost control and crashed into the barrier at the curve of the track.",
          "C. A red racing car skidded on the wet surface and collided with the guardrail on the straightaway.",
          "D. A yellow racing car spun out and hit the tire wall at the chicane."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_482_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:43]: The video begins with a view of a red and black RC car on a race track. The car is positioned diagonally on the track with red and white striped barriers around it. The car is decorated with various sponsor stickers and has a detailed design. [0:02:43 - 0:02:46]: The camera then cuts to a different point on the track where an orange and white RC car is driving. People and various pieces of RC equipment are visible in the background. The car appears to be maneuvering around the track. [0:02:46 - 0:02:48]: The orange and white RC car continues to drive, passing by more red and white barriers. The surroundings include a blue banner with text and other items related to the event. [0:02:48 - 0:02:50]: The car makes a turn, showing a better view of its details and the track. There is some sort of foliage in the background along with additional barriers. [0:02:50 - 0:02:53]: The car continues to drift along the track, showing its movement and the tire marks on the ground. It creates a playful and thrilling scene as it turns and speeds. [0:02:53 - 0:02:56]: The RC car approaches several large block-like barriers. The car maneuvers close to these barriers, giving a sense of the driving skill and precision involved. [0:02:56 - 0:02:58]: The car continues to drive around these obstacles. The driver's control over the car is evident as it navigates close proximity to the barriers without hitting them. [0:02:58 - 0:03:00]: The video ends with the RC car driving past more red barriers and what appears to be some landscaping or decorative elements on the track. It showcases the complexity and enjoyable aspects of this RC car racing event.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What unusual event just happened?",
        "time_stamp": "00:02:14",
        "answer": "D",
        "options": [
          "A. A red and white car skidded and slammed into the blue barrier at the edge of the circuit.",
          "B. A green and silver car lost control and collided with the yellow obstacle beside the straight track.",
          "C. A blue and orange car spun out and crashed into the red barrier near the corner.",
          "D. A black and gold car crashed into the orange obstacle on the side of the track after drifting."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_482_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:02]: A small, first-person view remote-controlled police car, with blue and red flashing lights, is moving on a gray concrete road. The car is mainly black with white doors featuring a badge. There are yellow and black caution stripes along the edge of the track. [0:04:03 - 0:04:06]: The police car moves along the track, passing by a green and yellow bulldozer model. It is near a barrier with yellow and black caution stripes, and some people and other models can be seen in the background. [0:04:07 - 0:04:08]: Several small, remote-controlled racing cars appear on a wider stretch of the track. There are various models in different colors, including a green car, a red car, white cars, and another silver car. The area is surrounded by a barrier on one side and has some decorative, red objects scattered along the track. [0:04:09 - 0:04:11]: The remote-controlled cars continue racing on the track. They are maintaining tight but controlled movements, staying within the white lines that mark the lanes on the gray asphalt surface. There is a blue sponsor banner visible in the background. [0:04:12 - 0:04:13]: The racing continues with the cars moving curve right, maintaining consistent speeds. The red car and silver car are side-by-side as they navigate the turn. [0:04:14]: The silver car moves ahead of the red car coming out of the curve into a short straightaway. The track has some scattered obstacles in the form of red items along the inside lane. [0:04:15]: Both cars continue racing, entering another turn. The silver car is ahead of the red car, while more cars appear on the other side of the course. The background includes barriers with advertising banners and a mock race setup. [0:04:16]: The green and white cars continue along the course in front of a large blue sponsor banner, while other cars navigate different parts of the track. The racing action maintains a high pace with tight control. [0:04:17 - 0:04:18]: Different perspectives show cars navigating the track, including a red truck and other vehicles moving through curves and straight sections. Colored barriers and scattered debris add elements of course complexity. [0:04:19 - 0:04:20]: The green car moves quickly along a straight section of the track, passing background elements, including other cars and a small building structure that seems part of the course’s environment. There is also a purple truck visible in the background.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the main color of the remote-controlled police car?",
        "time_stamp": "0:04:02",
        "answer": "A",
        "options": [
          "A. Black.",
          "B. White.",
          "C. Blue.",
          "D. Red."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Event Understanding",
        "question": "What happens just after the silver car moves ahead of the red car?",
        "time_stamp": "0:04:15",
        "answer": "D",
        "options": [
          "A. The red car overtakes the silver car again.",
          "B. Both cars stop racing.",
          "C. The green car crashes into the silver car.",
          "D. The red car knocked the silver car off the track."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_482_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the hat the man is wearing right now?",
        "time_stamp": "00:00:02",
        "answer": "C",
        "options": [
          "A. White.",
          "B. Blue.",
          "C. Gray.",
          "D. Black."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_418_real.mp4"
  },
  {
    "time": "[0:02:19 - 0:02:39]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the primary color of the navigation panel right now?",
        "time_stamp": "00:02:27",
        "answer": "B",
        "options": [
          "A. Yellow.",
          "B. Black.",
          "C. Blue.",
          "D. White."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_418_real.mp4"
  },
  {
    "time": "[0:04:38 - 0:04:58]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the general shape of the fields visible right now?",
        "time_stamp": "00:04:42",
        "answer": "C",
        "options": [
          "A. Circular.",
          "B. Triangular.",
          "C. Rectangular.",
          "D. Irregular."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_418_real.mp4"
  },
  {
    "time": "[0:06:57 - 0:07:17]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type of vehicles are visible right now?",
        "time_stamp": "00:07:06",
        "answer": "C",
        "options": [
          "A. Cars.",
          "B. Bicycles.",
          "C. Gliders.",
          "D. Trains."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_418_real.mp4"
  },
  {
    "time": "[0:09:16 - 0:09:36]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the predominant color of the mountains visible right now?",
        "time_stamp": "00:09:20",
        "answer": "B",
        "options": [
          "A. Brown.",
          "B. Green.",
          "C. Gray.",
          "D. White."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_418_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins with a black screen and then transitions to a scenic view of a Minecraft landscape. The scene showcases a mountainous area with lush greenery, trees scattered across the hills, and a body of water extending towards the right. A cave opening can be seen on the left side of the mountain, adding depth to the landscape. The sky is clear with a few clouds scattered across. [0:00:03]: A title card appears, indicating \"MINECRAFT 1.19\" in bold, block letters, superimposed on the same landscape view as the previous frame. The image is slightly blurred to make the text more prominent. [0:00:04 - 0:00:06]: The screen transitions to black with white text appearing gradually. The text initially reads \"AND I HAVE SOME\" and then \"SOME BIVERY,\" likely a typo or an incomplete word. [0:00:07 - 0:00:09]: The scene changes to a first-person view standing on a wooden dock in Minecraft. The player character, wearing a full set of iron armor, is visible on the dock. The dock is connected to a small fishing hut, with wooden barrels placed on the sides. The background consists of green hills and trees, providing a serene setting. The player's health and hunger bars are shown at the bottom of the screen.  [0:00:10 - 0:00:11]: The character continues to stand on the wooden dock in a similar position as the previous frame. The background includes a small house constructed from wooden planks and birch wood, accessible via a wooden staircase. The surrounding area is lush with greenery, and trees dot the landscape. [0:00:12 - 0:00:13]: The character moves slightly, providing a clearer view of the wooden fishing hut. Additional details of the hut become visible, such as a hanging lantern near the entrance and a small garden plot to the right of the hut. The dock extends into the water, suggesting a peaceful fishing spot. [0:00:14 - 0:00:15]: The camera shifts focus towards the fishing hut, emphasizing its construction details. The hut has a pitched roof with a small chimney on top, releasing smoke particles. Wooden barrels and crates are arranged around the entrance, adding to the rustic feel. The terrain includes tiered grassy platforms leading up the hill. [0:00:16 - 0:00:17]: The camera angle changes to a closer view of the fishing hut's front door. The architecture of the hut showcases the detailed use of different wood textures. Inside, a lantern hangs, giving a warm, inviting ambiance. There are no signs of other players or mobs. [0:00:18 - 0:00:19]: The scene transitions inside the fishing hut. The interior is compact but cozy, with wooden walls and floors. A brick chimney is prominent on the left side, adding a touch of homeliness to the hut. The inside lighting appears sufficient, casting soft shadows. The video concludes with this view of the interior.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What can be seen on the left side of the wooden house at the beginning of the video?",
        "time_stamp": "0:00:20",
        "answer": "D",
        "options": [
          "A. A waterfall.",
          "B. A village.",
          "C. A tower.",
          "D. A cave opening."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_200_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:20 - 0:03:40] [0:03:21 - 0:03:23]: The video begins at the corner of a wooden house. The walls are made of oak logs and planks, with a small area of brick visible near the doorway. There is a large open area to the right, with a view of a river flowing gently and land rising in the background.  [0:03:24]: Turning inside, the video moves down a narrow wooden hallway. There are two furnaces side by side with a bright yellow glow indicating they are active.  [0:03:25]: Exiting the house, the scene shifts to an open field with long grass and scattered trees. A body of water is visible in the distance. [0:03:26]: The video focuses on a cow standing in a grassy area. The cow is black and white with a wide stance. [0:03:27]: The first-person perspective approaches and hits the cow with an iron sword. The result is a puff of white smoke indicating the cow has been defeated.  [0:03:28]: Moving across rolling grassy hills, the perspective reveals more of the open landscape. In the background, clusters of trees are standing. [0:03:29]: Continuing to traverse the green hills, the video maintains this pace, passing a lone tree standing at the top of a small incline. [0:03:30]: A scene with a village in the background, several houses with wooden walls and thatched or wooden roofs can be seen. Another cow is struck, causing red particles to fly. [0:03:31]: Near a small pond surrounded by crops, the video shows another interaction with a cow. The animal is hit by the iron sword, sending more red particles flying. [0:03:32]: Moving closer to the water, the perspective switches to gather tall reeds growing near the shore. Houses from the village are visible in the background. [0:03:33]: Passing through the village, a range of houses, additional crops, and wooden fencing creates a picturesque agrarian scene. There is a hint of other beings in the area, but no clear view. [0:03:34-0:03:35]: The video concludes with a view inside a house, looking at an open chest. The chest contains black helmets, pieces of obsidian, and several saplings. The inventory below shows a variety of items obtained during the journey including beef, leather, a sword, and a bed.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action does the player take towards the cow standing in the grassy area?",
        "time_stamp": "0:03:27",
        "answer": "D",
        "options": [
          "A. Feeds the cow.",
          "B. Leads the cow to water.",
          "C. Ties the cow to a tree.",
          "D. Hits the cow with a sword."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_200_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:42]: The video begins in a cavernous space characterized by rugged terrain. There are patches of amethyst crystals on the right-hand side and a light source casting shadows on the walls. The view shows a sword and a shield icon at the bottom of the screen, indicating the first-person perspective of a character. The ground is littered with various types of blocks, and torches are placed sporadically to illuminate the area. [0:06:42 - 0:06:45]: The camera moves forward, approaching a stone block with two torches on either side. Ahead, there are various types of stone blocks creating a small mound. The right side of the scene remains dominated by the amethyst cluster, while the area to the left is more open and spacious. [0:06:45 - 0:06:47]: As the camera continues moving forward, it passes the stone block and approaches a rectangular wooden structure on stilts. This structure supports multiple chests. The perspective shifts slightly upward to capture the height of the supporting beams and the chests. [0:06:47 - 0:06:50]: The camera looks up at the wooden beam that supports the chest and captures more details of the upper area of the cavern. The view includes a mix of stone and dirt blocks overhead, enhancing the complexity and depth of the cavern's design. [0:06:50 - 0:06:53]: Moving closer to the wooden support beam, the camera orientation changes to provide a close-up view of the part supporting structure. The texture of the wooden blocks and the attached granite blocks becomes more evident, showcasing their pixelated appearance. [0:06:53 - 0:06:56]: The camera moves around the wooden beam, providing alternate angles and perspectives of the structure in the cavern. The surrounding area shows some additional detail, including the texture of the ground blocks and more torches placed strategically around. [0:06:56 - 0:06:58]: The camera moves back to the cavern space, further exploring the surroundings. It captures more of the varied block patterns on the ground and walls, and additional light sources continue to highlight certain areas within the cavern. [0:06:58 - 0:07:00]: As the camera turns to a different section of the cavern, more of the space becomes visible. Another section with a rock formation is revealed, partially illuminated by torches. The texture of the wall shows a mix of different stone blocks, contributing to a varied texture and appearance. [0:07:00 - 0:07:03]: The camera shifts to face another part of the cavern, which appears more expansive and less cluttered. The area includes several larger stone blocks stacked against the wall, likely as part of the layout and structure within the game environment. [0:07:03 - 0:07:05]: This section of the cavern includes a large wall covered in dark red blocks, partially lit by torches. The ground level appears slightly lower as the camera captures more of the surrounding textures and construction. [0:07:05 - 0:07:07]: The camera continues to move, capturing more of the extensive cavern space. A tall wooden column rises from the ground, and details of the far walls and various structures are further visible. [0:07:07 - 0:07:10]: The video shows the character moving forward across the extensive wooden floor. Torches keep the space well-lit, with varied block patterns on the walls enhancing the aesthetic diversity of the scene. [0:07:10 - 0:07:12]: Moving towards the far wall, the camera passes several dark stone blocks positioned against the wall. The layout and spacing suggest some structured design choices within the game's environment. [0:07:12 - 0:07:15]: The camera captures the far wall with greater detail, showing a large section overlaid with red and brown blocks, partly lit by torches placed evenly around the room's perimeter. [0:07:15 - 0:07:17]: As the character moves back, focus shifts to a large open floor space once more. The illuminated areas reveal more of the architectural elements and consistent texture patterns in the wooden floor and stone walls. [0:07:17 - 0:07:20]: The camera pans across the large open space, continuously revealing different textures and shades of the block elements. The upper parts of the cavern show intricate details that define the overarching design of the room. [0:07:20 - 0:07:23]: The final frames capture a broader view of the cavern's interior. The ceiling shows a complex array of blocks, while the ground level appears methodically structured with wooden planks and strategic torch placements, illuminating the vast space.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the box supported by right now?",
        "time_stamp": "00:06:56",
        "answer": "D",
        "options": [
          "A. Wood floor.",
          "B. soil block.",
          "C. stone.",
          "D. Nothing."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_200_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:11:00]",
    "captions": "[0:10:00 - 0:10:20] [0:10:00 - 0:10:02]: The video begins with a first-person perspective showing a view downward into a dark crimson landscape with blocky, pixelated textures. The player character, indicated by the heads-up display, holds a golden object in the right hand and a shield in the left, with various tools and items detailed in a hotbar at the bottom of the screen.  [0:10:03]: The camera angle shifts slightly to the right, providing another view of the blocky terrain.  [0:10:04]: The scene moves forward slightly, offering further perspective into the crimson environment from a higher ledge. [0:10:05 - 0:10:06]: The camera shifts again, glancing around the environment, revealing a lava pool in the distance. The deep red blocks dominate the view, with the character holding the same gold item and shield combination. [0:10:07]: Another shift in perspective shows the left screen edge with more of the peculiar red environment, including jagged block formations. [0:10:08 - 0:10:09]: The camera switches views, climbing upwards, and reveals some glowing white ore embedded in the red blocks. [0:10:10]: The scene now captures a broader expanse of the cavernous crimson world ahead, returning to a similar vantage as earlier but higher up. [0:10:11]: The player’s point of view shifts slightly forward, and to the right, offering a more centered view of the network of red blocks. [0:10:12]: The camera perspective lowers, focusing closer on the ground-level blocks where the character stands. [0:10:13 - 0:10:14]: The character moves through a narrow passageway, with the camera facing forward. [0:10:15]: The player rounds a corner, with a forward view of the red textured wall directly ahead, continuing through the narrow path. [0:10:16]: The character progresses further down the corridor, the red-tinged path continues, seemingly carved out by the character. [0:10:17]: The camera shifts downward, revealing a small creature wearing purple armor at the bottom of the passageway. [0:10:18]: The player’s focus remains on the armored creature, which appears to interact or acknowledge the player character. [0:10:19]: Another angle shows the character’s inventory screen briefly, indicating the various items and equipment in possession. The focus then returns to the creature, now being offered an item. [0:10:20]: The creature continues to acknowledge the player character while holding the item. The scene ends with the character continuing to move through the passage.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What happens when the player encounters the small creature wearing purple armor?",
        "time_stamp": "00:10:25",
        "answer": "D",
        "options": [
          "A. The creature attacks the player.",
          "B. The player hit the creature.",
          "C. The player kill the creature.",
          "D. The player left without doing anything."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_200_real.mp4"
  },
  {
    "time": "[0:12:00 - 0:12:42]",
    "captions": "[0:12:40 - 0:12:42] [0:12:40 - 0:12:41]: A player character dressed in iron armor is standing inside a large wooden structure with a vaulted ceiling. The structure is well-lit with torches placed along the walls and on the ground. The player is positioned behind a crafting table, holding a pink item in their hand. The crafting table is centrally located within the frame, with a chest and a stone cutter to the left of the player. The background includes wooden stairs leading to an upper level, with bookshelves and wooden beams visible. There are also multiple stone pillars and wooden railings visible, enhancing the interior architecture of the structure.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the player character holding in his left hand right now?",
        "time_stamp": "00:12:35",
        "answer": "D",
        "options": [
          "A. A beef steak.",
          "B. A sword.",
          "C. A red item.",
          "D. A shield."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_200_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:07]: A woman with dark hair is standing in a room with a floral-patterned dress. She is wearing a white slip dress with lace detailing at the top.  She wears a floral robe with red and pink flowers. The background shows a floral wallpaper, a wooden chest of drawers with a mirror and various items on it, a bed with a salmon-colored bedspread, and a white nightstand with a lamp. The woman begins to take off the floral robe and sets it down on the bed. [0:00:08 - 0:00:12]: The woman continues to tidy up, setting down the robe and some clothing items on the bed. She walks towards a nightstand on the left side of the room and picks up another item of clothing from there. The room's decor, including the floral wallpaper and the furniture arrangement, remains consistent. [0:00:13 - 0:00:19]: The woman sits on a chair next to the nightstand and starts putting on what appears to be white stockings or leggings. She is focused on adjusting the stockings, looking down at them as she works. The lighting is bright as daylight streams in through the sheer curtains behind her, illuminating the room. The bed in the foreground has neatly arranged articles of clothing laid out on top of it.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What pattern is on the woman's robe?",
        "time_stamp": "0:00:02",
        "answer": "C",
        "options": [
          "A. Stripes.",
          "B. Polka dots.",
          "C. Floral.",
          "D. Geometric."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_161_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:20]: The video takes place in a bedroom with floral wallpaper and a bright atmosphere, created by a large window with sheer white curtains allowing sunlight to stream through.  A woman with dark hair styled in an updo is in the room, initially wearing a light-colored camisole and standing near the bed. The bed has a peach-colored cover with two piles of clothing. A small table with a lamp is positioned next to the bed, while a wooden dresser with a mirror is situated against the wallpapered wall. Various objects such as bottles, jewelry boxes, and a brush, are neatly arranged on the dresser, indicating it is used for grooming or dressing. The woman picks up a green, yellow, and floral-patterned skirt and begins to step into it, pulling it up to her waist and adjusting the waistband. She then takes the shoulder straps of the dress and begins putting them on, ensuring the dress fits correctly by adjusting the straps and smoothing the fabric. She carefully places the floral pattern of the dress to align it properly on her chest, checking the fit and making minor adjustments. After ensuring the dress is properly worn, she moves to grab a pair of beige shoes. She sits down, gracefully lifting her dress slightly to put on the shoes, revealing a glimpse of her legs as she adjusts her attire to ensure comfort and proper fit. The video is characterized by smooth, deliberate motions as the woman dresses, with a calm and orderly background complementing her actions.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the cover on the bed?",
        "time_stamp": "0:01:20",
        "answer": "C",
        "options": [
          "A. Blue.",
          "B. Green.",
          "C. Peach.",
          "D. White."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Action Recognition",
        "question": "What does the woman do after putting on the dress?",
        "time_stamp": "0:01:29",
        "answer": "B",
        "options": [
          "A. Brushes her hair.",
          "B. Put on her high heels.",
          "C. Adjusts the mirror.",
          "D. Picks up a jewelry box."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Event Understanding",
        "question": "What is the woman primarily doing in the video?",
        "time_stamp": "0:01:20",
        "answer": "C",
        "options": [
          "A. Cleaning the room.",
          "B. Reading a book.",
          "C. Dressing herself.",
          "D. Writing a letter."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_161_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:13]: A woman is standing in front of a mirrored vanity in a room with floral wallpaper and soft lighting. She wears a green, gold-patterned dress and a beaded necklace. She is seen putting on a yellow scarf or shawl, adjusting it around her shoulders and arms. The bed with brass railings and a green chair are visible beside her. The vanity holds various small items, including a mirrored jewelry box and decorative items. [0:02:14 - 0:02:17]: The scene transitions to a close-up of two elegant glasses filled with a clear beverage, one with a lemon twist garnish. This part focuses on the delicate details of the glasses, their reflections, and the liquid they contain. [0:02:18 - 0:02:19]: The video shows a woman wearing a gold dress with intricate beadwork. She has short hair styled with a decorative headband, and she is smiling. The background appears to be an elegantly decorated indoor space, possibly with warm lighting.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the woman doing right now?",
        "time_stamp": "00:02:42",
        "answer": "B",
        "options": [
          "A. Singing.",
          "B. Dancing.",
          "C. Smiling.",
          "D. Reading."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_161_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video begins with a black screen. [0:00:01 - 0:00:04]: A close-up view of an RTX 4090 graphics card is shown. The card is placed on a cushioned surface against a gradient background with purple and blue hues. The graphics card has a large fan on the right and a smaller one on the left, with the label \"RTX 4090\" visible on its side. [0:00:05 - 0:00:08]: The scene transitions to a top view of two RTX graphics cards placed on a white table. Both cards are identical in design, featuring black and silver coloring, large fans, and prominently displayed \"RTX 4090\" labels. The cards lie parallel to each other with a slight overlap. The background is mostly composed of a white surface and some out-of-focus elements. [0:00:09 - 0:00:13]: A split-screen view shows a comparison between two gameplay footages. The left side displays \"RTX 3090,\" while the right side displays \"RTX 4090.\" Both frames show a sports car driving down a rural road with fields on either side. Performance indicators are shown on the screen, with the RTX 4090 side indicating higher frame rates (measured in FPS) compared to the RTX 3090. [0:00:14 - 0:00:17]: A chart is displayed comparing the average frame rates of the RTX 4090 versus the RTX 3090 across ten different video games at 4K, high/ultra settings. Games listed include \"DOOM Eternal,\" \"F1 22,\" \"Assetto Corsa Competizione,\" \"Forza Horizon 5,\" and others. The RTX 4090 shows higher performance across all games with noticeable differences in FPS. [0:00:18 - 0:00:19]: The final scene features a person seated at a table, presenting the RTX 4090 graphics card. The person wears a green shirt and is situated in a dark room with a subtle light source to the left. They hold the graphics card and speak to the camera.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which graphics card performs better in this game?",
        "time_stamp": "00:00:13",
        "answer": "A",
        "options": [
          "A. RTX 4090.",
          "B. RTX 3080.",
          "C. RTX 3090.",
          "D. RTX 4080."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_115_real.mp4"
  },
  {
    "time": "[0:02:40 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:42]: Two graphics cards are positioned side by side on a wooden surface with a black corrugated background. The card on the left is black with two fans, while the card on the right is silver with a single large fan and a distinctive X-shaped design. [0:02:43 - 0:02:46]: The screen showcases an advertisement for the GeForce RTX 4060 Family, featuring a sleek black and silver graphics card with an X-shaped design. The background has green and black streaks, and there is text highlighting specifications and pricing information starting from \"AUD $545.\" [0:02:47 - 0:02:51]: A person is holding a Radeon graphics card with both hands. The card is mostly black with some red accents. Initially, the person holds it horizontally, showing the top side with the Radeon branding, and then slightly tilts it, revealing the side with the ports and a large heatsink. [0:02:52 - 0:02:55]: A close-up side view of the Radeon graphics card is displayed, focusing on the heatsink and the power connector at the end of the card. [0:02:56]: A dark screen displays various specifications for two models, RX 6600 and RX 6700, but the text is dark and hard to read. [0:02:57 - 0:02:59]: The same specifications screen becomes clearer, highlighting details such as \"Silicon,\" \"Compute Units,\" \"Boost,\" \"Memory\" and \"Power\" for both RX 6600 and RX 6700, alongside their respective prices \"$329\" for RX 6600 and \"$269\" for RX 6700 in distinct columns.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What price is shown for the GeForce RTX 4060 Family graphics card right now?",
        "time_stamp": "00:02:43",
        "answer": "A",
        "options": [
          "A. AUD $545.",
          "B. AUD $499.",
          "C. USD $545.",
          "D. USD $499."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_115_real.mp4"
  },
  {
    "time": "[0:05:20 - 0:05:40]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:26]: The video features a first-person perspective sequence set in a vibrant, futuristic city filled with neon lights and towering buildings adorned with digital signage. A car with bright red rear lights is seen traveling on an elevated road, featuring large billboards on either side, displaying colorful advertisements. The screen is split into two views, each displaying the car's lane on the road under slightly different graphical settings, indicated by on-screen text. The left view shows \"RTX 4090\" with \"RT Psycho + DLSS 3 Perf,\" while the right view adds \"Frame Generation\" to the same settings. The city's architecture is detailed with a blend of modern and futuristic elements, including holographic billboards and bright lighting. [0:05:27 - 0:05:28]: The perspective shifts to a more open area of the city with the car passing through a section flanked by tall buildings. The road is clearly defined with yellow lines and curves gently. The screen still shows both views, comparing different graphical settings. A subtle change is noted in lighting and reflections on the road surface and buildings. [0:05:29 - 0:05:34]: The video then transitions to a scene featuring a person seated at a table in a quieter indoor environment. This person is wearing a green t-shirt and is discussing two graphics cards placed in front of him on the table. The surrounding area appears to be a controlled setting with dark walls and soft lighting. He gestures with his hands, explaining or comparing something about the graphics cards. There are two additional boxes on either side of the table, possibly related to the graphics cards. [0:05:35]: The same person continues to speak, gesturing with their hands to emphasize their points. The focus remains on him and the graphics cards on the table. [0:05:36 - 0:05:39]: The scene changes again to show a gameplay footage on a computer monitor. The game appears to be a fast-paced shooter, with the player using a futuristic weapon to navigate a complex environment filled with obstacles and other characters. The graphics are vivid and colorful, featuring a blend of blue, red, and orange hues in a dynamic in-game setting. Another screen is visible to the left, likely part of a multi-monitor setup.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What graphical setting is added to the right view that is not included in the left view right now?",
        "time_stamp": "00:05:26",
        "answer": "A",
        "options": [
          "A. Frame Generation.",
          "B. DLSS 3 Perf.",
          "C. RT Psycho.",
          "D. RTX 4090."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_115_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:08:20]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: A person sits at a table with various computer graphics cards in front of him, including several placed on the table. He is wearing a green t-shirt and is interacting with the objects on the table with both hands. The background is dark with vertical ridges on the left side. [0:08:02 - 0:08:04]: Close-up of an NVIDIA GeForce RTX 4070 graphics card placed on a white surface, showing the cooling fan and part of the card's body. [0:08:05 - 0:08:06]: A side view of the GeForce RTX graphics card, with the card's body slightly tilted to reveal the fan and part of the card's internal structure. The background is out of focus with green and black hues. [0:08:07 - 0:08:11]: A screen displays a comparison chart of the performance of RTX 4070 Ti vs. 4070, displaying average gains in various games at 1440p, high/ultra settings. Game titles and performance metrics are listed, with green bars representing the data visually. [0:08:12 - 0:08:15]: The view returns to the person sitting at the table. He is again interacting with the graphics cards, explaining something. His hands are moving over the cards while he speaks, showing them and pointing towards them. [0:08:16 - 0:08:19]: Close-up of multiple NVIDIA GeForce RTX graphics cards on the table, viewed from above. The person holds and adjusts one card, showing its design features and cooling fans. The cards are arranged to display their fans and top labels.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What comparison is displayed on the screen right now?",
        "time_stamp": "00:08:08",
        "answer": "A",
        "options": [
          "A. 4070Ti vs 4070 in various games.",
          "B. Cooling efficiency of RTX 4070 against previous models.",
          "C. 4070 in different regions.",
          "D. 4070."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_115_real.mp4"
  },
  {
    "time": "[0:09:40 - 0:09:46]",
    "captions": "[0:09:40 - 0:09:46] [0:09:40 - 0:09:42]: A hand is shown placing a black GeForce RTX graphics card on a box. The graphics card is seen from the side, with the GeForce RTX branding visible. The box underneath the card bears the text \"INSPIRED BY GAMERS, BUILT BY NVIDIA.\" The background is dark and blurred, hinting at a focused shot on the product. [0:09:43 - 0:09:45]: The scene shifts to a person sitting at a table with multiple Nvidia products in front of them. The person, wearing a green t-shirt, is interacting with the products, expressing something with a calm and friendly demeanor. Various models of Nvidia products are spread across the table: some types of graphics cards and a possibly related accessory. The background remains dark, maintaining focus on the person and the products on the table.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the message printed on the box underneath the GeForce RTX graphics card right now?",
        "time_stamp": "00:09:40",
        "answer": "A",
        "options": [
          "A. INSPIRED BY GAMERS, BUILT BY NVIDIA.",
          "B. DESIGNED FOR GAMERS, MADE BY NVIDIA.",
          "C. CREATED FOR GAMERS, MADE BY NVIDIA.",
          "D. DESIGNED FOR GAMERS, BUILT BY NVIDIA."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_115_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the man's sweatshirt right now?",
        "time_stamp": "00:00:04",
        "answer": "A",
        "options": [
          "A. Beige.",
          "B. Blue.",
          "C. Red.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_433_real.mp4"
  },
  {
    "time": "[0:02:17 - 0:02:22]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type of vehicle appears near the open parking lot right now?",
        "time_stamp": "00:02:06",
        "answer": "D",
        "options": [
          "A. Motorcycle.",
          "B. Helicopter.",
          "C. Bicycle.",
          "D. Car."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_433_real.mp4"
  },
  {
    "time": "[0:04:34 - 0:04:39]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the vehicle being operated right now?",
        "time_stamp": "00:04:35",
        "answer": "D",
        "options": [
          "A. Airplane.",
          "B. Boat.",
          "C. Car.",
          "D. Helicopter."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_433_real.mp4"
  },
  {
    "time": "[0:06:51 - 0:06:56]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "What type of headset is the man wearing right now?",
        "time_stamp": "00:06:53",
        "answer": "D",
        "options": [
          "A. HiFi.",
          "B. Reverberant.",
          "C. No function.",
          "D. Aviation."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_433_real.mp4"
  },
  {
    "time": "[0:09:08 - 0:09:13]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What shape is the island in the water right now?",
        "time_stamp": "00:09:17",
        "answer": "A",
        "options": [
          "A. Rectangular.",
          "B. Circular.",
          "C. Triangular.",
          "D. Oval."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_433_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:16",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_89_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:34",
        "answer": "A",
        "options": [
          "A. 3.",
          "B. 5.",
          "C. 6.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_89_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:21",
        "answer": "B",
        "options": [
          "A. 2.",
          "B. 4.",
          "C. 3.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_89_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:19",
        "answer": "B",
        "options": [
          "A. 2.",
          "B. 5.",
          "C. 4.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_89_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:06:19",
        "answer": "C",
        "options": [
          "A. 4.",
          "B. 8.",
          "C. 6.",
          "D. 7."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_89_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:16",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      },
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:50",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_97_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:01:00",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_97_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:29",
        "answer": "A",
        "options": [
          "A. 3.",
          "B. 4.",
          "C. 6.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_97_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:14",
        "answer": "B",
        "options": [
          "A. 2.",
          "B. 5.",
          "C. 3.",
          "D. 7."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_97_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why did the elderly woman step off the bus with a tray of tea?",
        "time_stamp": "00:00:21",
        "answer": "C",
        "options": [
          "A. To serve tea to the bus driver during his break.",
          "B. To offer refreshments to passengers waiting at the bus stop.",
          "C. To deliver food to the elderly lady in a pink dress who has a foot injury.",
          "D. To serve tea to passengers waiting at the bus stop."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_245_real.mp4"
  },
  {
    "time": "[0:02:12 - 0:02:42]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why is Mr. Bean in the hospital ward?",
        "time_stamp": "00:02:29",
        "answer": "B",
        "options": [
          "A. Because he is visiting a friend who is a patient.",
          "B. Because this is his fantasy.",
          "C. Because he mistakenly walked into the wrong building.",
          "D. Because he is participating in a medical study."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_245_real.mp4"
  },
  {
    "time": "[0:04:24 - 0:04:54]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the character try to manipulate the thermometer reading?",
        "time_stamp": "0:04:47",
        "answer": "A",
        "options": [
          "A. To pretend to be sick.",
          "B. To make the nurse angry.",
          "C. To make the nurse laugh.",
          "D. To test the accuracy of the thermometer."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_245_real.mp4"
  },
  {
    "time": "[0:06:36 - 0:07:06]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why is the patient prepped for surgery?",
        "time_stamp": "00:07:03",
        "answer": "A",
        "options": [
          "A. Because Mr. Bean tampered with the check, making himself appear very unhealthy.",
          "B. Because the patient requested the surgery as a precaution.",
          "C. Because the doctor misread the patient's medical chart.",
          "D. Because the patient was mistakenly identified as someone else."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_245_real.mp4"
  },
  {
    "time": "[0:08:48 - 0:09:18]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why do the babies start crying?",
        "time_stamp": "0:09:16",
        "answer": "C",
        "options": [
          "A. Because they are hungry.",
          "B. Because they need a diaper change.",
          "C. Because the person in the wheelchair enters the room.",
          "D. Because they are tired."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_245_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:05]: The video begins with a black notebook lying flat on a tabletop. Surrounding it, there is a red pencil on the left side, an eraser, and a tube of red paint towards the top right. The camera is set in a first-person perspective, capturing two hands as they reach towards the notebook and start to open it. [0:00:06 - 0:00:13]: As the notebook is opened, blank pages are revealed initially. The individual flips through the pages, revealing detailed paintings. The first painting depicts a creature with a long beak, and the subsequent page shows a feline face with expressive eyes, followed by a realistic painting of a fish. [0:00:14 - 0:00:20]: After flipping through a page with the fish painting, the person continues turning blank pages, seemingly towards the end of the notebook. The video concludes with the hands resting on a blank page with the notebook still open.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What sequence of paintings was revealed in the notebook?",
        "time_stamp": "0:00:17",
        "answer": "B",
        "options": [
          "A. A fish, a tree, and a landscape.",
          "B. A creature with a long beak, a feline face, and a realistic fish.",
          "C. An abstract design, a human portrait, and a cityscape.",
          "D. A bird, a flower, and a mountain."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_131_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:03]: The video shows an artist painting the head of a rooster on a piece of paper. The rooster's head is painted with vibrant colors, including a pink comb, a brown and white face, and an orange eye. The artist uses a paintbrush to add details to the rooster's head, particularly focusing on the eye and the area around it. [0:03:04 - 0:03:06]: The artist continues to work on the rooster's head, applying more paint to bring out the details and colors. The brush moves smoothly, adding various shades of color to enhance the painting's texture and depth. [0:03:07 - 0:03:12]: The artist shifts the focus to different parts of the rooster's head, applying paint to the comb and face. The brushstrokes are meticulous, showing the artist's careful attention to detail. The background remains a plain, light beige color, keeping the focus on the painted rooster. [0:03:13 - 0:03:16]: The artist begins to fill in more details on the rooster's neck and body area, adding layers of brown and white paint. The brush moves in smooth, deliberate strokes, highlighting the intricate features of the rooster. The overall composition of the painting becomes more defined and detailed. [0:03:17 - 0:03:19]: The artist continues to refine the painting, focusing on the lower part of the rooster's body. The brushstrokes create a textured effect, adding depth to the image. The colors blend seamlessly, showcasing the artist's skillful technique. The rooster's head and body appear more lifelike and detailed as the artist adds the finishing touches.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the content shown in the video just now?",
        "time_stamp": "00:03:19",
        "answer": "D",
        "options": [
          "A. The artist is painting a landscape with various animals.",
          "B. The artist is sketching a detailed rooster using a pencil.",
          "C. The artist is painting abstract shapes and patterns.",
          "D. The artist is painting a rooster's head and body with vibrant colors and detailed brushstrokes."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_131_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:19]: A hand is painting a realistic chicken image in a sketchbook using a paintbrush. The chicken's feathers are a mix of brown, black, and white colors, with a vibrantly colored red comb and wattle. The painter’s hand holds a wooden paintbrush with white metal ferrule. The painting is being meticulously detailed, specifically around the neck and feather areas, with varying brush strokes to add texture and depth. The background is a plain off-white surface, and the sketchbook is open flat on a table. Nearby are a few art supplies including a red pencil and a tube of paint. The brush's movement is consistent, adding layers to the painting to enhance realism. The overall scene remains static except for the painter’s hand and brush. [0:06:19 - 0:06:20]: The painting of the chicken remains the focus, with the painter’s hand continuing to refine the image using precise strokes.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the painter focusing right now?",
        "time_stamp": "0:06:12",
        "answer": "B",
        "options": [
          "A. Adding a background landscape.",
          "B. Detailing the chicken's neck and feathers.",
          "C. Painting the chicken's feet.",
          "D. Sketching a second chicken."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_131_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:04]: The video depicts a close-up scene of someone painting a rooster on a piece of paper or canvas. The background is filled with dark blue brush strokes. The rooster features a red comb and wattles, with a brightly colored head comprising shades of pink and red. The lower part of the rooster's neck and body is painted in shades of brown with lighter highlights. A hand holding a paintbrush can be seen working diligently on the painting, applying more blue paint to the background on the left side of the rooster's head. [0:09:05 - 0:09:09]: The hand continues to move quickly, applying more blue paint to the upper left corner of the painting. The rooster's details, such as the eye, comb, and wattles, become more defined as the painter works around these features, adding depth and contrast to the artwork. The use of contrasting colors accentuates the vibrant look of the rooster against the dark blue background. [0:09:10 - 0:09:14]: The painter continues to add details and refine the painting. They focus on enhancing the background and making sure the rooster's head stands out prominently. The strokes are purposeful and controlled, displaying the skill and technique of the painter. The brush moves from the top of the rooster's comb towards the upper part of the canvas, adding more depth to the background. [0:09:15 - 0:09:19]: The final stages of the painting process showcase the painter making final touches and adjustments. The brush moves steadily to fill in any remaining gaps and to create a well-blended background. The painter's hand appears steady and deliberate, ensuring the painting has a polished and finished look. By the end of the sequence, the rooster's head is vividly highlighted against the rich blue background, giving the painting a striking appearance.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action is the painter performing at the upper left corner of the painting?",
        "time_stamp": "0:09:09",
        "answer": "C",
        "options": [
          "A. Adding red paint.",
          "B. Adding green paint.",
          "C. Adding blue paint.",
          "D. Adding yellow paint."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Event Understanding",
        "question": "What is the painter focusing right now?",
        "time_stamp": "0:09:19",
        "answer": "C",
        "options": [
          "A. The painter begins a new painting.",
          "B. The painter adds blue paint to the rooster's body.",
          "C. The painter makes touches and adjustments to the background.",
          "D. The painter signs the painting."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_131_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:02]: A person is shown chopping a red onion on a wooden cutting board. The person uses a large knife to make precise cuts; the red onion is halved with the flat side facing down. On the counter surrounding the cutting board, there are ingredients such as olive oil, a block of butter, and some leafy greens in a bowl. [0:02:03 - 0:02:07]: The camera perspective widens, showing more of the kitchen setting. The individual, wearing a blue shirt, continues to finely chop the onion. The kitchen has white brick walls, shelves with plates and glasses, and stainless-steel knives hanging on the wall. On the kitchen counter, a frying pan is on the stovetop, and other ingredients, including yellow corn and cherry tomatoes, are visible on the counter. [0:02:08 - 0:02:09]: The view zooms back in on the chopping board as the person continues to slice the onion into thin pieces. The precision of the cuts reveals the layers of the onion, and more of the surrounding ingredients are visible, such as a glass bottle of olive oil and a small plate of butter chunks. [0:02:10 - 0:02:12]: The camera perspective shifts again, showing the person moving the chopped onion pieces to one side of the cutting board. The individual wipes the knife clean with their hand, and in the background, the kitchen shelves are stocked with various colorful plates and bowls, adding a vibrant touch to the setting. [0:02:13 - 0:02:13]: The person appears to be speaking or giving instructions; however, no audio details are available. They emphasize their actions with hand movements, making it clear that they are engaged in demonstrating the cooking process. [0:02:14 - 0:02:17]: The focus shifts to the stovetop where the person begins placing the chopped onion into a heated frying pan. The onions sizzle as they come into contact with the hot surface. In the pan, there are already pieces of yellow corn. The person's hands are busy distributing the sliced onions evenly across the pan. [0:02:18 - 0:02:19]: The view briefly switches to an overhead angle, showing the contents of the pan from above. The yellow corn and finely sliced onions mix together as the person continues to stir and ensure even cooking. Two other pans are seen on the stovetop, although they are not in use at this moment.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person using to chop the onion?",
        "time_stamp": "0:02:02",
        "answer": "B",
        "options": [
          "A. A small wooden knife.",
          "B. A sliver knife.",
          "C. A pair of scissors.",
          "D. A fork."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "How are the yellow corn and onions distributed in the frying pan?",
        "time_stamp": "0:02:19",
        "answer": "C",
        "options": [
          "A. Randomly scattered.",
          "B. Piled in the center.",
          "C. Evenly distributed.",
          "D. Grouped separately."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_32_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:01]: A stainless steel frying pan containing cooking ingredients is on the stove's burner, with a slice of what appears to be raw meat about to be added. Several small jars are placed in the background near the stovetop, one containing wooden items like a spoon and spatula, and another with cooking tongs. [0:04:01 - 0:04:02]: A person wearing a teal shirt is beside the stove, which has multiple burners. The background has white subway tiles and shelves with kitchenware and dishes. The person appears to be adjusting the heat on one of the stove burners. [0:04:02 - 0:04:03]: The individual in the teal shirt holds a pan in one hand and seems to be discussing or explaining something, pointing to emphasize. [0:04:03 - 0:04:04]: The person places the pan back onto the stove's burner, ready for the next step in the cooking process. They have a casual yet attentive manner. [0:04:04 - 0:04:05]: The individual gestures with both hands, likely explaining the next cooking step or ingredient in focus. [0:04:05 - 0:04:06]: The person stands to the right of the stove, with an extended hand toward a bowl of ingredients lying on the right-hand side. [0:04:06 - 0:04:07]: The top view reveals a chopping board with halved cherry tomatoes varying in color. The board is surrounded by several bowls of ingredients, such as fresh greens, butter, and salt. [0:04:07 - 0:04:08]: The person's hand coats several pieces of raw meat with flour or a similar substance, while the prepared ingredients remain positioned around the chopping board. [0:04:08 - 0:04:09]: The hand of the individual continues to work the raw meat into the flour mixture in the bowl, focusing on making sure they are well-coated. [0:04:09 - 0:04:10]: The person’s hand completes dusting the pieces of raw meat with flour, with the other ingredients awaiting further steps. [0:04:10 - 0:04:11]: The individual adds some seasoning to the raw meat placed in a bowl on the counter, preparing for the next cooking step. The surroundings include various bowls, bottles, and utensils neatly arranged. [0:04:11 - 0:04:12]: The person sprinkles seasoning over the raw meat, now further prepared for cooking, while the stove in the background has a pan frying other ingredients. [0:04:12 - 0:04:13]: The individual continues to season the raw meat in the bowl. Different colorful ingredients and cooking utensils are positioned around the kitchen counter. [0:04:13 - 0:04:14]: With one hand, the person holds the bowl of seasoned meat and uses the other to lift the pan off the stove, showing readiness to rotate between the preparations. [0:04:14 - 0:04:15]: The person hovers between the stove and counter, holding a bowl of seasoned meat in one hand and a frying pan in the other, aligning them for the upcoming steps. [0:04:15 - 0:04:16]: Over the stove, the person readies the pan for the seasoned meat while other kitchen tools and ingredients are visible on the counter. [0:04:16 - 0:04:17]: A view from above shows the frying pan containing colorful cooking ingredients such as corn and onions, along with an empty frying pan beside it. [0:04:17 - 0:04:18]: The person appears to ready another pan, likely in the preparation stage for the cooking process. [0:04:18 - 0:04:19]: Holding the bowl close, the person places the first piece of seasoned meat into an empty frying pan, beginning the cooking process. [0:04:19]: Additional pieces of seasoned meat are added to the empty frying pan as other ingredients continue to cook in the adjacent pan.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person holding in one hand at the stove?",
        "time_stamp": "0:04:02",
        "answer": "B",
        "options": [
          "A. A wooden spoon.",
          "B. A pan.",
          "C. A bowl.",
          "D. A knife."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_32_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:01]: In a modern kitchen setup with a white brick wall and blue cabinets, my hands are shown working over two stainless steel frying pans on a black stovetop. The left pan contains food that is being prodded with a fork, while the right pan holds a sauté mix of diced vegetables, predominately yellow and red. [0:06:01 - 0:06:02]: Using a spoon and fork, I turn over a piece of food in the left pan. The action is precise, and my fingers are close to the pan, indicating careful handling. The vibrant colors of the vegetables in the right pan remain visible.  [0:06:02]: My right hand uses a spoon to flip the piece of food in the pan again. My left hand is steadying the pan by holding its handle. [0:06:03 - 0:06:04]: Now, I use a spoon in my right hand to continue adjusting the food in the left pan. The focus is on ensuring even cooking, as the steam rises lightly from both pans. Background items, such as kitchen utensils and condiments, are neatly arranged. [0:06:04 - 0:06:05]: Pulling back slightly, I step away from the stove but keep my attention on the food. The kitchen's decor becomes more visible, including white shelves holding jars and a cutting board with small items. [0:06:05 - 0:06:06]: I continue managing the food with the spoon, leaning toward the stove. The blue overhead cabinets and various jars on the shelves are more prominent, enhancing the kitchen's modern aesthetic. [0:06:06 - 0:06:07]: I turn away from the stove and reach for a white towel to clean my hands. The expression indicates attentiveness, ensuring that everything is prepared correctly. The background includes a kettle and additional cooking tools. [0:06:07 - 0:06:08]: Holding the towel in both hands, I look at the kitchen counter, likely checking for any necessary adjustments or cleanups needed. [0:06:08 - 0:06:09]: Using a black-handled frying pan in my right hand, I transfer the food to another area in the kitchen. The view of the white brick wall and wooden shelves add context to the spacious cooking area. [0:06:09 - 0:06:10]: I hold the frying pan and walk towards a wooden countertop on the right. Plates and bowls on the counter await the food, creating a warm and organized cooking environment. [0:06:10 - 0:06:11]: Pouring the contents from the pan onto a white plate, I ensure the pieces are carefully placed in a specific arrangement. Bottles and bowls nearby suggest a setup for further preparation or garnishing. [0:06:11 - 0:06:12]: The pan is tilted further, allowing more food to slide onto the plate, filling it gradually. Limes and other ingredients on the counter prepare for final food presentation steps. [0:06:12 - 0:06:13]: Continuing the pour, the plate now shows neatly placed food pieces, while I adjust the pan to guide any remaining items. The organized kitchen space provides ample room for efficient cooking. [0:06:13 - 0:06:14]: The last of the food is guided from the pan to the plate with precision. My focus remains on ensuring all pieces are transferred without any spills. [0:06:14 - 0:06:15]: Any remaining food bits are being carefully poured onto the plate. The kitchen counter includes multiple cooking instruments and ingredients, highlighting a well-stocked cooking environment. [0:06:15 - 0:06:16]: I shake the pan slightly to nudge the last piece of food onto the plate. The pristine white plate contrasts with the rich colors of the food, making it visually appealing. [0:06:16 - 0:06:17]: All food now appears on the plate, neatly arranged, while I lift the pan away. Adjacent are bowls, a cutting board, and ingredients ready for the final touches. [0:06:17 - 0:06:18]: Placing the pan back on the stove, steam rises from the fully cooked food. The countertop remains organized with several different kitchen tools and some green vegetables. [0:06:18 - 0:06:19]: I finish the cooking process with a final look over the stove and kitchen. The kitchen wall's \"COOK\" sign is prominent, blending well within the white brick and wooden shelf decor.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Where does the person transfer the food after cooking?",
        "time_stamp": "00:06:24",
        "answer": "B",
        "options": [
          "A. Onto a white plate on the left countertop.",
          "B. Onto a white plate on the right countertop.",
          "C. Into a bowl on the left countertop.",
          "D. Into a bowl on the right countertop."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the towel used to clean the hands?",
        "time_stamp": "00:06:07",
        "answer": "B",
        "options": [
          "A. Blue.",
          "B. White.",
          "C. Red.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_32_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins inside a dealership featuring a modern, first-person perspective. The interior has checkerboard-patterned flooring and dark walls.  There is a counter on the left side and a waiting area with couches against the walls.  [0:00:03 - 0:00:06]: The character runs toward the center of the dealership, passing through a lobby area. Displayed cars and additional seating areas are visible throughout. [0:00:07 - 0:00:07]: The character makes a brief stop in the showroom area where vehicles are displayed, with cars parked on the checkered floor.  [0:00:08 - 0:00:11]: The character continues moving through the showroom area, approaching the counter where various car accessories are displayed. [0:00:12 - 0:00:12]: The character stops at the counter which is cluttered with various items, and moves towards the computer terminal. [0:00:13 - 0:00:15]: The character interacts with the computer terminal at the counter, initiating a car selection menu. [0:00:16 - 0:00:17]: The video shifts focus to a car selection interface showing different car models, each with its own price and name overlaying a grid. [0:00:18 - 0:00:20]: The character scrolls through various car models in the selection menu, examining different vehicles visually.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the character doing right now?",
        "time_stamp": "00:00:17",
        "answer": "D",
        "options": [
          "A. Running towards the center of the dealership.",
          "B. Stopping in the showroom area.",
          "C. Interacting with the computer terminal.",
          "D. Viewing the car selection interface."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_275_real.mp4"
  },
  {
    "time": "0:02:20 - 0:02:40",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:40]: The video appears to be a first-person perspective gameplay or streaming session, as indicated by the screen layout. On the left side corner, there is a webcam feed of the player, who is looking at the screen and interacting with the game. The player's room is visible in the background, featuring a lit gaming setup with multiple monitors. The main scene is centered on a character in the game, standing against a two-toned wall background. The character wears a green hoodie with \"AIR FORCE\" written on it and looks directly ahead, showing varied facial expressions. On the right side of the screen, a chat window displays messages from viewers, along with notifications or alerts. Also, there is a vertical menu indicating an \"Available Jobs\" list in the game, associated with different roles like Sanitation Worker, Lumberjack, Trucking, etc. The player scrolls through this job list as the video progresses. The overall context involves selecting or browsing job options within the game interface.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the player doing right now?",
        "time_stamp": "00:02:24",
        "answer": "C",
        "options": [
          "A. Interacting with the chat window.",
          "B. Selecting a game character.",
          "C. Browsing the job options list.",
          "D. Customizing the game character's appearance."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_275_real.mp4"
  },
  {
    "time": "0:04:40 - 0:05:00",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:43]: The video starts with a scene showing a detailed digital map of a city area in a software application, complete with streets, buildings, and various icons. A small window in the top left corner displays a person with various expressions and hand movements, presumably narrating or interacting with the map. On the right side of the screen, there is a chat section with colorful text messages from viewers, indicating a live-streaming environment. [0:04:44 - 0:04:46]: The map view remains, but now the webcam frame shows the person more intently focusing on the map. The interactions in the chat are still actively updating as the person continues their detailed explanations or actions related to the map content. [0:04:47 - 0:04:49]: Suddenly, the video transitions to a first-person perspective in a digital game environment. The scene depicts a field with high grass and flowers under a green night-vision filter. Trees and bushes are visible in the background. The lower left corner of the screen displays various icons and small text boxes, indicative of a game's HUD. The person in the webcam window reacts to this new environment. [0:04:50 - 0:04:55]: The first-person perspective remains in the grassy field, with subtle movements and slight changes in view angles. The interaction in the chat reflects reactions to the game scenario, and the person in the webcam continues to display focused engagement. [0:04:56 - 0:04:59]: The scene shifts to a different area, showing a tram station with urban surroundings, still under the night-vision filter. A tram evolves into view and comes to a stop. The HUD icons persist in the lower left corner, and the chat reactions vary with excitement and comments about the scene change. The person in the webcam seems to be providing commentary on the new scene. [0:05:00]: The scene advances to the tram interior, showcasing seats and windows from a first-person view. The night-vision filter persists, as does the interaction from the chat and the on-screen person’s reactions to the transition inside the tram.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person in the webcam window doing right now?",
        "time_stamp": "00:05:00",
        "answer": "C",
        "options": [
          "A. Narrating and interacting with a digital map.",
          "B. Reacting to a tram station scene.",
          "C. Waiting for the game to finish loading..",
          "D. Switching between different screens."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_275_real.mp4"
  },
  {
    "time": "0:07:00 - 0:07:20",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:03]: The video starts with a first-person perspective, showing a city intersection from inside a moving blue car. The car is situated on the left lane near the crosswalk, waiting for the traffic light to change. There are tall white buildings on either side of the street, and traffic lights are visible in the distance. To the left of the screen is a live feed of a streamer in the top corner. Some chat messages are displayed on the right side of the screen; [0:07:04 - 0:07:13]: The car begins to move forward through the intersection. The surroundings remain consistent with tall buildings on both sides of the street. The sky is overcast, indicating it might be early morning or late afternoon. The driver's view shows the straight path ahead, with fewer cars visible on the road; [0:07:14 - 0:07:19]: The car drives straight under an overpass, and the area becomes darker due to the shadows from the structure overhead. The street ahead continues straight, and minimal traffic is visible. The streamer remains engaged with the viewers, as shown in the live feed; [0:07:20]: The car veers slightly to the right and moves into a large vehicle parking area or garage. Inside, the light is dim and there are various vehicles, including white and green trucks, parked along the sides. A person is also visible standing near one of the trucks, and the car moves towards this person before coming to a stop. The live feed shows the streamer continuing to interact with the viewers. The chat messages persist on the right side of the screen.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the car doing right now?",
        "time_stamp": "00:07:13",
        "answer": "D",
        "options": [
          "A. Turning left at the intersection.",
          "B. Moving forward through the crosswalk.",
          "C. Driving straight under an overpass.",
          "D. Veering slightly to the right into a parking area."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_275_real.mp4"
  },
  {
    "time": "0:09:20 - 0:09:23",
    "captions": "[0:09:20 - 0:09:23] [0:09:20 - 0:09:23]: In a nighttime urban environment, a blue car is seen driving on a dark street with yellow lane markings. The vehicle stops and a person, clad in white clothing, appears near the driver's side door, seemingly after having been hit by the car. The individual on the ground is positioned sideways and motionless. In the top-left inset, a small window shows a person with headphones speaking animatedly into a microphone, indicating that this is a livestream. The chat window on the right contains multiple user comments reacting to the event. The in-game map and information show that this event is part of a video game.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the car doing right now?",
        "time_stamp": "0:09:21",
        "answer": "B",
        "options": [
          "A. Speeding down the street.",
          "B. Hit an electric scooter and stopping on the dark street.",
          "C. Turning at an intersection.",
          "D. Parking in a garage."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_275_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What brand's advertisement is visible on the back of the bus right now?",
        "time_stamp": "00:00:04",
        "answer": "D",
        "options": [
          "A. Toyota.",
          "B. Chevrolet.",
          "C. Nissan.",
          "D. Ford."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_451_real.mp4"
  },
  {
    "time": "[0:01:55 - 0:02:00]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What company's logo is visible on the left side of the building right now?",
        "time_stamp": "00:01:55",
        "answer": "D",
        "options": [
          "A. Intel.",
          "B. Tesla.",
          "C. AT&T.",
          "D. Optus."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_451_real.mp4"
  },
  {
    "time": "[0:03:50 - 0:03:55]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the current status of the traffic light ahead?",
        "time_stamp": "00:03:52",
        "answer": "D",
        "options": [
          "A. Red.",
          "B. Yellow.",
          "C. Flashing.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_451_real.mp4"
  },
  {
    "time": "[0:05:45 - 0:05:50]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What brand is visible on the building right now?",
        "time_stamp": "00:05:46",
        "answer": "D",
        "options": [
          "A. Citibank.",
          "B. Westpac.",
          "C. ANZ.",
          "D. Coleslocal."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_451_real.mp4"
  },
  {
    "time": "[0:07:40 - 0:07:45]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "Which brand's logo is visible on the right side of the road right now?",
        "time_stamp": "00:07:44",
        "answer": "D",
        "options": [
          "A. McDonald's.",
          "B. KFC.",
          "C. Subway.",
          "D. Starbucks."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_451_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "Which country's flag is hanging on the right side of the road now?",
        "time_stamp": "00:00:03",
        "answer": "C",
        "options": [
          "A. Australia.",
          "B. Italy.",
          "C. United States.",
          "D. UK."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_383_real.mp4"
  },
  {
    "time": "[0:02:04 - 0:02:09]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which store sign is currently visible on the right side of the road right now?",
        "time_stamp": "00:02:08",
        "answer": "C",
        "options": [
          "A. Victoria's Secret.",
          "B. Lululemon.",
          "C. Ann Taylor.",
          "D. Forever 21."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_383_real.mp4"
  },
  {
    "time": "[0:04:08 - 0:04:13]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "Which basketball-related sign is currently visible in the video right now?",
        "time_stamp": "00:04:11",
        "answer": "A",
        "options": [
          "A. NBA Store.",
          "B. Nike Store.",
          "C. Adidas Shop.",
          "D. Reebok Shop."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_383_real.mp4"
  },
  {
    "time": "[0:06:12 - 0:06:17]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which store sign is visible on the right side of the road right now?",
        "time_stamp": "00:06:14",
        "answer": "B",
        "options": [
          "A. Starbucks.",
          "B. Pret A Manger.",
          "C. Chipotle.",
          "D. McDonald's."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_383_real.mp4"
  },
  {
    "time": "[0:08:16 - 0:08:21]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which roads are currently visible in the video right now?",
        "time_stamp": "00:08:18",
        "answer": "B",
        "options": [
          "A. East 40th St and 3rd Ave.",
          "B. East 36th St and 5th Ave.",
          "C. East 42nd St and Madison Ave.",
          "D. East 30th St and Park Ave."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_383_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins with a view inside an art gallery. Various framed artworks are hung on white walls, prominently featuring a collection of photographs and paintings. Several pieces consist of abstract designs and dark backgrounds, while others are more detailed. People are seen moving around the space. [0:00:03 - 0:00:06]: The camera moves forward, offering a closer look at some of the displayed works. The artworks include a mix of photographic prints and a screen displaying text. One prominent frame contains green text on a black background that reads, \"WE BUY & SELL LUXURY.\" [0:00:07 - 0:00:10]: The camera focuses on an exhibit section labeled \"ARTSNITCH.\" This area displays four pieces of framed art, two of which feature digital or neon text. [0:00:11 - 0:00:12]: The camera angle shifts to capture more of the gallery interior, showing additional artwork along adjacent walls. Some of these pieces feature intricate, detailed designs. [0:00:13 - 0:00:14]: As the camera moves along the walls, more artwork comes into view. This includes detailed paintings with intricate patterns and a mix of dark and light backgrounds. [0:00:15 - 0:00:17]: The camera continues panning along the wall, showing a series of artworks that incorporate complex patterns and organic shapes. The lighting highlights the texture and detail of each piece. [0:00:18 - 0:00:20]: The video concludes with a close-up view of a large, intricate artwork that appears to be a symmetrical, detailed design in shades of yellow and white, alongside another abstract piece featuring a burst of colors.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What words are written on the painting in the lower left corner of the wall?",
        "time_stamp": "00:00:06",
        "answer": "C",
        "options": [
          "A. \"WE SELL ART\".",
          "B. \"BUY & SELL ART\".",
          "C. \"WE BUY & SELL LUXURY\".",
          "D. \"ART FOR SALE\"."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Counting",
        "question": "How many pieces of art paintings are displayed in the \"ARTSNITCH\" section?",
        "time_stamp": "00:00:10",
        "answer": "B",
        "options": [
          "A. Two.",
          "B. Three.",
          "C. Four.",
          "D. Five."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_464_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:22]: There are two large framed artworks on a white wall. The one on the left is a red and pink abstract piece depicting Mickey Mouse's head, while the one on the right features two white cartoonish hands pointing at each other on a blue background. Below these artworks, there are two colorful abstract canvases, one displaying vibrant flowers. [0:02:20 - 0:02:23]: To the left of the frames, there is a vertical colorful digital display, showing an underwater scene featuring a figure with blue and purple hues. Another framed canvas displays a toy-like painting. [0:02:21 - 0:02:25]: The underwater-themed digital art transitions, showing a more colorful view with a central figure having rainbow patterns on its face. Next to it, another digital portrait shows an animal with large antlers. [0:02:23 - 0:02:26]: Three digital frames are hanging on a white wall. The leftmost frame exudes gray-toned rural imagery; the center frame displays a vibrant depiction of a person with neon lighting; the rightmost frame continues to depict the antlered animal. [0:02:26 - 0:02:27]: On the far-left display, an artistic graphic portrait showing black and white text over a face is shown.  [0:02:26 - 0:02:28]: The far-right display transitions again showing more vibrant digital art pieces. [0:02:27 - 0:02:29]: As the view moves left, a fourth digital display shows a whimsical scene featuring a character holding an umbrella covered in colorful patterns. [0:02:28 - 0:02:30]: Hanging next to it is a black and white line drawing of a face. Underneath are colorful abstract squares some with prominent cartoon heads. [0:02:29 - 0:02:31]: A mix of digital frames and framed artworks adorn the wall being viewed in quick succession. The artworks on display range from monochrome portraits to vibrantly colorful digital paintings. [0:02:29 - 0:02:30]: A large painting with a black and white graphical pattern and iconic cartoon head. [0:02:31 - 0:02:34]: As the camera shifts, some attendees are seen interacting with the artwork. A person in a jacket crouches near the graphic painting, taking a picture.  [0:02:33 - 0:02:34]: The person crouching moves into a dynamic pose, and more attendees appear to be engaged. [0:02:34 - 0:02:36]: A bright dynamic scene, as more vibrant digital artworks are shown transitioning with bright light flashes ensuring the continuity to a digital screen on the right displaying pinkish tones. [0:02:35 - 0:02:36]: Attendees continue engaging; one person is seen enthusiastically explaining something to another person. The central figure moves forward, showing interaction dynamics. [0:02:36 - 0:02:39]: New artwork is visible with themes varying from minimalistic black lines on white to vivid color patterns. The digital displays continue showing different vibrant artworks, moving around another digital display. [0:02:37 - 0:02:39]: The group interaction continues, with several people involved in discussions or viewing displays. One attendee holds a drink while conversing, and another closely interacting with the art display. [0:02:38 - 0:02:40]: The viewing angle broadens revealing more parts of the exhibit with attendees mingling around different art pieces. The digital screens display dynamic images and changing artworks capture viewers’ attention.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action is being performed by a person in a jacket near the graphic painting?",
        "time_stamp": "0:02:34",
        "answer": "A",
        "options": [
          "A. Taking a picture.",
          "B. Painting.",
          "C. Cleaning the artwork.",
          "D. Pointing at the artwork."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_464_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:41]: The video begins with a view of an art gallery wall. A framed artwork depicting a muscular man holding a basket of lemons is on the left side. Below the artwork, the text \"Lemon Lad BRAND\" is visible. To the right of this painting, there are two alternatingly placed white shelves on the wall. The lower shelf holds a brown sculpture resembling a gourd. [0:04:41 - 0:04:45]: The scene continues to focus on the two shelves, with the brown gourd-shaped sculpture becoming more centered in the footage. Next to the sculpture, to the right, there's another framed painting of an old-fashioned car below a large, orange dome structure, and palm trees in the background. [0:04:45 - 0:04:48]: The camera pans up to reveal another blue, sculptural object positioned on the upper shelf. Next to these shelves, the framed painting showing the orange dome continues to be visible. [0:04:48 - 0:04:51]: The video captures an overall view of the blue sculpture on the shelf. Adjacent to it, the painting with the orange dome structure is clearly framed against a dark sky with searchlights.  [0:04:51 - 0:04:56]: Now, to the right side of the orange dome painting, another framed artwork depicting a variety of vibrant, abstract flowers comes into view. Both paintings are mounted on a white wall, and below them, a black folding chair is positioned.  [0:04:56 - 0:04:58]: The camera angle shifts to show both the 'orange dome' painting and the 'flower' painting fully. The black folding chair remains centered at the bottom. [0:04:58 - 0:05:00]: Finally, the video provides a broader view of the gallery wall, bringing several elements into frame: the \"Lemon Lad\" artwork on the far left, the blue sculptural piece on the top shelf, the brown gourd-like sculpture on the lower shelf, and both adjacent paintings. A second black folding chair is barely visible to the right.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What text is visible on the artwork of the muscular man holding a basket of lemons?",
        "time_stamp": "0:04:40",
        "answer": "B",
        "options": [
          "A. \"Lemon Art BRAND\".",
          "B. \"Lemon Lad BRAND\".",
          "C. \"Lemon Grove BRAND\".",
          "D. \"Lemon Orchard BRAND\"."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_464_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:03]: The video begins with a view of four paintings hanging on a white wall. The top left painting depicts a stormy scene with lightning striking down over a dark road lined with trees, and it is predominantly gray and black. To its right, the top right painting shows a vivid orange and red sunset over a hilly landscape. Below these two larger paintings are two smaller ones, each showing a monochromatic, foggy landscape with twisted trees. To the right of these, a horizontal painting features abstract splashes of black, blue, yellow, and orange. [0:07:03 - 0:07:06]: The camera shifts slightly to the left, revealing more artwork on the white wall. A colorful abstract painting with swirling patterns in green, blue, and other vibrant colors is visible to the left, alongside a small bronze sculpture placed on a black pedestal.  [0:07:06 - 0:07:09]: The focus shifts further left to reveal a large painting of a vivid sunset with orange, red, and yellow hues near the top. Below it, two vibrant pieces with vivid colors and apparent abstract themes are displayed. The lower left painting has bold paint strokes of pink, yellow, white, and black, depicting an abstract scene, while the center artwork shows colorful circular shapes on a blue background. [0:07:09 - 0:07:12]: The camera continues to move left, revealing more framed paintings. Two paintings featuring complex, abstract faces in rich colors are displayed. The top painting has swirling patterns in blue, yellow, and white, while the bottom painting features a face with elaborate gold, red, and white patterns. To their left, there are more colorful, nature-inspired landscapes in predominantly green, pink, and blue hues. [0:07:12 - 0:07:15]: The view pans further left, showing a series of landscape paintings in vibrant shades of pink, green, and blue. These depict various fields of flowers and pathways, capturing the beauty of nature. There are also paintings of different scenes like markets and urban views with intricate details and a rich color palette. [0:07:15 - 0:07:18]: The camera continues along the same trajectory, adding more artworks to the visible collection. The highlighted paintings capture wider countryside scenes with fields of vibrant wildflowers, a winding path, and distant mountains under pink and blue skies. A well-detailed urban scene with structures and streets painted with intricate details is also visible. [0:07:18 - 0:07:20]: The video concludes with the camera focusing on two landscape paintings filled with vivid floral blooms, featuring similar pathways leading through colorful fields and surrounded by lush greenery. The top painting also showcases a vibrant field, with the sky being depicted in soft, blended hues of pink, blue, and purple.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is depicted in the top left painting on the white wall?",
        "time_stamp": "00:07:00",
        "answer": "B",
        "options": [
          "A. A vivid sunset over a hilly landscape.",
          "B. A stormy scene with lightning over a dark road.",
          "C. A monochromatic, foggy landscape with twisted trees.",
          "D. An abstract painting with splashes of colors."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_464_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:20]: The video is set in a virtual environment featuring what appears to be a character creation screen from a video game. A person with blond hair, a black belt, black pants, and white shoes stands in the center of the room. They seem to be meant as the player's character. The room is dimly lit with shelves and equipment, predominantly showcasing various items. To the right of the screen, there is a menu that shows different customization options, seemingly for editing the character's appearance. There is another character visible only in partial view on the right, dressed in a dark outfit with high-heeled boots.  At the top left, a small in-game camera display shows another person, possibly the player, seated at a desk and focused on the screen. This secondary display features changing expressions and movements of this individual, suggesting active engagement with the customization menu. Various user interface elements are scattered across the screen, including text chats and clickable buttons indicating options like \"Character Creation\" and various clothing and accessory choices. The background consists of a partially visible metal gate, adding to the industrial ambiance of the environment. Through the video, there are changes in the character's clothing, moving from an orange safety vest to a plain black polo shirt, indicating active customization of the character.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person with blond hair doing right now?",
        "time_stamp": "00:00:20",
        "answer": "A",
        "options": [
          "A. Changing clothing.",
          "B. Sitting at a desk.",
          "C. Walking around the room.",
          "D. Exiting through the metal gate."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_281_real.mp4"
  },
  {
    "time": "0:03:00 - 0:03:20",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:06]: The video begins with a first-person perspective of a person standing in a room. The character on screen is wearing a green safety vest, a headset, and goggles. The backdrop consists of a wooden panel wall, a shelf with various items on it, including a small television, and a neatly made bed. In the upper left corner, there is an inset screen showing a person in a room with multiple monitors and LED lights. The chat section on the right side of the screen shows real-time messages from viewers, interacting with the stream. [0:03:07 - 0:03:14]: Gradually, the character remains in the same spot, with minimal movement. The chat section continues to be active with similar messages. The inset screen in the left corner shows the person talking and observing the scene attentively. [0:03:15 - 0:03:16]: A new prompt pops up in the middle of the screen, asking if the player is sure about saving their clothing changes. The options include \"Cancel,\" \"Save,\" and \"Discard.\" [0:03:17 - 0:03:19]: The scene transitions to the character starting to walk towards the exit. The inset screen and chat section remain unchanged. [0:03:20]: The perspective shifts slightly, showcasing the character still walking inside the room, now seen moving past various clothing displays and shelves. Other characters, presumably NPCs, can be seen standing around in the background, indicating the setting is inside a shop or store.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the character doing right now?",
        "time_stamp": "00:03:19",
        "answer": "B",
        "options": [
          "A. Standing still in a room.",
          "B. Walking towards the exit.",
          "C. Interacting with the chat section.",
          "D. Changing clothing."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_281_real.mp4"
  },
  {
    "time": "0:06:00 - 0:06:20",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:20]: The video is taken from the first-person perspective of someone sitting in a vehicle at night, driving through a city. The setting is urban with tall buildings illuminated by streetlights and colorful signage. The streets are relatively empty. The vehicle the person is driving is a large van with prominent red lights on its rear. The interior of the van is dark, and the dashboard has illuminated controls.  The video starts with the van turning left onto a street with red and blue lights illuminating a building on the left. As the van continues driving, it passes multiple buildings with various light displays. The streets have typical urban elements such as streetlights, commercial buildings, and sidewalks. The exterior view shows the van making turns and driving straight on the broad, well-lit road.  At [0:06:10-0:06:13], the perspective briefly changes for a second to a head-on view from inside the van, showing the driver's hands on the steering wheel and a brightly lit dashboard. The van later stops at a traffic light and then proceeds once the light turns green.  As the van drives, the scene shifts forward with buildings continuing to line both sides of the street. The cityscape is very active with artificial light sources illuminating the buildings and streets. The view switches back and forth between the van's rear with the red outline of lights and the dashboard views. The video ends with the van driving further down the street.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the driver do just now?",
        "time_stamp": "00:06:08",
        "answer": "A",
        "options": [
          "A. Turned the van left onto a street.",
          "B. Stopped at a traffic light.",
          "C. Drove straight down the street.",
          "D. Checked the dashboard."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_281_real.mp4"
  },
  {
    "time": "0:09:00 - 0:09:20",
    "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:03]: The video starts with the person standing in front of a building at night. There is a man wearing a white shirt and cap standing on the sidewalk, and the player character approaches him. The area is lit by streetlights, and there are steps to the right leading up to a doorway. [0:09:04 - 0:09:07]: The person approaches closer, engaging in a conversation with the man in the white shirt. A dialog window appears, identifying the man as Harry Miller, with options to \"Open Stock\" or \"Leave Conversation.\" [0:09:08]: The camera is focused on Harry Miller, showing a close-up of his face as the conversation continues, with the same options visible on the screen. [0:09:09]: The screen transitions to an inventory interface, displaying multiple slots containing different items. The player character's inventory is visible on the left, showing personal information and items. Harry Miller's stock is displayed on the right. [0:09:10]: The player character now has possession of a medium-sized cardboard box with blue tape on it. He is standing back on the sidewalk near the building. [0:09:11]: The character walks away from the building, holding the box. A white car is parked nearby on the street, and the area is lined with buildings and trees. [0:09:12 - 0:09:13]: The character continues to walk along the sidewalk, approaching a large dark van with \"Prime\" written on the side. The area remains well-lit by streetlights. [0:09:14 - 0:09:16]: The character reaches the back of the van and interacts with it, opening the cargo bay. Another inventory interface appears displaying both the character's inventory and the van's cargo space. [0:09:17]: The interface shows various items placed in the cargo space. The character completes the action and closes the inventory interface. [0:09:18]: The character steps back from the van and begins walking toward the cab of the vehicle, preparing to enter the driver’s seat. [0:09:19]: The character approaches the driver’s door, opens it, and starts climbing into the seat. [0:09:20]: The video ends with the character settled in the driver’s seat, preparing to drive. The street remains visible through the windshield with another stationary vehicle in front of the van.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the character just do?",
        "time_stamp": "00:09:15",
        "answer": "B",
        "options": [
          "A. Walked towards a white car.",
          "B. Opened the door of the van and climbed into the driver's seat.",
          "C. Stood in front of a building with Harry Miller.",
          "D. Placed items in a cardboard box."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_281_real.mp4"
  },
  {
    "time": "0:11:40 - 0:11:46",
    "captions": "[0:11:40 - 0:11:46] [0:11:40 - 0:11:41]: A person stands by the corner of a building, wearing a green safety vest and a headset. They are holding a phone or small tablet looking down. The view is in a cityscape at night, with buildings and streetlights illuminating the scene. A large screen on the left shows another figure, possibly the same individual, seemingly engaged in streaming or a video call. [0:11:41 - 0:11:42]: The person begins to move to the right side of the frame toward a black, rectangular structure, possibly a vehicle or a storage unit. Their back is facing the camera, continuing to hold the item in their hand. [0:11:42 - 0:11:43]: The movement continues as the person approaches the black object with a firm grip on their item. The city lights and buildings remain visible in the background. [0:11:43 - 0:11:44]: The person stands directly in front of the black object, leaning slightly forward, possibly inspecting or interacting with it. [0:11:44 - 0:11:46]: Now, within an inventory interface displayed on the screen, different grids showing categories like \"Player,\" \"Backpack,\" \"Cargo,\" and \"Ground\" are visible. The individual seems to be organizing or moving items between these categories. The streaming screen on the left shows the person concentrating on the task.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:11:38",
        "answer": "D",
        "options": [
          "A. Standing by a corner of a building.",
          "B. Moving to the right side of the frame.",
          "C. Inspecting or interacting with a black object.",
          "D. Organizing items within an inventory interface."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_281_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:14]: Throughout the video, the main focus is a large black vehicle with a rectangular shape and red outlines. The vehicle is being driven on a multi-lane highway that is lined with green trees, powerlines, and light poles. The highway leads through a rather wooded area, and traffic is light, with a few other vehicles visible ahead and in adjacent lanes. The overall scene appears to capture various perspectives from this ongoing drive, with the camera situated directly behind the vehicle. [0:00:02 - 0:00:12]: Side by side with the main scene, a smaller inset in the top-left corner features a person viewed from a frontal angle. The individual is wearing a headset and appears to be either focused on a screen or communicating during the drive. Below the individual, a game-like interface is superimposed over the video, displaying changing icons and text, indicating some user interactions or selections being made, especially towards the midsection of the video where the screen turns green multiple times, likely signaling menu navigation or selections by the user. [0:00:06 - 0:00:14]: Various radio or sound icons appear on the screen during these green overlays, suggesting that the driver is choosing media options while driving. This is revealed by the circular interface overlaying the driving scene, each time showing icons like music notes, microphones, or headphones.  [0:00:08], [0:00:15], and [0:00:19]: During intervals, the camera returns to the normal driving view, shifting briefly back, allowing an unobstructed focus on the vehicle progressing on the clear path ahead. [0:00:12 - 0:00:20]: The drive continues on the straight road as the video frames capture steady and uneventful highway driving, maintaining the focus on the moving vehicle and the driver's interactions through the interface.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the driver doing right now?",
        "time_stamp": "00:00:16",
        "answer": "D",
        "options": [
          "A. Talking to a passenger.",
          "B. Looking at the road ahead.",
          "C. Adjusting the rearview mirror.",
          "D. Selecting media options."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_283_real.mp4"
  },
  {
    "time": "0:02:40 - 0:03:00",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:58]: In the video, two individuals are present in an urban environment. The person on the left is wearing a neon yellow safety vest over a black outfit, white goggles, and headphones. They stand near a potted plant situated in the center of a tiled sidewalk. The person on the right, who faces the first individual, is wearing a black and white striped top, denim shorts, and white sneakers. Behind them, there is a modern building with large windows, on which green emblems and signs related to healthcare are displayed. In the background, a partially obscured cityscape with tall buildings is visible, and a vehicle is parked on the street. The video primarily captures their interaction as they converse. The person on the left occasionally gestures animatedly, while the individual on the right mostly maintains a relaxed posture. [0:02:59 - 0:03:00]: Throughout the scene, both individuals continue their conversation without any significant changes in their positions or surroundings. The ambiance remains consistent with few people and objects visible in the background.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person on the right doing right now?",
        "time_stamp": "00:03:00",
        "answer": "D",
        "options": [
          "A. Gesturing animatedly.",
          "B. Walking towards the building.",
          "C. Sitting near the potted plant.",
          "D. Standing still."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_283_real.mp4"
  },
  {
    "time": "0:05:20 - 0:05:40",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:23]: As the video starts, a delivery van is driving down a narrow street with buildings on either side. The camera shows a first-person view from the cabin of the vehicle, with a view ahead towards an underground parking entrance. A palm tree grows on the sidewalk to the left. [0:05:24 - 0:05:27]: The van continues to approach the parking entrance. A low barrier partially blocks the entrance, and the road ahead slightly curves to the left. [0:05:28 - 0:05:31]: The van gets closer to the entrance, and the camera focuses more on the red structure containing the barrier. Some signage on the building says \"We aim\" with partial words visible. [0:05:32 - 0:05:35]: Upon reaching the entrance, the barrier lifts automatically, and the van begins entering the parking area marked with “MAX CLEARANCE” overhead. To the left, there is a small control booth. [0:05:36 - 0:05:39]: The signs on the building become clearer, revealing \"We aim not to lose.\" The van fully enters the parking area, angling toward a left turn. [0:05:40 - 0:05:43]: Inside the parking area, the van turns left. On-screen text updates to indicate different incoming communications. The ground and walls inside the parking area are concrete, and there is a delivery bay painted red straight ahead. [0:05:44 - 0:05:47]: The van completes the left turn inside the parking bay, positioning itself parallel to the left wall. The camera continues to capture this first-person driving perspective. [0:05:48 - 0:05:51]: After coming to a stop inside the parking space, the view from inside the van shows it parked snugly against the wall on the left. [0:05:52 - 0:05:55]: The driver gets out of the van, revealing a delivery person in a high-visibility vest. The perspective now includes a mix of the inside of the parking area and the vehicle's interior. [0:05:56 - 0:05:59]: Stepping away from the vehicle, the camera follows the driver as they walk towards the back of the van. The background includes various parked vehicles and the structure of the parking area. [0:06:00 - 0:06:03]: The driver prepares to unload the van, standing at the back door. The van is now stationary, and the bustling environment of the parking garage is apparent, with various background details such as pipes and wall markings becoming clearer.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the driver do just now?",
        "time_stamp": "00:05:41",
        "answer": "D",
        "options": [
          "A. Drove out of the parking area.",
          "B. Reached the parking entrance.",
          "C. Entered the small control booth.",
          "D. Got out of the van."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_283_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:05]: The video starts with a first-person perspective of a van driving through a city street. The camera view is from behind the van. The van is heading towards an intersection where the light is red. On the left side of the screen, a small image-in-image displays a person, possibly the driver, with chat messages appearing to the right, indicating streaming activity. The street is lined with buildings and various shopfronts. [0:08:05 - 0:08:10]: The van progresses towards and stops at the intersection with a red traffic light. A dark-colored car is seen driving on the left lane crossing the intersection. [0:08:10 - 0:08:15]: The van continues moving forward after the intersection and passes by a few buildings with varying facades. The camera is fixed behind the van, showing a straight road ahead with more infrastructure in the distance. [0:08:16 - 0:08:20]: The scene shows the van driving under a series of large, elevated structures that appear to be bridges or overpasses. The background includes advertisements and an assortment of urban elements. Occasionally, the display briefly switches to a user interface showing audio options, overlaying the driving scene.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the van doing right now?",
        "time_stamp": "00:08:15",
        "answer": "D",
        "options": [
          "A. Stopping at a red traffic light.",
          "B. Reversing towards the intersection.",
          "C. Turning left at the intersection.",
          "D. Moving forward past the intersection."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_283_real.mp4"
  },
  {
    "time": "0:10:20 - 0:10:33",
    "captions": "[0:10:20 - 0:10:33] [0:10:20 - 0:10:23]: The video starts with a first-person perspective showing a screen with a \"Trucking Progression\" interface. The UI displays several missions with information like destination, reward, and status for various locations such as Los Santos Hospital and Sandy Shores Hospital. There are also multiple small icons and text columns. [0:10:23 - 0:10:25]: The interface remains on the screen, seemingly static. The viewer’s perspective indicates they are observing the details of the missions listed. [0:10:26 - 0:10:27]: There is slight movement within the video, suggesting the person is scrolling or navigating through the menu, but the primary view does not change drastically. [0:10:28 - 0:10:29]: A new screen appears within the interface. This screen provides additional options or settings related to the trucking missions, including details about distance, time, deliveries, and payout. [0:10:30]: The view quickly returns to the previous \"Trucking Progression\" mission list screen, similar to the initial frames. [0:10:31 - 0:10:32]: The perspective shifts from the screen and reveals an interaction with another person. The camera now shows a person in a white shirt holding a tablet or clipboard likely in the process of discussing or handling some tasks. [0:10:33]: The video concludes with a closer view of the person in the white shirt as they appear to be checking something on the tablet or clipboard.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person in the white shirt doing right now?",
        "time_stamp": "00:10:26",
        "answer": "D",
        "options": [
          "A. Scrolling through a menu.",
          "B. Navigating the \"Trucking Progression\" interface.",
          "C. Discussing missions with another person.",
          "D. Checking something on a clipboard."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_283_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins with a blurred, abstract background of dark purple, black, and bluish tones with some texture. As the seconds progress, the background remains abstract with slight variations in color intensity and features what appears to be static or interference lines. [0:00:02 - 0:00:04]: The display transitions to a clear image of a vibrant logo. The logo has a yellow, red, and blue triangular shape with the text \"GRAVITY THROTTLE RACING\" written in bold, white letters. [0:00:05 - 0:00:12]: The video then shows two miniature models of people standing side by side against a bright orange background with desert elements, such as cacti and a distant mountain range. The \"GRAVITY THROTTLE RACING\" logo remains in the backdrop. Both figures are holding microphones; the one on the left wears a brown jacket and gray pants, while the other, on the right, wears a white jacket and tan pants. [0:00:13 - 0:00:20]: The figures appear to be engaging in conversation, holding their microphones towards each other. There are no significant movements, but their postures suggest they are communicating, possibly interviewing each other. The backdrop remains consistent with the previously described elements, featuring the logo and desert landscape.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the license plate number of the car currently shown in the video?",
        "time_stamp": "00:00:30",
        "answer": "D",
        "options": [
          "A. 18.",
          "B. 197.",
          "C. 56.",
          "D. 46."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_494_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:22]: The video shows a snowy race track with several toy cars positioned in rows that appear ready for a race. The background includes snow-covered hills and a blue sky. Several cars are lined up on the left side of the track, and a digital starting sign with green lights is visible on the right side. [0:03:23 - 0:03:27]: The toy cars start to accelerate, and the orange and green cars move forward from the rest. The terrain remains snowy, with the cars now beginning to navigate through the initial curves of the track. Other cars in the row follow closely. [0:03:28 - 0:03:29]: The cars continue to race onward, moving further along the snow-covered and curvy track. The background reveals more of the snowy hilly terrain with sparse, miniature trees and a blue sky. The green and orange cars maintain their lead, while additional cars become visible on the track behind them. [0:03:30 - 0:03:31]: The scene transitions to an overview of the track's layout, which shows the winding path the cars are taking. The track features multiple curves and slopes, with the main competitors still being the orange and green cars. [0:03:32 - 0:03:33]: The cars are now seen racing on a sharp incline of the track, which elevates and wraps around a hilly landscape. The orange and green cars remain in the lead, while other cars maneuver the curve behind them. The entire race environment maintains a snowy and mountainous theme. [0:03:34 - 0:03:35]: The track showcases a long, snaking path with a deep curve. The toy cars speed through, maintaining their relative positions. More of the rugged environment becomes visible, featuring rocky formations and minimal plant life, in addition to the snow. [0:03:36 - 0:03:37]: The cars continue around the elevated, curved path. The road and surrounding terrain look steep and rocky. The green and orange cars remain at the front of the pack while the blue car joins the leading group. [0:03:38 - 0:03:39]: The race persists as the cars enter a tunnel segment of the track. A mix of rocky and snowy background transitions to semi-arid terrain, indicating a change in the racing environment. The leading cars maintain their speed and navigate the challenging course.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the car that is in the lead along with the green car?",
        "time_stamp": "00:03:39",
        "answer": "D",
        "options": [
          "A. Blue.",
          "B. Red and black.",
          "C. Yellow and red.",
          "D. Black and blue."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Event Understanding",
        "question": "Which racing car won first place in this competition?",
        "time_stamp": "0:05:51",
        "answer": "D",
        "options": [
          "A. The pink car.",
          "B. The dark green car.",
          "C. The light green car.",
          "D. The green car."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_494_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:41]: A white racing car is navigating a curved, uphill road next to a rocky cliff. There are two large advertising signs positioned on the rock face, one labeled \"Bluechrome Racing\" with a blue and white logo of a flame, and another advertisement with red elements slightly visible in the background and partly obscured by a tree. [0:06:42 - 0:06:45]: The scene transitions to a different perspective showing a winding mountain road covered in snow. There are several cars on this road, including a brown truck navigating a sharp incline. Below this road, viewers can see a red car parked at a roadside edge, trees dotting the landscape, and a \"Replay\" branding element at the bottom right corner. [0:06:46 - 0:06:50]: Continuing on the snowy road, the brown truck descends towards a sharp curve, with the camera perspective providing a top-down view. There are two other cars, blue and brown, visible in the distance, moving around the bend, with pine trees lining both sides of the road and snow-covered terrain. [0:06:51 - 0:06:57]: The blue and brown cars speed around the bend of the mountain road, with each still in movement. The setting emphasizes the sharpness of the curve with varying elevations, rocks, and decreasing snow as it transitions into a more asphalt-covered path. By the end of this segment, both the cars are starting to converge side by side at close quarters. [0:06:58 - 0:06:59]: The perspective switches to a closer, more detailed view of multiple cars making their way up what seems to be another snowy incline. The foreground includes cars with appearances suggesting a racing competition, amidst a backdrop of snow and a blue sky. The formations indicate a challenging climb typical of high-altitude mountain roads.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Which racing car won first place in this competition?",
        "time_stamp": "0:06:36",
        "answer": "D",
        "options": [
          "A. The pink car.",
          "B. The dark green car.",
          "C. The light green car.",
          "D. The brown car."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_494_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:10:00]",
    "captions": "[0:10:00 - 0:10:20] [0:10:00 - 0:10:02]: Two small cars, one green and one blue, race on a smooth, winding road surrounded by a dry, shrub-dotted landscape. The road is bordered by a cliffside on one side and barren terrain on the other. The green car is ahead of the blue one, both cars are oriented forward, heading towards an upward slope. [0:10:03 - 0:10:04]: A digital display board appears on the left side of the road, showing race information. The green car soon disappears from view while the blue one follows, moving past the board. The surrounding landscape remains unchanged. [0:10:05 - 0:10:11]: The scene shifts to a snowy mountainous area with multiple cars on a multi-lane track. Several cars, including the green and blue ones, race downhill on snow-covered slopes. Some cars are moving uphill, while others navigate the winding slopes. A crowd of people stand beside the track, watching the race. The background features high, snow-capped peaks and sparse trees. [0:10:12 - 0:10:13]: The racing continues in the snow-covered mountains, with cars maneuvering around the bends of the track. The green car leads, followed closely by the blue car. The track twists and turns, and more trees become visible as the landscape slopes downward. [0:10:14 - 0:10:19]: The viewpoint changes to an aerial view over a different section of the track, situated along the edge of a steep cliff with lush green terrain below. A train is seen traveling parallel to the road. The green car continues to race along the winding road, followed by other cars. The road descends and curves around the cliffside, with trees and billboards visible on the landscape.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Which racing car won first place in this competition?",
        "time_stamp": "0:06:36",
        "answer": "A",
        "options": [
          "A. The green car.",
          "B. The dark green car.",
          "C. The light green car.",
          "D. The brown car."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_494_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:01]: The video begins with a person in a blue t-shirt, holding a small red object in his left hand. The kitchen background has white brick walls, blue cabinets, various utensils, and a mid-sized window with a view of outdoor foliage. There’s an assortment of colorful vegetables and spices on the counter in the foreground. [0:02:01 - 0:02:02]: The person holds the red object closer to view, presumably inspecting it. The expression on their face seems focused as they glance towards the counter. Behind them, the word \"COOK\" is displayed on a shelf with various jars and bottles. [0:02:02 - 0:02:03]: The person turns slightly to the side, looking at several pots on the stove in front of them. The pots are of varying sizes and appear to contain different ingredients. The layout of the kitchen is designed for efficient cooking with all necessary utensils and ingredients within arm’s reach. [0:02:03 - 0:02:04]: The person starts moving the red object towards the pots on the stove. There are three pots, one with boiling liquid and the other two with ingredients being cooked.  [0:02:04 - 0:02:05]: The frame shifts to an aerial view of the cooking surface. There are four burners, two of which have pots on them. One pot contains eggs in water while another pot has a darker liquid with some solid food items. On the left side of the counter, there are various chopped vegetables and other cooking ingredients. [0:02:05 - 0:02:06]: The person begins moving some ingredients from the cutting board to one of the pots. Their movements are precise, suggesting they are experienced in cooking. The overall setup of the kitchen remains consistent, with clean counters and organized utensils. [0:02:06 - 0:02:07]: The person adds more ingredients to the pots on the stove, constantly observing and adjusting the contents as they cook. The aerial view continues to show the detailed arrangement of everything on the counter. [0:02:07 - 0:02:08]: The person uses a spoon to stir the contents of one of the pots. The close-up perspective highlights the careful attention they pay to the cooking process. Simultaneously, they seem to be making sure all ingredients are cooking evenly. [0:02:08 - 0:02:09]: The view shifts again to a front perspective, where the person is adjusting the heat on the stove. The blue cabinets and white-brick wall in the background continue to add an aesthetic touch to the modern kitchen setup. [0:02:09 - 0:02:10]: The person appears to be explaining something, possibly giving instructions or tips on cooking. Their focus alternates between the stove and the camera. [0:02:10 - 0:02:11]: The focus shifts back to the stove, where the person holds two pieces of cauliflower, examining them before placing them into a pot. The arrangement of kitchen utensils remains meticulous and practical. [0:02:11 - 0:02:12]: They add the pieces of cauliflower to the pot and stir the contents again, ensuring all ingredients blend well. The attention to detail continues to be evident as they manage multiple pots simultaneously. [0:02:12 - 0:02:13]: The individual continues to cook and stir, focusing on every detail. The kitchen's overall arrangement, with plates, jars, and knives neatly organized, enhances the cooking efficiency. [0:02:13 - 0:02:14]: The person checks the contents of the other pots, making sure everything is cooking properly. The kitchen's aesthetic, with the mix of modern and rustic elements, provides a visually pleasing background. [0:02:14 - 0:02:15]: The person tastes the food, adjusting the seasoning if necessary. The focus on perfection is evident as they ensure the dish meets certain standards. [0:02:15 - 0:02:16]: The frame returns to an aerial view, highlighting the stirring action in one of the pots. The dark liquid and noodles are visible, being carefully blended with a spoon. [0:02:16 - 0:02:17]: The person continues stirring the pot, making sure the noodles are cooked evenly. The aerial perspective provides a clear view of the cooking process, showing the consistency and texture of the food. [0:02:17 - 0:02:18]: The person stirs more vigorously, perhaps to achieve a specific consistency. The organized layout of the cooking tools and ingredients remains evident in this frame. [0:02:18 - 0:02:19]: The person continues to monitor the cooking process",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person holding in their right hand at the beginning of the video?",
        "time_stamp": "0:02:01",
        "answer": "B",
        "options": [
          "A. A green vegetable.",
          "B. A red bottle cap.",
          "C. A blue utensil.",
          "D. A white jar."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What are the colors of the cabinets and the walls in the kitchen?",
        "time_stamp": "0:02:40",
        "answer": "B",
        "options": [
          "A. Red cabinets and white walls.",
          "B. Blue cabinets and white brick walls.",
          "C. White cabinets and blue walls.",
          "D. Green cabinets and white brick walls."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_42_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:01]: In a bright kitchen with a prominent green plant by the window, a person in a blue shirt is standing at a wooden countertop. On the countertop, there are various kitchen utensils and cooking appliances, including a frying pan on the stove, two pots, a bowl with a spatula, and a set of knives in a holder. The person appears to be engaged in cooking. [0:06:01 - 0:06:02]: The person picks up a bowl filled with shredded ingredients, perhaps cheese or vegetables, from the countertop and holds it over the stove. [0:06:02 - 0:06:05]: The scene shifts to a wider view of the kitchen. A white brick wall with shelves holding plates, jars, and utensils is in the background. Decorative letters spelling \"COOK\" are prominently displayed on the top shelf. The person continues to stand in front of the stove, holding the bowl and preparing to pour its contents into a pan. On the counter in the foreground, there are more cooking utensils and ingredients. [0:06:05 - 0:06:06]: The person now has their focus on the pan while carefully adding the contents from the bowl. The scene remains in the same kitchen setup with the background unchanged. [0:06:06 - 0:06:07]: The person looks down intently at the pan while adding the ingredients from the bowl, making sure to distribute everything evenly. [0:06:07 - 0:06:10]: The view returns to the previous angle in the kitchen. The person is adjusting the pans and pots on the stove. The countertop remains cluttered with various kitchen essentials, and the green plant in the background adds a touch of nature to the setting. [0:06:10 - 0:06:11]: The person leans towards a pan on the stove and begins adding more ingredients from the bowl. The view remains focused on the cooking area with the same background elements. [0:06:11 - 0:06:13]: Close-up of the person’s hand holding the bowl and actively adding the shredded ingredients into a frying pan on the stove. The surrounding countertop shows multiple kitchen tools and ingredients, indicating a busy cooking session. [0:06:13 - 0:06:15]: The hand continues to add shredded ingredients from the bowl into the frying pan. Adjacent to the pan are spatulas, ladles, and other cooking essentials within arm's reach. [0:06:15 - 0:06:17]: The person's hand dips back into the bowl to grab another handful of ingredients, then proceeds to sprinkle them into the frying pan. The stovetop shows two additional pots, one covered and one boiling. [0:06:17 - 0:06:18]: A top-down view of the stove reveals three active burners. One pot has a lid and contains boiling food items, and the frying pan is filled with the shredded ingredients that are being cooked. The person's hand blurs slightly as it moves about, indicating continuous activity. The countertop holds various additional kitchen supplies and ingredients for the meal being prepared. [0:06:18 - 0:06:19]: The perspective shifts back to a wider shot of the person in front of the stove in the kitchen. The brick wall with shelves is still visible, along with the decorative \"COOK\" letters and multiple kitchen items on the countertop. The person seems to be finishing up their task and turning away from the stove, possibly to get more ingredients or tools.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What does the person hold over after touching the handle of the pan?",
        "time_stamp": "0:06:03",
        "answer": "B",
        "options": [
          "A. A frying pan.",
          "B. A bowl filled with shredded ingredients.",
          "C. A pot.",
          "D. A set of knives."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Counting",
        "question": "How many active burners are revealed in the top-down view of the stove?",
        "time_stamp": "0:06:25",
        "answer": "C",
        "options": [
          "A. One.",
          "B. Two.",
          "C. Five.",
          "D. Four."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Text-Rich Understanding",
        "question": "What do the decorative letters on the top shelf spell?",
        "time_stamp": "0:06:05",
        "answer": "C",
        "options": [
          "A. EAT.",
          "B. CHEF.",
          "C. COOK.",
          "D. FOOD."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_42_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the car parked near the Diner right now?",
        "time_stamp": "00:00:04",
        "answer": "D",
        "options": [
          "A. Blue.",
          "B. Red.",
          "C. Yellow.",
          "D. Pink."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_481_real.mp4"
  },
  {
    "time": "[0:00:45 - 0:00:50]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What object is the track passing under right now?",
        "time_stamp": "00:00:48",
        "answer": "C",
        "options": [
          "A. A chair.",
          "B. A table.",
          "C. A bed.",
          "D. A couch."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_481_real.mp4"
  },
  {
    "time": "[0:01:30 - 0:01:35]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the car on top of the cabinet right now?",
        "time_stamp": "00:01:30",
        "answer": "B",
        "options": [
          "A. Red.",
          "B. Blue.",
          "C. Green.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_481_real.mp4"
  },
  {
    "time": "[0:02:15 - 0:02:20]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What object is the track curving around right now?",
        "time_stamp": "00:02:19",
        "answer": "C",
        "options": [
          "A. A couch.",
          "B. A table.",
          "C. A sofa.",
          "D. A window."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_481_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:03:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the walls near the track right now?",
        "time_stamp": "00:03:00",
        "answer": "B",
        "options": [
          "A. White.",
          "B. Beige.",
          "C. Blue.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_481_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taking place just now?",
        "time_stamp": "00:00:10",
        "answer": "C",
        "options": [
          "A. The individual prepared fresh lettuce by washing it thoroughly in a sink.",
          "B. The individual took lettuce from a drawer, cooked it briefly, and then placed it on a preparation surface.",
          "C. The individual retrieved spinach from a compartment, cooked it on a flat top grill, and seasoned it.",
          "D. The individual prepared a salad by chopping tomatoes and cucumbers and adding them to a bowl."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_413_real.mp4"
  },
  {
    "time": "[0:02:56 - 0:03:06]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now in the kitchen?",
        "time_stamp": "00:03:06",
        "answer": "B",
        "options": [
          "A. The cook selected a frying pan, sautéed ingredients, and served them with a sauce.",
          "B. The cook retrieved pans, prepared a mixture, and added a creamy sauce to one pan.",
          "C. The cook diced vegetables, fried them, and placed them on a serving tray.",
          "D. The cook heated a pan, added oil, and mixed it with pre-prepared sauces."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_413_real.mp4"
  },
  {
    "time": "[0:05:52 - 0:06:02]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the recent activity just shown?",
        "time_stamp": "00:06:02",
        "answer": "C",
        "options": [
          "A. The individual was plating a dish, adding final garnishes, and preparing the plate for serving.",
          "B. The individual was cooking an omelet, flipping it multiple times, and placing it on a plate.",
          "C. The individual was preparing a dish by stirring ingredients in a pan and adding various seasonings.",
          "D. The individual was cleaning the kitchen station, wiping surfaces, and organizing utensils."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_413_real.mp4"
  },
  {
    "time": "[0:08:48 - 0:08:58]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual just now?",
        "time_stamp": "00:08:58",
        "answer": "B",
        "options": [
          "A. The individual cleaned the kitchen area and organized the utensils.",
          "B. The individual sautéed some vegetables, operated the stove, and managed multiple pans on a busy kitchen station.",
          "C. The individual prepared a salad by chopping and mixing fresh ingredients.",
          "D. The individual baked a cake, frosted it, and placed it in a refrigerator to cool."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_413_real.mp4"
  },
  {
    "time": "[0:11:44 - 0:11:54]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions performed just now?",
        "time_stamp": "00:11:54",
        "answer": "B",
        "options": [
          "A. The individual mixed a variety of raw ingredients in a bowl and started blending them.",
          "B. The individual sautéed green vegetables in a pan, transferred them to a serving dish, and prepared different sauce pans.",
          "C. The individual grilled several pieces of meat and then plated them with garnish.",
          "D. The individual baked a loaf of bread and buttered it before serving."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_413_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins with a view of a brightly colored, abstract painting on a white wall. The painting features various shapes and patches of colors including yellow, orange, purple, blue, green, and red. In the background, part of another white wall and some indistinguishable objects or people can be seen. [0:00:03 - 0:00:04]: The camera pans slightly to the right, revealing more of the white wall and another abstract painting next to the first. This second painting, by Stephen Rowe, is composed of numerous splashes and strokes of various colors including black, green, yellow, and red on a predominantly white background. [0:00:05 - 0:00:09]: The focus continues on the second painting by Stephen Rowe, fully revealing it. The painting is large, occupying most of the frame. The use of splashes and strokes creates a chaotic yet visually engaging composition.  [0:00:10 - 0:00:12]: As the video progresses, the camera gradually moves farther to the right, displaying the right side of the painting and part of another gallery wall that is white. Some visitors of the gallery are now visible in the background to the far right side. [0:00:13 - 0:00:15]: The camera further moves to the right, showing more of the gallery space. It reveals more paintings and exhibits on the walls and a few people walking around the gallery. The setting appears to be well-lit with high ceilings. [0:00:16 - 0:00:17]: The focus shifts to another artwork on the right side of the gallery. These are boxes mounted on the wall with various colorful, three-dimensional structures inside. The boxes have multilayered components with bright red, purple, and yellow pieces interwoven. [0:00:18 - 0:00:19]: The video ends with the camera capturing the upper portion of the box structures with intricate details, including some cartoonish eyes drawn on one of the higher boxes. More of the gallery space, including more artworks, is visible in the background.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which artist's work has appeared in the video?",
        "time_stamp": "00:00:09",
        "answer": "C",
        "options": [
          "A. Vincent van Gogh.",
          "B. Pablo Picasso.",
          "C. Stephen Rowe.",
          "D. Claude Monet."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_462_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:44]: The video starts with a view of an art gallery wall containing two framed paintings. The upper painting depicts an underwater scene in shades of green and blue, while the lower painting illustrates a wave breaking in the ocean. A person moves from the left side toward the center. [0:02:45 - 0:02:49]: The camera moves slightly to the right, revealing more of the gallery. In addition to the lower seascape painting, an adjacent larger painting of underwater rocks and reflections becomes visible. The colors are primarily green and blue with some white highlights illustrating the reflections. [0:02:50 - 0:02:51]: The camera continues panning to the right, further revealing details of the large underwater painting. Bright, diffused light appears reflected on the large green rocks submerged underwater. [0:02:52 - 0:02:54]: The camera pans more to the right, capturing another substantial painting with a similar underwater theme, featuring clear, blue-green water and submerged rock formations. Both paintings showcase realistic reflections on the water surface. [0:02:55 - 0:02:57]: The camera focuses on the third piece of art. Above it, a black sign with white text describing \"Irwin Cumberland\" is visible. A section of the ceiling implies indoor gallery lighting. [0:02:58 - 0:02:59]: The video concludes with a close-up of the upper portion of the last blue-green water painting and the artist's name sign. Reflections continue to be a focal point in the water's surface, enhancing the realism of the artwork.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the black sign with white text describing \"Irwin Cumberland\" located in relation to the paintings?",
        "time_stamp": "00:02:57",
        "answer": "C",
        "options": [
          "A. Below the painting.",
          "B. To the left of the painting.",
          "C. Above the painting.",
          "D. To the right of the painting."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Object Recognition",
        "question": "What is depicted in the painting shown in the video right now?",
        "time_stamp": "00:02:42",
        "answer": "A",
        "options": [
          "A. A wave breaking in the ocean.",
          "B. A forest scene.",
          "C. An abstract pattern.",
          "D. A cityscape."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_462_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:22]: On a white wall, the video captures several black and white photos framed in black, arranged in a grid layout. The images depict various scenes and people, each with distinct themes and settings. [0:05:23 - 0:05:26]: The camera pans to the right, revealing a framed painting of a rural landscape during sunset with a house and trees in the foreground. Above the painting, a sign reads \"Michael Miller.\" [0:05:27 - 0:05:28]: The camera continues to focus on the painting, with the name \"Michael Miller\" still visible at the top. [0:05:29 - 0:05:35]: The camera moves down, showing the painting more closely and then begins to pan to the right side, revealing more framed images on the walls. The painting of the rural landscape remains the focal point. [0:05:36 - 0:05:39]: The camera finishes panning right, showing a different wall with framed photographs featuring portraits of people, prominently displaying a woman with red hair and another person in different poses and attire. The video ends with the name \"Kira Tanelle\" visible at the top.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "which painting did the camera focus on before revealing the name \"Kesja Tabaczuk\"?",
        "time_stamp": "00:05:39",
        "answer": "A",
        "options": [
          "A. A painting of a rural landscape.",
          "B. A mirror.",
          "C. A bookshelf.",
          "D. A window."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      },
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the last name shown in the video?",
        "time_stamp": "00:05:54",
        "answer": "B",
        "options": [
          "A. Michael Miller.",
          "B. Kesja Tabaczuk.",
          "C. John Smith.",
          "D. Sarah Parker."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_462_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video starts with a first-person view in a supermarket or similar store. The viewer's perspective shows a stack of cardboard boxes on a cart in front of refrigerated display units. Inside the units at the top, packages of dairy products are visible. The floor is made of light-colored tiles. [0:00:01 - 0:00:02]: The viewpoint shifts slightly forward, showing a clearer view of the refrigerated display with various dairy products. [0:00:02 - 0:00:03]: The scene changes to look into the refrigerated unit, filled with various cartons of plant-based milk stacked neatly. The camera angle captures a bird's-eye view of the shelves. [0:00:03 - 0:00:10]: A person wearing gloves reaches into the refrigerated display. They start organizing and adjusting the milk cartons, ensuring they are neatly placed. The hands are consistently rearranging the cartons, moving them from one spot to another to ensure they are correctly aligned on the shelf. [0:00:10 - 0:00:15]: The person continues to organize the milk cartons on the shelves, ensuring all the items are front-facing and evenly positioned. The movement of hands and cartons is smooth, indicating careful arrangement. [0:00:15 - 0:00:16]: The perspective shifts back to a wider view, showing the stack of cardboard boxes on the cart, the refrigerated display, and more of the store surroundings. [0:00:16 - 0:00:19]: The person handling the camera focuses back on the boxes on the cart. They start opening one of the boxes, revealing more cartons similar to those being arranged on the shelf. The arrangement of boxes, cans, and cartons on the cart becomes clearer as the person continues to work.",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the person do next after revealing more cartons in the box?",
        "time_stamp": "0:00:19",
        "answer": "B",
        "options": [
          "A. Leave the store.",
          "B. Start arranging the new cartons on the shelf.",
          "C. Throw away the cartons.",
          "D. Close the refrigerated display."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_444_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:23]: The video starts with a first-person perspective view of a person placing cartons of milk on a refrigerated shelf in a store. The individual is wearing black gloves and seems to be organizing the cartons neatly in rows. The cartons have a white and blue design with some colorful graphics.  [0:02:24 - 0:02:31]: The camera angle shifts to reveal a wider view of the transactions, showing a cart loaded with cardboard boxes and packs of items possibly waiting to be shelved. The person then lifts some of the boxes off the cart and presumably begins to unpack them for shelving. [0:02:32 - 0:02:33]: The view returns to the refrigerated display where the individual places another carton on the shelf among the already arranged milk cartons. They continue to ensure the cartons are aligned correctly. [0:02:34 - 0:02:38]: The camera focuses back on the cart loaded with items. The person appears to continue unpacking and handling the cartons, moving them to the refrigerated section. They keep alternating between picking up new cartons and arranging them on the shelf. [0:02:39]: Finally, the view shows the individual making slight adjustments to the placement of the cartons, ensuring the neat alignment of the items on the shelf. The refrigeration unit and another set of milk cartons are visible in the background, displaying an organized and methodical stocking process.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is being placed on the refrigerated shelf right now?",
        "time_stamp": "0:02:23",
        "answer": "A",
        "options": [
          "A. Cartons of milk.",
          "B. Bottles of water.",
          "C. Packs of juice.",
          "D. Bags of vegetables."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_444_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:41]: In a supermarket, looking at a refrigerated display filled with various juices and dairy products. Predominantly yellow, purple, and green packages are on the top shelf, along with blue and yellow items on the bottom shelf. The person is removing items from a cardboard box. [0:04:42 - 0:04:43]: The camera shifts slightly to the right, revealing more of the display. The cardboard box has been placed on the bottom shelf. More yellow and green juice containers are visible on the top shelf. [0:04:44]: Close-up of the cardboard box in the person's hands. They inspect the box, positioning it over the drinks. [0:04:45]: The person lifts the box to reveal multiple cartons of purple and orange drinks on a cart beside the refrigerated display. [0:04:46]: More of the refrigerated display is shown. The cardboard box is now empty and lying flat, with two rows of purple and orange drink cartons neatly arranged next to it. [0:04:47]: The person continues to organize items inside the refrigerated display. Their gloved hands are visible, along with additional rows of drinks in purple and orange packaging. [0:04:48]: The same area of the refrigerated display with a neat arrangement of purple and orange drinks. The emptied cardboard box remains in place. [0:04:49]: A view showing more of the surroundings, such as the tiled floor and adjacent shelves. The drinks are neatly organized within the display. [0:04:50 - 0:04:51]: The camera shifts back to the refrigerated display, providing a wider view of the setup and showing more items to the left. There are bottles and cartons of various colors arranged neatly. [0:04:52]: Moving back to the cart, it is stacked with cartons of drinks, specifically orange, purple, and red ones. The person's gloved hand is visible. [0:04:53 - 0:04:54]: The person pushes the shopping cart, filled with colorful drink cartons, towards the right. [0:04:55]: The perspective shifts to the left, showing the refrigerated display full of dairy products and juices. Items are being organized and rearranged. [0:04:56]: The person is handling items, moving them between the cart and display. Their gloved hands continue arranging the colorful drink cartons. [0:04:57 - 0:04:59]: The person maneuvers the cart filled with various colored drinks on a tiled floor. Some drinks have been taken out and placed in the refrigerated display.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the cardboard box placed after the person removes items from it?",
        "time_stamp": "0:04:43",
        "answer": "A",
        "options": [
          "A. On the top shelf.",
          "B. On the floor.",
          "C. On the bottom shelf.",
          "D. On the cart."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_444_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:02]: The video begins in a supermarket, visible from a first-person perspective. The camera faces downward at a display of various yogurt cups. The yogurt cups are brightly colored, mostly in purples, oranges, and whites, arranged in organized rows within a refrigerated section. Beside the rows of yogurt cups, a stack of flat, folded cardboard trays is positioned to the right of the camera's view. [0:07:03 - 0:07:04]: The individual in the video moves their hands into view, each wearing black gloves. They lift one of the flat cardboard trays from the stack and begin unfolding it. Behind the tray, more refrigerated items, potentially dairy products, are visible. [0:07:05 - 0:07:07]: The camera shows the person placing the now unfolded cardboard tray down with the yogurt cups. The individual starts organizing the yogurt cups within the tray. The tiling on the floor reflects some overhead lighting, revealing a clean, well-maintained supermarket setting. [0:07:08 - 0:07:11]: The person continues handling the yogurt cups, placing more into the folded trays. They are methodically working, indicating an organized and systematic approach. Meanwhile, some more sections of refrigerated goods are visible in the background, along with the tiled floor and aisles of the store. [0:07:12 - 0:07:15]: The first-person perspective remains consistent, showing the person continuing their process of moving the yogurt cups and organizing them into the cardboard trays. The background reveals more items within the refrigeration units, likely providing perishables or other dairy-related products. [0:07:16 - 0:07:19]: The person, still wearing gloves, reaches for another flat cardboard tray from the stack. They unfold it and place it down, continuing the organized placement of yogurt cups into the tray. The tidy presentation of the supermarket’s aisles and clear lighting emphasizes the cleanliness and order of the environment.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What main activity was carried out just now?",
        "time_stamp": "00:07:19",
        "answer": "A",
        "options": [
          "A. Move the drinks from the cart to the refrigerated shelf.",
          "B. Cleaning the store aisles.",
          "C. Shopping for groceries.",
          "D. Checking out at the register."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the folded cardboard trays located in relation to the camera's view right now?",
        "time_stamp": "00:06:56",
        "answer": "B",
        "options": [
          "A. To the left of the camera.",
          "B. To the right of the camera.",
          "C. Directly in front of the camera.",
          "D. Behind the camera."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_444_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What object is pointing out the window right now?",
        "time_stamp": "00:00:10",
        "answer": "C",
        "options": [
          "A. A pencil.",
          "B. A pen.",
          "C. A finger.",
          "D. A stick."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_420_real.mp4"
  },
  {
    "time": "[0:02:03 - 0:02:23]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the weather condition right now?",
        "time_stamp": "00:02:09",
        "answer": "C",
        "options": [
          "A. Clear skies.",
          "B. Heavy rain.",
          "C. Overcast with clouds.",
          "D. Snowing."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_420_real.mp4"
  },
  {
    "time": "[0:04:06 - 0:04:26]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the current speed of the wind?",
        "time_stamp": "00:04:09",
        "answer": "C",
        "options": [
          "A. 20 kph.",
          "B. 25 kph.",
          "C. 30 kph.",
          "D. 35 kph."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_420_real.mp4"
  },
  {
    "time": "[0:06:09 - 0:06:29]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the horizon right now?",
        "time_stamp": "00:06:22",
        "answer": "C",
        "options": [
          "A. Red.",
          "B. Yellow.",
          "C. Orange.",
          "D. Blue."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_420_real.mp4"
  },
  {
    "time": "[0:08:12 - 0:08:32]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the sky right now?",
        "time_stamp": "00:08:32",
        "answer": "B",
        "options": [
          "A. Bright red.",
          "B. Dark grey.",
          "C. Clear with no clouds.",
          "D. Yellowish-orange."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_420_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located on the left side of the current road?",
        "time_stamp": "00:00:10",
        "answer": "B",
        "options": [
          "A. A dense forest.",
          "B. An open field.",
          "C. A river.",
          "D. A row of houses."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_174_real.mp4"
  },
  {
    "time": "[0:02:02 - 0:02:22]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the wire fence located right now?",
        "time_stamp": "0:02:20",
        "answer": "B",
        "options": [
          "A. On the right side of the road.",
          "B. On the left side of the road.",
          "C. Ahead on the left.",
          "D. Behind on the right."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_174_real.mp4"
  },
  {
    "time": "[0:04:04 - 0:04:24]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is situated on the right side of the path right now?",
        "time_stamp": "00:04:19",
        "answer": "B",
        "options": [
          "A. A lamp post.",
          "B. A bench.",
          "C. A trash can.",
          "D. A bicycle rack."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_174_real.mp4"
  },
  {
    "time": "[0:06:06 - 0:06:26]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the overhead power lines located right now?",
        "time_stamp": "00:06:17",
        "answer": "B",
        "options": [
          "A. Crossing the road ahead.",
          "B. Running parallel to the left of the road.",
          "C. Running parallel to the right of the road.",
          "D. Not visible at the moment."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_174_real.mp4"
  },
  {
    "time": "[0:08:08 - 0:08:28]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is visible on the left side of the path right now?",
        "time_stamp": "00:08:18",
        "answer": "A",
        "options": [
          "A. A field with some fencing.",
          "B. A dense forest.",
          "C. A village with a few houses.",
          "D. A river flowing parallel to the path."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_174_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: A pair of hands holds a green, conical object and is pressing it down onto a large rectangular piece of marble. There are some red splotches on the marble surface under the green object. Next to the marble slab is a wooden bowl containing two eggs and some dark colored small objects. [0:00:04 - 0:00:07]: The scene changes to an elderly person with a white beard and glasses, holding a quill pen. The person appears to be writing on a large, flat surface that is illuminated by a candle in the background. The setup appears like a desk with a pot nearby. [0:00:08 - 0:00:11]: The elderly person is shown in a close-up profile view, still holding the quill pen and continuing to write. This view emphasizes their focused expression as they work. [0:00:12 - 0:00:15]: The scene changes to a close-up view of an illuminated manuscript. The manuscript is decorated with gold and red colors, and has ornate text and designs, revealing that it is an old, possibly medieval, document. [0:00:16 - 0:00:19]: The final segment shows a younger person in a black robe sitting at a desk with an open book. They are holding a quill pen and writing in the book. This scene is set in a room with a large window made of small, square panes of glass. Different objects, including vessels and a candle, are present on the desk around them.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the elderly person with a white beard doing in the video?",
        "time_stamp": "0:00:10",
        "answer": "D",
        "options": [
          "A. Reading a book.",
          "B. Painting on a canvas.",
          "C. Cooking with a pot.",
          "D. Writing with a quill pen."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Counting",
        "question": "How many eggs are in the wooden bowl next to the marble slab?",
        "time_stamp": "0:00:03",
        "answer": "B",
        "options": [
          "A. One.",
          "B. Five.",
          "C. Three.",
          "D. Four."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_155_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:20 - 0:01:40] [0:01:20 - 0:01:28]: Inside a room with wood and plaster walls, there is a man in loose-fitting, beige pants, adjusting the waistband. Various pieces of clothing and objects are scattered around, including a bed with a fur blanket and a bench; [0:01:29 - 0:01:33]: The camera focuses on the close-up of the man’s hands as he adjusts a string on the waistband of his pants; [0:01:34 - 0:01:38]: The man stands up straight and begins to reach for a light-colored piece of clothing on a bench; [0:01:39]: A close-up of the man's face shows him looking down, possibly concentrating on what he is doing with the clothing.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action does the man perform after adjusting his waistband?",
        "time_stamp": "00:01:40",
        "answer": "D",
        "options": [
          "A. Sits down.",
          "B. Looks out the window.",
          "C. Lies down on the bed.",
          "D. Reaches for a piece of clothing."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_155_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:49]: A person is bending over while sitting on a bench or stool. The individual, wearing a light-colored shirt and blue jeans, is seen tying the laces on a pair of brown leather boots. The surrounding floor is covered in hay, suggesting a barn or rustic setting. The person's hands are engaged in adjusting and securing the boot laces.  [0:02:50 - 0:02:55]: The camera moves to a wider angle showing the full body of the person who is bent over and securing the boots. Wooden beams and a rustic interior can be seen in the background. The person, dressed in a long, loose shirt, continues to tighten and adjust the laces. [0:02:56]: The camera angle shifts to show a portion of the wall with wooden beams and some hanging items, possibly indicating a rural interior setting. [0:02:57]: The person in the video begins to stand up from a partially obscured position, leaning forward. [0:02:58 - 0:02:59]: The individual, now fully upright, is adjusting their clothing, possibly preparing to leave. The rustic wooden beams and plaster wall remain visible in the background.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What type of boots is the person wearing?",
        "time_stamp": "0:02:49",
        "answer": "A",
        "options": [
          "A. Brown leather boots.",
          "B. Rubber boots.",
          "C. Sneakers.",
          "D. Sandals."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_155_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:04:53]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:05]: A person with short brown hair and light facial hair, wearing a light brown coat and a dark purple scarf draped around their neck, is standing facing mostly towards the left. The background consists of a grassy field with some scattered flowers and a line of dense trees in the distance under a cloudy sky. The person appears to be looking somewhere off in the distance. [0:04:06 - 0:04:11]: The person starts walking away from the camera, revealing a back view of their coat and scarf. They continue walking towards the dense tree line in the distance. The field is vast and open with clusters of small yellow flowers scattered around. [0:04:12 - 0:04:20]: The person continues to walk further away, gradually becoming smaller in size until they are no longer discernible. The frame is dominated by the lush green grass and the dark green tree line in the background.\n[0:04:40 - 0:04:53] [0:04:40 - 0:04:41]: A field with numerous small yellow flowers stretches out in the foreground, occupying most of the frame. Green grass covers the ground, and there is a hedge line along the horizon. A tall tree stands on the right side, and the sky is overcast, casting a muted light over the scene. [0:04:42 - 0:04:47]: The scene transitions to an abstract blue geometric shape on a black background. The shape appears three-dimensional and consists of numerous elongated rectangular prisms radiating from a central point, forming a symmetrical pattern. The prisms move slightly, creating an impression of motion. [0:04:48 - 0:04:51]: The blue geometric shape continues to evolve, becoming more intricate as additional prisms appear and rotate. The background remains black. [0:04:48 - 0:04:50]: Text reading \"CROW'S EYE PRODUCTIONS\" appears below the geometric shape. A thin white beam of light points from the upper right towards the center of the shape, creating a spotlight effect. [0:04:51 - 0:04:52]: The scene fades to black.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person wearing around their neck right now?",
        "time_stamp": "00:04:05",
        "answer": "A",
        "options": [
          "A. A dark purple scarf.",
          "B. A dark green scarf.",
          "C. A light brown scarf.",
          "D. A dark blue scarf."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_155_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the cyclist with the number 894 located relative to the main pack right now?",
        "time_stamp": "00:00:19",
        "answer": "B",
        "options": [
          "A. Leading the main pack.",
          "B. Directly behind the main pack.",
          "C. In the middle of the main pack.",
          "D. To the side of the main pack."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_175_real.mp4"
  },
  {
    "time": "[0:02:05 - 0:02:25]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What structure is the cyclist approaching right now?",
        "time_stamp": "00:02:07",
        "answer": "A",
        "options": [
          "A. A National Championships banner.",
          "B. A McDonald's sign.",
          "C. A residential building.",
          "D. A traffic light."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_175_real.mp4"
  },
  {
    "time": "[0:04:10 - 0:04:30]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the relative position of the red brick building to the cyclist right now?",
        "time_stamp": "00:04:19",
        "answer": "C",
        "options": [
          "A. On the left side of the road.",
          "B. Directly ahead.",
          "C. On the right side of the road.",
          "D. Directly behind the cyclist."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_175_real.mp4"
  },
  {
    "time": "[0:06:15 - 0:06:35]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the \"National Championships\" banner located right now?",
        "time_stamp": "00:06:34",
        "answer": "C",
        "options": [
          "A. On the right side of the road.",
          "B. Above the cyclists.",
          "C. Hanging in the middle of the road.",
          "D. Directly behind the cyclists."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_175_real.mp4"
  },
  {
    "time": "[0:08:20 - 0:08:40]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the relative position of the cyclist in the green shoes to the group right now?",
        "time_stamp": "00:08:32",
        "answer": "C",
        "options": [
          "A. Leading the group.",
          "B. In the middle of the group.",
          "C. At the back of the group.",
          "D. Outside the group."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_175_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following actions best summarizes what the individual just now did?",
        "time_stamp": "00:00:12",
        "answer": "C",
        "options": [
          "A. The individual greeted the customers, prepared two glasses of wine, and served the drinks.",
          "B. The individual chatted with the customers, filled two glasses with ice, and arranged them on the counter.",
          "C. This person places three glass cups in front of the table.",
          "D. The individual cleaned the counter, arranged empty glasses, and interacted with other staff members."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_404_real.mp4"
  },
  {
    "time": "[0:02:04 - 0:02:14]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken by the person just now?",
        "time_stamp": "00:02:18",
        "answer": "C",
        "options": [
          "A. The person was rearranging glasses on the counter and chatting with colleagues.",
          "B. The person was cleaning the counter and organizing different bottles.",
          "C. The person was taking out a bottle from a refrigerator and pouring its contents into glasses with ice.",
          "D. The person was preparing food items and arranging plates and utensils."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_404_real.mp4"
  },
  {
    "time": "[0:04:08 - 0:04:18]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What best summarizes the actions performed by the individual just now?",
        "time_stamp": "00:04:18",
        "answer": "C",
        "options": [
          "A. The individual cleaned the bar area, organized supplies, and inspected the beverage inventory.",
          "B. The individual prepared a batch of food ingredients, cooked them, and plated the dishes.",
          "C. The individual prepared garnishes, assembled multiple drinks.",
          "D. The individual greeted guests, took their orders, and rang up the sales at the cash register."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_404_real.mp4"
  },
  {
    "time": "[0:06:12 - 0:06:22]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following actions best summarizes what the individual just now did?",
        "time_stamp": "00:06:22",
        "answer": "C",
        "options": [
          "A. The individual greeted customers, prepared a meal, and served it.",
          "B. The individual cleaned the bar area, organized supplies, and managed the inventory.",
          "C. The individual prepared a cocktail, stirred it, and placed it on the counter.",
          "D. The individual filled a glass with ice, poured a drink, and wiped the counter."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_404_real.mp4"
  },
  {
    "time": "[0:08:16 - 0:08:26]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following actions best summarizes what the individual just now did?",
        "time_stamp": "00:08:26",
        "answer": "C",
        "options": [
          "A. The individual took drink orders from customers while others were conversing.",
          "B. The individual cleared dishes from the table while customers waited for their drinks.",
          "C. The individual entered order details on a tablet while talking with the customers.",
          "D. The individual prepared cocktails using a shaker while engaging with customers."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_404_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video begins with a transition from a black screen to a lively street scene, showing a cobbled sidewalk and a modern building with glass windows reflecting the surroundings. A motorbike with a rider wearing a helmet is seen moving on the road, passing a traffic light that is red. [0:00:04 - 0:00:07]: The video provides a clearer view of the street, revealing more details of the surroundings. A large tree is visible in the center of the scene, with various buildings in the background showcasing traditional European architecture. The red traffic light remains in view, and additional vehicles, including a black car, are seen moving through the intersection. [0:00:08 - 0:00:09]: The motorbike from earlier proceeds through the intersection, while the black car moves closer to the camera. Pedestrians are seen walking on the opposite sidewalk. The video continues to depict the vibrant urban environment with shops and a broad pedestrian area. [0:00:10 - 0:00:14]: More vehicles, including a dark-colored car, are seen driving through the intersection. The motorbike from earlier continues forward. The camera angle slightly shifts, showing more details of the sidewalk and the surrounding buildings. Pedestrians and cyclists are also visible in the background. [0:00:15 - 0:00:20]: The camera continues to move along the sidewalk, providing a view of the street with a mix of traffic including cars, motorbikes, and bicycles. A red delivery scooter and a cyclist are spotted moving along the road in the same direction as the camera. The various storefronts, signs, and urban artwork add to the dynamic atmosphere of the scene.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What type of sidewalk is depicted in the video right now?",
        "time_stamp": "0:00:20",
        "answer": "C",
        "options": [
          "A. Metal sidewalk.",
          "B. Wooden sidewalk.",
          "C. Stone sidewalk.",
          "D. Concrete sidewalk."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Action Recognition",
        "question": "What did the red motorbike do just now?",
        "time_stamp": "0:00:09",
        "answer": "C",
        "options": [
          "A. Stops at the intersection.",
          "B. Turns left at the traffic light.",
          "C. Proceeds through the intersection and moves along the road.",
          "D. Parks on the sidewalk."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_325_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:59]: The video begins on a bright and sunny day with a view of a paved street lined with buildings on the left and a sidewalk on the right. The camera appears to be held by a person walking down the street. To the right, there's a small metal kiosk with various advertisements on it. A white van is parked next to the sidewalk on the right. The buildings on the left appear to be shops and are decorated with colorful signs. Some people are visible walking on the sidewalk; a few are heading towards the camera while others walk away. In the distance, more buildings can be seen, including a tall building that stands out due to its height. A line of trees is seen further down the street, adding greenery to the urban setting. As the camera moves forward, the left side shows shops with red chairs and tables outside, suggesting a small café or dining area. On both sides of the street, there are lampposts and various other signs, contributing to the lively atmosphere of the area. Close to the end of the video, on the right side, bicycles are parked against a fence, and to the right of the fence is a set of stairs leading upward.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type of vehicle is parked in the middle of the frame right now?",
        "time_stamp": "00:03:00",
        "answer": "B",
        "options": [
          "A. A red car.",
          "B. A white car.",
          "C. A black motorcycle.",
          "D. A yellow truck."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_325_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:31]: A bustling urban street stretches into the distance, lined with multi-story buildings on both sides. The buildings have large windows, some with balconies, adorned with railings. The street is paved with cobblestones, featuring tram tracks running down the center. Multiple cars are parked along the right side of the street, while a white van is parked near the center of the right lane. Pedestrians walk along the sidewalks; a woman in a black dress walks away from the camera on the right side of the screen, while a person in a green shirt walks away on the left. Shops with signs and displays line the ground floor of the buildings. In the background, the street slopes downhill, revealing more buildings further along. [0:05:32 - 0:05:39]: As the video continues, the scene changes slightly with pedestrians moving about. A group of individuals walks on the left sidewalk, gradually progressing further down. The woman with a red bag, previously near the center, moves more toward the right side. The lighting remains bright, suggesting a clear day with blue skies visible between the buildings. The street retains its cobblestone and tram tracks, and cars continue to be parked along the sides. The white van remains stationary, contributing to the scene's bustling yet orderly atmosphere.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the white van parked in relation to the rest of the cars on the street?",
        "time_stamp": "00:05:30",
        "answer": "D",
        "options": [
          "A. Near the center of the left lane.",
          "B. Near the center of the right lane.",
          "C. Along the left side of the street.",
          "D. Along the right side of the street."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_325_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:04]: The scene opens on a large stone building with detailed and ornate architecture. The sky is clear, and the weather is sunny. A man in a light blue shirt, dark pants, and blue shoes stands at the entrance of the building. The entrance is framed by large columns and features a wooden door. Several people are seen walking on the sidewalk, some at the entrance of the building, and a person wearing a neon yellow-green safety vest is visible on the right side next to a parked car.  [0:08:05 - 0:08:06]: Inside the building, it features a grand, large hall with high ceilings and ornate detailing. There are several large arched windows along the walls, which let in ample natural light. A large clock is prominently displayed on the wall above the entrance. The floor has a checkered black and white pattern near the entrance and a more subtle beige and brown pattern in the rest of the hall. People are seen walking and standing around, including a man with glasses and a gray shirt standing with his hands on his hips and a person sitting near the entrance. [0:08:07 - 0:08:20]: As the scene progresses, the interior of the hall becomes more visible. Beautiful large blue and white tile murals depicting historical scenes adorn the walls. The hall is spacious, with people walking around or standing in groups, some taking photos. The ceiling features intricate details, including plasterwork and large chandeliers. The hall has a calm and grand ambiance, with a blend of historical architectural styles. Towards the end of the sequence, a woman in a light blue shirt and a skirt stands near the middle of the hall, and the natural light streaming through the tall arched windows illuminates the intricate details of the murals and the architecture.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is prominently displayed on the wall above the entrance inside the building?",
        "time_stamp": "0:08:09",
        "answer": "B",
        "options": [
          "A. A painting.",
          "B. A clock.",
          "C. A sculpture.",
          "D. A mirror."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_325_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What logo is visible on the gas station sign right now?",
        "time_stamp": "00:00:05",
        "answer": "A",
        "options": [
          "A. Shell.",
          "B. Chevron.",
          "C. Exxon.",
          "D. BP."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_251_real.mp4"
  },
  {
    "time": "[0:01:06 - 0:01:11]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What brand is the steering wheel right now?",
        "time_stamp": "00:01:06",
        "answer": "C",
        "options": [
          "A. BMW.",
          "B. Audi.",
          "C. Mercedes-Benz.",
          "D. Porsche."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_251_real.mp4"
  },
  {
    "time": "[0:02:12 - 0:02:17]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the current speed of the car right now?",
        "time_stamp": "00:02:16",
        "answer": "B",
        "options": [
          "A. 150 km/h.",
          "B. 200 km/h.",
          "C. 225 km/h.",
          "D. 250 km/h."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_251_real.mp4"
  },
  {
    "time": "[0:03:18 - 0:03:23]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color are the gloves the driver is wearing right now?",
        "time_stamp": "00:03:20",
        "answer": "B",
        "options": [
          "A. Red and black.",
          "B. Blue and black.",
          "C. Black and white.",
          "D. Green and black."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_251_real.mp4"
  },
  {
    "time": "[0:04:24 - 0:04:29]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the current speed of the car right now?",
        "time_stamp": "00:04:23",
        "answer": "D",
        "options": [
          "A. 150 km/h.",
          "B. 200 km/h.",
          "C. 225 km/h.",
          "D. 250 km/h."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_251_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located on the left side of the cyclists right now?",
        "time_stamp": "00:00:19",
        "answer": "A",
        "options": [
          "A. A row of shops.",
          "B. A forest.",
          "C. A parking lot.",
          "D. A large park."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_168_real.mp4"
  },
  {
    "time": "[0:01:59 - 0:02:19]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the National Championships banner located right now?",
        "time_stamp": "00:02:17",
        "answer": "A",
        "options": [
          "A. Hanging above the cyclist.",
          "B. On the left side of the road.",
          "C. On the right side of the road.",
          "D. Directly behind the cyclist."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_168_real.mp4"
  },
  {
    "time": "[0:03:58 - 0:04:18]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the stop sign located right now?",
        "time_stamp": "00:04:13",
        "answer": "A",
        "options": [
          "A. On the right side of the cyclists.",
          "B. On the left side of the road.",
          "C. Directly ahead of the cyclists.",
          "D. Behind the cyclists."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_168_real.mp4"
  },
  {
    "time": "[0:05:57 - 0:06:17]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is positioned on the right side of the cyclists right now?",
        "time_stamp": "00:06:09",
        "answer": "A",
        "options": [
          "A. A green fence.",
          "B. A banner with an advertisement.",
          "C. A row of trees.",
          "D. A pedestrian sitting area."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_168_real.mp4"
  },
  {
    "time": "[0:07:56 - 0:08:16]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the cyclist with the orange socks located right now?",
        "time_stamp": "00:08:12",
        "answer": "B",
        "options": [
          "A. To the right of the cameraman.",
          "B. Directly in front of the cameraman.",
          "C. To the left of the cameraman.",
          "D. Behind the cameraman."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_168_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video starts with a completely black screen. [0:00:01 - 0:00:07]: Two computer processors with the labels \"AMD Ryzen 5 5600G\" and \"AMD Ryzen 7 5700G\" are placed side by side on a white surface. In the background, blurred images with various colors can be seen on a screen. [0:00:08]: The processors are now shown against a dark gradient background, with one side having a hint of red and the other blue. [0:00:09 - 0:00:17]: Information about the two processors is displayed on the screen. The comparison shows that both processors belong to the Zen 3 architecture for the CPU. For the GPU, the Ryzen 5600G has Vega 7 graphics, while the Ryzen 5700G has Vega 8 graphics. [0:00:18 - 0:00:19]: A person is seen sitting behind a table holding two boxes of the Ryzen processors. The setting appears to be a studio with various tech items on shelves in the blurred background. [0:00:20]: The person, still holding the Ryzen processors, is speaking directly to the camera. They seem to be gesturing as they describe or explain something.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What GPU does the AMD Ryzen 7 5700G have right now?",
        "time_stamp": "00:00:20",
        "answer": "C",
        "options": [
          "A. Vega 6.",
          "B. Vega 7.",
          "C. Vega 8.",
          "D. Vega 9."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_109_real.mp4"
  },
  {
    "time": "[0:03:20 - 0:03:40]",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:27]: A man is sitting at a white table, discussing two Ryzen processor boxes on the table in front of him. He is dressed in a white shirt, wearing glasses, and has a microphone attached to his shirt. The background consists of shelves with various tech items, including trophies, gadgets, and models, against a blue gradient wall. He gestures with his hands to emphasize his points, often pointing towards the Ryzen boxes. [0:03:27 - 0:03:32]: The video transitions to a series of graphics showing benchmarking results for different processors. The graph is titled 'TimeSpy 图形分' (TimeSpy Graphics Score). In the first graph, two processors, Ryzen 5700G (Radeon Vega 8) and Ryzen 5600G (Radeon Vega 7), are compared, with bars indicating their performance percentages. [0:03:32 - 0:03:33]: As the graph stays on the screen, it shows numerical scores: Ryzen 5700G at 1429 and Ryzen 5600G at 1312. The scores are represented by orange bars. [0:03:33 - 0:03:36]: The graph evolves to include another processor, i9-11900K (UHD 750 Integrated Graphics), which has a significantly lower score of 862. The bar for this processor is shown in blue, indicating a different color scheme for identification. [0:03:36 - 0:03:38]: The graph expands further and now also includes the i7-1165G7 (Iris Xe Graphics), with a score of 1656, depicted by a light blue bar. The comparisons indicate that the i7-1165G7 performs the best among the listed processors.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the score of the i7-1165G7 (Iris Xe Graphics) processor right now?",
        "time_stamp": "00:03:38",
        "answer": "B",
        "options": [
          "A. 862.",
          "B. 1656.",
          "C. 1312.",
          "D. 1429."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_109_real.mp4"
  },
  {
    "time": "[0:06:40 - 0:07:00]",
    "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:45]: The scene begins with a first-person perspective, showing a snowy, narrow pathway lined with bare trees and bushes. The path leads up to an open gate in the distance, and the viewer holds a handgun in their right hand, which is partly bandaged. [0:06:46 - 0:06:48]: The pathway continues, and the viewer moves closer to the gate, which has a tall, dark, and ominous appearance. Snow covers the ground, and some footprints and tire marks can be seen on the path. [0:06:49 - 0:06:51]: As the viewer approaches the gate, the surrounding environment remains snowy with dark, leafless trees on the sides. The gate appears closer, showing more details, including some rust and snow accumulation. [0:06:52 - 0:06:54]: The viewer walks beneath the gate's arch and heads into a darker, confined area with walls at the sides. The pathway still shows snow, but the environment becomes gloomier. [0:06:55]: The scene changes abruptly to a menu screen with system performance statistics and graphical settings. Options like \"Display,\" \"Graphics,\" and \"FidelityFX Super Resolution 1.0\" are visible. A grayscale background image with a dark, shadowy figure and a hand is partially visible. [0:06:56]: The same menu screen is shown, but there are slight variations in the system performance statistics. [0:06:57]: The menu screen remains the same, but there are further changes in the system performance statistics. [0:06:58 - 0:06:59]: The system performance statistics continue to change, showing different power measurements, temperatures, and frame rates, while the rest of the menu screen remains the same. [0:07:00]: The scene switches back to the first-person perspective. The viewer now stands on a snowy pathway with rugged terrain ahead. There is a handgun in their right hand, and snow-covered bushes are visible along the traill.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What graphical setting is mentioned on the menu screen right now?",
        "time_stamp": "00:06:45",
        "answer": "D",
        "options": [
          "A. Display.",
          "B. Graphics.",
          "C. Anti-Aliasing.",
          "D. FidelityFX Super Resolution 1.0."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_109_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:10:20]",
    "captions": "[0:10:00 - 0:10:20] [0:10:00 - 0:10:01]: The video begins with a detailed shot of a computer motherboard BIOS interface. It displays settings for overclocking, CPU configuration, and GPU settings. The options shown include \"Precision Boost Overdrive,\" \"Core Control,\" \"SMT Control,\" and more technical terms, with values shown such as 2500 MHz for \"GFX Clock Frequency\" and \"GFX Voltage\" set to 1.400V. [0:10:02]: This transition then reveals a man in a white shirt is sitting at a white table. He appears to be in a studio with a blue gradient background. Behind him are shelves with various items such as computer hardware and collectibles. Two AMD Ryzen processor boxes are positioned on the table in front of him. [0:10:03]: The man begins speaking, gesturing animatedly with his hands. He is seen holding up a finger as if to make a point or explain something while continuing his conversation directly to the camera. [0:10:04]: He then changes his gesture, spreading his hands apart. His focus remains on the audience, maintaining an engaging and informative demeanor. [0:10:05]: The man continues speaking, now bringing his hands closer together as he describes something, his expression showing enthusiasm. [0:10:06 - 0:10:07]: He shifts to a more relaxed stance, folding his hands together on the table as he continues to address the audience. His demeanor is friendly and informative. [0:10:08 - 0:10:09]: The man's gestures become more dynamic again, with him making a circular motion with his hands. His facial expression suggests he is explaining a point in more detail. [0:10:10]: He continues his narrative, occasionally glancing to the side while maintaining a conversational tone. His hands rest on the table near the Ryzen boxes. [0:10:11]: He looks directly at the camera and resumes speaking, potentially returning to a main point or summarizing the previous discussion. [0:10:12]: The man continues explaining, using his right hand to emphasize his points. The boxes labeled Ryzen 7 and Ryzen 5 remain prominently displayed on the table. [0:10:13]: He articulates a new point by bringing both hands closer to each other. His expression becomes more earnest as he addresses the camera. [0:10:14]: The next segment shows a close-up of a benchmark software interface on a computer screen. The software, 3DMark, shows performance metrics, including a \"Graphics score\" of 1,869 and a \"CPU score\" of 10,813.  [0:10:15 - 0:10:16]: The camera focuses on details of the 3DMark interface, highlighting the performance scores. The interface displays additional data such as estimated game performance and monitoring chart, along with a score of 21,133 under \"Time Spy.\" [0:10:17]: The camera zooms in on the \"Graphics score\" of 1,869 and the \"CPU score\" of 10,813, emphasizing these figures. Other details such as \"Estimated game performance\" and \"Monitoring\" charts are also visible but less emphasized in this shot. [0:10:18 - 0:10:20]: The focus remains on the same software interface with the performance metrics reiterated. The camera holds steady, ensuring the viewer notices the significant scores displayed for graphical and CPU performance.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What value was set for the \"GFX Voltage\" just now?",
        "time_stamp": "00:10:01",
        "answer": "D",
        "options": [
          "A. 1.200V.",
          "B. 1.500V.",
          "C. 1.250V.",
          "D. 1.400V."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_109_real.mp4"
  },
  {
    "time": "[0:12:20 - 0:12:26]",
    "captions": "[0:12:20 - 0:12:26] [0:12:20 - 0:12:23]: A person with short black hair and glasses is seen sitting at a white table in a room with a blue background. They are wearing a plain white T-shirt. In front of them on the table is a boxed CPU labeled \"AMD Ryzen 5.\" The room has shelves behind the person with various items, including gadgets and figurines. The person appears to be speaking and making gestures with their right hand. [0:12:23 - 0:12:26]: The screen transitions to a black background with white, glowing text and graphics, including a gear symbol and some Chinese characters. The glow effect and the gear symbol suggest a tech-related context. The text and graphics appear to remain static, highlighting the transition or a logo screen.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the label on the boxed CPU seen right now?",
        "time_stamp": "0:12:23",
        "answer": "B",
        "options": [
          "A. Intel Core i7.",
          "B. AMD Ryzen 5.",
          "C. NVIDIA GeForce GTX.",
          "D. Intel Core i5."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_109_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a first-person perspective focused on a wooden surface. In the center, there is a white cotton canvas with the dimensions 28cm by 23cm (11 inches by 9 inches) written on it. To the right of the canvas is a paint palette with visible paint smudges at the edges. [0:00:01 - 0:00:04]: A hand appears holding a tube of black acrylic paint, which is squeezed onto the palette. Next, a tube of cerulean blue paint is also squeezed onto the palette. [0:00:04 - 0:00:06]: The hand squeezes white paint onto the palette, and the labeled colors “Black,” “Cerulean Blue,” and “White” appear above their respective blobs of paint. [0:00:07 - 0:00:10]: The canvas' labeled text disappears, leaving just the blank canvas. A 25mm flat brush comes into view, and the hand holding the brush dips it into the black paint, followed by the white paint, and then the cerulean blue paint on the palette. [0:00:10 - 0:00:17]: The brush is moved around, mixing the colors on the palette. The palette now has a mixture of black, white, and cerulean blue. [0:00:18 - 0:00:20]: The hand begins painting on the blank canvas with the mixed paint, starting from the top-left corner, applying the color with horizontal strokes.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the paint palette located relative to the canvas?",
        "time_stamp": "00:00:01",
        "answer": "B",
        "options": [
          "A. To the left of the canvas.",
          "B. To the right of the canvas.",
          "C. Below the canvas.",
          "D. Above the canvas."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_142_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:09]: In a close-up view, a hand holds a fine paintbrush and applies white paint to a curved, grey surface that occupies the center and right portion of the canvas. The background is dark. The brush initially makes contact near the top of the curved surface, creating a small white patch that gradually expands with gentle strokes. The hand consistently moves the brush in a controlled manner, adding delicate brushstrokes to create a subtle texture. [0:02:10 - 0:02:19]: The hand continues applying white paint, extending the white patch downward along the curved edge. The strokes become slightly longer, filling in more of the dark background area and gradually increasing the size of the white-painted section. The texture of the white area begins to resemble a pattern, possibly clouds or another soft element, with the brush making varied, small swirling motions as more paint is applied. The contrast between the white paint and the dark background becomes more pronounced.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the hand doing in the video?",
        "time_stamp": "00:02:19",
        "answer": "A",
        "options": [
          "A. Applying white paint to a curved surface.",
          "B. Drawing a straight line with a pencil.",
          "C. Cleaning the paintbrush.",
          "D. Mixing colors on a palette."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_142_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:03]: The video begins with a close-up of an artist's hand holding a paintbrush, which moves over a circular, cratered surface resembling the moon. The moon is painted in shades of white and gray, occupying the left and lower side of the canvas, while the background is a dark gradient, giving a space-like feel. On the right, there's a palette with various colors; light and dark blue paint is visible. [0:04:04 - 0:04:09]: The brush continues to paint subtle details on the moon's surface. The canvas is centered, and the palette, labeled \"Liner Brush,\" is on the right. The palette exhibits different colors, including white and dark blue. The artist's hand picks up light blue paint and mixes it on the palette. [0:04:10 - 0:04:11]: The artist's hand holds the paintbrush and dips it into the light blue paint on the palette before continuing to paint on the canvas. [0:04:12 - 0:04:19]: The brush moves meticulously, adding fine details to the surface. The artist switches between adding fine lines and small patches of white, incorporating intricate textures that mimic the moon’s craters and surface irregularities. The hand's movement shows careful attention to enhancing the moon's realistic appearance.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the artist doing right now",
        "time_stamp": "00:04:11",
        "answer": "B",
        "options": [
          "A. Mixing colors on the palette.",
          "B. Applying paint to the moon's surface.",
          "C. Cleaning the paintbrush.",
          "D. Adjusting the canvas."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_142_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:06]: The video shows the detailed process of painting, starting with a hand holding a brush and painting black vertical lines of varying heights on a canvas. The canvas background features a detailed moon illustration, depicting craters and texture against a dark sky. The hand making the lines is steady and precise.  [0:06:07 - 0:06:12]: After finishing the lines, the video shifts to show an artist's palette on the right side of the frame. The palette holds multiple blobs of paint in different colors, including black, white, and shades of blue. The hand picks up a larger paintbrush and mixes some of the black paint.  [0:06:13 - 0:06:14]: The video then focuses on the canvas again, where the artist begins adding foliage to one of the vertical lines using the larger paintbrush dipped in black paint. [0:06:15 - 0:06:19]: The artist continues adding detail to the tree, creating a realistic texture by dabbing and dragging the brush. The tree's appearance becomes more defined, and the brushstrokes suggest individual branches. [0:06:20]: The video likely continues with the same detailed painting process, focusing on adding more details to the trees and the overall composition.",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the artist pick up a larger paintbrush after finishing the lines?",
        "time_stamp": "00:06:21",
        "answer": "A",
        "options": [
          "A. To adds foliage details.",
          "B. To clean the previous paintbrush.",
          "C. To start a new painting.",
          "D. To add a new color to the palette."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the artist's actions just shown in the video",
        "time_stamp": "00:06:20",
        "answer": "C",
        "options": [
          "A. The artist sketches a rough outline of a landscape.",
          "B. The artist adds fine details to a previously sketched portrait.",
          "C. The artist paints vertical lines, mixes paint, and adds foliage details to the canvas.",
          "D. The artist starts a new painting with a fresh palette and blank canvas."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_142_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: A blank white canvas is lying flat on a wooden surface. The wood has a light brown hue with a visible grain pattern running horizontally. [0:00:02 - 0:00:04]: The name \"CorreaArt\" appears in black handwritten text in the center of the white canvas. [0:00:05]: A translucent plastic sheet is being placed over the canvas, partially obscuring the text \"CorreaArt.\" [0:00:06 - 0:00:10]: The plastic sheet is now fully covering the canvas. The text \"CorreaArt\" remains visible through the plastic. The sheet has a few scattered paint stains of various colors. [0:00:11 - 0:00:12]: A hand holding a blue tube of paint appears from the top left and begins to dispense paint onto the plastic sheet. [0:00:13]: The hand is still dispensing the blue paint onto the top section of the plastic sheet. [0:00:14]: The hand appears again, now holding a green tube of paint, and starts dispensing green paint onto the sheet below the blue paint. [0:00:15]: The same hand now holds a red paint tube and dispenses red paint onto the sheet below the green paint. [0:00:16]: The hand holds a yellow tube of paint, dispensing yellow paint onto the sheet below the red paint. [0:00:17]: The hand, now holding a black tube of paint, is about to dispense paint onto the sheet. [0:00:18 - 0:00:19]: White paint is being dispensed onto the bottom section of the plastic sheet by the hand holding a white paint tube.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best describes the initial actions shown in the clip?",
        "time_stamp": "00:00:12",
        "answer": "D",
        "options": [
          "A. A hand is painting directly on a canvas.",
          "B. A plastic sheet is covered with paint stains, then a name appears.",
          "C. A blank canvas is being prepared for painting.",
          "D. Paint is being dispensed onto a plastic sheet covering a canvas."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      },
      {
        "task_type": "Prospective Reasoning",
        "question": "What is likely the next step after dispensing all the paint colors onto the plastic sheet?",
        "time_stamp": "00:00:19",
        "answer": "B",
        "options": [
          "A. The hand will spread the paint across the sheet.",
          "B. The plastic sheet will be removed from the canvas.",
          "C. Additional colors will be added.",
          "D. The text \"CorreaArt\" will be erased."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_137_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:01]: The video shows an artist's hand holding a paintbrush and approaching a canvas. The canvas is already prepared with a gradient sky background transitioning from a light blue at the top to a warm orange near the horizon. The artist adds dark purple cloud shapes in the sky. Below the sky, the canvas features lightly sketched outlines indicating future landscape elements.  [0:05:01 - 0:05:08]: The artist continues painting the dark purple clouds in a sweeping motion across the top of the sky. On a wooden table to the right of the canvas, there are various squeezed paint tubes, including colors like red, yellow, green, blue, and beige mixed on a palette. The artist’s movements are deliberate, filling in the outlines with the dark purple color. [0:05:09 - 0:05:15]: The artist focuses on building up the contour of the clouds, adding and spreading the dark purple paint to create more defined and fuller cloud shapes. The brush strokes are varied, suggesting a textured and dynamic sky. [0:05:15 - 0:05:20]: The lower portion of the canvas remains untouched, with only the initial sketches visible. The artist continues to enhance the sky, adding more clouds and shaping the existing ones to create a dramatic effect. The background objects stay in place, while the artist's attention remains on the upper part of the canvas.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the various squeezed paint tubes located in relation to the canvas?",
        "time_stamp": "0:05:08",
        "answer": "D",
        "options": [
          "A. To the left of the canvas.",
          "B. On a shelf above the canvas.",
          "C. Below the canvas on the floor.",
          "D. On a wooden table to the right of the canvas."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_137_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:11:00]",
    "captions": "[0:10:00 - 0:10:20] [0:10:00 - 0:10:01]: A canvas painting of a serene beach with clear blue water and a colorful sunset sky is placed on the left side of the frame. A palette with a variety of paint colors, including red, yellow, green, blue, and several shades of mixed colors, is visible on the right side. A person's finger is seen at the bottom center, holding the palette steady. [0:10:02 - 0:10:06]: The person begins to mix different paint colors together with a palette knife, primarily working on blending the white, blue, and red paints. The mixed paint has a range of hues, transitioning from dark blue to a purplish color. The palette knife moves back and forth, creating swirls and patterns in the paint. [0:10:07 - 0:10:09]: The person continues to mix the paints, adding more blue and white to the mixture, creating a lighter, more varied shade of purplish-blue. The palette knife continues to move in circular and sweeping motions. [0:10:10 - 0:10:14]: The painter further mixes the paints, incorporating more red into the mixture, resulting in a deeper and richer purple color. The palette knife is mostly focused on the center region of the paint mix. [0:10:15 - 0:10:19]: The painter adds white to the mixture, lightening the color once again. The process involves careful blending to ensure the hues are well-integrated, resulting in a more complex and nuanced range of purples. The palette knife's movements are precise and controlled, indicating a methodical approach to achieving the desired color. The entire scene is set against a wooden texture background.",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the paint mixture become deeper and richer in purple color right now?",
        "time_stamp": "00:10:16",
        "answer": "B",
        "options": [
          "A. Because more orange paint was added.",
          "B. Because more red and blue paint was added.",
          "C. Because purple paint was added.",
          "D. Because the palette knife was cleaned."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_137_real.mp4"
  },
  {
    "time": "[0:15:00 - 0:16:00]",
    "captions": "[0:15:00 - 0:15:20] [0:15:00 - 0:15:04]: A painting of a beach sunset is visible to the left of the frame. It depicts a vibrant sunset sky with hues of purple, pink, and orange, a calm ocean with gentle waves, and sandy shores. To the right, there is a palette containing various colors of paint, organized in blobs. The colors range from reds and yellows at the bottom to blues, greens, and purples towards the top. A paint knife is seen close to the palette, mixing yellow paint with white paint. [0:15:05 - 0:15:09]: The paint knife picks up the mixed paint and moves towards the painting. As it reaches the canvas, it adds more paint to the waves, refining the texture and enhancing the highlights. The colors on the palette appear slightly more mixed, suggesting ongoing blending of shades. The beach painting now shows enhanced details in the wave. [0:15:10 - 0:15:15]: A brush returns to the palette, picking another color before further detailing the artwork. The paint knife continues to add and refine textures on the waves, capturing more realistic water movement and reflections. The bright colored sunset contrasts strikingly against the blue-green tones of the waves. [0:15:16 - 0:15:20]: The painter continues to apply details to the crashing waves, emphasizing white caps and the movement of water. The palette's colors now show signs of multiple mixes, and the painting gains more life with each brushstroke, bringing out the vibrancy of the sunset and the dynamic atmosphere of the sea waves.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the artistic process just shown in the clip?",
        "time_stamp": "00:15:20",
        "answer": "D",
        "options": [
          "A. A painter adding final touches to a portrait.",
          "B. A painter cleaning their tools after finishing a painting.",
          "C. A painter starting a new abstract piece.",
          "D. A painter refining the details of ocean waves."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_137_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:09]: The video starts with a display of artwork on a gallery wall. There are three black-framed, rectangular pieces on the left, positioned one above the other. The top two frames feature abstract designs with silver and red elements. On the right side, there are four canvases with black brushstroke designs. Below the frames, there are two QR codes mounted on the wall. As the video progresses, the camera starts moving to the right, revealing more of the gallery space and faintly displaying other visitors and artworks in the background. [0:00:10 - 0:00:19]: The camera continues to pan right, transitioning into another section of the gallery. Here, a wide view of the gallery space is visible, showcasing various artworks on display on both sides. The perspective shifts, focusing on a wall with a collection of framed pictures arranged in different shapes and sizes, all featuring a monochromatic theme. As the camera moves forward, it captures a detailed view of each piece, ending on the highest row of artworks that include both rectangular and circular frames. The video concludes with a frontal shot of these pieces.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What happens as the camera starts moving to the left?",
        "time_stamp": "00:00:13",
        "answer": "A",
        "options": [
          "A. It reveals more of the gallery space and visitors.",
          "B. It zooms in on the top frame.",
          "C. It transitions to a close-up of a single artwork.",
          "D. It focuses on a sculpture in the middle."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_470_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:23]: The video begins by showcasing three artworks hanging on a white wall. Each artwork consists of cartoon figures in dynamic poses set against a gold background. The figures are simplified with white and other colors, appearing in action poses such as running and jumping. Under each artwork is a small label.   [0:02:24 - 0:02:26]: The camera slowly moves to the left, revealing another artwork on the wall. This artwork is colorful and abstract, depicting a variety of floral shapes and elements with vibrant hues such as red, purple, green, and blue. The paint appears to be applied in thick, three-dimensional layers, creating a textured effect. [0:02:27 - 0:02:30]: Attention is focused entirely on the newly revealed abstract artwork, with the camera capturing it from different angles. The painting continues to grow in complexity, featuring a mix of colors and shapes that overlap and intertwine. [0:02:31 - 0:02:33]: The camera reveals another artwork next to the previous one. This new one is on a blue background with an abstract tree consisting of multi-colored, swirling paint, resembling leaves and branches. The colors are vibrant, creating a stark contrast with the solid blue background. [0:02:34 - 0:02:36]: The camera continues to move left, revealing more of the wall. There is an artwork similar to the second abstract painting but with a different color palette and another unique, abstract tree. This piece has more serene undertones with a mix of pastel colors against a gray background. [0:02:37 - 0:02:39]: The video continues to explore the last artwork in this sequence. It remains focused on intricate details, showcasing the thick, textured paint application, and vibrant color combinations. The camera finally pulls away, giving a broader perspective of the art on display.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is depicted in the three artworks shown in the video right now?",
        "time_stamp": "0:02:23",
        "answer": "B",
        "options": [
          "A. Abstract floral shapes.",
          "B. Cartoon figures in dynamic poses.",
          "C. Realistic portraits of people.",
          "D. Landscapes with mountains."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "What is the background color of the artwork shown right now?",
        "time_stamp": "0:02:33",
        "answer": "C",
        "options": [
          "A. Red.",
          "B. Green.",
          "C. Blue.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_470_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:44]: The video starts with views of an art gallery wall displaying several framed paintings. On the left side, there are two small square paintings aligned vertically next to a larger rectangular painting with an oceanic scene, showing waves and clouds, along with abstract forms in the sky. These paintings are framed in a metallic color. [0:04:45 - 0:04:48]: The movement continues, focusing closely on the large painting with the oceanic scene. The camera then slightly shifts to the right, gradually moving past the paintings. [0:04:49 - 0:04:52]: The camera moves around a corner to reveal another section of the gallery. There are people milling around and examining the art. The perspective then shifts to a new wall displaying several small framed abstract paintings, each featuring a circular form against a colorful background. [0:04:53 - 0:04:58]: The camera continues to showcase the series of abstract paintings, revealing a grid of nine similar paintings on this section of the gallery wall. The artworks are symmetrical and neatly organized. [0:04:59]: The video ends with the camera capturing a perpendicular view of the nine abstract paintings, focusing on the intricate details and color variations of the circular forms within them.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What kind of forms do the small framed abstract paintings shown right now feature?",
        "time_stamp": "00:04:58",
        "answer": "C",
        "options": [
          "A. Rectangular forms.",
          "B. Triangular forms.",
          "C. Circular forms.",
          "D. Star-shaped forms."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_470_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:03]: The scene starts with a blue table. On the table, a square-shaped, colorful art piece with numerous small squares in a grid pattern is seen in the center. This grid pattern features various vibrant colors and small images inside each square. To the left, two letters 'L' and 'P' are mounted on the blue wall, and to the right, a painting depicts a city scene with cars and tall buildings under a partly cloudy sky. [0:07:04 - 0:07:11]: The camera then focuses and zooms in on a three-part portrait that appears to be a stylized or pixelated representation of a person's face, seen from different angles and emotions. The main color tones are red, black, and beige. The camera captures the three sections of the face sequentially, starting from the top middle area comprising the eyes and lips, then the left segment, capturing each part of the face in more detail until it reaches the lips. [0:07:12 - 0:07:16]: The focus continues close-up on the pixelated art, emphasizing the eyes and the red lips composed of small, circular elements. Each of these small elements has alphanumeric characters on them, indicating the intricate details used to create the portrait.  [0:07:17 - 0:07:19]: The camera then zooms out slightly, showing parts of the face near the lips and slowly moving downwards to showcase the lower portion of the portrait. The texture and details of the small elements forming the portrait are visible, showing a mix of red, black, and beige colors arranged to portray the image. [0:07:20]: The final segment extends outward to reveal the bottom of the framed art piece, where another artwork or object partially comes into view. The overall portrayal includes the varied and detailed close-ups of the pixel-art style portrait and other pieces around the blue tabletop.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What letters are mounted on the blue wall shown just now?",
        "time_stamp": "00:07:10",
        "answer": "B",
        "options": [
          "A. L and T.",
          "B. L and P.",
          "C. P and T.",
          "D. L and M."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_470_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: In the video, a first-person perspective reveals an art gallery space characterized by white walls and high ceilings. In the background, multiple works of art are displayed, including a sculpture of two seated, nude, mannequin-like figures wearing white headgear. Near the center and to the right of the frame, a large, glossy red gorilla sculpture is visible. This sculpture appears to be made of a shiny, reflective material and stands on a white pedestal. [0:00:03 - 0:00:08]: Next to the red gorilla is another sculpture of a blue gorilla holding a cylindrical object labelled \"USA.\" This sculpture, also on a white pedestal, features a patriotic theme with red, blue, and white colors, as well as stars and various decorative elements. As the viewer progresses further, another colorful gorilla sculpture comes into view, adorned with vibrant and eclectic patterns in shades of purple, green, pink, and yellow. [0:00:08 - 0:00:12]: The last two sculptures observed are part of the same series as the gorillas, displaying their bold and diverse colors. The colorful gorilla is positioned in front of the white wall, separating them from the rest of the exhibition. Near the end of this segment, a man in a gray jacket and a white cap appears on the right, walking towards the sculptures. [0:00:12 - 0:00:17]: The video continues to show the detailed designs on the sculptures, highlighting their polished finishes. The observer pans to the right, capturing more of the gallery space. The walls display other artworks, including a brightly painted surfboard on the left side of the frame. A more prolonged look at the gorilla sculptures stands out. [0:00:17 - 0:00:19]: As the viewer moves further into the gallery, the surrounding areas become more visible, including another room in the background where additional visitors are present. On the right side, a child in green clothing and a woman pushing a stroller can be seen approaching the gorilla sculptures. The background includes an additional artwork, likely a painting or photograph of a person's back, reinforcing the gallery's diverse exhibits.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What object is the blue gorilla holding?",
        "time_stamp": "00:00:08",
        "answer": "D",
        "options": [
          "A. A banana.",
          "B. A small sculpture.",
          "C. A book.",
          "D. A cylindrical object labeled \"USA\"."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_466_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:44]: The video begins with a clear view of a white wall displaying text about an exhibition titled \"RAINTYPE\" by Lorenzo Marini at the Bruce Lurie Gallery, curated by Luca Beatrice. The text provides an overview of the installation, describing it as inspired by a rainy day, with letters gaining meaning when combined with others in the immersive environment. The background shows other parts of the gallery with artworks visible on the walls. [0:02:45 - 0:02:46]: The camera begins to pan to the right, revealing more of the gallery space. A few visitors are seen, some standing and looking at exhibits. The text on the wall starts to move out of the frame's center. [0:02:47 - 0:02:50]: The view opens up to a larger section of the gallery. There is a significant installation in the center, consisting of numerous small, colorful items hanging from the ceiling, reflecting light and creating a visually complex and immersive scene. The floor is polished, reflecting the hanging installation and visitors. People are walking around, observing the artwork. [0:02:51 - 0:02:56]: The camera continues to capture the expansive installation. Visitors of various ages and appearances move around, some taking photos and others interacting with the space. The hanging objects are densely packed, creating a mosaic-like effect with reflections on the floor. [0:02:57 - 0:02:59]: The camera maintains its focus on the central installation. The space is well-lit, with bright and colorful details from the hanging pieces standing out. Visitors are engaged, some taking pictures or closely examining parts of the installation. The pattern of hanging items persists, enhancing the immersive nature of the exhibition.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the title of the text written on this wall?",
        "time_stamp": "00:02:44",
        "answer": "A",
        "options": [
          "A. \"RAINTYPE LORENZO MARINI\".",
          "B. \"SUNSHINE\".",
          "C. \"LIFE IN COLOR\".",
          "D. \"NIGHT LIGHTS\"."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Object Recognition",
        "question": "What significant installation is revealed in the center of the gallery right now?",
        "time_stamp": "00:02:51",
        "answer": "D",
        "options": [
          "A. A large sculpture in the center.",
          "B. A collection of paintings on the walls.",
          "C. A video projection.",
          "D. Numerous small, colorful items hanging from the ceiling."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_466_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:22]: The video begins with a view of a wall displaying five framed artworks. The background wall is white. The artworks vary in size and are rectangular in shape, featuring abstract designs with different colors. The frames are arranged in two horizontal rows. The largest frame on the right contains a composition of green and yellow hues, while the leftmost artwork combines red, beige, and gray tones. [0:05:23 - 0:05:24]: The camera moves slightly to the left, revealing more of the exhibit. More framed artworks come into view. A group of six smaller, black-and-white photographs are visible, arranged in a 3x2 grid. Each photograph is individually framed with a black border and has a white matting. [0:05:25 - 0:05:26]: The view continues to focus on the grid of six photographs. Each photograph appears to depict architectural elements. Small labels are positioned to the right of each frame, likely providing details about the artwork. [0:05:27 - 0:05:29]: Adjacent to the grid of photographs, there are more framed pieces. To the left of the grid, there are four smaller square frames, arranged in a vertical column, and each containing abstract compositions. The frames are dark and offset by a light background. [0:05:30 - 0:05:33]: As the camera continues to pan, the focus moves to the artwork on the left. Below the four smaller frames is an empty white wall space. To the left, there is another artwork featuring a large white area with vivid splashes of color, including yellow, red, pink, black, and blue. The painting captures attention with its dynamic and vibrant appearance. [0:05:34 - 0:05:36]: The central part of the colorful painting on the left becomes more visible. It features a yellow splash that transitions into various other colors, giving it an energetic and lively feel. Drippings and brush strokes are evident, enhancing the sense of movement. [0:05:37 - 0:05:39]: The final frames focus entirely on the colorful painting. The camera settles, providing a clear and centered view of the large, vibrant artwork. The painting stands out against the white wall background, highlighting the contrast and expressive use of color.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What appears below the four smaller frames as the camera pans to the left?",
        "time_stamp": "00:05:40",
        "answer": "B",
        "options": [
          "A. Another set of photographs.",
          "B. An empty white wall space.",
          "C. A sculpture.",
          "D. A window."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_466_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: The scene depicts an art gallery with white walls. On the left side, a complex sculpture is mounted on the wall. Several people, including a man in a dark blazer and blue jeans, are walking around. [0:08:01 - 0:08:04]: The camera moves forward, capturing more of the gallery space. The focus shifts to a white wall displaying artworks. A group of people, including a woman in a magenta jacket, stands in front of the exhibits. A television screen mounted on the wall displays an image of an older man with glasses, identified by the text \"LARRY KAGAN.\" [0:08:04 - 0:08:09]: The camera centers on the television screen showing sequences of a man working with metal objects, identified as Larry Kagan. Additional text above the screen reads \"THOMAS PAUL FINE ART presents LARRY KAGAN.\" [0:08:09 - 0:08:19]: The television continues to display various clips of Larry Kagan working on his sculpture. It shows close-up views of his hands manipulating metal and different stages of his art process. The camera remains focused on the TV screen, which occupies most of the frame. The gallery visitors are visible in the background, subtly illuminated by the light from the screen.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the man doing in the sequences shown on the television screen?",
        "time_stamp": "00:08:10",
        "answer": "C",
        "options": [
          "A. Painting.",
          "B. Drawing.",
          "C. Working with metal objects.",
          "D. Writing."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_466_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What type of weather condition is present right now?",
        "time_stamp": "00:01:35",
        "answer": "A",
        "options": [
          "A. Sunny.",
          "B. Stormy.",
          "C. Foggy.",
          "D. Rainy."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_460_real.mp4"
  },
  {
    "time": "[0:01:56 - 0:02:01]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the current color of the sky?",
        "time_stamp": "00:01:59",
        "answer": "A",
        "options": [
          "A. Orange.",
          "B. Blue.",
          "C. Grey.",
          "D. Pink."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_460_real.mp4"
  },
  {
    "time": "[0:03:52 - 0:03:57]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the traffic light right now?",
        "time_stamp": "00:06:02",
        "answer": "A",
        "options": [
          "A. Red.",
          "B. Green.",
          "C. Yellow.",
          "D. Blue."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_460_real.mp4"
  },
  {
    "time": "[0:05:48 - 0:05:53]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the number displayed on the electronic screen of the bus in front?",
        "time_stamp": "00:05:50",
        "answer": "A",
        "options": [
          "A. 590.",
          "B. 591.",
          "C. 1314.",
          "D. G1314."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_460_real.mp4"
  },
  {
    "time": "[0:07:44 - 0:07:49]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which supermarket logo is visible on the right side right now?",
        "time_stamp": "00:07:46",
        "answer": "D",
        "options": [
          "A. Aldi.",
          "B. Woolworths.",
          "C. IGA.",
          "D. Coles."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_460_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:04]: In a room with a grayish-blue wall, a person's hand is adjusting the position of a gray metal easel on a wooden table. The person is wearing a gray long-sleeved shirt. To the left of the easel, there is a plant with broad green leaves in a blue and white striped pot, with a small spotlight illuminating it from below. There is also a large circular object on the table behind the easel. [0:00:05 - 0:00:09]: The person places a piece of white paper with a pencil sketch of a human face onto the easel. The sketch appears to be a portrait of a woman with long hair. The person uses both hands to align the paper straight. [0:00:10 - 0:00:12]: The paper with the portrait sketch is now centered and secured on the easel. The person makes small adjustments by touching the paper. [0:00:13 - 0:00:14]: The person runs their hand over the paper, possibly to smooth it out or check if it is firmly attached. [0:00:15 - 0:00:19]: The camera focuses on the portrait sketch on the easel. A paintbrush with a cloth wrapped around it is held next to the paper, possibly used for shading or blending previous marks. The plant and the same desk clutter are still visible in the background.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What does the person do immediately after aligning the paper on the easel?",
        "time_stamp": "0:00:19",
        "answer": "A",
        "options": [
          "A. Makes small adjustments by touching the paper.",
          "B. Places a new sketch on the easel.",
          "C. Moves the easel to a different spot.",
          "D. Changes the spotlight angle."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_132_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:19]: An artist's hand, holding a paintbrush, is actively working on a portrait of a woman drawn on a white canvas. The portrait is characterized by its realistic style, focusing on the woman’s face, which has blue eyes, a slightly open mouth, and detailed features. The artist is applying light brown and peach tones to the neck area. The background is a neutral gray wall, with a potted plant on the left side and various art supplies visible on the right. The brush movements are meticulous, adding shading and depth to the neck of the female subject. The hand moves back and forth, carefully blending the colors to achieve a smooth gradient. The process emphasizes the gradual build-up of layers to enhance the three-dimensional effect of the portrait.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the potted plant located in relation to the portrait?",
        "time_stamp": "00:04:20",
        "answer": "C",
        "options": [
          "A. On the right side.",
          "B. Behind the canvas.",
          "C. On the left side.",
          "D. Under the table."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the artist's activity just shown in the video?",
        "time_stamp": "00:04:20",
        "answer": "B",
        "options": [
          "A. Creating a landscape painting.",
          "B. Working on a realistic portrait of a woman, focusing on the neck area.",
          "C. Sketching a preliminary outline for a new artwork.",
          "D. Mixing colors on a palette for future use."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_132_real.mp4"
  },
  {
    "time": "[0:12:00 - 0:13:00]",
    "captions": "[0:12:00 - 0:12:20] [0:12:00 - 0:12:20]: The video showcases the process of painting a portrait on a canvas. The hand of the painter, holding a brush, is actively engaged in the painting process, moving across the canvas to apply and blend the colors. The painting depicts a woman’s face with blonde hair and realistic details, particularly focusing on her bluish eyes, red lips, and subtle shading to reflect depth and dimension. The artist uses a variety of brush strokes, from broad to careful detailing, to refine the image. In the background, there are several objects, including a potted plant with green leaves on the left side and bird decorations on a shelf to the right. The environment appears to be a well-organized and creative workspace, with the canvas and painting tools positioned centrally and the focus mainly on the fine details being added to the portrait. The background colors are muted, featuring gray walls, directing the attention towards the vibrant colors of the painting itself.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the bird decorations located in the workspace?",
        "time_stamp": "00:12:20",
        "answer": "C",
        "options": [
          "A. On the left side.",
          "B. In front of the canvas.",
          "C. On a shelf to the right.",
          "D. Next to the potted plant."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the painting process just shown in the video?",
        "time_stamp": "00:12450",
        "answer": "C",
        "options": [
          "A. The artist is organizing painting tools in a vibrant studio.",
          "B. The painter is applying broad strokes to a landscape painting.",
          "C. The video captures the detailed process of painting background of a portrait with various brush strokes.",
          "D. The artist is blending colors on the palette while preparing to paint."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_132_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:06]: In this video, a virtual inventory screen is displayed, showing different compartments labeled as \"Player,\" \"Cargo,\" \"Backpack,\" and \"Ground.\" Various items like documents, boxes, and pieces of paper appear in these compartments. On the left side of the screen, a character's profile with personal information is visible, featuring sections like \"ID,\" \"Age,\" and \"Skills.\" [0:00:07 - 0:00:13]: The viewpoint swaps from the virtual inventory interface to a first-person perspective scene. The person appears to be carrying a large cardboard box and stands next to a large vehicle, possibly a truck, with its back door open. [0:00:14 - 0:00:17]: As they walk while holding the box, a pathway and a building with red and white walls are visible in the background. Another person dressed in a delivery uniform becomes visible near the door. [0:00:18 - 0:00:20]: The individual carrying the box approaches the person in the delivery uniform and stops next to them. They seem to be interacting, and the delivery person holds a tablet or a similar device. The screen then switches back briefly to the virtual inventory interface. [0:00:21 - 0:00:23]: The inventory screen reveals details about the person being interacted with (\"Harry Miller - Trucking License\"). Various items are moved between different compartments like \"Trucking Cargo\" and \"Backpack,\" indicating organization or preparation for delivery.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:00:18",
        "answer": "D",
        "options": [
          "A. Organizing items in the virtual inventory.",
          "B. Carrying a large cardboard box.",
          "C. Walking towards the building.",
          "D. Interacting with the delivery person and delivering the goods."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_288_real.mp4"
  },
  {
    "time": "0:02:40 - 0:03:00",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:44]: The video showcases a first-person perspective of driving on a multi-lane road, with a large bridge visible in the background. Vehicles on the road include a white car on the right and another vehicle further ahead. A man is visible in an inset video at the top left corner, appearing to be the person controlling the vehicle. [0:02:45 - 0:02:51]: The vehicle continues to move forward, passing more parked and moving cars on the right side, including some utility poles and trees along the sidewalk. The bridge in the background becomes more prominent as the vehicle approaches. [0:02:52 - 0:02:54]: The road slightly curves to the right, and the driver approaches an intersection. The bridge is now closer in view, and several more vehicles can be seen on the road, including another parked car on the right side. [0:02:55 - 0:02:58]: The driver appears to slow down as they approach the intersection. Traffic lights and street signs become visible. The bridge now looms overhead, and there are several vehicles around, including a large truck on the left. [0:02:59 - 0:03:04]: The driver continues forward, passing beneath the bridge structure. Palm trees are noticeable on the sidewalk, and a large sign with advertisements is visible on a street lamp to the right. [0:03:05 - 0:03:09]: The vehicle proceeds straight toward another intersection. Traffic lights are visible, and several vehicles, including a blue car, are seen moving ahead. The road looks clear, with visible greenery and industrial buildings adjacent. [0:03:10 - 0:03:12]: The driver proceeds through the intersection under the bridge. There are more palm trees and street signs in view. The road continues to a slight incline, heading towards what seems like a shipping port area. [0:03:13 - 0:03:17]: The vehicle drives on an elevated section of the road, with a large body of water and ships visible to the left. The road ahead appears clear, with minimal traffic, and the inset of the man is still present in the top left corner. [0:03:18 - 0:03:21]: Continuing forward, the surrounding area shows more industrial buildings and infrastructure. The waterway and shipping containers are visible on the left. Vehicles, including a van, are seen traveling in the opposite direction. [0:03:22 - 0:03:24]: The vehicle drives along the elevated road, passing more industrial elements and signs indicating the direction of traffic. The road appears to be in good condition, with clear markings. [0:03:25 - 0:03:27]: The vehicle continues towards a more open stretch of the road, with palm trees lining the center divider and both sides of the street. The weather is clear, indicating daytime. [0:03:28 - 0:03:32]: The vehicle progresses further, and more commercial buildings and infrastructure elements become visible. The road remains straight and well-maintained with minimal traffic in sight. [0:03:33 - 0:03:35]: The driver continues on the straight road, passing trees and commercial structures on the right. A sense of progression is visible as the scene reveals more of the urban environment. [0:03:36 - 0:03:40]: Movement is steady forward, showcasing an expansive road with clear skies and orderly vehicle flow. The surrounding environment indicates an industrial area transitioning to commercial zones.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the driver doing right now?",
        "time_stamp": "0:02:40",
        "answer": "C",
        "options": [
          "A. Turning left at an intersection.",
          "B. Slowing down to stop at a traffic light.",
          "C. Driving on a straight road.",
          "D. Parking the vehicle on the side of the road."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_288_real.mp4"
  },
  {
    "time": "0:05:20 - 0:05:40",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:26]: The video shows a large, grey delivery truck parking next to a white car in an industrial area. Barriers and other vehicles are seen nearby.  [0:05:27 - 0:05:30]: The delivery truck finishes parking and a person wearing a light green safety vest gets out from the driver's side, walking towards the back of the truck.  [0:05:31 - 0:05:34]: The individual, who appears to be a delivery person, continues walking around the truck. The background features other vehicles, some industrial equipment, and a large building.  [0:05:35 - 0:05:38]: The delivery person starts running towards a group of people and a building. There is a mix of standing and walking individuals in various outfits, indicating an active work area.  [0:05:39 - 0:05:42]: The delivery person continues running towards a man in a white shirt leaning against a brick wall, with some debris and scattered items on the ground nearby.  [0:05:43 - 0:05:46]: The person in the safety vest stops in front of the man in the white shirt, appearing to initiate a conversation.  [0:05:47 - 0:05:50]: A close-up view shows the man in the white shirt, who is wearing a blue wristwatch and holding some papers.  [0:05:51 - 0:05:55]: The man in the white shirt, identified as \"Frank Miller\" through an overlay, continues the conversation. Different options appear on the screen, indicating possible interactions.  [0:05:56 - 0:06:01]: The focus remains on Frank as the conversation progresses, with the overlay showing the player's dialogue choices.  [0:06:02 - 0:06:06]: A close-up of the character \"Frank Miller\" shows his detailed facial features and expression.  [0:06:07 - 0:06:09]: The view shifts back to the delivery person in the safety vest, turning around and walking away from Frank.  [0:06:10 - 0:06:13]: The delivery person heads towards the delivery truck called \"grime,\" which is parked in the same area as before.  [0:06:14 - 0:06:16]: The delivery person walks towards the back of the truck as some individuals stand around talking near the truck.  [0:06:17 - 0:06:20]: The delivery person approaches the rear door of the truck, triggering an interaction option. The characters and other environmental details remain visible in the background.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the delivery person doing right now?",
        "time_stamp": "00:06:20",
        "answer": "B",
        "options": [
          "A. Walking away from Frank.",
          "B. Run towards the harvester in white clothes.",
          "C. Engaging in a conversation with Frank.",
          "D. Running towards the building."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_288_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:05]: The video opens with a first-person view of a character wearing a reflective safety vest standing next to a gray van with the word \"grime\" written in light blue on its side. The character is facing the van and there is an interaction menu present. [0:08:06 - 0:08:11]: The character continues to stand stationary next to the van, with the interaction menu in view. Small red objects are scattered on the ground near the van. [0:08:12 - 0:08:14]: The interaction menu remains in view as the character continues to face the van. The background shows an industrial area with another van visible in the distance. [0:08:15 - 0:08:19]: The camera angle remains consistent, maintaining focus on the character and the van. The interaction menu options show \"Wire large\" and \"Deploy trip.\" [0:08:20]: The character steps back and turns slightly away from the van, revealing more of the surrounding industrial environment. Multiple vehicles and other characters are visible in the distance, with a light blue van standing out.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are the three people in the video doing now?",
        "time_stamp": "00:08:20",
        "answer": "A",
        "options": [
          "A. Standing quietly.",
          "B. Deployed a tripwire next to the van.",
          "C. Talking to each other.",
          "D. Interacted with another character in the background."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_288_real.mp4"
  },
  {
    "time": "0:10:20 - 0:10:39",
    "captions": "[0:10:20 - 0:10:39] [0:10:20 - 0:10:22]: The scene opens with a first-person perspective in a lush grassy area with numerous flowers. The background contains some trees and a building partially visible through the foliage. A small inset screen on the upper left displays a person in a room with various electronic equipment, appearing to stream or play a game. Various chat messages are visible on the right side of the screen. [0:10:23 - 0:10:26]: The view shifts to a road, looking at a truck in the center. This large, white vehicle has a visible license plate and is surrounded by trees and utility poles. The person in the inset screen remains in the same position, continuing to interact with their chat. [0:10:27]: A black rabbit appears on the road, positioned near the truck, and is looking toward the camera. The truck is still centered in the frame. [0:10:28 - 0:10:31]: The rabbit is now fully centered and sitting upright in the foreground. In the background, the white truck remains stationary. [0:10:32 - 0:10:34]: The rabbit continues to sit on the road while the frame remains focused on the truck with the background showing trees and utility poles. [0:10:35 - 0:10:37]: The camera zooms slightly closer to the truck. The trees to the left side become more prominent, and the rabbit is no longer in the frame. [0:10:38 - 0:10:39]: The perspective changes back to a park-like setting with large trees and bushes. In the background, buildings are visible, indicating an urban environment. The streaming individual is still present in the upper left inset with their room and setup.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the camera doing right now?",
        "time_stamp": "00:10:39",
        "answer": "C",
        "options": [
          "A. Zooming in on the truck.",
          "B. Focusing on the streaming individual.",
          "C. Panning across the park-like setting.",
          "D. Capturing the rabbit sitting on the road."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_288_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "Right now, what is the main color of the bus in front?",
        "time_stamp": "00:00:19",
        "answer": "A",
        "options": [
          "A. Yellow.",
          "B. Red.",
          "C. Blue.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_457_real.mp4"
  },
  {
    "time": "[0:01:55 - 0:02:00]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the yellow vehicle on the left side of the street right now?",
        "time_stamp": "00:01:56",
        "answer": "A",
        "options": [
          "A. DHL van.",
          "B. Police car.",
          "C. Fire truck.",
          "D. Food truck."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_457_real.mp4"
  },
  {
    "time": "[0:03:50 - 0:03:55]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Right now, what is the speed limit indicated on the yellow bus?",
        "time_stamp": "00:03:52",
        "answer": "A",
        "options": [
          "A. 90 km/h.",
          "B. 30 km/h.",
          "C. 60 km/h.",
          "D. 80 km/h."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_457_real.mp4"
  },
  {
    "time": "[0:05:45 - 0:05:50]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "Right now, what is the background color of the sign indicating changed traffic conditions?",
        "time_stamp": "00:05:45",
        "answer": "A",
        "options": [
          "A. Yellow.",
          "B. Blue.",
          "C. Red.",
          "D. White."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_457_real.mp4"
  },
  {
    "time": "[0:07:40 - 0:07:45]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "Right now, what architectural style is predominant in the buildings visible on the left side of the street?",
        "time_stamp": "00:07:48",
        "answer": "A",
        "options": [
          "A. Georgian.",
          "B. Gothic.",
          "C. Modern.",
          "D. Art Deco."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_457_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video begins with a logo for \"3D BotMaker Diecast Racing League\" on a textured background. [0:00:01 - 0:00:04]: The perspective switches to a miniature racing track scene. Multiple small diecast cars, including orange, yellow, and black vehicles, are seen on a curved track. The setting includes faux grass, trees, billboards, and spectators. [0:00:05 - 0:00:10]: The logo for \"3D BotMaker Diecast Racing League\" reappears, with additional text indicating it is the \"Fiero Tournament, Race 1 of 3.\" [0:00:11 - 0:00:15]: The scene shows a steep track on a hillside. The environment is a well-crafted diorama, featuring a track with railings, small trees, and detailed landscaping. [0:00:16]: A close-up of a miniature off-road scene shows several model cars on rocks, with miniature figures posed around them, emphasizing a realistic, dynamic setup. [0:00:17 - 0:00:18]: The view shifts to a camping area beside the track. There are multiple caravans, a few parked cars, and miniature figures, indicating a detailed and vibrant setting. [0:00:19]: The final frame focuses on a miniature race control tower with small figures. Two portable toilets are positioned nearby, surrounded by detailed foliage and structures.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What does the additional text indicate about the \"3D BotMaker Diecast Racing League\"?",
        "time_stamp": "00:00:10",
        "answer": "B",
        "options": [
          "A. It is the \"Final Race\".",
          "B. It is the \"Fiero Tournament, Race 1 of 3\".",
          "C. It is the \"Champion's Cup\".",
          "D. It is the \"Qualifying Round\"."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_500_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:40 - 0:02:00] [0:01:40 - 0:01:43]: The video begins with a group of model cars on a slot car racing track. The track loops through a miniature landscape with green slopes and small, evenly spaced trees. The group of cars, including a red, an orange, a green, and a yellow car, races along a curved section of the track. [0:01:44 - 0:01:45]: As the cars continue racing, they speed past a grassy hillside. The red car appears to be leading, followed by a yellow car. The landscape maintains its lush, green appearance with occasional trees along the track. [0:01:46 - 0:01:47]: The cars navigate another section of the track, moving downhill. The red car is slightly ahead of the yellow car. The greenery around the track adds a sense of realism to the miniature environment. [0:01:48 - 0:01:50]: The track circles a central area with parked miniature vehicles, spectators, and various structures. The red car leads closely followed by other cars around the bend, with a red and white barrier lining the track's edge. In the background, an electronic billboard is visible. The red car continues while the other cars join the track. [0:01:51 - 0:01:54]: The scene shifts to a high-speed straight section. The red car, followed by a yellow car, races past a series of billboards and miniature trees. The track is bordered by grassy areas and more background elements such as miniature spectator stands and signage. [0:01:55 - 0:01:57]: The track runs parallel to structures, including billboards for \"Slammin Customs\" and \"Model Cars Houston\". The red car is slightly blurred from its speed as it crosses a finishing line marked with a digital timer. [0:01:58 - 0:02:00]: The video concludes with a display showing the race results. An orange car is prominently featured on the screen with a list ranking the cars by their positions, indicating that \"Randy\" driving a red car has finished first. The setup includes more structures and a miniature race control center.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Which car is leading right now?",
        "time_stamp": "0:01:43",
        "answer": "C",
        "options": [
          "A. Black car.",
          "B. Red car.",
          "C. Yello car.",
          "D. Green car."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      },
      {
        "task_type": "Text-Rich Understanding",
        "question": "According to the race results displayed at the end, who is the driver of the red car?",
        "time_stamp": "00:02:00",
        "answer": "B",
        "options": [
          "A. George Milidrag.",
          "B. Randy Ferrero.",
          "C. Crazy Jimmy.",
          "D. Rad Cunningham."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_500_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:23]: The video starts with a first-person view looking at a model car racetrack. There is a curved section of the track with several model cars racing around it, including a bright yellow car in the foreground. In the background, there are small trees and some billboards. The left half of the frame contains sand and a small grassy area with miniature trees. [0:03:24 - 0:03:27]: As the video progresses, the view shifts to a straightaway portion of the track, where more model cars are seen racing towards a small building on the right side. The building has some figures (likely model people) standing around. Along the track, there are more spectators and trees creating a bustling scene of a model racing event. [0:03:28 - 0:03:30]: Near the finish line, the racetrack has a timing tower and several spectators gathered around, cheering for the cars as they speed by. The grassy hill remains prominent in the background. [0:03:31 - 0:03:35]: The view transitions to the starting line on an elevated track section. Four brightly colored model cars (red, green, orange, yellow) are lined up beneath a starting gate equipped with lights. The adjacent track is visible with several elevation changes and a few trees lining the sides. [0:03:36 - 0:03:39]: The starting lights turn green, signaling the start of the race. The model cars begin moving swiftly down the track, with the red car slightly leading. The track curves and descends, with more trees and a racetrack visible on the left side of the frame as well.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What colors are the four model cars lined up at the starting line?",
        "time_stamp": "00:03:35",
        "answer": "D",
        "options": [
          "A. Blue, yellow, green, red.",
          "B. Red, blue, orange, green.",
          "C. Green, yellow, orange, blue.",
          "D. Red, green, orange, yellow."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_500_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:05:47]",
    "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:05]: The video begins on a racetrack with a few cars on the track. The camera captures a tight corner with red and white barriers. Several vehicles and trailers are parked on the grassy area adjacent to the track. Three racing cars—yellow, green, and orange—are in the middle of taking the curve. [0:05:06 - 0:05:09]: As the frame changes, the yellow car leads on the track while the red and orange cars follow closely. The terrain is grassy, and the track smoothly transitions into a straight path. [0:05:10 - 0:05:11]: The yellow car continues to lead, followed by the red car in the left lane, while the spectators and a few structures, including a sign and a small building, appear to the left side of the track.  [0:05:12 - 0:05:14]: The yellow car remains in the lead, passing under a raised structure displaying a digital timer. The crowd stands along the track, watching as cars race by. Some tents and a porta-potty can also be seen on the sidelines. [0:05:15 - 0:05:16]: The camera zooms in on the parking area where the yellow car is parked next to a red car adorned with an American flag design. A few people stand nearby, one of them taking photos. The background includes parked cars, trees, and a sign for \"Slamman Customs.\" [0:05:17 - 0:05:19]: The view remains focused on the yellow and red cars parked side-by-side. More details of the parking area and the background, including blue porta-potties and spectators, are visible.\n[0:05:40 - 0:05:47] [0:05:40 - 0:05:47]: The video shows a close-up view of a textured black asphalt surface. Positioned horizontally across the top of the frame is a black banner with bold white text that reads \"3D BotMaker DIECAST RACING LEAGUE.\" An orange hexagon with the white letters \"3D\" is on the left side of the banner. This banner spans the width of the frame, and the asphalt surface occupies the remaining area, providing a consistent and uniform background.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What design is on the red car parked next to the yellow car right now?",
        "time_stamp": "00:04:58",
        "answer": "D",
        "options": [
          "A. Flames.",
          "B. Stripes.",
          "C. Checkerboard.",
          "D. American flag."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_500_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video begins with a first-person view of a person standing on the sidewalk. He is wearing a blue and white striped shirt and white shorts. He is looking towards a multi-lane road with several cars passing by. In the background, there is an elevated highway and a few palm trees. Other pedestrians can be seen in the vicinity. [0:00:04 - 0:00:07]: The person starts walking along the sidewalk, approaching an intersection. The traffic light is visible, and a few more cars are seen driving by on the road ahead. A tall building under construction can be seen across the street. [0:00:08 - 0:00:11]: Continuing along the sidewalk, the person moves closer to the intersection. More details of the construction site are visible, including scaffolding and cranes. Traffic continues to flow in the background. [0:00:12 - 0:00:14]: The person begins to run, heading towards the intersection on the opposite side of the street. There are still multiple cars on the road, and the elevated highway remains in view. [0:00:15 - 0:00:17]: The person runs across the street towards another section of the sidewalk. A white car is seen waiting at the intersection, and other vehicles are moving in the distance. The construction site is still prominent in the background. [0:00:18 - 0:00:20]: The person continues running past the white car and up the sidewalk, approaching a pedestrian walking ahead. The buildings and elevated highway serve as a backdrop to the scene.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:00:14",
        "answer": "B",
        "options": [
          "A. Sit down on a bench.",
          "B. Running along the sidewalk.",
          "C. Taking a photograph.",
          "D. Turning around."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_271_real.mp4"
  },
  {
    "time": "0:02:40 - 0:03:00",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:50]: The video begins with a first-person perspective of a person running along a sidewalk lined with a blue construction wall on the left and parked cars on the right. The person, wearing a striped shirt and shorts, is steadily moving forward. The background shows a cityscape with buildings and a traffic light in the distance. Several objects like cans and debris are scattered along the sidewalk. [0:02:50 - 0:02:55]: As the person continues running, they approach a section of the sidewalk with more scattered trash, suggesting an urban setting. The blue construction wall slightly curves ahead, indicating the person is about to turn a corner. [0:02:55 - 0:03:00]: The person turns right at the corner of the blue construction wall, heading towards a crosswalk. The view ahead shows more city buildings with advertisements, a few parked and moving cars. The person crosses the street and approaches a blue car stopped on the opposite side.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located to the right of the person right now?",
        "time_stamp": "00:02:57",
        "answer": "B",
        "options": [
          "A. A blue construction wall.",
          "B. A blue car.",
          "C. A crosswalk.",
          "D. City buildings with advertisements."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_271_real.mp4"
  },
  {
    "time": "0:05:20 - 0:05:40",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:25]: The scene takes place in a parking lot near a building labeled \"24 HR PARKING.\" The person is walking towards the building's entrance, passing by a small booth painted blue and white. The ground has white painted lines marking parking spots.  [0:05:26 - 0:05:29]: The person continues towards the entrance, veering slightly to the right. Two individuals are standing near the entrance, appearing to be engaged in a conversation. Nearby, a potted plant is placed on the ground beside them. [0:05:30 - 0:05:32]: The person pauses for a moment and looks around. The background reveals other buildings, vehicles, and some traffic signs. The setting appears urban with some construction or renovation activity visible in the distance. [0:05:33 - 0:05:37]: Continuing to walk towards the two individuals, the person breaches the edge of the parking lot and approaches the building. The two individuals are still engaged in a conversation near an emergency exit door. [0:05:38 - 0:05:40]: The person stops and stands near the two individuals, who seem to be talking.  [0:05:41 - 0:05:43]: The scene shifts, showing the person turning around to face the parking lot, where some litter and disorganized parking spaces are visible. They stand still momentarily. [0:05:44 - 0:05:47]: The perspective turns back towards the parking lot, showing the person's back to the camera. The parking lot appears somewhat rundown with scattered debris around. [0:05:48 - 0:05:59]: The view switches to a map interface displaying a detailed map of the city. Various locations and routes are marked in different colors. The map shows urban planning, streets, buildings, and some notable landmarks by the coastline. [0:06:00 - 0:06:10]: The map interface continues being displayed, with the person possibly deciding on a route or examining different marked locations. The screen remains static, focusing on the detailed map. [0:06:10]: The final frame displays the person continuing to examine the map interface, planning their next move. The map remains the central focus, showcasing various streets and points of interest throughout the city.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located near the entrance of the building right now?",
        "time_stamp": "00:05:23",
        "answer": "B",
        "options": [
          "A. A blue and white booth.",
          "B. Two potted plants.",
          "C. A small kiosk.",
          "D. A security camera."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_271_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:02]: In the video, an individual with short blond hair wearing a light blue and white striped shirt, white shorts, and white sneakers is seen running along a sidewalk which runs parallel to a blue construction wall. The sky is clear, and daylight illuminates the scene.   [0:08:03 - 0:08:07]: The character continues running, and the video reveals a street on the left with a store named \"Los Santos Vapid\" in the background. More shops and several signposts line the street as well.   [0:08:08 - 0:08:10]: The person is still moving along the sidewalk with the blue construction wall on the right and starts moving past a pole. [0:08:11 - 0:08:15]: As the character runs further down the sidewalk, a black car appears on the road to the left, heading towards an intersection. [0:08:16 - 0:08:18]: The camera angle slightly shifts, providing a close-up view of the individual’s upper body and the nearby pole. [0:08:19 - 0:08:21]: The runner continues along the sidewalk, and the intersection with a visible pedestrian crossing signal can be seen ahead. [0:08:22 - 0:08:24]: As the person keeps running, the blue construction wall continues alongside him, with various signs posted on it. [0:08:25 - 0:08:27]: The person maintains their speed, approaching a bridge visible in the distance, under which the street and sidewalk continue. [0:08:28 - 0:08:30]: Both the runner and the black car progress towards the intersection where a crossing may happen. [0:08:31 - 0:08:33]: The individual approaches closer to the bridge, with a red building visible to the right beyond the blue construction wall. [0:08:34 - 0:08:36]: As the person runs past the red building, they make a slight turn, continuing along the pathway.  [0:08:37 - 0:08:40]: The runner approaches the end of the blue construction wall, revealing a stairway leading up to another level. [0:08:41 - 0:08:43]: The person moves towards the intersection, showing more signs of urban infrastructure and traffic signals. [0:08:44 - 0:08:46]: The view shifts slightly, showing the runner closer to various urban elements like streetlights, trash bins, and passing by a transit building with \"Transit\" written on it. [0:08:47 - 0:08:50]: The runner continues through the intersection area, displaying more urban scenery including traffic lights and buildings ahead. [0:08:51 - 0:08:54]: The individual crosses the road onto a pavement, moving through the cityscape with tall buildings in the background. [0:08:55 - 0:08:58]: The camera adjusts as the person makes a sharp turn, revealing a wide road ahead and more buildings and infrastructure. [0:08:59 - 0:08:20]: The runner proceeds into an open boulevard, running parallel to a row of tall buildings and finally, veers right onto a quieter side street.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located to the right of the runner as they approach the bridge?",
        "time_stamp": "00:08:15",
        "answer": "B",
        "options": [
          "A. A blue construction wall.",
          "B. A red wall.",
          "C. A pedestrian crossing signal.",
          "D. A black car."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_271_real.mp4"
  },
  {
    "time": "0:10:00 - 0:10:15",
    "captions": "[0:10:00 - 0:10:15] [0:10:00 - 0:10:04]: The video begins with a first-person perspective of a character in a striped shirt running along a concrete sidewalk, next to a short concrete wall lined with green hedges on the right side. The sidewalk is bordered by a patch of grass on the left side, behind which there's a road. Tall buildings and trees can be seen in the distance, suggesting an urban environment. There is an overlay in the top left corner showing a different person's figure, possibly a streamer, and a chatroom interface is visible on the right side of the screen. [0:10:05 - 0:10:07]: The character slows down and comes to a stop, facing the same direction down the sidewalk. The streamer overlay in the top left remains stable, with continuous activity in the chatroom interface on the right. [0:10:08]: The character begins to walk slowly down the sidewalk again, maintaining the same direction. The scene around remains consistent, with greenery on the right and urban architecture in the background. [0:10:09]: The character temporarily stops again on the sidewalk. The overlay and chatroom interface remain unchanged in position and content. [0:10:10]: A map interface pops up, overlaying the direction and location. This map shows a detailed view of the surrounding area, highlighting the current position and path. [0:10:11]: The map disappears, returning to the first-person view of the character standing on the sidewalk, with the chatroom on the right still active. [0:10:12 - 0:10:14]: The character resumes running down the sidewalk. The urban scenery remains unchanged, displaying tall buildings, green hedges, and palm trees. [0:10:15]: The character continues to run forward, with more urban structures and greenery seen ahead. The streamer overlay remains in the same position, with vibrant chat activity ongoing.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the position of the greenery in relation to the character right now?",
        "time_stamp": "00:10:02",
        "answer": "B",
        "options": [
          "A. On the left side.",
          "B. On both sides.",
          "C. Behind the character.",
          "D. In front of the character."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_271_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why is Mr. Bean constantly posing right now?",
        "time_stamp": "00:01:04",
        "answer": "D",
        "options": [
          "A. Because he is trying to practice his modeling skills.",
          "B. Because he is preparing for a photo shoot.",
          "C. Because he is showing off his new outfit in front of a mirror.",
          "D. Because he is imitating the characters on the television."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_237_real.mp4"
  },
  {
    "time": "[0:02:05 - 0:02:35]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the bed get pulled out through the window?",
        "cause": "The man ties the rope to the bed",
        "effect": "The bed gets pulled out through the window",
        "time_stamp": "00:02:34",
        "answer": "A",
        "options": [
          "A. Because the man ties the rope to the bed.",
          "B. Because the bed is on wheels.",
          "C. Because the window is too large.",
          "D. Because the bed is not secured properly to the floor."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_237_real.mp4"
  },
  {
    "time": "[0:04:10 - 0:04:40]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the Mr. Bean peer through the door's mail slot or window?",
        "time_stamp": "0:04:28",
        "answer": "C",
        "options": [
          "A. Because he lost his keys and is trying to see if anyone is home.",
          "B. Because he heard a strange noise and wants to check what it is.",
          "C. Because he wanted to observe the activities of the four people outside.",
          "D. Because he is waiting for an important package to be delivered."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_237_real.mp4"
  },
  {
    "time": "[0:06:15 - 0:06:45]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does Mr.Bean drop their coat and bag?",
        "time_stamp": "00:06:27",
        "answer": "C",
        "options": [
          "A. Because he realized he grabbed the wrong coat and bag.",
          "B. Because he was startled by a sudden loud noise.",
          "C. Because his disguise mission has ended.",
          "D. Because he spotted someone he knows and wanted to quickly greet them."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_237_real.mp4"
  },
  {
    "time": "[0:08:20 - 0:08:50]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the police officer react and possibly make a call for backup or additional assistance?",
        "time_stamp": "0:08:45",
        "answer": "C",
        "options": [
          "A. Because the police officer needs to record the event.",
          "B. Because the police officer needs to clock off for the day.",
          "C. Because the police officer believes the photographs show a potential crime.",
          "D. Because the police officer wants to review the photos later."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_237_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:09]: A hand holds a set of keychains, displayed prominently against a beige background. The keychains are made of clear acrylic discs with different colored backdrops—pink, purple, blue, and mint green. Each keychain has a name written on it: Holly on the pink disc, Katie on the purple, Millie on the blue, and Eloy on the mint green. Each name is accompanied by a small white heart underneath. The keychains are attached to rings and have tassels in various colors: pink, blue, and green, which complement the discs. The clear discs show a glossy finish, reflecting some light. The hand holding them is slightly rotated to showcase different angles of the keychains. [0:00:09 - 0:00:10]: The screen transitions to black. [0:00:10 - 0:00:13]: White text on a black background appears, reading \"Personalized Acrylic Key Chain.\" [0:00:13 - 0:00:14]: The screen remains black. [0:00:14 - 0:00:18]: More white text appears, reading \"If you would like to see how I designed these keychains, watch until the end.\" [0:00:18]: The screen fades to black again. [0:00:19]: The scene changes to a workspace. A compact, teal-colored Cricut cutting machine is positioned at the top of the frame. Below it, there's a green cutting mat, gridded with white lines, placed on a black-checkered work surface. Two hands are visible, preparing the materials for crafting.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:00:20",
        "answer": "D",
        "options": [
          "A. Holding a set of keychains.",
          "B. Displaying a text message.",
          "C. Operating a Cricut cutting machine.",
          "D. Preparing materials for crafting."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_49_real.mp4"
  },
  {
    "time": "0:02:00 - 0:02:20",
    "captions": "[0:02:00 - 0:02:20] [0:02:01 - 0:02:02]: Both hands, wearing a magenta sweater, are handling several pieces of translucent, circular plastic sheets and square white papers placed on a gray grid-patterned surface. A small round piece with a light blue color is being lifted by the right hand. [0:02:03]: The left hand holds a piece of paper with a pastel pink color, while the right hand is peeling off a sticker with blue paint. [0:02:04]: The right hand now holds a purple sticker, previously peeled off from the white backing paper, while the left hand holds the same pastel pink paper. [0:02:05]: The purple sticker is now being attached to one of the circular, translucent plastic sheets with the hands carefully positioning it. [0:02:06]: The hands continue positioning and pressing down the purple sticker onto the translucent sheet, ensuring it is aligned. [0:02:07]: The hands are still pressing and adjusting the sticker to ensure it adheres correctly to the circular sheet. [0:02:08 - 0:02:09]: The hands remain focused on fixing the purple sticker onto the circular plastic sheet, smoothing out any bubbles or creases. [0:02:10]: The right hand reaches out for another piece of paper, while the left continues to handle the circle with the purple sticker. [0:02:11 - 0:02:12]: The left hand places the circular sheet with the purple sticker back onto the surface. A new piece of white backing paper with what appears to be a pink sticker is being lifted with both hands. [0:02:13 - 0:02:14]: The right hand begins peeling off a pink sticker from the backing paper while the left hand supports the paper. [0:02:15]: Both hands work together to peel off the entire pink sticker from the white backing paper. [0:02:16]: The pink sticker is now fully removed, and the right hand holds it mid-air while the left hand sets the backing paper aside. [0:02:17 - 0:02:18]: Both hands begin positioning the pink sticker onto another translucent circular sheet, ensuring it is correctly aligned. [0:02:19 - 0:02:20]: The hands continue adjusting and fixing the pink sticker onto the circular sheet, smoothing out its surface to ensure proper adhesion.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are the hands doing right now?",
        "time_stamp": "0:02:14",
        "answer": "C",
        "options": [
          "A. Peeling off a blue sticker.",
          "B. Attaching a purple sticker to a circular sheet.",
          "C. Adjusting and smoothing out a pink sticker.",
          "D. Lifting a piece of pastel pink paper."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_49_real.mp4"
  },
  {
    "time": "0:04:00 - 0:04:20",
    "captions": "[0:04:00 - 0:04:20] [0:00:01 - 0:00:05]: From the first-person perspective, a pair of hands is visible manipulating keyrings with the use of a blue-handled tool. The backdrop features a gray grid-patterned mat. There are four round keychains near the top of the frame, each with names and pastel colors. The keychains have different colored tassels: blue, pink, green, and purple. [0:00:06 - 0:00:12]: The hands continue working on the keyring. The tool is held in the right hand, used to manipulate a silver component attached to a pink-colored keychain that bears the name \"Holly” along with a small heart symbol underneath it. The activity focuses on attaching or modifying the keychain attachments. [0:00:13 - 0:00:15]: The person’s left hand moves the pink keychain with \"Holly\" positioned centrally, suggesting completion of the adjustment. Meanwhile, the other hand prepares to adjust or attach another part of the keychain. [0:00:16 - 0:00:20]: The right hand with the tool moves closer to the camera, forming a more detailed view of the attachment process. Moments later, the hands present all the keychains in one hand, showing the names \"Millie\", \"Holly\", \"Betty\", and \"Katie\", each with their distinctive color and small tassel. [0:00:21 - 0:00:25]: The keychains are held more steadily, each clearly displayed. The hands briefly shift the position to allow a full view of the names and tassels against the gray background. [0:00:26 - 0:00:30]: A transition occurs where the hand moves to display the keychains against a plain background instead of the grid mat. The names and tassels remain in clear view, capturing the keychains' intricacies and details. The colors and design elements, such as small hearts beside the names, are distinctly visible.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are the hands doing right now?",
        "time_stamp": "00:04:05",
        "answer": "C",
        "options": [
          "A. Stitching a piece of fabric.",
          "B. Writing on a notepad.",
          "C. Adjusting a pink keychain.",
          "D. Sorting different colored beads."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_49_real.mp4"
  },
  {
    "time": "0:06:00 - 0:06:20",
    "captions": "[0:06:00 - 0:06:20] [0:06:05 - 0:06:06]: The video begins with the screen displaying a software interface with a grid background. On the screen, various design elements are positioned. At the top, there are eight blobs of colors including beige, light blue, pink, pastel purple, yellow, lavender, green, and another shade of pink, all aligned in a row. Below these colored blobs, there are three large black circles placed horizontally across the middle of the screen. To the right of these black circles, the word \"Shelby\" is written in a stylish script font three times at different locations and sizes. [0:06:06 - 0:06:07]: The word \"Katie\" is typed and added beneath the second black circle, replacing one of the \"Shelby\" texts. The words \"Shelby\" and \"Katie\" appear to be used for designing purposes on this grid-based interface. [0:06:07 - 0:06:09]: \"Katie\" continues to be adjusted and repositioned under the second black circle. The text \"Shelby\" still remains under the first black circle and to the far right. [0:06:09 - 0:06:11]: No significant changes occur. The design on the screen remains mostly the same with an emphasis on the text \"Katie,\" \"Shelby,\" and the black circles. [0:06:11 - 0:06:12]: Another text box appears next to the third circle, but no text has been typed in it. The cursor may be preparing to add another name or word within the editable space. [0:06:12 - 0:06:13]: The new text box remains empty while the design involves the black circles and the various positioned text elements such as \"Shelby\" and \"Katie.\" [0:06:13 - 0:06:14]: The text box continues to exist to the right of the third black circle without any content. The design remains consistent with black circles and corresponding names. [0:06:14 - 0:06:15]: The empty text box remains. The design interface still shows no additional movements or adjustments. [0:06:15 - 0:06:16]: The word \"Millie\" is typed out within the previously empty text box to the right of the third black circle. The layout now includes \"Shelby,\" \"Katie,\" and \"Millie\" under the respective black circles. [0:06:16 - 0:06:17]: The design elements such as \"Shelby,\" \"Katie,\" and \"Millie\" are being fine-tuned possibly for alignment under each black circle within the software interface. [0:06:17 - 0:06:18]: The word Millie is now clearly seen along with \"Shelby\" and \"Katie,\" arranged under the black circles. [0:06:18 - 0:06:19]: The addition of another text box to the right of \"Shelby\" but it remains empty. Text editing and positioning tools visible on the right side of the interface. [0:06:19 - 0:06:20]: The previously empty text box next to \"Shelby\" has its cursor visible, preparing for text input. Other elements remain unchanged. [0:06:20 - 0:06:21]: The design screen with black circles, colored blobs, and texts \"Katie\", \"Shelby,\" and \"Millie\" remains mostly constant. [0:06:21 - 0:06:22]: The empty text box appears again next to \"Shelby.\" The positioning of elements on the screen remains the same. [0:06:22]: The newly added text, \"Holly,\" is placed in the previously empty text box next to \"Shelby.\" All text elements are well-aligned and visible. [0:06:22 - 0:06:23]: Enlarged view of the text shows the names are now clearly seen, with \"Katie\", \"Shelby\", \"Millie\", and newly added \"Holly\" arranged orderly. [0:06:23 - 0:06:24]: The design focuses on the three black circles while the text elements, \"Katie,\" \"Shelby,\" \"Millie,\" and \"Holly\" are arranged for a clear view. [0:06:24 - 0:06:25]: A closer look at \"Shelby,\" showing positioning and editing tools being used. Text alignment features are visible for precision in the design. [0:06:25]: The video ends with the focus on the text elements and their proper alignment. The words \"Katie\", \"Shelby\", \"Millie,\" and \"Holly\" are neatly arranged under the row of black circles.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What name is being typed out right now?",
        "time_stamp": "00:06:07",
        "answer": "C",
        "options": [
          "A. Shelby.",
          "B. Katie.",
          "C. Millie.",
          "D. Holly."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_49_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [8:08:05 - 8:08:07]: Four circular designs are displayed horizontally across a grid background. From left to right, the circles contain names: \"Shelby\" on a mint green background, \"Katie\" on a purple background, \"Millie\" on a light blue background, and \"Holly\" on a pink background, all in black cursive writing. [8:08:08 - 8:08:11]: The \"Shelby\" circle is removed, leaving three circles. The \"Katie\" circle now has a small pink heart shape, while the \"Millie\" and \"Holly\" circles remain unchanged. [8:08:12 - 8:08:15]: A small red heart shape is added to the \"Millie\" circle. The \"Katie\" and \"Holly\" circles remain the same. [8:08:16 - 8:08:18]: Another small black heart is added to the \"Millie\" circle. The \"Katie\" and \"Holly\" circles remain the same. [8:08:19 - 8:08:21]: The \"Millie\" circle now has a total of three small hearts: one red and two black. The \"Katie\" and \"Holly\" circles remain the same. [8:08:22 - 8:08:25]: The \"Katie\" circle remains the same with its single pink heart. The \"Millie\" circle is unchanged with its three small hearts. A small black heart is added to the \"Holly\" circle. [8:08:26 - 8:08:29]: Both the \"Millie\" and \"Holly\" circles now each have an additional small heart, making three small hearts in the \"Millie\" circle and one in the \"Holly\" circle. The \"Katie\" circle remains the same. [8:08:30 - 8:08:33]: A black heart is added below the names in all three circles: \"Katie,\" \"Millie,\" and \"Holly.\" [8:08:34 - 8:08:37]: A black heart appears in all three circles. The circles remain in place on the grid background. The \"Katie\" circle is moving out of view to the left. [8:08:38 - 8:08:41]: The frames show the three circles \"Katie,\" \"Millie,\" and \"Holly,\" each with a black heart below their names. The background shows a grid layout.  [8:08:41]: The \"Katie\" circle has moved completely out of sight, leaving just the \"Millie\" and \"Holly\" circles. The grid background remains the same.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the \"Katie\" circle doing right now?",
        "time_stamp": "0:08:10",
        "answer": "A",
        "options": [
          "A. Moving to the left.",
          "B. Adding a heart shape.",
          "C. Displaying a light blue background.",
          "D. Removing the label."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_49_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:08]: Two individuals are standing beside a large black vehicle. One person is wearing a striped shirt and light-colored shorts, and the other is dressed in a blue hoodie and dark pants. They are positioned close to the rear of the vehicle, which is facing a wall. The individual in the striped shirt holds a device, possibly a smartphone, looking at it attentively. The other individual appears to be conversing with the first person. Various icons and text are displayed on the right side of the frame, suggesting a streaming interface. The background has some parked cars and urban elements visible under dim lighting, indicating it might be nighttime. [0:00:09 - 0:00:20]: The person in the striped shirt continues to interact with their device, occasionally looking up. The second individual in the blue hoodie keeps observing or talking to them. After interacting for a while, the person in the striped shirt starts to move around the vehicle, walking briskly towards the front of the large black vehicle with “grip” written on the side in dark letters. The corridors seem urban with some street lighting visible, suggesting they are in a city or town at night. As the individual runs, their motion suggests urgency or purpose.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person in the striped shirt doing right now?",
        "time_stamp": "00:00:18",
        "answer": "A",
        "options": [
          "A. Walking briskly towards the front of the vehicle.",
          "B. Standing beside the vehicle and looking at their device.",
          "C. Conversing with the individual in the blue hoodie.",
          "D. Observing the background elements."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_278_real.mp4"
  },
  {
    "time": "0:02:40 - 0:03:00",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:45]: The video starts with a first-person perspective of a character walking on a sidewalk at night, accompanied by two other characters. One character is wearing a striped shirt and another is in a blue hoodie. There are buildings and street lights in the background. [0:02:46 - 0:02:50]: The characters move closer to a bus or truck parked along the sidewalk. The view shifts slightly to follow the character in the striped shirt who starts to crouch. [0:02:51 - 0:02:53]: The character in the striped shirt continues moving towards the parked vehicle, now in a crouching position. Another character follows closely behind. [0:02:54 - 0:02:56]: The perspective changes to an inventory screen showing various items. Personal information and statistics are visible on the left side, while the inventory grid is on the right. [0:02:57 - 0:03:01]: The inventory screen remains open, showing assorted items and available storage spaces. The actions of the character are not visible but seem focused on organizing or inspecting the inventory. [0:03:02 - 0:03:04]: Back to the first-person perspective, the character now stands behind the vehicle. A prompt appears on the screen offering options to \"toggle item\" or \"toggle engine.\" [0:03:05 - 0:03:08]: The character moves slightly to the side of the vehicle, and the same interaction prompt remains on the screen. [0:03:09 - 0:03:10]: The character continues to walk around the vehicle, examining it from different angles.  [0:03:11 - 0:03:12]: The interaction prompt changes as the character approaches the back side of the vehicle more closely. [0:03:13 - 0:03:14]: The character stops walking and pulls out a phone.  [0:03:15 - 0:03:17]: The character looks down at their phone, possibly interacting with it, while still standing next to the vehicle. [0:03:18 - 0:03:19]: The character puts away the phone and continues standing beside the vehicle, still looking at it. [0:03:20]: The character moves again, starting to walk around the front side of the vehicle. [0:03:21 - 0:03:23]: Now, the character proceeds towards the driver's side door and makes a motion to open it. [0:03:24 - 0:03:26]: The character opens the door and begins to climb into the driver's seat of the vehicle. [0:03:27 - 0:03:29]: With the character now seated inside the vehicle, the view switches to a third-person perspective showing the vehicle from above and behind. [0:03:30]: The vehicle starts moving forward, driving along the street. The dashboard and navigation icons are visible on the screen.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the character do just now?",
        "time_stamp": "00:02:54",
        "answer": "A",
        "options": [
          "A. Put away his phone.",
          "B. Open the driver's side door.",
          "C. Crouch next to the vehicle.",
          "D. Toggle the engine."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_278_real.mp4"
  },
  {
    "time": "0:05:20 - 0:05:40",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:29]: A dark-colored delivery van, marked with the word \"prime\" on the side, is parked near the sidewalk of a dimly lit street. The street appears to be in a commercial area with buildings close by. The streetlights and building windows provide some illumination. A person in a blue shirt stands near the back of the van, slightly obscured by the vehicle. In the background, city lights can be seen, indicating it's nighttime. [0:05:29 - 0:05:32]: The person near the van's back door begins to open it. Their posture suggests they are preparing to load or unload something. The surrounding buildings remain the same, with the same illuminated windows and low lighting conditions. [0:05:32 - 0:05:36]: The back door of the van is fully opened. The person is now clearly visible, seemingly involved in handling something inside the van. The rest of the scene remains static, with the illuminated windows providing the primary light source in the otherwise dark environment. [0:05:36 - 0:05:40]: The person at the back of the van continues to handle items. Their movements indicate they are either loading or unloading the van. The surrounding area remains unchanged, with consistent lighting from the nearby buildings and streetlights.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person near the van doing right now?",
        "time_stamp": "00:05:28",
        "answer": "A",
        "options": [
          "A. Handling items in the back of the van.",
          "B. Standing near the van without any movement.",
          "C. Opening the van's side door.",
          "D. Walking towards the street."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_278_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: The player is driving a van in a dark alley, showing the front of the van with headlights on. The surroundings are dimly lit with some red indication markers on the ground. [0:08:02 - 0:08:04]: The van is seen from a higher angle, navigating slightly to the left in the alleyway still enclosed by the red markers. [0:08:05 - 0:08:08]: The van moves forward under a low roof or possibly a parking garage entrance; the front part is illuminated with the red and black colored route on the ground. [0:08:09 - 0:08:10]: The van continues straight, visible from the rear and slightly elevated angle, as it aligns itself correctly in the parking area. [0:08:11 - 0:08:12]: The scene shifts back to a closer view of the van's side as it approaches a bay marked in green light. [0:08:13 - 0:08:14]: The van stops in the highlighted green area. The driver seems to be aligning the vehicle with a designated point. [0:08:15 - 0:08:16]: The van is seen from the rear, parked entirely within the green-marked area, inside what appears to be a loading dock. [0:08:17 - 0:08:18]: The driver exits the van into the dimly lit garage. The player character, dressed in striped clothes, steps out of the vehicle. [0:08:19]: Walking around the van, the character stops partially hidden behind the van's side. [0:08:20]: The character approaches another person standing nearby. The interface opens to show an inventory screen with various items and storage options visible.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the driver do just now?",
        "time_stamp": "00:08:15",
        "answer": "A",
        "options": [
          "A. Exiting the van.",
          "B. Entering the van.",
          "C. Turning on the headlights.",
          "D. Navigating the alley."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_278_real.mp4"
  },
  {
    "time": "0:09:40 - 0:09:45",
    "captions": "[0:09:40 - 0:09:45] [0:09:40 - 0:09:41]: The video shows a first-person perspective of someone playing a driving video game at night. The camera is positioned to show the road ahead and a portion of the right side sidewalk. The urban environment is illuminated by streetlights, building lights, and headlights from the vehicle and other cars. In the top left corner, a small window shows a person playing the game, noticeable from their focused expression and gaming setup. The vehicle's rear lights are prominently visible as red outlines in a rectangular shape. [0:09:41 - 0:09:43]: The vehicle continues moving down the road. The surroundings include palm trees on the left side and tall buildings on both sides of the street. Some windows in the buildings are lit, adding to the nighttime ambiance. The road appears slightly wet, reflecting the lights over the ground, suggesting either recent rain or a general reflective surface. [0:09:43 - 0:09:44]: As the vehicle moves forward, the player keeps a steady hand on the controls. On the right side of the image, light poles and a few small trees can be seen on the sidewalk. The digital dashboard displays current speed and other driving-related information, shown in white text. [0:09:44]: The vehicle approaches an intersection with traffic lights ahead. Red traffic lights and street signs become more visible, indicating the need to stop or slow down. The player's focus remains on the screen, ensuring accurate navigation through the urban streets. [0:09:45]: The vehicle reaches the intersection, showing a clearer view of the traffic lights now turned red. Another car is visible on the left side, ready to cross the intersection. The player remains engaged with the game, reacting to the traffic signals and planning the next move. The surroundings maintain the same urban nighttime environment with illuminated buildings and slightly wet roads.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the player focusing on right now?",
        "time_stamp": "00:09:43",
        "answer": "A",
        "options": [
          "A. Driving a truck on the road.",
          "B. Adjusting the game settings.",
          "C. Quiting the game.",
          "D. Watching a cut scene."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_278_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a black screen. [0:00:01 - 0:00:05]: The video reveals a scene in an outdoor plaza on a sunny day. Prominently featured is a large metallic globe sculpture with golden letters spelling out \"UNIVERSAL\" encircling it. The globe is set inside a circular water feature with mist rising around its base. At least two people are visible in the scene; one person is capturing a photo or video using their phone, and another person is walking to the left of the globe. The background features several palm trees and street lamps. [0:00:06 - 0:00:10]: The scene remains largely the same. However, one person, dressed in black and standing to the right of the globe, is more visible now and seems to be posing for a picture, looking towards the camera. The person who was walking before is now closer to the left side of the frame. The mist from the water feature continues to rise around the globe. [0:00:11 - 0:00:13]: The camera angle begins to shift to the right, providing a broader view of the plaza. The focus moves away from the globe sculpture to reveal more people in the background, walking around the area. The red carpet starts becoming visible in later frames. [0:00:14 - 0:00:16]: The scene captures more of the walkway, bordered by palm trees, leading towards an arched entrance in the distance. Two signs with arrows indicating \"ALREADY HAVE TICKETS?\" and \"PURCHASE TICKETS\" are visible. Several people are present, walking along the pathway. The red carpet extends further, leading towards the arched entrance. [0:00:17 - 0:00:20]: The camera continues to shift right, fully showing a wide red carpet leading towards the arched entrance of a building in the distance. More pedestrians are walking along, taking in the surroundings. The streetlights and palm trees line the path, providing a picturesque and welcoming entrance. The overall scene suggests an inviting and crowded area, with people likely visiting for entertainment purposes.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What prominent feature is located in the center of the outdoor plaza?",
        "time_stamp": "0:00:05",
        "answer": "B",
        "options": [
          "A. A large fountain.",
          "B. A metallic globe sculpture.",
          "C. A statue of a person.",
          "D. A giant screen."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Text-Rich Understanding",
        "question": "What word is encircling the large metallic globe sculpture?",
        "time_stamp": "0:00:10",
        "answer": "A",
        "options": [
          "A. UNIVERSAL.",
          "B. GLOBAL.",
          "C. EARTH.",
          "D. WORLD."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_308_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:42]: People walk along a paved pathway in a theme park setting, featuring various themed buildings designed to look like a small-town street. On the left, a group of people walks toward the camera while on the right, a child in a pink shirt moves away. [0:02:43 - 0:02:45]: The scene shows the exteriors of colorful, stylized buildings, including a green storefront and a red café named \"Power Up Café\" with a yellow question mark sign above the door. [0:02:46 - 0:02:48]: Additional elements of the setting include a sign for \"Stage 52\" on the right side and the backgrounds feature thematic posters and decorations, creating an immersive environment. [0:02:49 - 0:02:52]: The pathway is relatively empty except for occasional individuals moving through the scene. The \"Power Up Café\" continues to be a prominent feature, with its bright signage and adjacent red umbrella. [0:02:53 - 0:02:57]: Two people are standing near the entrance of the \"Power Up Café.\" One individual, wearing yellow, interacts with the door while another, dressed in red, faces inward. The background highlights more details of the café, including posters and decorative elements. [0:02:58 - 0:02:59]: A closer view of the \"Power Up Café\" shows specific details like a large billboard featuring \"Greetings from New Donk City\" and familiar-themed graphics that enhance the café's playful and vibrant aesthetic.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What was the name of the café shown just now?",
        "time_stamp": "00:03:04",
        "answer": "C",
        "options": [
          "A. The Green Café.",
          "B. The Fun Café.",
          "C. Power Up Café.",
          "D. Stage 52 Café."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_308_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:29]: The video depicts a first-person perspective as the viewer walks down a street. The street is lined with various buildings on either side, displaying an urban setting with a mix of architectural styles. On the left, there is a laundromat with a bright blue storefront, with other shops and a fire escape above it. People walk on the zebra crossing and sidewalks, including a man pushing a stroller and others walking in groups. On the right, buildings display beige facades with detailed window frames and green awnings. The street is bustling with pedestrians, some wearing backpacks, children accompanying parents, and a mix of casual summer clothing. The sky is partly cloudy. Further ahead, more structures of different colors, including a pastel blue building and others with peach and green accents, are visible. [0:05:30 - 0:05:39]: The viewer continues to walk down the street, passing more shops and people. A building with a green awning reads \"MacGuff Cinema,\" and people gather around the area. Across the street, patrons sit under green umbrellas at a cafe. The background reveals more of the street, with buildings displaying classic facades and balconies, and people engaged in various activities. The video continues to show the continuous movement of the crowd and street features, maintaining the urban atmosphere and lively environment. The flora includes some potted plants and small trees along the road.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is one of the activities that people were doing on the zebra crossing just now?",
        "time_stamp": "0:05:29",
        "answer": "B",
        "options": [
          "A. Jogging.",
          "B. Pushing a stroller.",
          "C. Riding a bicycle.",
          "D. Selling goods."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_308_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:03]: A staircase with wooden steps and railings on both sides is seen. The left wall is plain, while the right wall is made of red bricks. A person is visible near the top of the stairs, who moves away from the stairs. [0:08:04 - 0:08:08]: Reaching the top of the stairs, the layout transitions into a hallway with wooden flooring. A door with a blue color is on the right, opening into an adjacent room. [0:08:09 - 0:08:10]: Entering a room with brick walls, containing a small dining area with a wooden table and metal chairs. In the background, a kitchen area is visible, with people inside and various kitchen appliances. [0:08:11 - 0:08:14]: The scene moves towards a section with a bookshelf filled with books and decorative objects. Then, it progresses towards a room with a baby gate and people walking in and out. [0:08:15 - 0:08:19]: Entering a children's room, decorated with playful elements. The room features a bed, a small table, shelves with toys and books, colorful curtains, and decorative wall art. The presence of a green dinosaur-shaped lamp adds to the room's kid-friendly ambiance.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What material are the stairs made of?",
        "time_stamp": "00:08:03",
        "answer": "B",
        "options": [
          "A. Metal.",
          "B. Wood.",
          "C. Marble.",
          "D. Concrete."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_308_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: Several Christmas ornaments are showcased closely. The first ornament, prominently in the foreground, is a vivid blend of pink, blue, and gold hues with the text \"JINGLE bells\" inscribed in white. Surrounding it are other ornaments partially visible, with similarly vibrant mixtures of colors. The background is decorated with festive greenery. [0:00:04 - 0:00:06]: The view expands to show three ornaments fully. Positioned in the center is the \"JINGLE bells\" design, while to its left, an ornament reads \"Joyeux Noel\" in white cursive against a dark blue and gold mix. To the right, an ornament reads \"Merry Christmas\" in white cursive with a blue and purple base highlighted with gold. These ornaments are arranged among green pine branches adorned with red berries and golden ribbons. [0:00:07 - 0:00:08]: The frame becomes darker and transitions to a title screen. The text \"ALCOHOL INK ORNAMENTS\" appears in white on a solid black background. [0:00:09 - 0:00:10]: The title screen remains the same, with \"ALCOHOL INK ORNAMENTS\" still displayed. [0:00:11 - 0:00:13]: A new text appears against a soft pink background, stating, \"Templates and Materials used are listed in the description.\" [0:00:14]: A pair of hands wearing black gloves is seen holding a can of Cabot's Cabothane Clear. The text \"For this project you will need:\" appears at the bottom of the frame. [0:00:15]: The hands firmly grasp the top and bottom of the can, showcasing it to the camera. The design of the can prominently features yellow and blue colors with detailed text and branding. [0:00:16 - 0:00:19]: The hands continue holding the can as the text \"Water based Polyurethane\" appears below the can image.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What are the prominent colors of the ornament in the middle of the video?",
        "time_stamp": "00:00:03",
        "answer": "A",
        "options": [
          "A. Pink, blue, and gold.",
          "B. Red, green, and silver.",
          "C. Black, white, and blue.",
          "D. Yellow, orange, and purple."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_48_real.mp4"
  },
  {
    "time": "0:01:40 - 0:02:00",
    "captions": "[0:01:40 - 0:02:00] [0:01:40 - 0:01:50]: A pair of hands wearing black gloves is holding a translucent, spherical ornament with a matte, slightly cloudy surface. The ornament is being held above a small, clear plastic cup that is situated on top of an open tin containing a golden rim. The background is a blue grid mat.  [0:01:50 - 0:01:56]: The hands continue to hold the ornament as it is being tilted slightly to one side, aligning the opening of the ornament with the cup in the tin below, suggesting the intention to pour out excess liquid from the ornament.  [0:01:56 - 0:01:58]: The view remains the same, with the ornament's opening now directly above the cup, continuing to suggest the pouring action without any visible liquid. The text on the frame reads \"Pour excess liquid back into the tin.\" [0:01:58 - 0:02:00]: The ornament remains in the same position, held steadily above the cup, ensuring that any residual liquid drains out completely. [0:02:00 - 0:02:05]: The process of pouring continues as the text changes to \"Let the ornament drain for 10 minutes,\" indicating that the ornament is to be left in this position for proper drainage. The hands maintain their grip on the ornament to prevent any movement.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are the black-gloved hands doing right now?",
        "time_stamp": "0:02:00",
        "answer": "A",
        "options": [
          "A. Holding the ornament steadily above the cup for drainage.",
          "B. Placing the ornament on a table.",
          "C. Shaking the ornament to remove excess liquid.",
          "D. Filling the ornament with a new liquid."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_48_real.mp4"
  },
  {
    "time": "0:03:20 - 0:03:40",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:22]: A person wearing black gloves holds a round glass ornament, decorated with vibrant swirls of blue, purple, pink, and gold. On a white cloth beneath the ornament, several small bottles of different colors are placed alongside some bottle caps. [0:03:22 - 0:03:24]: The gloved hands position the ornament closer to the camera while also holding a small bottle. [0:03:24 - 0:03:26]: The person starts applying the contents of the small bottle to the opening of the ornament. [0:03:26 - 0:03:28]: The hands carefully manipulate the ornament and the small bottle, spreading the liquid inside. [0:03:28 - 0:03:30]: The person closely examines the ornament, ensuring that the liquid inside it is evenly distributed. [0:03:30 - 0:03:32]: The ornament is placed back onto the white cloth while the person retrieves a blue funnel. [0:03:32 - 0:03:34]: The funnel is inserted into the ornament's opening. [0:03:34 - 0:03:35]: The person adjusts the funnel and prepares to use it. [0:03:35 - 0:03:40]: The person holds a pink container labeled “Glitter,” and begins pouring fine white glitter into the funnel. The glitter fills the gaps inside the ornament, adding a sparkling effect.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now with the pink container labeled \"Glitter\"?",
        "time_stamp": "0:03:38",
        "answer": "A",
        "options": [
          "A. Pouring fine white glitter into the funnel.",
          "B. Sprinkling glitter on the white cloth.",
          "C. Decorating the ornament with paint.",
          "D. Mixing different colored liquids inside the ornament."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_48_real.mp4"
  },
  {
    "time": "0:05:00 - 0:05:20",
    "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:01]: Two black-gloved hands are holding a spherical object. The sphere has various colorful patterns, including blue, purple, and gold. One hand holds a bottle with a metallic cap, currently applying a golden liquid to the sphere's opening. The sphere is positioned above a white cloth on a blue surface, with a couple of other small bottles and a white container nearby. There is a grid visible on the blue surface in the background. [0:05:01 - 0:05:02]: The hands are continuing to apply the golden liquid from the bottle to the opening of the colorful sphere. The sphere has distinctive patterns that include areas of blue, purple, teal, and golden shades blending together. The white cloth underneath, on a grid-patterned blue background, remains in place. The additional bottles and a white container are still visible on the right side of the frame. [0:05:02 - 0:05:03]: The right hand is now holding the liquid bottle away from the sphere, which the left hand is rotating slightly, showing a full view of the colorful design, including patches of gold, blue, and purple colors. The white cloth below is still visible, laying on the blue grid-patterned background. The additional materials from the previous frames are placed to the right. [0:05:03 - 0:05:04]: Holding the sphere steady, the right hand tilts the bottle, with the golden cap still attached, closer to the opening of the sphere. The colors on the sphere, including blues, purples, and gold, remain vivid and intricate. The white cloth remains spread on the blue grid-patterned background, with additional items still in their previous positions. [0:05:04 - 0:05:05]: The right hand is pouring the golden liquid into the opening of the colorful sphere. The sphere's patterns are clear, with swirling blue, purple, and gold designs. The white cloth underneath continues to lie on the blue grid background. The surrounding objects remain unaltered, positioned slightly right of the sphere. [0:05:05 - 0:05:06]: The hands hold the sphere and the bottle in a similar manner to the previous frames. The golden liquid continues to be applied to the opening of the sphere. The intricate colorful patterns, including blue, purple, and gold, are clear on the sphere. The white cloth on the blue grid-patterned surface and the surrounding bottles maintain their positions. [0:05:06 - 0:05:07]: The hands slightly rotate the sphere while continuing to apply the golden liquid from the bottle into the opening. The sphere’s colorful design remains prominent, showcasing swirls of blue, purple, and gold. The white cloth still lies on the blue grid background, with additional items on the right remaining in place. [0:05:07 - 0:05:08]: The right hand holds the liquid bottle slightly away from the sphere, which is being rotated by the left hand. The colorful patterns of the sphere, consisting of blue, purple, and gold, are still visible. The white cloth on the blue grid-pattern background and the nearby small bottles and container remain unchanged. [0:05:08 - 0:05:09]: The right hand brings the liquid bottle closer to the sphere's opening, continuing to pour the golden liquid. The left hand holds and slightly rotates the colorful sphere, showcasing the various shades of blue, purple, and gold. The white cloth under the sphere lies on the blue grid background, with the additional items standing to the right. [0:05:09 - 0:05:10]: The sphere is held steady, with the right hand positioning the bottle close to its opening. The intricate patterns of blue, purple, and gold on the sphere remain visible. The white cloth on the blue grid background and the surrounding bottles and container are still present in the same arrangement. [0:05:10 - 0:05:11]: The right hand continues to pour the golden liquid into the opening of the sphere, while the left hand holds it securely. The colorful patterns, blending blue, purple, and gold, are clear. The white cloth on the blue grid background, with the additional bottles and container to the right, remain in place. [0:05:11 - 0:05:12]: The hands rotate the sphere slightly while applying the golden liquid around its opening. The colorful patterns of the sphere, featuring blue, purple, and gold hues, are visible. The white cloth, spread on the blue grid-patterned surface, and the nearby bottles and container maintain their positions. [0:05:12 - 0:05:13]: The right hand tilts the bottle, applying more golden liquid near the sphere’s opening. The colorful sphere, with",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are the hands doing right now?",
        "time_stamp": "0:05:07",
        "answer": "A",
        "options": [
          "A. Rotating the sphere while applying golden liquid.",
          "B. Placing the sphere on a white cloth.",
          "C. Cleaning the sphere with a cloth.",
          "D. Removing the golden liquid bottle."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_48_real.mp4"
  },
  {
    "time": "0:06:40 - 0:07:00",
    "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:50]: A pair of gloved hands is seen holding a circular ornament in the left hand and a bottle of metallic-colored paint in the right hand. The ornament, predominantly pink and purple with a glossy finish, has two white spots near its surface. The background is a blue, grid-patterned work mat with several bottles of colored paints arranged neatly to the right side of the frame. A small transparent plastic cup is positioned between the hands, just below the ornament. The metallic-colored paint bottle is tilted towards the ornament, and some paint is applied on its surface.  [0:06:51 - 0:06:59]: The ornament is set down on the work mat, its colorful surface now adorned with additional paint. The pair of hands then picks up a bottle containing blue-colored paint. The bottle is held close to the ornament, and its cap is being unscrewed. The scene includes the same components, such as the work mat, the paint bottles, and the small plastic cup remaining in their positions on the right side of the frame.  [0:06:53 - 0:06:56]: The gloved hands begin to apply the blue paint to the ornament, introducing new streaks and patches of color. The ornament is rotated to ensure even coverage. The background layout remains the same, with the ornament taking center stage and the paint bottle being handled with precision. [0:06:57 - 0:06:59]: The paint bottle is set back on the work mat, and the ornament, now displaying a vibrant mixture of pink, purple, and blue hues, is closely examined. The focus is on the painted ornament, while the bottles of paint and the small plastic cup stay positioned to the right of the frame. The created design on the ornament becomes more intricate and defined with each application of paint.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the person do just now with the metallic-colored paint bottle?",
        "time_stamp": "00:06:50",
        "answer": "A",
        "options": [
          "A. Applied paint to the ornament.",
          "B. Shook the bottle vigorously.",
          "C. Poured paint into a plastic cup.",
          "D. Closed the bottle's cap."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_48_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the predominant vehicle type visible on the right side of the street right now?",
        "time_stamp": "00:00:11",
        "answer": "B",
        "options": [
          "A. Red truck.",
          "B. Yellow car.",
          "C. White van.",
          "D. Blue bicycle."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_398_real.mp4"
  },
  {
    "time": "[0:01:56 - 0:02:01]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the weather condition right now?",
        "time_stamp": "00:01:56",
        "answer": "C",
        "options": [
          "A. Sunny.",
          "B. Snowy.",
          "C. Rainy.",
          "D. Clear."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_398_real.mp4"
  },
  {
    "time": "[0:03:52 - 0:03:57]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which type of establishment has a visible sign on the left side of the street right now?",
        "time_stamp": "00:03:52",
        "answer": "C",
        "options": [
          "A. A cafe.",
          "B. A library.",
          "C. A tea room.",
          "D. A bookstore."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_398_real.mp4"
  },
  {
    "time": "[0:05:48 - 0:05:53]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which company logo is visible on a truck right now?",
        "time_stamp": "00:05:50",
        "answer": "B",
        "options": [
          "A. FedEx.",
          "B. Ryder.",
          "C. UPS.",
          "D. Amazon."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_398_real.mp4"
  },
  {
    "time": "[0:07:44 - 0:07:49]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which establishment's sign is visible in the video right now?",
        "time_stamp": "00:07:47",
        "answer": "B",
        "options": [
          "A. Starbucks.",
          "B. Dunkin'.",
          "C. McDonald's.",
          "D. Subway."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_398_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the traffic light now?",
        "time_stamp": "00:00:02",
        "answer": "C",
        "options": [
          "A. Yellow.",
          "B. Red.",
          "C. Green.",
          "D. Black."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_184_real.mp4"
  },
  {
    "time": "[0:01:58 - 0:02:18]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the fenced area located right now?",
        "time_stamp": "00:02:03",
        "answer": "D",
        "options": [
          "A. Only on the left side of the road.",
          "B. Directly behind the cyclist.",
          "C. Directly ahead of the cyclist.",
          "D. On both sides of the road."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_184_real.mp4"
  },
  {
    "time": "[0:03:56 - 0:04:16]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the stop sign located right now?",
        "time_stamp": "00:03:54",
        "answer": "D",
        "options": [
          "A. On the left side of the road.",
          "B. In the middle of the crossroad.",
          "C. Above the traffic lights.",
          "D. On the right side of the road."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_184_real.mp4"
  },
  {
    "time": "[0:05:54 - 0:06:14]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is placed along the side of the road right now?",
        "time_stamp": "00:05:55",
        "answer": "D",
        "options": [
          "A. Tree barriers.",
          "B. Cones.",
          "C. Metal fences.",
          "D. Hay bales."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_184_real.mp4"
  },
  {
    "time": "[0:07:52 - 0:08:12]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located in front of the cyclists right now?",
        "time_stamp": "00:08:07",
        "answer": "D",
        "options": [
          "A. A finishing line.",
          "B. A straight road.",
          "C. A roundabout.",
          "D. A sharp left turn."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_184_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:03]: A person wearing a teal shirt stands in front of a kitchen counter beginning to cook. Two frying pans rest on the stove, with the one on the left emitting a small amount of steam. A wooden shelf filled with glass containers and dishes decorates the white-tiled backsplash behind them. The person grasps a cauliflower head with both hands, positioning it in front of them;  [0:02:04 - 0:02:10]: The person lowers the cauliflower towards one of the frying pans on the stove, tilting it to a vertical position. Various kitchen utensils, bottles, and a cutting board are visible on the left side of the counter. They place the cauliflower into the pan with both hands;  [0:02:11 - 0:02:13]: An overhead view focuses on two frying pans—the left one containing the cauliflower. The person's right hand picks up a pinch of seasoning or salt from a nearby container and sprinkles it over the cauliflower;  [0:02:14 - 0:02:16]: The person's left hand retrieves another pinch of seasoning or salt from the container and again sprinkles it over the cooking cauliflower;  [0:02:17 - 0:02:19]: After seasoning, the person steps aside to grab a small plate from the left side of the counter. The kitchen is equipped with an array of utensils, jars, and bottles neatly arranged near the stove.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What does the person in the teal shirt grasp with both hands?",
        "time_stamp": "0:02:04",
        "answer": "A",
        "options": [
          "A. A cauliflower head.",
          "B. A frying pan.",
          "C. A cutting board.",
          "D. A glass container."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Action Recognition",
        "question": "What does the person do after picking up a pinch of seasoning or salt?",
        "time_stamp": "0:02:18",
        "answer": "D",
        "options": [
          "A. Places it on the cutting board.",
          "B. Puts it back in the container.",
          "C. Hands it to someone else.",
          "D. Sprinkles it over the cauliflower."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_38_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:05]: A person is seen from a first-person perspective, holding a small bottle of golden liquid above a glass mixing bowl placed on a wooden cutting board. The person is pouring the liquid into the bowl containing chopped green ingredients while stirring with a spoon. The setting is a kitchen with a stove and a saucepan on the left side. Cabinets in blue and wooden tones are visible in the background.  [0:06:06 - 0:06:12]: The scene zooms out, showing more of the kitchen and the person. There are various kitchen items on the countertop, such as plates, and small glass containers holding spices. The person is continuing to pour the golden liquid into the mixing bowl and stirring. The background reveals a brick wall with shelves holding kitchen utensils and ingredients.  [0:06:13 - 0:06:15]: The view shifts to an overhead perspective, showing the person stirring the mixture in the glass bowl placed on the wooden cutting board with other ingredients like butter and pepper in small bowls positioned around it.  [0:06:16 - 0:06:18]: The person adds another ingredient from a small container into the mixing bowl and continues mixing. The overhead perspective provides a clear view of the various ingredients and utensils arranged around the workspace.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person holding above the glass mixing bowl?",
        "time_stamp": "0:06:06",
        "answer": "A",
        "options": [
          "A. A small bottle of golden liquid.",
          "B. A jar of honey.",
          "C. A bottle of olive oil.",
          "D. A cup of vinegar."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Object Recognition",
        "question": "Which utensil is the person using to stir the mixture?",
        "time_stamp": "0:06:15",
        "answer": "A",
        "options": [
          "A. Spoon.",
          "B. Fork.",
          "C. Whisk.",
          "D. Spatula."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Clips Summarize",
        "question": "What is the overall activity taking place in the kitchen?",
        "time_stamp": "0:06:18",
        "answer": "A",
        "options": [
          "A. Making a delicate cauliflower dish.",
          "B. Baking a cake.",
          "C. Making a sandwich.",
          "D. Cooking pasta."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_38_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which famous fast-food chain's logo can be seen on the left side of the street right now?",
        "time_stamp": "00:00:04",
        "answer": "D",
        "options": [
          "A. Starbucks.",
          "B. McDonald's.",
          "C. Subway.",
          "D. Chipotle."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_402_real.mp4"
  },
  {
    "time": "[0:01:58 - 0:02:03]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the traffic light right now?",
        "time_stamp": "00:01:03",
        "answer": "D",
        "options": [
          "A. Red.",
          "B. Yellow.",
          "C. Blue.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_402_real.mp4"
  },
  {
    "time": "[0:03:56 - 0:04:01]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the name of the parking facility visible on the street sign right now?",
        "time_stamp": "00:03:56",
        "answer": "D",
        "options": [
          "A. Central Parking.",
          "B. Manhattan Parking.",
          "C. New York Parking.",
          "D. Lincoln Center Parking."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_402_real.mp4"
  },
  {
    "time": "[0:05:54 - 0:05:59]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What type of weather condition is being experienced right now?",
        "time_stamp": "00:05:57",
        "answer": "D",
        "options": [
          "A. Sunny and clear.",
          "B. Snowy and blowing.",
          "C. Foggy and misty.",
          "D. Rainy and wet."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_402_real.mp4"
  },
  {
    "time": "[0:07:52 - 0:07:57]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "Which type of vehicle can be seen parked on the left side of the street right now?",
        "time_stamp": "00:07:53",
        "answer": "C",
        "options": [
          "A. A truck.",
          "B. A bicycle.",
          "C. A white car.",
          "D. A motorcycle."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_402_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the green banner located right now?",
        "time_stamp": "00:00:04",
        "answer": "D",
        "options": [
          "A. Hanging above the cyclists.",
          "B. On the left side of the road.",
          "C. Directly behind the cyclists.",
          "D. On the both sides of the road."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_179_real.mp4"
  },
  {
    "time": "[0:02:01 - 0:02:21]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the flag located right now?",
        "time_stamp": "00:02:18",
        "answer": "D",
        "options": [
          "A. Hanging on the left side of the road.",
          "B. Hanging above the cyclists.",
          "C. Hanging behind the cyclists.",
          "D. Hanging on the right side of the road."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_179_real.mp4"
  },
  {
    "time": "[0:04:02 - 0:04:22]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the cyclist located relative to the National Championships banner right now?",
        "time_stamp": "00:04:05",
        "answer": "D",
        "options": [
          "A. Far down the road from the banner.",
          "B. Before reaching the banner.",
          "C. Turned away from the banner.",
          "D. Just crossing underneath the banner."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_179_real.mp4"
  },
  {
    "time": "[0:06:03 - 0:06:23]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the position of the tall building with many windows right now?",
        "time_stamp": "00:06:09",
        "answer": "D",
        "options": [
          "A. On the left side of the road.",
          "B. Directly in front of the cyclists.",
          "C. Behind the cyclists.",
          "D. On the right side of the road."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_179_real.mp4"
  },
  {
    "time": "[0:08:04 - 0:08:24]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the current positioning of the National Championships banner right now?",
        "time_stamp": "00:08:17",
        "answer": "D",
        "options": [
          "A. Behind the cyclists.",
          "B. On the left side of the road.",
          "C. On the right side of the road.",
          "D. Directly above the cyclists."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_179_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is noticeable about the item emitting smoke on the right side of the street?",
        "time_stamp": "00:00:05",
        "answer": "C",
        "options": [
          "A. It is completely covered in white paint.",
          "B. It has black stripes.",
          "C. It is orange and white with caution tapes around it.",
          "D. It is blue and white with warning signs around it."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_396_real.mp4"
  },
  {
    "time": "[0:01:35 - 0:01:40]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What type of vehicle is on the left-hand side traveling through the intersection right now?",
        "time_stamp": "00:01:35",
        "answer": "B",
        "options": [
          "A. Sedan.",
          "B. SUV.",
          "C. Taxi.",
          "D. Motorcycle."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_396_real.mp4"
  },
  {
    "time": "[0:03:10 - 0:03:15]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the traffic light color right now?",
        "time_stamp": "00:03:10",
        "answer": "C",
        "options": [
          "A. Red.",
          "B. Yellow.",
          "C. Green.",
          "D. Blinking."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_396_real.mp4"
  },
  {
    "time": "[0:04:45 - 0:04:50]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the traffic light color right now?",
        "time_stamp": "00:04:36",
        "answer": "A",
        "options": [
          "A. Red.",
          "B. Yellow.",
          "C. Green.",
          "D. Blinking."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_396_real.mp4"
  },
  {
    "time": "[0:06:20 - 0:06:25]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the man wearing on his head right now?",
        "time_stamp": "00:06:22",
        "answer": "C",
        "options": [
          "A. A red cap.",
          "B. A white hat.",
          "C. A blue cap.",
          "D. A black hat."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_396_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The scene starts with an individual, dressed in a dark blue denim shirt and white t-shirt, seated behind a desk with a black laptop bearing a green emblem in front of them. He is leaning slightly forward, hands gestured in front, seemingly speaking to the camera.  [0:00:01 - 0:00:03]: The individual is seen continuing his explanation. The background has a geometric blue pattern, and there are shelves on both sides with decorative items like awards, books, and devices. [0:00:03]: The screen turns mostly black with a small green highlighted rectangle containing three vertical circles. [0:00:04 - 0:00:07]: A close-up of a laptop screen displaying a futuristic cityscape with sci-fi elements. Several parts are highlighted in neon green lines focused from left to right on different laptop components and performance metrics. [0:00:08 - 0:00:12]: A split-screen comparison between two graphics cards, labeled \"RTX 3080\" and \"RTX 3070,\" accompanied by their respective diagrams and computational capabilities. Then shifts to a more detailed internal structure of GPU components labeled \"GA104\" and \"GA102\". [0:00:13 - 0:00:14]: Continues to show a detailed comparison between \"GA104\" and \"GA102\" with additional labels and metrics, focusing on the differences in GPU build and performance. [0:00:15 - 0:00:16]: Comparison of two laptop models. On the left, features including \"RTX 3080 Ti\", \"GA103\", \"16GB GDDR6\", \"90-150W\" are shown. On the right, the features of \"RTX 3070 Ti\", \"GA104\", \"8GB GDDR6\", \"80-125W\" are highlighted. A laptop showing a high-action war video game scene on the display is in the foreground. [0:00:17 - 0:00:20]: The comparison and graphics details continue with the gaming laptop remaining in focus. The specifications of \"RTX 3080 Ti\" and \"RTX 3070 Ti\" appear in green text on the left, indicating the types and performance levels of the graphics cards. The action game display remains unchanged.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which graphics cards are being compared right now?",
        "time_stamp": "00:00:20",
        "answer": "A",
        "options": [
          "A. RTX 3080 Ti and RTX 3070 Ti.",
          "B. RTX 3060 and RTX 3070 Ti.",
          "C. RTX 3080 Ti and RTX 3090.",
          "D. RTX 3070 and RTX 3080 Ti."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_112_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:03:20]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:04]: The video starts with a computer screen displaying various performance monitoring software. These include readings of CPU and GPU usage, along with a list of system processes and their temperatures. There is a graph showing CPU usage over time and a separate window displaying GPU temperatures and power consumption. [0:03:05 - 0:03:08]: The video transitions to a screen with text in Chinese characters and two measurements: \"CPU Package Power: 44.996 W\" and \"GPU Board Power Draw: 150.4 W.\" [0:03:09 - 0:03:11]: The screen remains static with the same information displayed regarding CPU and GPU power consumption. [0:03:12 - 0:03:14]: The video shows a man sitting at a desk with a laptop displaying the Razer logo. He is mid-action, gesturing with his hands while speaking. [0:03:15 - 0:03:16]: The man at the desk has stopped gesturing and is now clasping his hands together while continuing to speak. Various objects are visible on the shelves behind him. [0:03:17 - 0:03:18]: The man at the desk continues to speak calmly while resting one hand on the laptop, maintaining a focus on the camera. [0:03:19 - 0:03:20]: The video shows a different screen labeled \"3DMark Ti\" and \"3DMark Time Spy,\" displaying comparison benchmarks between \"RTX 3080 Ti\" and \"RTX 3080.\" The benchmarks show performance scores for the two graphics cards with accompanying bar graphs displaying relative performance levels.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which graphics cards are being compared right now?",
        "time_stamp": "00:03:11",
        "answer": "A",
        "options": [
          "A. RTX 3080 Ti and RTX 3080.",
          "B. RTX 3060 and RTX 3070 Ti.",
          "C. RTX 3080 Ti and RTX 3090.",
          "D. RTX 3070 and RTX 3080 Ti."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_112_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:06:20]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:02]: The video starts with a static image of a benchmarking screen. The screen features performance comparisons between two graphics cards: the RTX 3080 Ti and the RTX 3080. The benchmark results display metrics in 2K and 4K resolutions for a game. Numerical values for FPS, clock speed, temperature, power, and CPU usage are shown on the right side for each card. The background of the screen shows a blurry game scene with some indistinguishable objects. [0:06:03 - 0:06:04]: The next sequence features a man in a blue shirt and glasses seated at a desk with a black laptop in front of him. He appears to be presenting or talking about the laptop, gesturing with his hands as he speaks. Behind him is a wall with a geometric blue pattern, and shelves holding various items like a globe, speakers, and awards. [0:06:05 - 0:06:08]: The man continues to talk, shifting his gaze occasionally between the laptop and the camera. His facial expressions change as he explains something, indicating engagement with his presentation. The background remains the same with slight variations in his posture and gestures. [0:06:09 - 0:06:13]: The scene transitions to an in-game view from the God of War video game. The camera follows the main character, a muscular man with a bald head, wearing Norse-inspired armor and carrying an axe. He walks through a rugged, snowy landscape, crossing a river. Water splashes around, and rocky terrain is visible in the background. [0:06:14 - 0:06:18]: The character continues to navigate through the environment, making his way towards a large cave entrance. He climbs into a small boat and rows forward, illuminated by a torchlight that adds a warm glow contrasting with the cold surroundings. The graphics showcase detailed textures and dynamic lighting effects in the game environment. [0:06:19 - 0:06:20]: As the character rows deeper into the cave, the surrounding environment becomes darker, with stalactites hanging from the ceiling and light reflections on the water surface. The character's movements are smooth, and the overall ambiance suggests exploration and adventure. The screen also displays performance metrics similar to those shown earlier in the benchmarking segment.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What metrics are displayed on the benchmarking screen for each graphics card right now?",
        "time_stamp": "00:06:00",
        "answer": "A",
        "options": [
          "A. FPS, clock speed, temperature, power, and CPU usage.",
          "B. Memory usage, CPU temperature, and power draw.",
          "C. Ping rate, jitter, and packet loss.",
          "D. Frame time, GPU load, and VRAM usage."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_112_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:20]",
    "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:02]: The video displays a bar chart with the title \"《荒野大镖客：救赎2》游戏平均帧\" (Red Dead Redemption 2 - Game Average FPS). Three graphics card setups are compared in terms of maximum frame rates (fps) using 2K+DLSS and 4K+DLSS settings. The RTX 3080 Ti with Intel i9 12900K and 32G DDR5 4800 achieves 125 fps at 2K+DLSS and 91 fps at 4K+DLSS. The RTX 3080 Ti in the Razer Blade 17 setup records 91 fps at 2K+DLSS and 66 fps at 4K+DLSS. The RTX 3080 in the Lenovo Y900K setup achieves 84 fps at 2K+DLSS and 58 fps at 4K+DLSS.  [0:09:03 - 0:09:08]: The scene remains consistent, with the same bar chart and data displayed, focusing on clarity and readability. The background has a gradient effect, adding a polished appearance to the graphical data presentation. [0:09:09 - 0:09:12]: The video transitions to another bar chart, this one with the title \"《战神4》游戏平均帧\" (God of War 4 - Game Average FPS). In this chart, the RTX 3080 Ti with Intel i9 12900K and 32G DDR5 4800 achieves the highest frame rates, recording 135 fps at 2K+DLSS and 101 fps at 4K+DLSS. The RTX 3080 Ti in the Razer Blade 17 setup records 91 fps at 2K+DLSS and 66 fps at 4K+DLSS. The RTX 3080 in the Lenovo Y900K setup achieves 87 fps at 2K+DLSS and 63 fps at 4K+DLSS.  [0:09:13 - 0:09:17]: This segment continues to display the same bar chart and data for \"God of War 4\", maintaining focus on the comparative performance of the different setups. The background also includes a blurred depiction of scenes from the game, giving context to the performance data. [0:09:18]: The video transitions to another benchmark comparison chart titled \"《极限竞速：地平线5》游戏平均帧\" (Forza Horizon 5 - Game Average FPS). In this chart, the RTX 3080 Ti with Intel i9 12900K and 32G DDR5 4800 achieves 109 fps at 2K+DLSS and 82 fps at 4K+DLSS. The Razer Blade 17 setup records 78 fps at 2K and 57 fps at 4K. The Lenovo Y900K setup achieves 78 fps at 2K and 58 fps at 4K. The background of this chart includes an image of a racing car, giving context to the gaming performance data provided.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What frame rate does the RTX 3080 Ti with Intel i9 12900K and 32G DDR5 4800 achieve for \"God of War 4\" at 4K+DLSS right now?",
        "time_stamp": "00:09:20",
        "answer": "A",
        "options": [
          "A. 101 fps.",
          "B. 135 fps.",
          "C. 91 fps.",
          "D. 87 fps."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_112_real.mp4"
  },
  {
    "time": "[0:11:00 - 0:11:17]",
    "captions": "[0:11:00 - 0:11:17] [0:11:00 - 0:11:05]: The video starts with a close-up shot of a computer motherboard, focusing on the NVIDIA GPU which is surrounded by several small components like capacitors and resistors. The GPU is rectangular with a shiny silver surface that displays the NVIDIA logo and some specifications. Nearby, a secondary chip can be seen within a red frame, and several circular mounting points are positioned around the components. The motherboard has a dark blue or black background with detailed circuit patterns. [0:11:05 - 0:11:08]: The scene shifts to a man sitting at a desk in a modern, well-lit room. He is wearing glasses and a denim jacket over a graphic t-shirt. He appears to be explaining something, gesturing with his hands while a laptop with a green logo on the back is open in front of him. The background has a blue geometric pattern with shelves on either side, holding various items like books, speakers, and a helmet. [0:11:08 - 0:11:13]: The man continues speaking, emphasizing his points with hand gestures. His expression is attentive, and he occasionally looks directly at the camera. Overlay graphics appear on the screen showing various icons, such as a thumbs-up button, a coin with a slash through it, a star, and an arrow, suggesting options like liking, voting, saving, and sharing the content. These graphics move across the screen as he speaks. [0:11:13 - 0:11:17]: The video transitions to an animated outro featuring a spinning logo and Chinese characters. The logo resembles a gear and a piece of electronic equipment, glowing in white against a black background filled with small, star-like dots. The words next to the logo also glow brightly, pulsating as the logo moves. The outro gives a dynamic and energetic feel, suggesting the end of the content.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What appears in the overlay graphics on the screen right now?",
        "time_stamp": "00:11:13",
        "answer": "A",
        "options": [
          "A. User interaction options.",
          "B. Weather icons.",
          "C. Application logos.",
          "D. Navigation tools."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_112_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the content of the video just shown?",
        "time_stamp": "00:00:10",
        "answer": "A",
        "options": [
          "A. An individual adjusts the settings on a coffee grinder, preparing it for use.",
          "B. An individual sets the timer on a toaster oven, ready to bake.",
          "C. An individual calibrates an oven's temperature for baking pastries.",
          "D. An individual measures the exact amount of coffee grounds for a recipe."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_361_real.mp4"
  },
  {
    "time": "[0:01:55 - 0:02:05]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What is the best summary of the actions performed just now?",
        "time_stamp": "00:02:05",
        "answer": "A",
        "options": [
          "A. The individual prepared a coffee by grinding the beans and operating an espresso machine.",
          "B. The individual assembled a sandwich with various ingredients and served it to a customer.",
          "C. The individual organized a stationary set, placing pens and pencils in a container.",
          "D. The individual arranged some tools on a workbench, preparing for a repair task."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_361_real.mp4"
  },
  {
    "time": "[0:03:50 - 0:04:00]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the content of the video just shown?",
        "time_stamp": "0:04:00",
        "answer": "A",
        "options": [
          "A. A person is preparing for steaming the milk.",
          "B. A barista prepares and serves a complex coffee drink, adjusting various machines and ingredients to ensure perfection.",
          "C. An individual prepares various ingredients for a large meal, organizing and setting them on a counter.",
          "D. A chef expertly places different kitchen tools in an organized manner to prepare for a cooking session."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_361_real.mp4"
  },
  {
    "time": "[0:05:45 - 0:05:55]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual just now?",
        "time_stamp": "0:05:55",
        "answer": "A",
        "options": [
          "A. The individual ground coffee beans, tamped the coffee grounds, and prepared an espresso shot.",
          "B. The individual operated a juicer, filled a glass with orange juice, and served it to a customer.",
          "C. The individual brewed a pot of tea, poured it into cups, and arranged cookies on a plate.",
          "D. The individual prepared a milkshake, added toppings, and served it with a straw."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_361_real.mp4"
  },
  {
    "time": "[0:07:40 - 0:07:50]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the action just taken by the individual?",
        "time_stamp": "00:07:50",
        "answer": "A",
        "options": [
          "A. The individual is steaming the milk.",
          "B. The individual is arranging paper cups for use.",
          "C. The individual is preparing a drink by adding a mixer.",
          "D. The individual is cleaning a stainless steel jug."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_361_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why are they two people shaking hands now?",
        "time_stamp": "0:01:57",
        "answer": "B",
        "options": [
          "A. Because they just reached an important business agreement.",
          "B. Because they appeared together in a photo when they were young.",
          "C. Because they are meeting for the first time after many years.",
          "D. Because they are congratulating each other on a job well done."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_238_real.mp4"
  },
  {
    "time": "[0:02:09 - 0:02:39]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why is Mr. Bean very angry now?",
        "time_stamp": "00:03:00",
        "answer": "A",
        "options": [
          "A. Because another person ate all the food in the refrigerator.",
          "B. Because someone spilled a drink on his favorite chair.",
          "C. Because his television stopped working during his favorite show.",
          "D. Because he just realized he lost his wallet."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_238_real.mp4"
  },
  {
    "time": "[0:04:18 - 0:04:48]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the person in the smaller bed become annoyed and awake?",
        "cause": "The person with a larger build interacts with the other person.",
        "effect": "The person in the smaller bed becomes visibly annoyed and awake.",
        "time_stamp": "0:04:28",
        "answer": "B",
        "options": [
          "A. Because the person with a larger build speaks loudly.",
          "B. Because the snoring sound from sleeping on the ground is too loud.",
          "C. Because the person in the smaller bed has a nightmare.",
          "D. Because a noise from outside wakes them up."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_238_real.mp4"
  },
  {
    "time": "[0:06:27 - 0:06:57]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why is Mr. Bean very angry now?",
        "time_stamp": "0:06:42",
        "answer": "B",
        "options": [
          "A. Because someone borrowed his car without asking.",
          "B. Because another person also ate the ice cream that belonged to Mr. Bean.",
          "C. Because his favorite TV show was canceled.",
          "D. Because his favorite shirt got ruined in the wash."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_238_real.mp4"
  },
  {
    "time": "[0:08:36 - 0:09:06]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does Mr. Bean want to escape?",
        "time_stamp": "0:10:17",
        "answer": "B",
        "options": [
          "A. Because he accidentally broke something in the store.",
          "B. Because he doesn't want to pay the bill.",
          "C. Because he saw someone he is trying to avoid.",
          "D. Because he is late for an important appointment."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_238_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the bus zone sign visible right now?",
        "time_stamp": "00:00:04",
        "answer": "A",
        "options": [
          "A. Red and white.",
          "B. Blue and white.",
          "C. Green and white.",
          "D. Black and yellow."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_453_real.mp4"
  },
  {
    "time": "[0:03:20 - 0:03:25]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the traffic light right now?",
        "time_stamp": "00:03:19",
        "answer": "A",
        "options": [
          "A. Green.",
          "B. Yellow.",
          "C. Red.",
          "D. Blue."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_453_real.mp4"
  },
  {
    "time": "[0:06:40 - 0:06:45]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What can be identified about the current traffic light status for vehicles?",
        "time_stamp": "00:06:42",
        "answer": "C",
        "options": [
          "A. Red light.",
          "B. Yellow light.",
          "C. Green light.",
          "D. Flashing light."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_453_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:10:05]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the time displayed on the dashboard right now?",
        "time_stamp": "00:10:06",
        "answer": "B",
        "options": [
          "A. 7:10:30.",
          "B. 7:11:02.",
          "C. 7:12:15.",
          "D. 7:11:45."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_453_real.mp4"
  },
  {
    "time": "[0:13:20 - 0:13:25]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the current speed limit for this road?",
        "time_stamp": "00:13:24",
        "answer": "C",
        "options": [
          "A. 50.",
          "B. 60.",
          "C. 70.",
          "D. 80."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_453_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: A white rectangular shape and a round white clock with a fork and knife at the center are displayed on a blue background. [0:00:01 - 0:00:03]: A person in a white chef's coat stands with arms crossed in front of a blue background, with the words \"RAMSAY in 10\" displayed prominently in bold red letters. [0:00:04 - 0:00:06]: The scene changes to a modern kitchen with blue cabinets, glass-fronted upper cabinets, a tiled backsplash, and various kitchen appliances such as a microwave, toaster, and coffee maker on the counter. There are also books and utensils. [0:00:05 - 0:00:07]: In the foreground, a steel countertop with a stove has two large burners and a frying pan on top, along with a griddle. The camera angle changes slightly, showing a hand moving into the frame on the left side. [0:00:07 - 0:00:08]: The person wearing a navy blue t-shirt enters the frame, standing beside the stove, and appears to be engaging with the camera, possibly speaking. [0:00:08 - 0:00:11]: The person is leaning forward, closer to the camera, showing a closer view of the stove and kitchen behind. They seem animated and expressive, indicating a dynamic interaction. [0:00:09 - 0:00:13]: The camera captures the person pausing momentarily, with the text \"Previously Recorded Live\" appearing at the top right of the screen. [0:00:13 - 0:00:14]: The person resumes speaking and moves slightly, gesturing with their hands. The kitchen setting remains consistent in the background. [0:00:15 - 0:00:17]: They continue talking, occasionally looking at the stove and then back at the camera. The frying pan on the stove remains central in the frame. [0:00:17 - 0:00:18]: The person looks directly at the camera and continues to engage, with their hands resting on the countertop. The background stays the same with the kitchen in view. [0:00:18 - 0:00:19]: In the final frames, the person steps back slightly, continuing to speak with an expressive demeanor, maintaining the focus on the kitchen environment.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What text appears at the top right of the screen right now?",
        "time_stamp": "00:00:13",
        "answer": "B",
        "options": [
          "A. \"Live Recording\".",
          "B. \"Previously Recorded Live\".",
          "C. \"Recording Now\".",
          "D. \"Live Now\"."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_28_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:21]: A man is standing in a kitchen with blue cabinets and a white tile backsplash, placing sliced potatoes into a black frying pan on a gas stove. He is wearing a dark blue t-shirt and a black wristwatch. Various kitchen appliances and utensils are visible on the counter behind him. [0:05:21 - 0:05:22]: The man continues to place additional potato slices into the frying pan. The potatoes are cut into thin rounds and are being laid flat in a single layer. [0:05:22 - 0:05:24]: He adjusts the positions of the potato slices within the pan, ensuring they are evenly spread out for cooking. His left hand holds a few more potato slices ready to be placed in the pan. [0:05:24 - 0:05:27]: The man meticulously arranges the potato slices while occasionally looking down at the pan to check their placement. In the background, the kitchen counter holds a variety of cooking items, including bottles and a knife block. [0:05:27 - 0:05:28]: He reaches over to the side, possibly to grab more potato slices from a wooden cutting board located next to the stove. His movements are precise and careful, indicating attention to detail in his cooking process. [0:05:28 - 0:05:29]: A close-up view of the man placing more potato slices into the frying pan. There are audible sizzling sounds as the potatoes come in contact with the hot surface. [0:05:29 - 0:05:30]: He adjusts the heat on the gas stove, ensuring the temperature is optimal for cooking the potatoes. His focus remains on the pan as he works. [0:05:30 - 0:05:31]: The man arranges the last few potato slices in the pan, making sure they are not overlapping and have enough space to cook evenly. His hand movements are quick and practiced. [0:05:31 - 0:05:32]: He shifts his gaze from the pan to another part of the kitchen, possibly checking on another cooking task. Behind him, there are glass cabinets with neatly arranged glassware and other kitchen items. [0:05:32 - 0:05:35]: The man briefly steps away from the pan, walking over to the cutting board where there are a few more potato slices left. He picks them up and returns to the pan, placing them inside it carefully. [0:05:35 - 0:05:36]: He stirs the potatoes slightly, ensuring they are all evenly cooked. The kitchen has a modern and clean ambiance, with everything in its place and well-organized. [0:05:36 - 0:05:37]: The man wipes his forehead with his forearm, showing that he is engaged in an activity that requires focus and effort. He then resumes his attention to the stove. [0:05:37 - 0:05:39]: The camera focuses on the frying pan showing the potatoes sizzling in oil. The potatoes are starting to turn golden brown on the edges indicating they are cooking well. The scene ends with the man giving the pan a slight shake to ensure even cooking.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action does the man take after placing the potato slices in the frying pan?",
        "time_stamp": "0:05:24",
        "answer": "B",
        "options": [
          "A. He adds seasoning to the potatoes.",
          "B. He sprinkles seasonings on the potatoes.",
          "C. He washes his hands.",
          "D. He prepares another dish."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What does the man do to ensure the potatoes cook evenly in the pan?",
        "time_stamp": "0:05:31",
        "answer": "B",
        "options": [
          "A. He stirs them constantly.",
          "B. He arranges them to avoid overlapping.",
          "C. He covers the pan.",
          "D. He adds more oil."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_28_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:11:00]",
    "captions": "[0:10:40 - 0:11:00] [0:10:40 - 0:10:52]: A person wearing a dark blue t-shirt stands at a kitchen counter, chopping green onions on a wooden cutting board. Various objects are present on the counter including a frying pan to the left and a white bowl with vegetables on the right. Behind the person, a grey cupboard with glass doors contains various dishes and a few bottles. The countertop itself is white with a small portion of a sink visible. The person continues chopping and adjusting the green onions with a focused expression;  [0:10:53]: The person shifts their focus from the cutting board to the bowl on the counter, holding both items. They are captured in mid-motion, appearing to smile or laugh while engaging with someone off-screen;  [0:10:54 - 0:10:56]: The person momentarily steps backward from the counter, facing away from the camera towards a kitchen shelf and microwave in the background. In this shot, more of the kitchen is visible, including a large clock mounted on the back wall. Another individual seated at a table is semi-visible in the distant background through a doorway;  [0:10:57 - 0:10:58]: The camera angle shifts, zooming back in on the countertop area. Numerous kitchen utensils and a stovetop with partially visible food being cooked are captured. The person resumes their previous position at the counter, holding the bowl with vegetables;  [0:10:59]: The person lifts the bowl towards the camera while using a spoon to mix the contents. The expression on their face is cheerful and engaging, as they look towards the camera. The kitchen setup in the background remains the same with the grey cupboards and various kitchen items.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is this person doing right now?",
        "time_stamp": "0:10:39",
        "answer": "B",
        "options": [
          "A. He is chopping green onions.",
          "B. He is slicing carrots.",
          "C. He is dicing tomatoes.",
          "D. He is mincing garlic."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Event Understanding",
        "question": "What is the person doing with the bowl right now?",
        "time_stamp": "0:11:00",
        "answer": "C",
        "options": [
          "A. Putting it in the microwave.",
          "B. Washing it in the sink.",
          "C. Mixing its contents with a spoon.",
          "D. Placing it back on the counter."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_28_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:00:10",
        "answer": "B",
        "options": [
          "A. An individual browsed through items in a refrigerator section of a store with a cloth in hand.",
          "B. An individual walked through an establishment, apparently a café, holding a cloth and prepared to clean a table.",
          "C. An individual picked up a jacket from a chair and folded it neatly to place it back.",
          "D. An individual took a tray of food to serve a seated customer at a table."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_352_real.mp4"
  },
  {
    "time": "[0:02:05 - 0:02:15]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:02:15",
        "answer": "C",
        "options": [
          "A. An individual was washing their hands and drying them with a towel.",
          "B. An individual was organizing various kitchen utensils and cutlery in a drawer.",
          "C. An individual was washing and rinsing a batch of dishes and cups in a sink.",
          "D. An individual was preparing ingredients for a meal by chopping vegetables."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_352_real.mp4"
  },
  {
    "time": "[0:04:10 - 0:04:20]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:04:20",
        "answer": "B",
        "options": [
          "A. An individual rearranged items in a refrigerator and then wiped the floors.",
          "B. An individual disposed of trash, retrieved cleaning supplies, and started mopping the floor.",
          "C. An individual washed dishes, emptied the trash, and cleaned the sink.",
          "D. An individual served food to a customer and then started cleaning the countertops."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_352_real.mp4"
  },
  {
    "time": "[0:06:15 - 0:06:25]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:06:25",
        "answer": "A",
        "options": [
          "A. An individual adjusted various settings on an electronic device, selected payment options.",
          "B. An individual searched through web pages on a tablet, bookmarked a few articles, and saved them for later reading.",
          "C. An individual played a game on a tablet, paused to check messages, and then defeated the final boss in the game.",
          "D. An individual followed a recipe on a tablet screen, added ingredients to a bowl, and began the baking process."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_352_real.mp4"
  },
  {
    "time": "[0:08:20 - 0:08:30]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:09:33",
        "answer": "A",
        "options": [
          "A. An individual made a cappuccino and a flat white with oat milk, and served them in takeaway cups.",
          "B. An individual made an espresso and a latte, serving them in ceramic mugs for dine-in.",
          "C. An individual prepared tea and hot chocolate for takeaway, using regular milk for both.",
          "D. An individual brewed a strong filter coffee and an americano, serving them in large takeaway containers."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_352_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a completely black screen.  [0:00:01 - 0:00:04]: The screen transitions to display the text \"RESIN WITH ME\" centered with a thin horizontal line beneath it. [0:00:03 - 0:00:04]: The text \"Acrylic Floral Sign\" appears below the horizontal line.  [0:00:05]: The perspective changes to a work surface covered with a blue grid mat. Two plastic containers filled with small flowers and other colorful decorations are held by a person wearing black gloves. A round brown object is placed in the center of the mat. [0:00:06 - 0:00:11]: The camera zooms in to the two plastic containers. On close inspection, the containers each hold multiple compartments with various dried flowers of different colors and shapes arranged neatly. [0:00:12]: The view shifts slightly, showing both containers being set down around the round object placed in the center of the mat. [0:00:13]: The person removes a craft knife from the left side of the frame. [0:00:14 - 0:00:17]: The person's gloved hands use the knife to carefully cut along the line dividing the brown round object, methodically scoring through the material. [0:00:18 - 0:00:19]: The person peels off a section of the brown object, revealing the surface underneath.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:00:19",
        "answer": "C",
        "options": [
          "A. Zooming in on the plastic containers.",
          "B. Cutting along the line dividing the brown object.",
          "C. Peeling off a section of the brown object.",
          "D. Arranging flowers in the containers."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_68_real.mp4"
  },
  {
    "time": "0:01:40 - 0:02:00",
    "captions": "[0:01:40 - 0:02:00] [0:00:00 - 0:00:01]: The scene opens to a blue grid-mat background with a round, tan-colored coaster-like object in the center. Various small, decorative elements, such as flowers and leaves in different colors (including red, purple, and green), are placed on the coaster. In the top left corner, there is a small circular container filled with a translucent liquid. To the right of the coaster, there is a rectangular container with multiple compartments, each holding tiny, colorful embellishments. A skewer stick is placed diagonally near the top right of the coaster. The view suggests the perspective of someone working on a craft project, possibly resin art. A gloved hand holding another small plastic box filled with colorful embellishments is seen entering the frame from the bottom. [0:00:02 - 0:00:03]: The gloved hand adjusts the small plastic box, revealing compartments with various small embellishments, including flowers and leaves. The focus is on selecting the embellishments for placement on the coaster. [0:00:04 - 0:00:06]: The gloved hand picks a flower embellishment from the small plastic box using a white tool with a flat edge. The other hand continues to hold the plastic box steady. [0:00:07]: The flower selected is yellow. The gloved hand uses the white tool to lift the flower out of the compartment. [0:00:08 - 0:00:10]: The gloved hand carefully places the yellow flower onto the coaster. It is positioned near the bottom right of the arrangement of existing embellishments. [0:00:11 - 0:00:13]: The gloved hand continues to hold the white tool and adjust the yellow flower to the desired position on the coaster. [0:00:14 - 0:00:16]: Additional adjustment is made to ensure the yellow flower fits well with the surrounding embellishments. The hand continues to refine the placement using the white tool. [0:00:17 - 0:00:19]: After securing the yellow flower in place, the gloved hand moves back towards the small plastic box to select another embellishment. The frame includes a clear view of the utensil, flowers, and leaves in the plastic box. [0:00:20]: The gloved hand opens another compartment of the plastic box, revealing more embellishments to choose from. The arrangement on the coaster remains in view, displaying a deliberate and meticulous crafting process.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the gloved hand doing right now?",
        "time_stamp": "00:02:00",
        "answer": "C",
        "options": [
          "A. Adjusting the heat on the stove top.",
          "B. Painting the coaster into a solid color.",
          "C. Adjusting the position of the flower on the brown ring.",
          "D. Picking up the coaster."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_68_real.mp4"
  },
  {
    "time": "0:03:20 - 0:03:40",
    "captions": "[0:03:20 - 0:03:40] [0:00:20 - 0:00:21]: A round wooden disk is centered on a grid-patterned blue cutting mat. The surface of the disk is adorned with various small, colorful items including flowers and leaves in hues of pink, purple, yellow, red, green, and white. A white ceramic bowl with a paintbrush standing in it sits in the top left corner of the frame. [0:00:22 - 0:00:23]: The setup remains unchanged with the wooden disk and ceramic bowl on the blue grid mat.  [0:00:24 - 0:00:25]: A black-gloved hand holding a bamboo stick appears from the right side of the frame. The stick is aimed at a yellow flower near the center of the disk. [0:00:26 - 0:00:27]: The black-gloved hand continues holding the bamboo stick as it approaches the yellow flower on the disk. [0:00:28 - 0:00:29]: The bamboo stick makes contact with the yellow flower, gently pressing or stirring it. [0:00:30 - 0:00:31]: The black-gloved hand continues to press or stir the yellow flower delicately with the bamboo stick. [0:00:32 - 0:00:33]: The bamboo stick remains pressed or stirring the yellow flower, and the black glove remains unchanged in its position. [0:00:34 - 0:00:35]: The bamboo stick continues its interaction with the yellow flower, demonstrating some movement but remaining focused on the same spot. [0:00:36 - 0:00:37]: The bamboo stick still interacts with the yellow flower with slight movement, showing refined attention to detail by the black-gloved hand. [0:00:38 - 0:00:39]: The black-gloved hand presses or stirs a little more firmly but continues its fine-tuned work on the yellow flower with cautious motions. [0:00:40 - 0:00:41]: The bamboo stick slightly shifts upwards, focusing on the adjacent green leaves. The hand adjusts its position slightly. [0:00:42 - 0:00:43]: The stick repositions further towards the green leaves, but its movement remains subtle and precise. [0:00:44 - 0:00:45]: The bamboo stick moves over to interact with the pink flower near the edge of the disk, maintaining the same gentle and detailed approach. [0:00:46 - 0:00:47]: The bamboo stick continues to interact with the pink flower carefully, maintaining its gentle touch. [0:00:48 - 0:00:49]: The stick shifts towards the orange flower on the disk, with the black glove maintaining control and precision. [0:00:50 - 0:00:51]: The black-gloved hand holds the bamboo stick while focusing on the green leaf beside the orange flower, showing meticulous detail in every movement. [0:00:52 - 0:00:53]: The bamboo stick and black-gloved hand stay attentive on the green leaf beside the orange flower, continuing smooth, delicate movements. [0:00:54 - 0:00:55]: The stick moves back towards the yellow flower near the center of the disk, continuing its delicate and meticulous intervention. [0:00:56 - 0:00:57]: The bamboo stick remains over the yellow flower, and the black gloved hand continues its careful, detailed work. [0:00:58 - 0:00:59]: The hand and bamboo stick maintain their position over the yellow flower, demonstrating consistent and detailed attention throughout the interaction.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the black-gloved hand doing right now?",
        "time_stamp": "0:03:26",
        "answer": "C",
        "options": [
          "A. Repositioning the white ceramic bowl.",
          "B. Stirring the paintbrush in the bowl.",
          "C. Adjusting the flowers with a bamboo stick.",
          "D. Adjusting the blue cutting mat."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_68_real.mp4"
  },
  {
    "time": "0:05:00 - 0:05:20",
    "captions": "[0:05:00 - 0:05:20] [0:00:00 - 0:00:19]: The video begins with a pair of hands wearing black gloves, holding a small, transparent plastic cup in the left hand and a thin wooden stick in the right hand. The hands are positioned over a circular, flat surface with a blue grid mat in the background. The circular surface appears to be a mold filled with various colorful elements, including flowers and leaves, embedded within. These elements are arranged in an aesthetically pleasing pattern with colors such as pink, purple, green, orange, and blue. The surface also includes scattered gold flakes.  Initially, the right hand uses the thin wooden stick to move and position the elements within the mold, ensuring they are evenly spaced and well-aligned. As the video progresses, the left hand tilts the plastic cup, pouring a clear liquid—likely resin—into the mold. The right hand continues to adjust the positions of the elements with the wooden stick, occasionally stirring the liquid to distribute it evenly across the mold.  Throughout the video, the actions are carried out meticulously, with the focus on achieving a balanced arrangement within the mold. The clear liquid slowly fills the spaces, surrounding the colorful elements and gold flakes, giving the appearance of creating a decorative piece.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:04:55",
        "answer": "A",
        "options": [
          "A. Pouring resin into the circular mold.",
          "B. Mixing colors in a bowl.",
          "C. Arranging flowers on a table.",
          "D. Stacking plastic cups."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_68_real.mp4"
  },
  {
    "time": "0:06:40 - 0:07:00",
    "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:43]: The video begins with a pair of hands holding a translucent circular object adorned with various multicolored floral and leaf designs. The surface of the object glistens and reflects light. The right hand is carefully peeling off a piece of gold-colored material from the circular object’s surface. This scene is set against a backdrop of a gray, grid-patterned work surface. [0:06:44 - 0:06:46]: As the gold-colored material continues to be peeled away, more of the floral and leaf designs are revealed. These designs include shades of purple, pink, blue, green, yellow, and orange. Some leaves are also visible among the intricate floral patterns. [0:06:47 - 0:06:48]: The circular object is now fully uncovered, revealing the entire colorful design beneath. The hand continues to hold the object steady against the gray grid-patterned surface, capturing the details of the vivid flowers and leaves embedded in the clear material. [0:06:49 - 0:06:51]: A closer view is shown, with the object held directly in front of the camera. This view highlights the various colorful floral and leaf designs more prominently. The designs are scattered evenly across the circular object, with a slight golden hue accentuating the edges of the flowers and leaves. [0:06:52 - 0:06:53]: The camera captures the object slightly rotated between the fingers. The floral patterns, which appear three-dimensional, shimmer under the light. The intricate details and vibrant colors of each flower and leaf are clearly visible. [0:06:54 - 0:06:55]: The object is held up closer to the camera, giving a much more detailed view. The round surface showcases flowers in purple, pink, yellow, blue, and orange, along with green leaves and small golden accents scattered throughout. [0:06:56 - 0:06:57]: The camera maintains focus on the circular object, held steadily by the hand. The intricate designs and shimmering colors are clearly visible against the consistent gray grid-patterned background. [0:06:58 - 0:07:00]: The perspective shifts as the circular object is placed on the gray grid-patterned work surface. The hand reaches down to grab a white rectangular piece of paper, placing it in front of the circular object. [0:07:01 - 0:07:02]: The hand picks up a pen-like tool and begins to manipulate the rectangular piece of paper. The tool is placed precisely at one corner of the paper, indicating the start of a new crafting process. [0:07:03 - 0:07:04]: The hand carefully makes precise movements with the pen-like tool along the edge of the paper, while the other hand holds the paper steady. The circular object remains visible in the background. [0:07:05 - 0:07:06]: The pen-like tool continues to work its way along the edge of the paper, creating a series of cut-out shapes. The focus remains on the careful and deliberate hand movements guiding the tool. [0:07:07 - 0:07:08]: The hand shifts, adjusting the paper slightly to ensure a precise cut. The pen-like tool continues to create detailed cut-out shapes along the edge of the rectangular paper. [0:07:09 - 0:07:12]: The hand partially lifts the paper to reveal the cut-out shapes. The focus remains on the delicate crafting process, with the pen-like tool carefully following the pre-determined path. Both the cut-out shapes and the circular object in the background are clearly visible. [0:07:13 - 0:07:15]: The camera captures the detailed cut-out shapes forming on the paper. The pen-like tool is lifted momentarily while the hand adjusts the paper, revealing the progress made thus far. The cut-outs are intricate and complement the floral designs on the circular object. [0:07:16 - 0:07:17]: The hand continues to manipulate the pen-like tool, ensuring that the shapes being cut out are precise and clean. The paper, now partially cut, begins to show the developed pattern as the other hand holds it in place. [0:07:18 - 0:07:20]: The paper is fully lifted, showing the intricate pattern created by the cut-out shapes. The camera captures both the detailed cut-outs and the floral designs on the circular object in the background, emphasizing the craftsmanship involved. [0:07:21 - 0:07:22]: The rectangular paper, now featuring several cut-out shapes, is held in place while the pen-like tool is repositioned. The hand prepares to further refine the design. The circular object remains a constant in the background, showcasing its vibrant floral",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action is being performed with the hands right now?",
        "time_stamp": "0:06:43",
        "answer": "B",
        "options": [
          "A. Painting the circular object.",
          "B. Peeling off a gold-colored material.",
          "C. Polishing the circular object.",
          "D. Cutting the circular object into pieces."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_68_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins in a commercial storage room or a supermarket stock room. A person is looking at a metal shelving unit filled with various goods, mainly food products. The lower shelves contain multiple yogurt containers. A distinctive red crate is visible on the floor. [0:00:03 - 0:00:04]: The person moves down the aisle and their hand, wearing a black glove, becomes visible as they push a shopping cart filled with more yogurt containers. There is a cardboard box with an image of an egg on it nearby, and a pallet jack indicating a storage or stocking area. [0:00:05 - 0:00:06]: The perspective returns to the metal shelving, focusing on rows of dairy products. The shelves are divided into sections, each labeled with clear signage. Small cartons and containers are neatly lined up. [0:00:07 - 0:00:08]: Attention shifts to a lower shelf where several yogurt cups are arranged. Further down, larger yogurt containers are stacked. The floor is mostly clean but shows signs of heavy foot traffic typical of storage areas. [0:00:09 - 0:00:10]: The person's gloved hands pick up a blue yogurt container from the shelf and appear to restock it or examine it closely. More yogurt containers are visible on the shelves, stacked in rows. [0:00:11 - 0:00:12]: A close-up look at the yogurt containers indicates various brands and flavors, all neatly organized on the shelf. The clear labeling helps in easy identification of the products, which are stacked from front to back. [0:00:13 - 0:00:14]: The view pans back out to show a wider area of the shelving unit, with a focus on the middle shelf. The yogurt containers are mostly white with different label colors. The lower shelves also hold some boxed items. [0:00:15 - 0:00:16]: Close-up of the middle shelf with more yogurt containers, and a hand reaches in to adjust or inspect one. The products are well-lit, emphasizing the cleanliness and organization of the storage area. [0:00:17 - 0:00:18]: Movement again to the shelving with the person actively restocking yogurt. Each container is placed carefully next to others, maintaining the orderly arrangement. [0:00:18 - 0:00:20]: The video concludes with the perspective widening to include more of the storage room. Shelves filled with products line the walls, and multiple boxes and pallets are spread around, showing the context of the space as a busy, well-organized storage area.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What type of container is the person placing on the shelves?",
        "time_stamp": "0:00:18",
        "answer": "A",
        "options": [
          "A. Blue and white yogurt containers.",
          "B. Small cardboard boxes.",
          "C. Large glass jars.",
          "D. Metal cans."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_442_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:20 - 0:04:40] [0:04:20 - 0:04:23]: The footage shows boxes stacked on the floor in a storage room. The boxes are labeled \"BRAVO\" and a person wearing gloves is visible, arranging or removing boxes. Cartons of eggs can be seen. [0:04:24]: Shelves filled with various items like packaged juice, milk products, and other cartons are visible. Price tags can be seen on the shelves. [0:04:25 - 0:04:29]: The camera turns back to focus on an area with more stacked boxes, the same gloved hands arranging and handling boxes. The gloved individual is adjusting cartons on top of the boxes. [0:04:30 - 0:04:32]: The gloved person places items on the shelf next to juice cartons. There are various flavors of juice with price tags. [0:04:33]: The footage becomes momentarily blurry as the camera moves. [0:04:34 - 0:04:35]: The camera focuses back on the storage area with stacked boxes. One large box has cut-outs, possibly handles, and the gloved hands are moving another box. [0:04:36 - 0:04:38]: The view captures more of the storage room, showing shelves on the right side with various items organized on them. The person continues handling boxes. [0:04:39]: Again, more of the storage room is visible, showing a variety of stacked items on the left side.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What label is visible on the boxes stacked on the floor in the storage room right now?",
        "time_stamp": "00:04:19",
        "answer": "C",
        "options": [
          "A. ALPHA.",
          "B. CHARLIE.",
          "C. Valio and BRAVO.",
          "D. DELTA."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_442_real.mp4"
  },
  {
    "time": "[0:13:00 - 0:14:00]",
    "captions": "[0:13:00 - 0:13:20] [0:13:00 - 0:13:02]: The video begins with a first-person perspective focusing on a metal shelf stocked with large juice cartons labeled \"EXOTISK JUICE.\" The cartons are green with some orange detailing. The person's black-gloved hands are visible as they handle the cartons, seemingly restocking or organizing them. The shelf is labeled with a price of 28.50. [0:13:03 - 0:13:04]: The camera angle moves downward, revealing a brown liquid spill on the white floor next to a cardboard box filled with more cartons. The contents of the box include similar green cartons, possibly the same as those being placed on the shelf. [0:13:05 - 0:13:07]: The motion continues as the person transfers more cartons from the box on the floor to the shelf, making sure they are positioned neatly. More green juice cartons are added to the row on the lower shelf. [0:13:08 - 0:13:12]: The perspective shifts and the person picks up another carton from the box on the floor and places it on the shelf, organizing the items carefully. The upper shelf holds several pink cartons, contrasting with the green cartons on the lower shelf. [0:13:13 - 0:13:15]: The camera moves to show the wider storage area, which includes a partially open door, tall shelves, and more large boxes filled with various items. The flooring is white, maintaining a clean and organized warehouse appearance. [0:13:16 - 0:13:19]: The video concludes with the person moving slightly away from the shelf and placing the remaining box lid onto the box labeled \"BRAVO.\" Other full boxes labeled \"BRAVO\" are stacked on a dolly, hinting that the person is in a stockroom or inventory space used for organizing product deliveries.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What brand of juice cartons is the person handling?",
        "time_stamp": "00:13:02",
        "answer": "A",
        "options": [
          "A. BRAVO.",
          "B. EXOTISK JUICE.",
          "C. TROPICANA.",
          "D. DELIGHT JUICE."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What are the colors of the juice cartons being handled by the person?",
        "time_stamp": "00:13:14",
        "answer": "A",
        "options": [
          "A. Green with white and yello detailing.",
          "B. Blue with yellow detailing.",
          "C. Red with white detailing.",
          "D. Pink with green detailing."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_442_real.mp4"
  },
  {
    "time": "[0:16:00 - 0:16:50]",
    "captions": "[0:16:40 - 0:16:50] [0:16:40 - 0:16:41]: The scene shows a storage room with shelves of brightly colored packaged goods on the left and a slightly open door at the back. There's a cart with dairy products in the foreground. [0:16:42 - 0:16:45]: The view shifts to focus on refrigerated shelves stocked with cartons of yogurt. Two hands wearing black gloves are seen rearranging or picking up the yogurt cartons. [0:16:46]: The perspective returns to the broader storage room. The shelves with colorful packages remain to the left, the cart is in the foreground, and the door at the back center is fully visible. [0:16:47 - 0:16:48]: The perspective moves towards a corner where multiple boxes are stacked on the floor and a wire rack holds more boxes and some green products. [0:16:49 - 0:16:50]: A shelving unit stocked with containers, possibly dairy products, is visible on the right. In the foreground, there are open cardboard boxes on the floor.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color are the gloves seen in the scene?",
        "time_stamp": "00:16:45",
        "answer": "A",
        "options": [
          "A. Black.",
          "B. Red.",
          "C. White.",
          "D. Blue."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_442_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:22]: In a well-lit kitchen with tiled walls and light blue cupboards, a person is seen wearing a dark blue shirt. The person is using a box grater to grate a yellow ingredient, likely cheese, over a white plate on a wooden cutting board. To the left, there's a frying pan on the stove, and to the right are some kitchen utensils like a pair of forks and possibly a bottle or jar. [0:03:23 - 0:03:27]: The grating continues methodically, with the person applying pressure and moving the yellow ingredient downward against the grater. More of the ingredient accumulates on the plate. The actions remain consistent with grating the yellow substance. [0:03:28 - 0:03:31]: The kitchen settings including the surrounding shelves with glassware come into clearer focus. The person's focused expression is visible as they continue grating the yellow ingredient vigorously. [0:03:32 - 0:03:35]: A small timer appears in the bottom right corner, counting down from 10:00 minutes. Meanwhile, the grating continues with consistent motion. The left arm holds the grater steady while the right arm moves the yellow ingredient across its surface. [0:03:36 - 0:03:39]: The grating process continues steadily, with the person looking focused on the task. The kitchen background features a sink area with various kitchen items orderly placed. The grated yellow ingredient is now significantly piling up on the plate, indicating considerable progress in the grating task.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color are the kitchen cupboards?",
        "time_stamp": "00:03:22",
        "answer": "B",
        "options": [
          "A. Dark blue.",
          "B. Light green.",
          "C. White.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing with the box grater?",
        "time_stamp": "00:03:27",
        "answer": "D",
        "options": [
          "A. Slicing bread.",
          "B. Chopping vegetables.",
          "C. Peeling potatoes.",
          "D. Grating a yellow ingredient."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_36_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:44]: A person in a blue shirt is seen chopping red bell pepper on a wooden cutting board. The chopping action is quick and precise, with the knife moving smoothly and consistently. The person's arms are positioned to stabilize the pepper while slicing it into thin strips. The kitchen background is clearly visible, with various kitchen utensils, a sink, and some cooking ingredients placed on a countertop;  [0:06:45 - 0:06:48]: The chopping continues as the person focuses intently on the task. Here, the sliced pieces of bell pepper start to accumulate on the board. On the left side of the frame, part of the stove with a pan visible. The person maintains a steady rhythm, ensuring each slice is uniform;  [0:06:49 - 0:06:51]: After chopping the bell pepper, the person shifts slightly to the side, looking down, presumably to inspect the slices. More kitchen tools and ingredients come into view in the background, neatly arranged on shelves or the countertop;  [0:06:52 - 0:06:55]: The person gathers the chopped bell pepper with the knife, transferring the pieces into a pan on the stove. The other pan on the stove has some ingredients in it, likely already cooking. The person moves with efficiency and purpose;  [0:06:56 - 0:06:59]: The person starts stirring the contents of the pan with a wooden spoon, ensuring even cooking and mixing of ingredients. Visible on the countertop is a microwave and additional cooking supplies, with the person's focus now entirely on the stove. The food in the pan looks vibrant and colorful, indicating a mix of ingredients. The entire kitchen setup portrays a well-organized and functional cooking space.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person in a blue shirt chopping on the wooden cutting board?",
        "time_stamp": "0:06:42",
        "answer": "A",
        "options": [
          "A. Red bell pepper.",
          "B. Carrots.",
          "C. Green bell pepper.",
          "D. Onions."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_36_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:11:00]",
    "captions": "[0:10:00 - 0:10:20] [0:10:00 - 0:10:02]: A person is standing in a modern kitchen, holding a wooden spoon and stirring food in a black frying pan on the stove. The kitchen features dark blue cabinets with glass doors showcasing dishware. Behind the person, there is a white tile backsplash and various kitchen utensils on the countertop. To the person's left, there is a stand mixer and a kettle. The person is wearing a navy blue shirt and green pants and a checkered cloth is tucked into the waistband. [0:10:02 - 0:10:06]: The person continues to stir the food in the frying pan, now using both the wooden spoon and another utensil to mix the ingredients. Their attention is focused on the pan. They briefly gesture with their free hand while still stirring. [0:10:06 - 0:10:09]: The video angle changes slightly, revealing a closer view of the pan. The food in the pan appears to be a mixture of vegetables and possibly some protein, with vibrant colors including yellow, red, and green. The person continues to mix the ingredients, lifting the pan slightly off the stove at one point. [0:10:09 - 0:10:11]: The person places the pan back on the stove, and the camera focuses closely on the pan, capturing detailed view of the ingredients being stirred. [0:10:11 - 0:10:13]: The person picks up a small bowl filled with a red liquid, possibly a sauce, and begins to pour it into the frying pan. They use the wooden spoon to ensure all the sauce is added. [0:10:13 - 0:10:16]: The person continues to empty the bowl into the pan, using the spoon to scrape out any remaining sauce. They then discard the empty bowl and resume mixing the contents of the frying pan. [0:10:16 - 0:10:19]: The camera returns to focus on the person’s face, who briefly looks up and then continues to stir the food meticulously, ensuring it is well-mixed with the sauce.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person holding right now?",
        "time_stamp": "0:10:02",
        "answer": "A",
        "options": [
          "A. A wooden spoon.",
          "B. A metal spatula.",
          "C. A whisk.",
          "D. A ladle."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_36_real.mp4"
  },
  {
    "time": "[0:13:00 - 0:14:00]",
    "captions": "[0:13:20 - 0:13:40] [0:13:20 - 0:13:21]: The scene shows an individual in a kitchen, holding a spatula in their right hand while standing in front of a stovetop. There are two frying pans on the stovetop; one is empty, and the other contains a yellowish dish cooking. The background features cabinets with glass doors, some utensils on the countertop, and a microwave. [0:13:21 - 0:13:23]: The person turns slightly to their left, moving an item away from the stovetop. The lower part of the kitchen, including drawers and cabinets, is visible. [0:13:23 - 0:13:24]: The individual is seen working on the countertop on their right while facing the stovetop on the left. There is a bowl of green leaves, a glass, and a ladle on the countertop. [0:13:24 - 0:13:25]: They are seen handling some dough on the wooden cutting board, which is lightly floured. The individual’s hands are covered in flour, and they are using a wooden rolling pin. [0:13:25 - 0:13:26]: The person continues to roll out the dough, flattening it into a circular shape. The countertop now shows additional items: a bottle of oil, a stack of plates, and some jars. [0:13:26 - 0:13:27]: The individual places the flattened dough onto the wooden cutting board and continues rolling another piece of dough. The stovetop and cooking pan with the yellowish contents are still active, with steam visible. [0:13:27 - 0:13:28]: Moving the rolling pin back and forth, the person flattens another piece of dough. Several prepared balls of dough are visible on a plate nearby. [0:13:28 - 0:13:29]: The individual rolls out the last piece of dough, ensuring it is thin and even. Other kitchen items remain undisturbed in their places. [0:13:29 - 0:13:30]: They place the dough aside and adjust the rolling pin. The person is focused on the preparation, with the cooking pans emitting steam. [0:13:30 - 0:13:31]: The individual continues their dough preparation, leaning slightly forward over the countertop. The background shelves with glassware and kitchen supplies are visible. [0:13:31 - 0:13:32]: Rolling another piece of dough, the person maintains a steady pace. The organized kitchen setting remains consistent, with various utensils and jars positioned around the countertops. [0:13:32 - 0:13:33]: The individual carefully presses the dough, ensuring it is properly flattened. Their attention stays on the task, with the stovetop still visible in the background. [0:13:33 - 0:13:34]: The person continues working with precision, rolling out the dough. The shelves behind them hold various kitchen equipment and glassware. [0:13:34 - 0:13:35]: They examine the dough, stretching it gently to achieve the desired shape and consistency. Items on the countertop remain neatly arranged. [0:13:35 - 0:13:36]: The individual then places the dough on the board and readjusts the rolling pin. The kitchen background and countertop setup remain unchanged. [0:13:36 - 0:13:37]: They work systematically, ensuring each piece of dough is evenly processed. The organized kitchen setting continues to provide a clear workspace. [0:13:37 - 0:13:38]: Their focus stays on rolling the dough, with all necessary utensils within easy reach. The kitchen remains tidy and functional, with all tools in their designated spots. [0:13:38 - 0:13:39]: Finally, the individual completes the dough preparation, with the countertop and surrounding utensils remaining orderly. The background shelves still display kitchenware and glass items.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person holding in the right hand right now?",
        "time_stamp": "0:13:19",
        "answer": "A",
        "options": [
          "A. A pen.",
          "B. A ladle.",
          "C. A knife.",
          "D. A rolling pin."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_36_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which option best summarizes what was just shown?",
        "time_stamp": "00:00:10",
        "answer": "B",
        "options": [
          "A. A person was preparing and serving hamburgers to a line of customers at a busy street corner.",
          "B. A mobile food truck opened its serving window near a shopping area in the morning.",
          "C. A vendor inside a food truck was grilling burgers on a barbecue and handing them to customers.",
          "D. A food truck got stuck in traffic and had to be manually pushed to the side of the road by several people."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_409_real.mp4"
  },
  {
    "time": "[0:02:38 - 0:02:48]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which option best summarizes what was just shown?",
        "time_stamp": "00:02:47",
        "answer": "B",
        "options": [
          "A. A person was cleaning the kitchen area and restocking spices and sauces.",
          "B. A vendor was arranging different types of drinks on the countertop in a food truck.",
          "C. Several people were unloading supplies from a truck into a storage area.",
          "D. A chef was heating oil in a pan to start cooking a meal."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_409_real.mp4"
  },
  {
    "time": "[0:05:16 - 0:05:26]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which option best summarizes what was just shown?",
        "time_stamp": "00:05:26",
        "answer": "C",
        "options": [
          "A. A person was unloading kitchen equipment from a truck and placing it on a counter.",
          "B. A person was stocking a refrigerator with beverages and condiments.",
          "C. A person was organizing cooking ingredients.",
          "D. A person was preparing ingredients for a recipe by mixing spices in different containers."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_409_real.mp4"
  },
  {
    "time": "[0:07:54 - 0:08:04]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which best summarizes what was just shown?",
        "time_stamp": "00:08:04",
        "answer": "A",
        "options": [
          "A. A burger was being freshly seasoned and prepared for customers.",
          "B. A chef was preparing a vegetarian dish by adding spices.",
          "C. A worker was cleaning a grill after a long day of cooking.",
          "D. An ice cream vendor was serving customers on a sunny day at the park."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_409_real.mp4"
  },
  {
    "time": "[0:10:32 - 0:10:42]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which option best summarizes what was just shown?",
        "time_stamp": "00:10:42",
        "answer": "C",
        "options": [
          "A. A chef was slicing vegetables and placing them on a plate.",
          "B. A person was manually operating a fryer to prepare French fries.",
          "C. Several patties and buns were being grilled on a hot surface by a worker.",
          "D. A person was cleaning up the kitchen area, wiping counters and washing dishes."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_409_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What kind of car is parked on the left side of the road now?",
        "time_stamp": "00:00:07",
        "answer": "A",
        "options": [
          "A. Police car.",
          "B. Bus.",
          "C. Ambulance.",
          "D. Taxi."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_393_real.mp4"
  },
  {
    "time": "[0:01:45 - 0:01:50]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the traffic light now?",
        "time_stamp": "00:02:32",
        "answer": "A",
        "options": [
          "A. Red.",
          "B. Green.",
          "C. Yellow.",
          "D. Orange."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_393_real.mp4"
  },
  {
    "time": "[0:03:30 - 0:03:35]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "Right now, what type of vehicle is the black car on the left?",
        "time_stamp": "00:03:34",
        "answer": "A",
        "options": [
          "A. Infiniti.",
          "B. Toyota.",
          "C. Honda.",
          "D. Lexus."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_393_real.mp4"
  },
  {
    "time": "[0:05:15 - 0:05:20]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the traffic light now?",
        "time_stamp": "00:05:08",
        "answer": "A",
        "options": [
          "A. Red.",
          "B. Green.",
          "C. Yellow.",
          "D. Orange."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_393_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:07:05]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which street name is visible on the green sign right now?",
        "time_stamp": "0:07:00",
        "answer": "C",
        "options": [
          "A. Broadway.",
          "B. Canal St.",
          "C. Centre St.",
          "D. East Broadway."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_393_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the rabbit strike the door with its fist several times?",
        "time_stamp": "0:00:25",
        "answer": "D",
        "options": [
          "A. Because the rabbit is practicing its strength.",
          "B. Because the rabbit is knocking to check if someone is inside.",
          "C. Because the rabbit wants to open the door.",
          "D. Because the rabbit wants to get food."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_233_real.mp4"
  },
  {
    "time": "[0:01:29 - 0:01:59]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why would the rabbit inside the iron gate be very panicked now?",
        "time_stamp": "00:02:14",
        "answer": "D",
        "options": [
          "A. Because he just realized the gate is locked, and he can't get out.",
          "B. Because he heard loud footsteps approaching from outside.",
          "C. Because the lights suddenly went out, and he is afraid of the dark.",
          "D. Because the toilet paper he was holding in his hand fell."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_233_real.mp4"
  },
  {
    "time": "[0:02:58 - 0:03:28]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "What causes the bunny character to react with shock and sweat?",
        "time_stamp": "0:03:28",
        "answer": "C",
        "options": [
          "A. Because the bunny just realized it forgot something important.",
          "B. Because a loud noise startled the bunny from behind.",
          "C. Because the rabbit wearing red and white clothes is perfectly fine.",
          "D. Because the bunny suddenly noticed a large shadow looming over it."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_233_real.mp4"
  },
  {
    "time": "[0:04:27 - 0:04:57]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why do the rectangular characters arm themselves?",
        "time_stamp": "0:04:41",
        "answer": "B",
        "options": [
          "A. Because they are excited to see the rabbit-like figure.",
          "B. Because the rabbit-like figure has glowing eyes and a menacing demeanor.",
          "C. Because they want to celebrate with the rabbit-like figure.",
          "D. Because they see another group approaching."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_233_real.mp4"
  },
  {
    "time": "[0:05:56 - 0:06:26]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the rabbit break through the wall to escape?",
        "time_stamp": "00:06:16",
        "answer": "D",
        "options": [
          "A. Because the rabbit wants to catch a thief.",
          "B. Because the rabbit is afraid of a predator.",
          "C. Because the rabbit wants to retrieve the advertisement.",
          "D. Because the rabbit is inspired by the advertisement for new shoes."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_233_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "What is the sticker on the windshield saying right now?",
        "time_stamp": "00:00:04",
        "answer": "B",
        "options": [
          "A. PLEASE PAY FARE.",
          "B. PLEASE HAND UP.",
          "C. PLEASE EXIT.",
          "D. PLEASE TAKE SEATS."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_459_real.mp4"
  },
  {
    "time": "[0:01:59 - 0:02:04]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What device is located to the right side of the steering wheel right now?",
        "time_stamp": "00:02:03",
        "answer": "A",
        "options": [
          "A. A digital device.",
          "B. A water bottle.",
          "C. A map.",
          "D. A book."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_459_real.mp4"
  },
  {
    "time": "[0:03:58 - 0:04:03]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What street is visible on the signpost right now?",
        "time_stamp": "00:04:02",
        "answer": "D",
        "options": [
          "A. Oak Street.",
          "B. Maple Street.",
          "C. Elm Street.",
          "D. Jersey Street."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_459_real.mp4"
  },
  {
    "time": "[0:05:57 - 0:06:02]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What sign is visible to the right of the intersection right now?",
        "time_stamp": "00:06:00",
        "answer": "D",
        "options": [
          "A. STOP.",
          "B. YIELD.",
          "C. NO ENTRY.",
          "D. GIVE WAY."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_459_real.mp4"
  },
  {
    "time": "[0:07:56 - 0:08:01]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the first car on the left side of the road right now?",
        "time_stamp": "00:07:58",
        "answer": "D",
        "options": [
          "A. Green.",
          "B. Blue.",
          "C. Silver.",
          "D. Red."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_459_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:00:24",
        "answer": "C",
        "options": [
          "A. Calculating the perimeter of shapes.",
          "B. Understanding different geometric figures.",
          "C. The concept of Area.",
          "D. The concept of volume."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_214_real.mp4"
  },
  {
    "time": "[0:02:09 - 0:02:39]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What formula might the speaker likely introduce next?",
        "time_stamp": "00:02:03",
        "answer": "D",
        "options": [
          "A. Perimeter of a rectangle.",
          "B. Volume of a cube.",
          "C. Circumference of a circle.",
          "D. Area of a triangle."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_214_real.mp4"
  },
  {
    "time": "[0:04:18 - 0:04:48]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:04:45",
        "answer": "D",
        "options": [
          "A. How to convert between different units of length.",
          "B. The concept of volume measurement.",
          "C. The perimeter of different shapes.",
          "D. How to calculate the area of another shapes."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_214_real.mp4"
  },
  {
    "time": "[0:06:27 - 0:06:57]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:06:55",
        "answer": "D",
        "options": [
          "A. The angle of the base.",
          "B. The formula for area.",
          "C. The hypotenuse of the triangle.",
          "D. The height of the triangle."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_214_real.mp4"
  },
  {
    "time": "[0:08:36 - 0:09:06]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:08:45",
        "answer": "D",
        "options": [
          "A. Discussing the properties of different quadrilaterals.",
          "B. Solving for the perimeter of the triangle.",
          "C. Simplifying algebraic expressions.",
          "D. Calculating the area of a different triangle."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_214_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:02 - 0:00:08]: A close-up view reveals four circular keychains held in a hand. Each keychain features an intricately designed acrylic disc attached to a metal ring, which is further connected to a metal cap and a colored tassel. The design on each disc is detailed and unique: the first keychain has a teal background with a geometric pattern and a decorative name \"Kayla\" written in white. The second keychain has a pink background with a chevron pattern and a monogram \"L\" in a fancy script. The third keychain has a purple background with a quatrefoil pattern, featuring the name \"Natalie\" in white alongside a heart and a monogram \"N\" in black with a heart. The tassels for the first, second, and third keychains are teal, pink, and lavender, respectively. The four keychains are neatly arranged in the hand, providing a clear view of each decorative element. [0:00:09 - 0:00:13]: The screen goes black, and the text \"Vinyl on Acrylic Key Chains\" appears, centered and in white font. [0:00:14]: The screen remains black. [0:00:15 - 0:00:17]: Another set of white text appears, reading \"How to personalized them in Cricut Design Space,” centered and in the same white font. [0:00:18 - 0:00:19]: The text changes to \"To watch the assembly skip to 2:08,\" maintaining the same centered, white font.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What was shown just now?",
        "time_stamp": "00:00:08",
        "answer": "A",
        "options": [
          "A. A detailed view of three circular keychains held in a hand.",
          "B. A person assembling a keychain.",
          "C. A tutorial on Cricut Design Space.",
          "D. A black screen with white text."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_50_real.mp4"
  },
  {
    "time": "0:02:00 - 0:02:20",
    "captions": "[0:02:00 - 0:02:20] [0:00:00 - 0:00:05]: The video begins with a selection interface on a screen, presenting options for how to load materials for a project. The screen features a white background with black and grey text, and four icons in blue outlines, associated with options: \"Without Mat,\" \"On Mat,\" \"On Card Mat,\" and \"Multiple Ways.\" [0:00:06 - 0:00:05]: The interface remains on the screen, with no changes in the selection. Below the options, instructions in black font urge to \"Choose settings according to your Vinyl type and cut out.\" [0:00:06 - 0:00:09]: The video transitions to a table with a dark grid pattern as the backdrop. At the center of the frame is a teal-colored Cricut device with the word \"Cricut\" engraved prominently on its front. Positioned beneath it is a standard green Cricut mat with a grid layout for aligning materials. [0:00:10 - 0:00:12]: A pair of hands, assumed to be the viewer’s, come into view. They hold a small piece of teal vinyl. The viewer positions the vinyl on the green Cricut mat, aligning it within the grid. [0:00:13 - 0:00:14]: With careful placement, the hands smooth out the teal vinyl on the mat, ensuring it sticks properly without any bubbles or creases. The hands then gently press down to secure it. [0:00:15 - 0:00:16]: The viewer lifts the mat slightly and adjusts it, preparing to insert it into the Cricut device. [0:00:17 - 0:00:15]: The hands move the Cricut mat towards the Cricut device and align it with the machine's entrance. [0:00:16 - 0:00:18]: The viewer carefully inserts the mat into the open slot of the Cricut machine, ensuring it is correctly aligned and ready for cutting. [0:00:19]: The video ends with the Cricut mat fully loaded into the machine, with the vinyl piece visible and ready for the cutting process to commence.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:02:20",
        "answer": "D",
        "options": [
          "A. Arranging a selection of materials on a screen.",
          "B. Smoothing out the vinyl on the mat.",
          "C. Pressing down the teal vinyl to secure it.",
          "D. Inserting the mat into the machine named \"Cricutjoy \"."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_50_real.mp4"
  },
  {
    "time": "0:04:00 - 0:04:20",
    "captions": "[0:04:00 - 0:04:20] [0:00:00 - 0:00:01]: Two keychains with tassels attached lie at the top of the frame on a grid-patterned surface. One keychain is purple with the name \"Natalie\" written on it, and the other is pink with a design resembling a knot. Below the keychains is a small white card. A pair of hands, wearing a purple sweater, are positioned at the bottom center of the frame, holding a semicircular piece of transparent paper with a teal design. [0:00:01 - 0:00:02]: The hands move the transparent paper with the teal design upwards, closer to the white card. The hands still hold the paper gently with their fingers on either side of the paper. [0:00:02 - 0:00:03]: The transparent paper with the teal design is now almost touching the white card. The detailed teal design on the paper becomes more apparent against the white background. [0:00:03 - 0:00:04]: The hands are now adjusting the position of the transparent paper over the white card. Both hands have fingers positioned symmetrically, ensuring careful alignment. [0:00:04 - 0:00:05]: The hands continue to adjust the transparent paper over the white card, ensuring it is perfectly aligned. The teal design, now fully visible, fits neatly over the white card. [0:00:05 - 0:00:06]: Once the transparent paper is aligned, the hands gently press down on it. The fingertips make sure the paper is secured onto the white card without any creases. [0:00:06 - 0:00:07]: The hands maintain their position as they apply even pressure across the entire surface of the transparent paper to ensure it sticks properly to the white card. [0:00:07 - 0:00:08]: The hands slowly begin to release the pressure, having successfully affixed the transparent paper to the white card.  [0:00:08 - 0:00:09]: The hands are now lifting slightly away from the card to inspect if the transparent paper with the teal design has properly adhered. [0:00:09 - 0:00:10]: The hands move the now-secured transparent paper and white card slightly sideways for further inspection. [0:00:10]: The hands continue adjusting the position of the teal design on the white card to ensure accuracy and avoid any misalignment. [0:00:11]: The hands continue to secure the transparent paper while slightly adjusting and pressing where needed.  [0:00:12]: With the card and transparent paper now firmly in place, the hands carefully move away, indicating that adjustments are complete. [0:00:13]: The hands return to the card, holding a new piece of transparent paper with a purple design, getting ready to repeat the process. [0:00:14]: The hands position the new paper over the white card. The delicate purple design contrasts against the teal design. [0:00:15]: The hands closely align this new transparent paper over the previously affixed teal design, ensuring it layers correctly. [0:00:16]: The hands press down gently again, ensuring the new layer of transparent paper adheres neatly over the first design. [0:00:17]: The hands work to remove any air bubbles and secure the new layer firmly, making sure the design remains intact. [0:00:18]: With both layers now firmly attached, the hands lift away for a final inspection, ensuring everything is smooth and aligned. [0:00:19]: The hands make minor adjustments to ensure the designs align perfectly, before taking a step back, revealing the completed layered design on the white card.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the hands doing right now?",
        "time_stamp": "0:04:07",
        "answer": "B",
        "options": [
          "A. Move the hands closer to the white card.",
          "B. Lift slightly away from the card to inspect the adhesion.",
          "C. Secure the paper by pressing down firmly.",
          "D. Place a new piece of transparent paper with a purple design."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_50_real.mp4"
  },
  {
    "time": "0:06:00 - 0:06:20",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:07]: In the first sequence, a hand is shown holding a set of keychains with tassels. There are four keychains in total, each with a different name: \"Holly,\" \"Katie,\" \"Millie,\" and \"Elby.\" The keychains are circular with transparent edges and are held together by metal rings. Each keychain has a unique background color behind the name: \"Holly\" has a light pink background, \"Katie\" has a purple background, \"Millie\" has a light blue background, and \"Elby\" has a mint green background. A small heart graphic is displayed beneath each name. The tassels attached to the keychains match the respective background colors and hang in an organized fashion. [0:06:08]: The scene transitions to show a Cricut cutting machine with a green Cricut mat positioned below it on a grey grid background. The focus is on the setting, suggesting preparation for a crafting task. [0:06:09]: A pair of hands in a purple sweater begins placing a small piece of mint green material at the top right corner of the green Cricut mat, aligning it carefully with the grid lines. [0:06:10]: The hands position the mint green piece precisely on the mat, ensuring alignment within the grid. [0:06:11]: A piece of pink material is now being positioned in the grid’s bottom right section, following the same careful alignment process. [0:06:12 - 0:06:13]: The hands place a purple piece of material at the bottom left section of the mat, ensuring it sticks properly to the adhesive mat surface. Alignment follows the grid lines meticulously to avoid any overlap. [0:06:14]: A blue piece of material is added above the purple piece, completing the arrangement on the mat. The green mat now displays four different colored pieces in each quadrant: mint green in the top right, blue in the top left, pink in the bottom right, and purple in the bottom left. [0:06:15]: The hands make final touches to the blue piece to ensure it is securely adhered to the mat. The overall arrangement shows a well-organized pattern, suggesting an upcoming multi-color crafting project. [0:06:16]: The hands pick up the mat and start guiding it into the Cricut cutting machine's opening slot, preparing it for the cutting process. [0:06:17 - 0:06:18]: The green mat with the four pieces of colored material is now partially inserted into the machine's slot, ready for the cutting operation. [0:06:19]: The mat is fully inserted into the machine, with the four pieces of material perfectly aligned, indicating readiness for the Cricut to begin the cutting task. The setup is complete, transitioning into the crafting action phase.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the person do just now?",
        "time_stamp": "00:06:12",
        "answer": "B",
        "options": [
          "A. Arrange a pink material at the top right.",
          "B. Place a purple piece of material at the bottom left.",
          "C. Position a blue piece of material at the bottom right.",
          "D. Set a mint green piece at the top left."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_50_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:08:07 - 0:08:09]: Several square and circular pieces of translucent material, each bearing a spot of color, are arranged on a grid-patterned work surface. The squares display hues of pink, blue, and other pastels, while the circles remain plain. A pair of hands, one holding a circular piece with a purple design and the other using a small tool, are positioned in the lower half of the frame. The fingers precisely apply pressure to attach or adjust a piece of strip tape onto the circular material. [0:08:09 - 0:08:13]: The hands continue their meticulous work by placing the purple-colored circle onto the work surface. Then, with the tool in one hand, they apply a white rectangular label to one of the squares, ensuring the label's edges align perfectly with the square's grid pattern. [0:08:13 - 0:08:21]: The hands adjust the position of the purple circlular piece, verifying its attachment. Concentrated efforts are directed towards affixing the purple circle further, ensuring its adherence to the surface. Subsequently, the focus shifts from the purple circle to a blue-colored piece. With deliberate movements, they peel off the label from the blue square, securing it onto another transparent circular material. The process demonstrates a methodical approach to applying the materials for a precise finish.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the person do just now?",
        "time_stamp": "00:08:19",
        "answer": "B",
        "options": [
          "A. Attach a purple circular piece with a strip tape.",
          "B. Apply a blue pattern to a square.",
          "C. Arrange pieces of pastel colors on the grid-patterned surface.",
          "D. Hold a tool to adjust the position of the purple circle."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_50_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What concept might the speaker explain next?",
        "time_stamp": "00:00:40",
        "answer": "C",
        "options": [
          "A. How to visualize data on a graph.",
          "B. The types of charts used in data representation.",
          "C. Differences between continuous and discrete data.",
          "D. Examples of data collection methods."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_216_real.mp4"
  },
  {
    "time": "[0:02:36 - 0:03:06]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:02:59",
        "answer": "B",
        "options": [
          "A. How to create a bar graph.",
          "B. How to fill in the data in the data table.",
          "C. How to interpret the data in the data table.",
          "D. How to convert the data table into a pie chart."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_216_real.mp4"
  },
  {
    "time": "[0:05:12 - 0:05:42]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:05:38",
        "answer": "B",
        "options": [
          "A. The average precipitation in May.",
          "B. The meaning of the scale on the graph.",
          "C. The total annual precipitation.",
          "D. The method used to calculate the average precipitation."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_216_real.mp4"
  },
  {
    "time": "[0:07:48 - 0:08:18]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:08:01",
        "answer": "D",
        "options": [
          "A. The use of the negative coordinate axis.",
          "B. How the fluctuation in investment income impacts yearly analysis.",
          "C. Strategies to improve investment returns.",
          "D. The overall trend of investment income throughout the year."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_216_real.mp4"
  },
  {
    "time": "[0:10:24 - 0:10:54]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "0:10:09",
        "answer": "A",
        "options": [
          "A. Interpret the trends shown in the graphs.",
          "B. Introduce new data sets for comparison.",
          "C. Calculate the mean of the data sets.",
          "D. Convert the temperatures to Celsius."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_216_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video shows a small, white canvas placed on a flat surface. A transparent palette with mixed colors is positioned on the right side of the canvas. A hand appears holding a tube of dark paint, dispensing it onto the palette, and another hand holding a lighter-colored paint tube is shown moments later. [0:00:03 - 0:00:05]: The palette contains blobs of pink and white paint. The person dips a brush into the pink paint and begins to paint a horizontal stroke of pink at the top edge of the canvas. [0:00:06 - 0:00:08]: The individual continues to paint horizontally, moving slightly down the canvas, adding more pink paint with the brush. The strokes are even and consistent. [0:00:09 - 0:00:12]: The brush is reloaded with paint from the palette, and additional horizontal strokes are made below the previous layer. The color appears to be blended slightly with white, creating a gradient effect with a softer pink hue. [0:00:13 - 0:00:17]: The process continues with more paint being added gradually. The brushstrokes blend smoothly, and the gradient becomes more noticeable as the lighter pink transitions to white. [0:00:18 - 0:00:20]: The gradient effect becomes more pronounced as the brush moves lower on the canvas. The painting now exhibits a seamless transition from pink at the top to white at the bottom, with each stroke carefully blended to eliminate any harsh lines.",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the gradient effect become more pronounced?",
        "time_stamp": "0:00:20",
        "answer": "C",
        "options": [
          "A. Because more dark paint is added.",
          "B. Because the brushstrokes become vertical.",
          "C. Because the brushstrokes blend smoothly from pink to white.",
          "D. Because the person stops adding paint."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the painting process just shown in the video?",
        "time_stamp": "0:00:20",
        "answer": "A",
        "options": [
          "A. The person paints a gradient from pink to white with smooth horizontal strokes.",
          "B. The canvas is covered with random colors and patterns.",
          "C. The palette is used to mix blue and green colors for a landscape.",
          "D. The person uses a sponge to create a textured effect."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_140_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:19]: The video shows the painting of a landscape on a small square canvas. The scene includes a pink sky with a gradient transitioning to light yellow near the horizon. In the background, there is a silhouette of a distant mountain in light blue. Beneath the mountain, there are hills painted in various shades of green. The artist uses a thin brush to add fine details to the foreground hills, blending yellows and greens to create texture and depth. The artist’s hand meticulously moves the brush, applying small strokes to enhance the grassy area on the right side of the canvas. A palette with mixed green and yellow paint is visible on the right edge of the frames. The workspace is well-lit, and the focus remains on the painting process, with the artist's hand occasionally obscuring parts of the canvas momentarily as they work.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action is the artist performing right now?",
        "time_stamp": "00:02:19",
        "answer": "C",
        "options": [
          "A. Mixing colors on the palette.",
          "B. Framing the finished painting.",
          "C. Applying fine details to the grass lawn.",
          "D. Cleaning the brush with water."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_140_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:20]: The video shows a close-up view of someone painting on a small canvas using a thin paintbrush. The canvas depicts a landscape scene with a path leading through green fields towards a small white house with a red roof, positioned at the center. The background features blue mountains under a pink and yellow gradient sky. The painter’s hand holding the brush is visible at the bottom right of the frames, and the brush is applying red paint to the roof of the house. A palette with mixed paint colors, including shades of yellow, white, blue, and brown, is positioned on the right side of the video frames.  Throughout the frames, the movement of the brush strokes is observed, adding fine details and adjusting the color on the roof. The video is shot from a first-person perspective, emphasizing the process of painting the roof of the miniature house to blend with the rest of the serene landscape. A section of text displaying paint color names “Burnt umber” and “White” appears in the top left corner of the image starting from the frame at [0:04:11] and continuing until the end. There are no significant background elements visible other than the painting and the palette.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the painting just shown in the video?",
        "time_stamp": "00:04:20",
        "answer": "A",
        "options": [
          "A. A landscape painting with a path leading to a small white house with a red roof.",
          "B. A portrait of an elderly man being painted with fine details.",
          "C. A close-up of a flower being painted with vibrant colors.",
          "D. An abstract painting with various geometric shapes."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_140_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:20]: In a detailed view, a person uses a small paintbrush with a light purple handle marked \"3/0 ART RIGHT\" to paint on a small canvas. The scene being painted features a landscape with a large dark brown tree with slender branches and a pinkish-red foliage. The background shows a gradient sky, transitioning from pink at the top to light blue towards the horizon. There is a mountain silhouette shaded in a light blue hue. Below the tree, a winding pathway takes a curved route towards a small, reddish-brown roofed house. The foreground comprises lush green grass, adding a vibrant touch to the pastoral setting.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located below the large dark brown tree in the painting?",
        "time_stamp": "00:06:20",
        "answer": "D",
        "options": [
          "A. A lake with a boat.",
          "B. A group of animals.",
          "C. A river with a bridge.",
          "D. A grass lawn."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_140_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:41]: At the beginning of the scene, the camera is positioned at a viewing area that leads to an escalator covered by a green and glass structure. The sky is predominantly blue with white clouds. In the foreground, there is a food stand selling \"REEL TREATS\" to the left, and a few people are seen walking towards the escalator. [0:02:42 - 0:02:48]: The camera perspective shifts to the descent of the escalator. The semi-transparent green and glass arched roofing covers the escalator. Several people are seen ahead of the camera, descending the escalator at a steady pace. A mix of greenery and buildings is visible through the glass structure. The white flooring at the bottom of the escalator is gradually becoming more visible as the descent continues. [0:02:49 - 0:02:52]: The camera continues to descend, drawing closer to the bottom. More people are visible walking on the ground level, including a couple dressed in matching red and black ensembles. They are heading past some signs indicating directions for the \"STUDIO TOUR\" and \"RESTROOMS.\" [0:02:53 - 0:02:59]: As the camera nears the bottom of the escalator, more people appear walking in various directions, and the camera captures the bustling activity. At the bottom, several directional signs and another escalator entrance are clearly seen. The people walking by vary in their movement - some individuals are heading towards the signs, while others are seen walking away from it, highlighting the busy and dynamic atmosphere of the area.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What was the name of the food stand shown just now?",
        "time_stamp": "00:02:44",
        "answer": "B",
        "options": [
          "A. TASTY TREATS.",
          "B. REEL TREATS.",
          "C. SWEET SNACKS.",
          "D. FOOD HAVEN."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_312_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:30]: The video shows a journey on a tram or similar vehicle moving along a road adjacent to a wooded area. The view begins inside the tram with passengers seated, looking out the window which provides an outside view. The road is bordered by metallic guardrails and trees with thick foliage. The seats are facing forward with passengers looking towards a screen at the front of the tram. The exterior area visible through the windows includes a mixture of leafless and leafy trees surrounded by dense greenery. As the vehicle advances, the perspective gradually shifts from inside the tram to focus more on the unfolding scenery, highlighting the transition; [0:05:31 - 0:05:39]: There is an increase in the visibility of the outside scenery, including an area with clearer views past the trees, revealing a distant urban environment with buildings and a mountainous landscape in the background. The road continues to follow a gentle curve, with the surrounding trees slowly giving way to a more open view. The sky is clear with a few wispy clouds, and the environment looks bright and serene. The perspective shows fewer close-up views of the tram's interior and more of the open surroundings as the journey continues.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What kind of area is visible through the tram's windows during the journey?",
        "time_stamp": "0:05:30",
        "answer": "B",
        "options": [
          "A. A sandy beach.",
          "B. A wooded area with trees.",
          "C. A bustling city street.",
          "D. A snowy mountain path."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Object Recognition",
        "question": "How is the road next to the tram bordered?",
        "time_stamp": "0:05:46",
        "answer": "C",
        "options": [
          "A. With a wooden fence.",
          "B. With stone walls.",
          "C. With metallic guardrails.",
          "D. With hedges."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_312_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:02]: The view begins with a first-person perspective driving along a road. On the left, there is a large building with the word \"Justice\" written in bold white and black letters. Directly in front, several billboards are visible, including one advertising \"MILK\" featuring a man’s face. There’s a small vehicle moving quickly on the left side of the road. [0:08:03 - 0:08:06]: The drive continues, revealing more billboards. These advertise movies such as \"Bridesmaids\" and \"ted\", among others. The background consists of tall buildings with reflective windows and a clear blue sky. [0:08:07 - 0:08:08]: As the vehicle moves further along, additional billboards become visible, such as an advertisement for \"Les Misérables\" and a couple more with unknown titles. [0:08:09 - 0:08:10]: The perspective shows the edge of the vehicle from which the recording is done, with additional billboards promoting what appears to be other movies or media. The surrounding area includes more buildings, some painted in light colors. [0:08:11 - 0:08:16]: The path continues past more advertisements and a large, long building on the right. There is a mix of concrete and other structures along the roadside. The road slightly curves. [0:08:17 - 0:08:19]: The video concludes with a view of a large grey building on the right side, several trucks and vehicles are parked along the side, and large movie posters appear on the building’s exterior walls.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What types of advertisements were seen on the billboards just now?",
        "time_stamp": "0:08:16",
        "answer": "C",
        "options": [
          "A. Food products.",
          "B. Clothing brands.",
          "C. Movie promotions.",
          "D. Electronics."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      },
      {
        "task_type": "Action Recognition",
        "question": "What is the perspective of the view as the vehicle drives along the road?",
        "time_stamp": "0:08:10",
        "answer": "B",
        "options": [
          "A. Aerial view.",
          "B. First-person perspective.",
          "C. Third-person perspective.",
          "D. Rear view."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_312_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The scene begins with a person holding a folded, transparent umbrella in their hand, walking up a set of wet stairs tiled with a safety pattern. The background shows a tiled wall and a handrail. [0:00:03 - 0:00:05]: As the person ascends, they begin to open the umbrella. The street-level background becomes more visible, with a few pedestrians walking under umbrellas. [0:00:06 - 0:00:09]: The person holding the umbrella approaches the street, surrounded by various buildings and objects. Signs and maps are visible on the wall, with a yellow-tactile paving and wet pavement leading onto a zebra crossing. [0:00:09 - 0:00:12]: The person starts walking over the wet zebra crossing while holding the opened umbrella. Other people are walking with umbrellas, and a few cars are seen on the road. Some greenery and road signs are visible to the left. [0:00:12 - 0:00:15]: The person continues to walk on the zebra crossing, heading towards a sidewalk. More pedestrians with umbrellas are visible, along with another building with various store signs. [0:00:15 - 0:00:19]: As the person reaches the sidewalk, they turn right and continue walking along a row of shops. The pathway ahead shows more pedestrians with their umbrellas open. The building textures, signs, and greenery to the left reflect the wet conditions due to the rain. [0:00:19 - 0:00:20]: The scene continues with the person walking along the sidewalk, passing storefronts with closed shutters and illuminated signs. Other pedestrians walk ahead under their umbrellas, and more of the cityscape is visible in the background with damp sidewalks and overcast skies.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What object is the person holding in the hand?",
        "time_stamp": "0:00:03",
        "answer": "A",
        "options": [
          "A. A folded umbrella.",
          "B. A map.",
          "C. A shopping bag.",
          "D. A book."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_324_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:07]: The scene is set on a rainy day in an urban environment. The camera view shows a wet street with various individuals walking under umbrellas. A row of buildings lines the left side of the street, and there is a sidewalk adjacent to the buildings. Some buildings are several stories high with modern facades. People are walking along the sidewalk, some carrying umbrellas. A police car is driving along the street with its headlights on. The street itself has two lanes divided by a crosswalk marked with white stripes. There are metal railings along the sidewalk to separate pedestrians from the road. In the distance, more buildings and vehicles are visible, indicating a bustling city environment. [0:03:08 - 0:03:13]: The frame captures more pedestrians walking along the sidewalk and crossing the street. The camera moves forward along the sidewalk. The street to the right shows parked cars and more buildings. The wet ground reflects the surroundings, enhancing the rainy atmosphere. Leafy green trees in metal planters are placed intermittently along the sidewalk. A pedestrian can be seen riding a bicycle down the sidewalk. The crosswalk in view appears to be frequently used as more people approach and cross it.  [0:03:14 - 0:03:20]: The camera angle shifts slightly to capture more of the crosswalk and the view further down the street. The buildings continue to line both sides of the street, with shops, offices, and residential windows visible. Pedestrians with umbrellas continue to walk briskly, trying to avoid the rain. The street surface is reflective from the rain, and the crosswalk lines are clear and distinct. The overall movement of pedestrians and occasional vehicles creates a sense of active city life despite the dreary weather conditions.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the distinctive feature of the crosswalk?",
        "time_stamp": "00:03:20",
        "answer": "B",
        "options": [
          "A. It has red and white stripes.",
          "B. It is marked with white stripes.",
          "C. It is marked with yellow stripes.",
          "D. It has green and white stripes."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_324_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:03]: On a rainy day, a first-person perspective video captures a wet sidewalk with red brick pavement. In the background, a mix of modern and older buildings line the street. Several people are visible carrying umbrellas while walking down the sidewalk. A blue truck is parked on the right side of the road, and a few pedestrians can be seen walking near it. [0:06:04 - 0:06:07]: As the person moves along the sidewalk, the view shifts slightly to the left, showing more pedestrians crossing a wet street at a crosswalk. A white van is waiting at the crosswalk, and people holding umbrellas are walking in various directions. [0:06:08 - 0:06:11]: The movement continues down the sidewalk, where more people with umbrellas are seen. A black taxi with its lights on is driving down the wet road beside the sidewalk. The sidewalk is separated from the road by a metal railing. [0:06:12 - 0:06:15]: The rain continues to pour, visible by the droplets on the camera lens. The camera captures more people walking both on the sidewalk and across the street, all holding different colored umbrellas. Some people walk alone while others walk in pairs or small groups. [0:06:16 - 0:06:19]: The video continues to show more of the sidewalk, with the rain still falling. Pedestrians are seen walking toward and away from the camera. The sidewalk is lined with green plants and trees, adding a bit of nature to the urban setting. The black taxi is still visible in the distance, moving down the street past the blue truck.",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "What is the overall weather condition depicted in the video?",
        "time_stamp": "0:06:19",
        "answer": "C",
        "options": [
          "A. Sunny.",
          "B. Snowy.",
          "C. Rainy.",
          "D. Foggy."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_324_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:02]: The scene begins on a rainy street viewed from a first-person perspective. Multiple vehicles, including a black car and a grey van, are driving on the wet road, lined with modern buildings and lush green trees. Some pedestrians hold umbrellas. [0:09:03 - 0:09:04]: The camera angle shows more of the sidewalk, presenting several bicycles parked against the pavement railing. There are also more pedestrians in the scene, some carrying umbrellas. [0:09:05 - 0:09:08]: The perspective shifts further down the sidewalk, revealing more of the street's expanse and the arrangement of trees, vehicles, and buildings. The rain is clearly visible on the umbrella and wet pavement, indicating ongoing drizzle. [0:09:09 - 0:09:10]: The angle now targets a biking area with parked bikes, showing more details of the wet conditions as the rain continues to fall. The empty space on the sidewalk highlights the rainfall's effect on the typically busy street. [0:09:11 - 0:09:13]: As the video progresses, the viewer's gaze shifts towards colorful signs written in Japanese on handrails and building walls, emphasizing the vibrant urban environment. [0:09:14 - 0:09:15]: The first-person perspective navigates towards a crosswalk, revealing more parked vehicles, including two white vans, and additional Japanese signage. The atmosphere remains consistently wet and rainy. [0:09:16 - 0:09:18]: Further down, the video showcases a more extended part of the sidewalk. The presence of rain significantly impacts the visual clarity and creates reflections from the pavement. [0:09:19 - 0:09:20]: The video wraps up with a view down the street, prominently featuring multiple parked vehicles, signage, and a clear path extending into the urban environment under persistent rain.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What was a noticeable feature of the street just now?",
        "time_stamp": "0:09:20",
        "answer": "C",
        "options": [
          "A. It is lined with colorful flowers.",
          "B. The road is empty.",
          "C. It has a lot of parked bicycles and is wet due to rain.",
          "D. It is empty and dry with clear skies."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_324_real.mp4"
  },
  {
    "time": "[0:11:00 - 0:11:59]",
    "captions": "[0:11:40 - 0:11:59] [0:11:40 - 0:11:54]: The video takes place in a large, open courtyard surrounded by tall buildings. Trees line the courtyard, with two prominent trees at the center. The trees have thick trunks and lush green leaves. To the right, there is a pathway with some people walking. The ground is wet from rain, creating a reflective surface. There is a small, white building at the far end of the courtyard. The viewer's perspective moves forward slowly, and it appears to be raining, as indicated by the raindrops visible on an umbrella held above the camera.  [0:11:55 - 0:11:59]: As the perspective continues to move forward, more details of the surroundings become visible. On the left, there is a playground with slides and climbing structures, while the buildings in the background are varied in height and architectural style. The trees, still in focus, add a natural element to the urban environment. The overall setting remains consistent, with wet surfaces and raindrops on the umbrella.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the small white building located in the courtyard?",
        "time_stamp": "00:11:59",
        "answer": "B",
        "options": [
          "A. To the left of the pathway.",
          "B. At the far end of the courtyard.",
          "C. Next to the playground.",
          "D. In the center of the courtyard."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_324_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:01]: A person is standing in a kitchen by a large countertop. They are stirring food in a black frying pan with a red spatula. Various ingredients and utensils are spread out on the counter, including a lemon and some herbs. The background includes a refrigerator and cabinets. [0:02:01 - 0:02:02]: The person continues to stir the contents of the frying pan, which appears to contain sliced onions. They are holding the pan with their left hand and the spatula with their right hand. [0:02:02 - 0:02:03]: A close-up view of the frying pan shows the sliced onions being stirred and sautéed. The person is using the spatula to mix the onions while holding the pan over the stove. [0:02:03 - 0:02:04]: The person continues stirring the onions in the frying pan. The camera remains focused on the pan and their hands, showing a controlled and steady stirring motion. [0:02:04 - 0:02:05]: The person gestures with their left hand while holding the red spatula upright in their right hand. They seem to be explaining something while maintaining eye contact, with the frying pan still on the stove. [0:02:05 - 0:02:06]: The person continues using the spatula in the frying pan with their right hand, looking down at the pan. The countertop and objects on it remain visible. [0:02:06 - 0:02:07]: The person continues stirring the contents in the frying pan. The background, including the cabinets and window, remains unchanged. [0:02:07 - 0:02:08]: The person steps away from the stove, moving towards the counter. They reach for something on the counter while the pan remains on the stove. [0:02:08 - 0:02:09]: The person picks up a utensil from the counter, holding it with their left hand. The frying pan remains on the stove while they prepare to use the utensil. [0:02:09 - 0:02:10]: A close-up of the person sharpening a knife. They hold the sharpening tool with their left hand and the knife with their right hand, moving it steadily across the tool. [0:02:10 - 0:02:11]: The person holds the sharpened knife up, displaying it. They appear to be explaining something, with their attention focused ahead. [0:02:11 - 0:02:12]: The person begins to gesture with their left hand while still holding the knife. The green cup and the bottle of oil on the counter remain in their positions. [0:02:12 - 0:02:13]: The person continues to gesture with their left hand, appearing to explain further. The countertop with the various ingredients and utensils is visible behind them. [0:02:13 - 0:02:14]: The person reaches towards the counter with their left hand, appearing to pick up one of the ingredients. The pan is still on the stove. [0:02:14 - 0:02:15]: The person picks up an ingredient from a small bowl on the counter. They continue to explain something while holding the ingredient. [0:02:15 - 0:02:16]: The person moves back towards the stove with the ingredient in their left hand. They are preparing to add it to the frying pan. [0:02:16 - 0:02:17]: The person adds the ingredient to the frying pan with a precise motion. The green cup and bottle of oil are still visible on the counter. [0:02:17 - 0:02:18]: The person continues to focus on the frying pan, stirring the contents with the red spatula. [0:02:18 - 0:02:19]: The person continues stirring the frying pan’s contents with the added ingredient, ensuring it mixes well. Their attention is fully on the frying pan.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person using to stir the contents in the frying pan?",
        "time_stamp": "00:02:01",
        "answer": "C",
        "options": [
          "A. A wooden spoon.",
          "B. A metal spatula.",
          "C. A red spatula.",
          "D. A plastic spoon."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_27_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:02]: A man stands in a modern, brightly lit kitchen in front of a stainless steel refrigerator and white subway-tiled wall. He places a bottle of white wine on the marble countertop, next to a variety of ingredients, including a lemon, fresh herbs, and a plate of prawns. [0:04:02 - 0:04:04]: The man picks up the bottle of white wine and begins pouring it into a frying pan on the stove. The pan contains a mixture of ingredients, including cherry tomatoes and herbs, which begin to sizzle as the wine is added. [0:04:04 - 0:04:06]: He continues to pour wine into the frying pan while stirring the contents with a red spatula. The contents in the pan are a vibrant mix of colors, primarily red and yellow from the cherry tomatoes. [0:04:06 - 0:04:08]: The camera zooms in to show a close-up of the pan. As the wine is added, steam rises, and the tomatoes and other ingredients continue to cook, releasing more aroma. [0:04:08 - 0:04:10]: The man briefly stops pouring wine and places the bottle back on the countertop. He then stirs the ingredients in the pan with a red spatula, ensuring even cooking and mixing. [0:04:10 - 0:04:12]: He continues to stir the contents of the frying pan, which now have a rich, simmering texture. Steam continues to rise from the pan, indicating the heat and ongoing cooking process. [0:04:12 - 0:04:13]: The camera shows a closer view of the frying pan, focusing on the tomatoes and other ingredients as they simmer and combine with the wine, creating a bubbling effect. [0:04:13 - 0:04:15]: The man stirs the mixture with the red spatula, ensuring that all ingredients are well-combined and cooked evenly. The sizzling sound intensifies, and more steam rises as the mixture cooks down. [0:04:15 - 0:04:17]: The man continues stirring and then briefly gestures with his free hand. He speaks, possibly explaining what he’s doing or offering cooking tips. The background remains consistent, with a well-organized kitchen and natural light streaming in from a window. [0:04:17 - 0:04:19]: The man then moves away from the stove, briefly placing the spatula down. He reaches out to grab another ingredient from the countertop, preparing to add it to the frying pan. The assortment of kitchen tools and ingredients spread out on the counter reflects a well-prepared cooking setup.",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What is the man likely to do next after reaching out to grab another ingredient from the countertop?",
        "time_stamp": "0:04:19",
        "answer": "B",
        "options": [
          "A. Serve the dish immediately.",
          "B. Add the new ingredient to the frying pan.",
          "C. Clean the countertop.",
          "D. Turn off the stove."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_27_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:01]: The video opens with a kitchen scene where a man in a navy blue shirt is preparing ingredients on a wooden cutting board placed on a marble countertop. Several items, including lemons, mushrooms, a green ceramic container, and a bowl, are placed on the counter. Stainless steel appliances and white tiled walls are visible in the background. The man is focused on chopping or arranging something on the board. [0:06:01 - 0:06:02]: The man reaches towards the bowl on his left and starts to move it closer to him, positioning it better for the next step of his preparation. His attention seems to be fully on the bowl and its contents. [0:06:02 - 0:06:03]: He uses both hands to handle the bowl, ensuring it is steady. Meanwhile, the countertop with the various ingredients and utensils remains organized, aiding in a smooth cooking process. [0:06:03 - 0:06:04]: Concentration remains high as he adjusts the position of the bowl. The lemons and other ingredients on the counter stay intact but await their purpose in the recipe. Natural light from the window nearby illuminates the scene, indicating it's daytime. [0:06:04 - 0:06:05]: After adjusting the bowl, the man resumes his focus on the cutting board, this time seemingly ready to incorporate the contents of the bowl into his preparation. The hand movements are deliberate and precise. [0:06:05 - 0:06:06]: His attention shifts as he places a piece from the bowl onto the cutting board, maintaining the cleanliness and order of his workspace. The bright natural light from outside continues to light up the kitchen. [0:06:06 - 0:06:07]: With the bowl properly positioned, the man retrieves another ingredient or utensil from the side, perhaps preparing to add it to what he was handling previously. His movements are calm and measured, highlighting his experience. [0:06:07 - 0:06:08]: Next, he begins seasoning the food item on the board with a pepper mill, suggesting that the preparation is nearing the cooking stage. The countertop remains organized, with ingredients lying in wait for further use. [0:06:08 - 0:06:09]: A close-up view zooms in on the shrimp on the cutting board as it starts getting seasoned with black pepper. The detail of the pepper falling onto the shrimp is clearly visible, indicating careful seasoning. [0:06:09 - 0:06:10]: The hand holding the pepper mill is shown up close while peppering the shrimp, emphasizing the action and detail involved in ensuring even seasoning across all pieces. [0:06:10 - 0:06:11]: Back to the wider kitchen view, the man places the pepper mill back on the counter and reaches for another ingredient, perhaps salt, as he continues the seasoning process. [0:06:11 - 0:06:12]: He picks up a container of salt and starts sprinkling it onto the shrimp, ensuring they are properly seasoned. His meticulous attention to detail is evident. [0:06:12 - 0:06:13]: The camera focuses closely again on the shrimp being seasoned with salt. The granules are visible as they land on the seafood, adding to the flavoring process. [0:06:13 - 0:06:14]: A pile of seasoned shrimp on the cutting board is shown up close, with the mix of black pepper and salt visible on them. The man's fingers slightly adjust the shrimp to ensure even coverage of spices. [0:06:14 - 0:06:15]: The man continues to handle the shrimp, giving them one last adjustment to ensure they are evenly coated with the pepper and salt. Each movement is purposeful and controlled. [0:06:15 - 0:06:16]: The man adjusts the shrimp one last time to make sure the seasoning is spread well, readying the seafood for cooking.  [0:06:16 - 0:06:17]: Another close-up shows the shrimp resting on the cutting board with seasoning all around, ready for the next step in preparation. The man's fingers are visible, indicating his preparedness for subsequent actions. [0:06:17 - 0:06:18]: Returning to the broader kitchen view, the man might be discussing or considering the next steps in the recipe. His hands rest on the cutting board near the shrimp, indicating a brief pause before continuing. [0:06:18 - 0:06:19]: The man resumes a more involved stance, appearing to talk or explain while pointing towards something near the shrimp. The kitchen remains orderly, with ingredients and utensils ready for further use.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the main activity the man is engaged in right now?",
        "time_stamp": "00:06:24",
        "answer": "A",
        "options": [
          "A. Cooking shrimp.",
          "B. Preparing a salad.",
          "C. Washing vegetables.",
          "D. Baking bread."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_27_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: In a bright kitchen with white tiled walls and cabinets, there is a large stainless steel refrigerator on the left side. A chef in a navy blue shirt stands behind a marble kitchen island. Various cooking utensils and ingredients are placed on the countertop and stoves. He gestures with his hands above a pot on the stove, which has steam rising from it. Two pans and a bottle of olive oil are also on the stove.  [0:08:01 - 0:08:02]: The chef bends slightly toward the pot, stirring it with a spoon. His left hand rests on the counter. The stainless steel refrigerator and kitchen counters are visible in the background. [0:08:02 - 0:08:03]: The chef's right hand continues stirring the pot while his left hand is now raised, possibly gesturing for emphasis. The countertops surround the cooking area with another stove containing a metal bowl and bottle of olive oil. [0:08:03 - 0:08:04]: The chef leans forward, focusing on the pot he is stirring. The kitchen behind him remains orderly, featuring various kitchen tools. He reaches over the pot positioned on the stove. [0:08:04 - 0:08:05]: With his right hand, the chef raises a utensil from the pot, inspecting its contents. His left hand is now positioned on the countertop for support. [0:08:05 - 0:08:06]: The chef looks toward the small pan on his left, and his hand reaches toward it. Olive oil bottles and metal bowls are still on the counter. [0:08:06 - 0:08:07]: The chef picks up the small pan and carefully flips the food inside. Behind him, natural light streams in from the window, illuminating the space. [0:08:07 - 0:08:08]: He continues to flip the food in the pan, ensuring even cooking. The kitchen's neat and organized setup with essential utensils and ingredients is visible around. [0:08:08 - 0:08:09]: The chef places the flipped food back onto the burner, adjusting its position on the stovetop. The kitchen background remains consistent, with the window and cabinets well designed. [0:08:09 - 0:08:10]: The chef's focus remains on the small pan as he continuously flips the food inside, ensuring it's perfectly cooked. The pot, utensils, and other cooking items are orderly on the countertop. [0:08:10 - 0:08:11]: As he flips the food inside the pan, the chef checks its cooking progress. His movements are precise and controlled, indicative of his professional expertise. [0:08:11 - 0:08:12]: A close-up shot of a different pan reveals shrimp and vegetables sizzling inside, releasing steam upward. [0:08:12 - 0:08:13]: The close-up continues, showing the sizzling shrimp and vegetables, with bits of green and red indicating spices and herbs. [0:08:14 - 0:08:15]: The chef steps back, holding a white cloth, gesturing toward the cooking food. The kitchen's clean and organized setup with essential implements remains visible. [0:08:15 - 0:08:16]: The chef picks up the pan, the white cloth in his left hand, focused on maintaining the food's presentation. Kitchen elements like the refrigerator and cabinets remain in the frame. [0:08:16 - 0:08:17]: He adjusts the position of the pan with the white cloth, ensuring that the food cooks evenly, maintaining the neat setting of the kitchen. [0:08:17 - 0:08:18]: Another close-up shot of the sizzling shrimp and vegetables in the pan on the stovetop, continuing to cook. [0:08:18 - 0:08:19]: The continuing close-up shows the same pan angle, highlighting the intense cooking action and rich colors of the shrimp and vegetables. [0:08:19 - 0:08:20]: The chef leans over a pot on the stove, using a pair of tongs to handle the food inside, with the kitchen in the background.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the chef stirring in the pot?",
        "time_stamp": "0:08:02",
        "answer": "C",
        "options": [
          "A. A sauce.",
          "B. Vegetables.",
          "C. noodles.",
          "D. Shrimp."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Object Recognition",
        "question": "What item is used by the chef to handle the food inside the pot?",
        "time_stamp": "0:08:25",
        "answer": "C",
        "options": [
          "A. A spoon.",
          "B. A spatula.",
          "C. A pair of tongs.",
          "D. A fork."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_27_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:05]: Two sketchbooks are on a table. One is open with a black elastic strap laying across the left page. Behind them is an out-of-focus portrait of a person. A hand holding a tube with the text \"GOLDEN 100\" is at the right side; A paintbrush is placed on the table between the sketchbooks. [0:00:06 - 0:00:07]: A small bottle dispenses a liquid onto the open page of the sketchbook. [0:00:08 - 0:00:09]: The liquid continues to collect in a small pool on the paper. [0:00:10 - 0:00:15]: The paintbrush is being used to spread the liquid onto the entire page of the open sketchbook in horizontal motions.  [0:00:16 - 0:00:17]: The page is now evenly coated with the liquid, and the brush moves away. [0:00:18 - 0:00:19]: Overhead, a black object (possibly a light or another tool) is near the open sketchbook. A pencil and pen are on the table next to the book, and the page appears dry.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the position of the paintbrush relative to the sketchbooks initially?",
        "time_stamp": "0:00:20",
        "answer": "B",
        "options": [
          "A. In front of the open sketchbook.",
          "B. Between a sketchbook and a white item.",
          "C. Behind the closed sketchbook.",
          "D. To the left of the open sketchbook."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_133_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:24]: The video shows a close-up of a person's hand painting a portrait. The portrait is partially completed with a focus on the face, which features a mix of vibrant colors including blue, yellow, and brown. The painting is on a white paper, with the artist using a flat brush to add color to the bottom part of the face, near the mouth and chin. The artist's hand is visible holding a brush. [0:03:25 - 0:03:31]: The hand continues to apply paint to the portrait, focusing on blending the colors around the mouth and chin area. The strokes are precise, adding depth and texture to the portrait. The person's features in the portrait become more defined with each brushstroke, and the use of multiple colors gives the portrait a dynamic and expressive feel. [0:03:32 - 0:03:39]: The artist makes final adjustments and details to the face, smoothing and blending the colors around the chin and neck. The background remains plain, keeping the focus entirely on the portrait's face. The expressive use of color and the careful brushwork make the portrait appear more vivid and lifelike as the video progresses.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the artist focusing on right now?",
        "time_stamp": "00:03:39",
        "answer": "B",
        "options": [
          "A. Adding a background.",
          "B. Painting the portrait's face.",
          "C. Sketching the portrait.",
          "D. Cleaning the palette."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_133_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:59]: The video captures the process of painting a detailed portrait of a man with dark hair and a serious expression. The canvas is white with shades of teal, blue, and green used for the background and the man's shirt. The artist's hand is visible holding a brush, applying strokes to the portrait. The brush moves mainly around the man's face and background, adding details and blending colors. The book, in which the painting is set, shifts slightly as the artist works, and the brush strokes become more precise over time. The scene appears to be set on a flat surface, probably a table, and occasional glimpses of other painting tools can be seen in the background.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the painting set while the artist works on it?",
        "time_stamp": "0:07:00",
        "answer": "C",
        "options": [
          "A. On an easel.",
          "B. On the floor.",
          "C. On a table.",
          "D. On a wall."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the painting process shown just now?",
        "time_stamp": "0:07:00",
        "answer": "B",
        "options": [
          "A. A rough sketch of a landscape.",
          "B. Add background color to the portrait.",
          "C. An abstract painting with vibrant colors.",
          "D. A still life painting of fruits and flowers."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_133_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:11:00]",
    "captions": "[0:10:00 - 0:10:20] [0:10:00 - 0:10:09]: The video shows a close-up view of a painting in progress. The painting is of a person's face, characterized by dark hair and a serious expression, rendered in shades of green, blue, and white. The brushstrokes are bold, with noticeable texture. The artwork is being created in an open sketchbook with white pages. A paintbrush is actively applying paint to the canvas, moving around the facial features, particularly focusing on the areas near the eyes and cheeks. A dark-colored mug with the text \"THAT'S TRUST IN PIXIE DUST\" in golden letters is visible to the right side of the sketchbook. The background surface is light and neutral, likely a desk or table. [0:10:10 - 0:10:19]: The camera continues to focus on the painting while the brush moves, touching up the details around the eyes and other facial features. The perspective remains consistent, maintaining a clear view of the painting and the ongoing artistic process. The brush techniques vary slightly, applying different shades and blending colors to enhance the texture and depth of the portrait. The overall appearance of the painting remains vibrant and expressive.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the painting process in the video?",
        "time_stamp": "00:10:19",
        "answer": "C",
        "options": [
          "A. Bold brushstrokes on a colorful landscape.",
          "B. Fine details on a still life painting.",
          "C. Detailing a portrait of a man with dark hair and a serious expressions.",
          "D. Subtle shading on an abstract design."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_133_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:18",
        "answer": "A",
        "options": [
          "A. 0.",
          "B. 1.",
          "C. 3.",
          "D. 4."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      },
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:58",
        "answer": "A",
        "options": [
          "A. 0.",
          "B. 1.",
          "C. 3.",
          "D. 4."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_71_real.mp4"
  },
  {
    "time": "0:02:40 - 0:03:00",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:02:18",
        "answer": "A",
        "options": [
          "A. 0.",
          "B. 1.",
          "C. 3.",
          "D. 4."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      },
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:02:58",
        "answer": "A",
        "options": [
          "A. 0.",
          "B. 1.",
          "C. 3.",
          "D. 4."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_71_real.mp4"
  },
  {
    "time": "0:05:20 - 0:05:40",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:18",
        "answer": "A",
        "options": [
          "A. 0.",
          "B. 1.",
          "C. 3.",
          "D. 4."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_71_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Which side of the trail has more dense foliage right now?",
        "time_stamp": "00:00:10",
        "answer": "C",
        "options": [
          "A. The left side.",
          "B. The right side.",
          "C. Both sides equally.",
          "D. Neither side has foliage."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_166_real.mp4"
  },
  {
    "time": "[0:01:47 - 0:02:07]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the large tree located right now?",
        "time_stamp": "00:02:06",
        "answer": "D",
        "options": [
          "A. On the left side of the road.",
          "B. Ahead, slightly to the left.",
          "C. Overhead, providing shade.",
          "D. On the right side of the road."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_166_real.mp4"
  },
  {
    "time": "[0:03:34 - 0:03:54]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the shadowed area located right now?",
        "time_stamp": "00:03:53",
        "answer": "B",
        "options": [
          "A. On the left side of the road.",
          "B. Across the entire road.",
          "C. On the right side of the road.",
          "D. No shadow at all."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_166_real.mp4"
  },
  {
    "time": "[0:05:21 - 0:05:41]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located immediately ahead in the clearing right now?",
        "time_stamp": "00:05:33",
        "answer": "A",
        "options": [
          "A. An intersection.",
          "B. A steep hill.",
          "C. A large boulder.",
          "D. A narrow bridge."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_166_real.mp4"
  },
  {
    "time": "[0:07:08 - 0:07:28]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is mainly visible on the left side of the road right now?",
        "time_stamp": "00:07:20",
        "answer": "B",
        "options": [
          "A. A lake.",
          "B. Trees and dense foliage.",
          "C. A building complex.",
          "D. Farmland."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_166_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the activity that transpired just now?",
        "time_stamp": "00:00:10",
        "answer": "D",
        "options": [
          "A. The person cooked fish filets, garnished them with vegetables, and presented them for serving.",
          "B. The person assembled burgers by adding patties, placing condiments, and wrapping them.",
          "C. The person restocked sandwich ingredients, such as lettuce and tomatoes, in preparation areas.",
          "D. The person cleaned a used tray by discarding waste, and preparing it for reuse."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_343_real.mp4"
  },
  {
    "time": "[0:01:15 - 0:01:25]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the activity that transpired just now?",
        "time_stamp": "00:01:25",
        "answer": "D",
        "options": [
          "A. The worker inspected the kitchen utensils, adjusted the oven temperature, and cleaned the grill.",
          "B. The worker arranged cooked patties on a slide, cleaned the grill surface, and prepared for the next batch.",
          "C. The worker deep-fried chicken, assembled it on a plate, and garnished it with herbs.",
          "D. The worker flipped burger patties, placed them on a tray,."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_343_real.mp4"
  },
  {
    "time": "[0:02:30 - 0:02:40]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the activity that transpired just now?",
        "time_stamp": "00:02:40",
        "answer": "D",
        "options": [
          "A. The worker arranged hotdogs on a grill, added condiments, and wrapped them for serving.",
          "B. The worker cooked and seasoned bacon strips and placed them into the fryer for crispiness.",
          "C. The worker assembled fish fillets, sprinkled them with seasoning, and placed them in the oven for baking.",
          "D. The worker transferred cooked onion onto a tray, cleaned the surface of the grill, and prepared for the next cooking batch."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_343_real.mp4"
  },
  {
    "time": "[0:03:45 - 0:03:55]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the activity that transpired just now?",
        "time_stamp": "00:03:55",
        "answer": "D",
        "options": [
          "A. The individual prepared a batch of scrambled eggs, added herbs, and placed them on a hot plate.",
          "B. The individual restocked a freezer with packages of frozen vegetables, organized them by color, and labeled each bin.",
          "C. The individual cleaned and sanitized a workstation, organized cooking utensils, and prepared for the next task.",
          "D. The individual prepared strips of bacon by separating them from frozen packaging."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_343_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:05:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the activity that transpired just now?",
        "time_stamp": "00:05:10",
        "answer": "D",
        "options": [
          "A. The worker cleaned the grill, prepared eggs by cracking and whisking them, and started cooking pancakes.",
          "B. The worker poured liquid eggs into a container, organized kitchen supplies, and cleaned the preparation area.",
          "C. The worker cracked eggs into a bowl, mixed them with spices, and set them aside for cooking later.",
          "D. The worker filled a container with liquid eggs."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_343_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:06]: A person with long curly hair is standing against a rough, pale green wall. The individual is wearing a loose, beige garment. Initially, they are looking down and appear to be adjusting their clothes with their hands near their hair.  [0:00:07 - 0:00:11]: The person turns slightly and begins to pick up a large piece of beige fabric. The background includes a draped cloth, some wooden beams, and a glimpse of darker fabric or objects near the floor. [0:00:12 - 0:00:15]: The person lifts the cloth and starts draping it over themselves, threading their arms through the sleeves. Their movements are deliberate, and they seem focused on their task. [0:00:16 - 0:00:20]: The person continues adjusting the garment, ensuring it is properly positioned. Their arms move in a fluid manner as they finalize the arrangement, looking slightly upwards towards the end. The background remains consistent, with the same draped cloth and wooden elements visible.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person picking up in the video?",
        "time_stamp": "00:00:11",
        "answer": "B",
        "options": [
          "A. A small piece of fabric.",
          "B. A large piece of beige fabric.",
          "C. A wooden beam.",
          "D. A piece of darker fabric."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Action Recognition",
        "question": "What action does the person perform with the large piece of fabric?",
        "time_stamp": "00:00:15",
        "answer": "B",
        "options": [
          "A. Ties it around their waist.",
          "B. Drapes it over themselves and threads arms through the sleeves.",
          "C. Folds it neatly and places it on the floor.",
          "D. Wraps it around their head."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_160_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:20 - 0:01:40] [0:01:20 - 0:01:22]: A person with long curly hair wearing a light-colored dress stands in a rustic room with wooden walls and a window. They are holding and inspecting a small object, possibly a necklace, with greenery visible through the window. [0:01:23 - 0:01:25]: The person bends over a wooden table to place or pick up several small objects. A wicker basket containing various items is on the table, and a dark red cloak is hanging on the wall nearby. [0:01:26 - 0:01:33]: The person stands upright again, facing the window, and begins adjusting or tying a braided belt around their waist. The window provides natural light, illuminating the interior of the room and highlighting the person's concentrated expression. [0:01:34 - 0:01:35]: The person secures a small brown pouch to their belt. The pouch hangs on their left hip, displaying ornamental detailing and embroidery on the belt. [0:01:36 - 0:01:40]: Close-up shots of a metallic pendant or key attached to the braided belt provide a detailed view of its intricate design and craftsmanship. The pendant/key is long with a distinctive pattern and multiple holes, adding a decorative touch to the overall attire.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What does the person do after picking up or placing several small objects?",
        "time_stamp": "0:01:33",
        "answer": "C",
        "options": [
          "A. Opens the window.",
          "B. Sits down at the table.",
          "C. Adjusts a braided belt around their waist.",
          "D. Leaves the room."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_160_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:45]: Hands are tying a belt with an attached pouch and a key at the waist. A person, seen from the torso down, is adjusting a burgundy and beige garment. The sleeves of the outfit have decorative patterns, and the hands are in the process of fastening the belt securely; [0:02:46 - 0:02:51]: The perspective changes to show the person’s face as they pull a white fabric over their head, likely a hood or headscarf. The person has long, dark hair and appears focused on adjusting their attire. The background reveals a room with dim lighting and wooden wall paneling; [0:02:52 - 0:02:54]: The individual, now fully covered with the fabric that drapes over their head and shoulders, turns away from the camera and faces a closed wooden door. The room shows a mix of medieval or historical decor elements, including a loom and other rustic furnishings; [0:02:55 - 0:02:57]: The person opens the wooden door, allowing natural light to flood into the dark room. They step outside, revealing part of a sunny, green outdoor area. The doorway frames this transition from the dim, cozy interior to the bright exterior; [0:02:58 - 0:02:59]: The scene changes to an outdoor setting with the person standing in front of a closed, weathered wooden door. The exterior of the building features rough, wooden siding, indicating an old or rustic structure. They are seen from behind, with the light fabric covering their head and shoulders flowing down their back.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "0:02:57",
        "answer": "D",
        "options": [
          "A. Adjusting their belt.",
          "B. Pulling a hood over their head.",
          "C. Opening a wooden door.",
          "D. Stepping outside."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_160_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:02]: The video shows the lower part of a person lying down. The individual is wearing a cream-colored garment underneath a blue one, both adorned with red embroidered edges. There is a dark object, possibly a strap or belt, positioned horizontally across the image, likely part of the person's attire. The person's lower body remains still during this time. [0:04:03 - 0:04:17]: A hand appears in the frame from the right side, reaching towards the dark object. The hand begins to touch and adjust the strap or belt, checking its buckle and positioning. The background remains dimly lit, focusing on the textures and colors of the fabric and the hand's movement. [0:04:18 - 0:04:20]: The scene zooms out slightly, revealing more of the person's upper body and part of the head, which is covered in hair. The person is lying down on a dark surface, possibly a bed or a cushioned area, wearing a dark robe with fur trim around the neck. The hand moves away from the buckle, and the person remains still.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What are the colors of the garments worn by the person lying down?",
        "time_stamp": "00:04:28",
        "answer": "A",
        "options": [
          "A. Cream and blue with red embroidered edges.",
          "B. White and blue with gold embroidered edges.",
          "C. Cream and green with silver embroidered edges.",
          "D. White and red with blue embroidered edges."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_160_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:12",
        "answer": "A",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_73_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:31",
        "answer": "B",
        "options": [
          "A. 1.",
          "B. 4.",
          "C. 2.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_73_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:18",
        "answer": "D",
        "options": [
          "A. 1.",
          "B. 3.",
          "C. 2.",
          "D. 5."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_73_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:07:18",
        "answer": "C",
        "options": [
          "A. 1.",
          "B. 3.",
          "C. 7.",
          "D. 5."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_73_real.mp4"
  },
  {
    "time": "[0:12:00 - 0:13:00】",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:12:21",
        "answer": "B",
        "options": [
          "A. 10.",
          "B. 11.",
          "C. 9.",
          "D. 12."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_73_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: A bustling street scene in a city at night. Various vehicles, including a double-decker bus, are visible in the road to the right. Across the street, buildings with brightly lit windows and advertisements add vibrancy. Individuals on the left sidewalk appear to be exploring or walking around. [0:00:03 - 0:00:06]: The street scene continues to develop, with more emphasis on the surrounding architecture. To the left, a building with a distinct slanted design becomes more prominent. Streetlights illuminate the path, and the iconic clock tower stands out in the background. [0:00:07 - 0:00:10]: The first-person perspective progresses forward along the bustling sidewalk. The path is flanked by palm trees, faint silhouettes of pedestrians, and a prominent clock tower creating focal points in the background. Vibrant lights and modern skyscrapers are visible in the distance. [0:00:11 - 0:00:14]: Moving closer to the clock tower, more details of the surrounding architecture and environment are observed. The modern buildings contrasting with the historical clock tower create a dynamic cityscape. More pedestrians and dynamic urban elements add life to the scene. [0:00:15 - 0:00:18]: Continuing along the sidewalk, a slight turn reveals more of the city skyline and passing vehicles. Advertisements on the side of the building highlight cultural events. The street remains busy with pedestrians and lined with palm trees, adding to the urban environment. [0:00:19 - 0:00:20]: Approaching the tall clock tower, the view captures the rich combination of modern urban life and historical architecture. Individuals continue to fill the bustling street, contributing to the lively city atmosphere.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type of bus is visible right now?",
        "time_stamp": "00:00:16",
        "answer": "C",
        "options": [
          "A. Single-decker bus.",
          "B. Minibus.",
          "C. Double-decker bus.",
          "D. School bus."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_336_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:23]: The scene starts in a well-lit, covered pedestrian walkway with a view to the right of a waterfront cityscape at night. The ceiling has bright light fixtures arranged in a row. To the left, there's a poster with unclear details. Several pedestrians can be seen in the distance walking toward the camera, and a person on the right is looking out over the water. [0:02:24 - 0:02:26]: As the camera moves forward, more details of the people become clearer. A group of three people wearing light-colored tops and shorts are walking toward the camera, others are scattered along the path, some standing and some walking. There are also people on the waterfront, engaged in looking out over the water. [0:02:27 - 0:02:28]: The clarity improves as the group of three people draws nearer, with some people walking by on the left side of the walkway. A sitting area with benches appears on the left, used by some resting individuals. The waterfront cityscape is vivid with skyscrapers illuminated. [0:02:29 - 0:02:30]: The group of three has passed by the camera, and two men in casual clothes are now prominent on the left side of the frame. People continue walking along the path, with more benches and people visible on the left. The illuminated buildings across the water remain visible in the background. [0:02:31 - 0:02:34]: The scene reveals a broader view of the walkway as the camera progresses. More people are visible, including those sitting on benches to the left and others walking in the distance. A vibrant blue poster is more discernible on a column. The walkway is bustling with activity. [0:02:35 - 0:02:39]: As the camera angle shifts slightly to the right, a street performer or speaker dressed in orange comes into view. This performer is seen adjusting equipment near a microphone stand. Pedestrians, including families and individuals, continue to move along, viewing the performance. The urban skyline in the background remains illuminated and provides a striking contrast against the night sky.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the primary activity of the pedestrians seen in the walkway?",
        "time_stamp": "00:02:26",
        "answer": "B",
        "options": [
          "A. Running.",
          "B. Walking.",
          "C. Cycling.",
          "D. Skating."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_336_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:59]: The video captures a vibrant cityscape from a first-person perspective at night, showcasing a panoramic view over a body of water. The sky is dark, interspersed with a few clouds. The city in the background is brightly lit with numerous multicolored lights from various buildings, including blue, red, green, and yellow lights reflecting on the water's surface, adding a dynamic array of colors to the scene. A high-rise building with a distinct, well-lit spire is prominent among the skyline. In the foreground, near the bottom-left corner, there is a portion of a person's hand and arm, suggesting they are on a boat or near the water's edge. The water gently ripples, reflecting the city lights in a dance of colors. The scene is static, primarily focusing on the lively, illuminated city skyline against the dark, night backdrop.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the setting of the video scene?",
        "time_stamp": "0:05:00",
        "answer": "C",
        "options": [
          "A. A busy market street during the day.",
          "B. A quiet village at sunset.",
          "C. A vibrant cityscape at night.",
          "D. A beach during the evening."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_336_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:19]: The video shows a panoramic view of a city skyline at night, reflecting vibrantly in the water in the foreground. The scene is filled with numerous high-rise buildings adorned with colorful lights, creating a dazzling display against the dark sky. One prominent building with a distinctively lit spire stands tall in the center, serving as a focal point amidst the other illuminated structures. The water reflects the city’s myriad lights, causing a beautiful interplay of colors on its surface. The sky above is partly cloudy, with some clouds visible but not obstructing the view. The video appears to be static, with no significant movement or changes happening throughout.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the overall setting depicted in the video?",
        "time_stamp": "0:07:20",
        "answer": "A",
        "options": [
          "A. A city skyline at night.",
          "B. A rural landscape at sunset.",
          "C. A beach during daytime.",
          "D. A forest in the morning."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the colorful lights primarily reflected?",
        "time_stamp": "0:07:20",
        "answer": "C",
        "options": [
          "A. In the sky.",
          "B. On the buildings.",
          "C. In the water in the foreground.",
          "D. On the streets."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_336_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a view of someone holding and breaking down several cardboard boxes on a floor with a grayish texture. Shelves with various products, including milk cartons and red plastic crates, are on the right. [0:00:01 - 0:00:07]: The person shifts their focus towards a shelf, eventually kneeling down and using their gloved hands to organize boxes of milk. A nearby metal cart filled with red plastic crates is visible to the right of the shelves. [0:00:08 - 0:00:11]: The person picks up a box labeled \"Valio\" and proceeds to place milk cartons from the box onto the shelf. The milk cartons are predominantly green and white and labeled with prices, such as \"20.95.\" [0:00:11 - 0:00:12]: The person continues placing milk cartons on the lower shelves, ensuring there is an orderly arrangement. [0:00:12 - 0:00:17]: The person picks up additional milk cartons from the box on the floor and places them on the shelf. The action alternates between reaching into the box and arranging the cartons on the shelf. The surrounding floor has scattered pieces of cardboard. [0:00:18 - 0:00:20]: The person continues to organize the remaining cartons. A more comprehensive view shows the shelves filled with various milk products and other items arranged neatly. The dismantled cardboard boxes lie on the ground to the left.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action does the person perform after kneeling down?",
        "time_stamp": "0:00:08",
        "answer": "B",
        "options": [
          "A. Breaking down cardboard boxes.",
          "B. Placing and organizing boxes of milk on a shelf.",
          "C. Placing cartons on a metal cart.",
          "D. Removing gloves."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_447_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:42]: The video shows a person in a storage room or supermarket aisle, reaching for yogurt cups on a shelf. The individual is using both hands, and an assortment of dairy products, mainly yogurts, is visible on the shelves. The person's hands are gloved, suggesting they may be handling cold items. [0:02:43 - 0:02:45]: The person continues to reach for different yogurt cups on the shelf. A wider view of the area shows more yogurt cups and other dairy products neatly organized on the shelves. The shelves have product labels and price tags, indicating a retail environment. [0:02:46 - 0:02:49]: The perspective shifts downward, showing a shopping cart with various items including a few yogurt cups and other packaged goods. The floor is grey and looks like the back storage area or warehouse of a retail store. The cart is partially filled. [0:02:50 - 0:02:53]: The individual appears to be picking up a box from a nearby stack of items. Several boxes and a red cart filled with more products are visible. The camera captures the person’s gloved hands as they handle the box. [0:02:54 - 0:02:56]: The person places the box into the shopping cart which already contains several similar boxes. The cart is pulled slightly closer while repositioning the items for better organization. [0:02:57 - 0:02:59]: The individual continues organizing items in the shopping cart, picking up and placing a smaller box that contains multiple white containers with red caps. The shelves are stocked with various food items, and more carts with similar products are in the vicinity.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What kind of items is the person reaching for on the cart?",
        "time_stamp": "0:02:43",
        "answer": "B",
        "options": [
          "A. Cheese blocks.",
          "B. Yogurt cups.",
          "C. Milk cartons.",
          "D. Bread loaves."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_447_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:39]: The video shows a person, presumably the viewer, stocking shelves in a store. The perspective is first-person, and the viewer's gloved hands are visible in almost every frame as they handle items. The shelves are filled with various products, primarily cartons and boxes of what appears to be food or beverage items. The items have different colors and labels, while the shelves are white and metallic.  A cart or some sort of mobile rack is positioned to the left side of the viewer, loaded with the items to be stocked, and the floor has a gray texture. The video captures the viewer fitting products into the shelves, removing items from cardboard boxes, and organizing them. The viewer's arms extend occasionally to the shelves to place the items accurately. Throughout the segment, there are additional cardboard boxes and packaging materials on the floor around the viewer. As the viewer stocks the shelves, they move products from the cart to their designated spots on the shelf. The area seems to be part of a stockroom or a storage section of a retail store based on the item arrangement and the environment. [0:05:20 - 0:05:29]: The viewer is fitting small containers, possibly yogurt or similar products, into a cardboard packaging box on the floor before placing the entire box on the shelf above. [0:05:30 - 0:05:31]: The viewer reaches toward a part of the shelf and adjusts their positioning. [0:05:32 - 0:05:36]: The viewer shifts their attention to a stack of larger cardboard boxes on the floor beside them. They begin to interact with these boxes, possibly identifying what items to stock next. [0:05:37 - 0:05:39]: The viewer resumes stocking the shelf, placing items carefully in their respective positions, reaching slightly upwards to fit the items in the correct spots.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type of items is the viewer primarily handling in the video?",
        "time_stamp": "00:05:40",
        "answer": "B",
        "options": [
          "A. Electronic gadgets.",
          "B. Dairy products.",
          "C. Clothing accessories.",
          "D. Stationery products."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_447_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:11]: The video shows a first-person perspective of a person working in what appears to be a storage room or warehouse. The individual is wearing gloves and is placing items on a shelf. There are cardboard boxes on the floor and a metal trolley cart next to the individual. On the shelf, there are various items including milk cartons. The person is systematically picking up and placing items from the box onto the shelf. The shelves have price labels, indicating different price points for the items.  [0:08:12 - 0:08:16]: The scene shifts as the individual moves away from the shelf, carrying a cardboard box. The camera follows a movement towards a stack of boxes on the floor. The stack appears to be organized with some boxes already opened and others still sealed. The individual seems to be preparing to stack more boxes or rearrange them. [0:08:17 - 0:08:20]: The individual picks up a manual pallet jack, indicating they might be preparing to move the stack of boxes. The camera position and movements suggest the person is making preparations to transport or rearrange the boxes. The video concludes with the individual still in the process of handling the pallet jack and preparing for further actions in the storage room.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What does the individual do after moving away from the shelf?",
        "time_stamp": "00:08:16",
        "answer": "B",
        "options": [
          "A. Picks up another item to place on the shelf.",
          "B. Carries a cardboard box towards a stack of boxes.",
          "C. Sits down to take a break.",
          "D. Leaves the storage room."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_447_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:59]",
    "captions": "[0:09:40 - 0:09:59] [0:09:40 - 0:09:41]: The video shows a person wearing a dark jacket and gloves with a gold bracelet on their wrist. They are reaching towards a white shelf filled with cartons labeled \"Julebrus.\"  [0:09:41 - 0:09:42]: The person continues to grab cartons from the shelf. Visible below are some cardboard boxes and the ground. [0:09:42 - 0:09:43]: The person has taken a carton off the shelf and is moving it towards a cart.  [0:09:43 - 0:09:44]: The camera angle changes to show the person's perspective as they move cartons from a shelf into a large, red shopping cart.  [0:09:44 - 0:09:45]: The person takes more cartons and begins placing them on the cart. Boxes are scattered on the ground nearby. [0:09:45 - 0:09:46]: More cartons are placed in the cart, while the person continues moving amidst a pile of cardboard boxes.  [0:09:46 - 0:09:47]: The person picks up a large, flat piece of cardboard from the pile on the floor and places it against the side of their cart. [0:09:47 - 0:09:48]: The view shifts back to the person organizing the boxes in the cart, making sure that they are stacked securely. [0:09:48 - 0:09:49]: The person adjusts several cartons in the cart, preparing to stack more boxes on top. [0:09:49 - 0:09:50]: They continue to organize the boxes, ensuring the pile is stable. Shelves filled with products can be seen in the background. [0:09:50 - 0:09:51]: The person stacks another box on the cart, pressing it down to make sure it is secure. [0:09:51 - 0:09:52]: They reach for another carton and begin to move it towards the cart, placing it carefully within the existing stack. [0:09:52 - 0:09:53]: Another carton is being held and positioned by the person above a large red basket and a pile of flattened boxes on the ground. [0:09:53 - 0:09:54]: The camera angle changes to show a shelf filled with product, with the person reaching up to grab an item. [0:09:54 - 0:09:55]: The person has taken a few cartons from the shelf and is securing them in the cart. [0:09:55 - 0:09:56]: They continue to stack the cartons in the cart, being more cautious with placement. [0:09:56 - 0:09:57]: They pull out another set of cartons from the shelf and place them on the cart. [0:09:57 - 0:09:58]: The person adjusts and makes sure the cartons are securely placed on the lower shelf. [0:09:58 - 0:09:59]: The person is seen moving more cardboard pieces towards the cart, managing the boxes efficiently.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the person wearing on his right wrist?",
        "time_stamp": "00:09:41",
        "answer": "C",
        "options": [
          "A. A silver watch.",
          "B. A leather bracelet.",
          "C. A gold bracelet.",
          "D. A rubber band."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_447_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video starts with a close-up view of a cluttered worktable. In the foreground, a paintbrush with a golden handle is out of focus. In the background, various art supplies such as paintbrushes and tools are scattered on the tabletop.  [0:00:02 - 0:00:03]: The perspective shifts to a top-down view of a wooden board that appears to be a canvas with a sketch outlined on it. A hand is holding a small paintbrush, applying red paint to a specific area of the sketch. [0:00:04 - 0:00:05]: The camera moves closer to the painting, focusing on a detailed section where the artist is painting a human ear. The hand holding the paintbrush adds precision to the brush strokes, enhancing the contours of the ear with shades of brown and pink. [0:00:06]: The scene transitions to showing a hand carefully peeling off a sheet of gold leaf. The gold leaf is placed on a piece of fabric on the table, with other sheets of similar material nearby. [0:00:07 - 0:00:08]: The camera shows the process of applying gold leaf to a painted surface. A brush gently presses the thin gold material onto the surface, which already has some gold leaf applied. [0:00:09 - 0:00:13]: The video returns to the previously seen painting, now showing more of the canvas. The painting appears to be a portrait of a person with leaves in their hair. The lighting creates a warm reflection on the freshly applied paint, which glistens under the light. The scene gradually darkens. [0:00:14]: The scene changes, showing a hand about to start painting on a blank white board or canvas. The surroundings indicate an indoor studio setting. [0:00:15 - 0:00:19]: The hand begins to apply a base layer of golden paint onto the white board. The process is shown with broad strokes, as the paint covers more of the surface. The detail and consistent motion of the brush are captured, illustrating the beginning stages of a new artwork.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the artist doing with the gold leaf in the video?",
        "time_stamp": "00:00:08",
        "answer": "A",
        "options": [
          "A. Applying it to a painted surface.",
          "B. Cutting it into smaller pieces.",
          "C. Mixing it with paint.",
          "D. Discarding it."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_126_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:45]: The video shows a detailed close-up view of a painting in progress. It is a portrait of a person's side profile with a background of textured gold color. A hand is holding a brush, applying paint to the area near the nose of the figure. The brush is flat and moves smoothly over the canvas, adding details to the side of the nose and cheek area.  [0:02:46 - 0:02:50]: The brush continues to move, refining the contours of the face. The colors used include soft skin tones that blend into the gold background. The lighting highlights the texture of the brush strokes both in the background and the portrait. [0:02:51 - 0:02:56]: The hand changes the brush's position, now working on adding depth and shading to the cheek and lower face area. The figure’s ear, hair, and part of the eye are clearly visible, emphasizing the precision of the painting process. [0:02:57 - 0:02:59]: The artist focuses on painting the area near the forehead and the top of the face. The strokes appear careful and measured, with the brush applying varying shades to create a realistic representation of the face.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the sequence of actions just performed by the artist?",
        "time_stamp": "00:03:00",
        "answer": "B",
        "options": [
          "A. Painting the background, then the nose, followed by the cheek, and finally the forehead.",
          "B. Painting the nose, then the cheek, followed by the forehead.",
          "C. Painting the forehead, then the nose, followed by the cheek.",
          "D. Painting the cheek, then the forehead, followed by the nose."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_126_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:39]: A detailed close-up of an artist painting a human ear on a canvas can be observed. The scene showcases the artist's hand holding a green paintbrush, applying meticulous strokes to the ear area. The painting itself features a side profile view of a person's face, predominantly painted in warm tones such as shades of brown, beige, and some golden hues. The ear is central in the frames, while the surrounding area displays segments of the cheek, jawline, and neck, illustrating an evident attention to detail. The hair is depicted in dark, almost black, brushstrokes, complemented by lighter highlights. A yellow and slightly red segment, possibly a part of the background or the person's clothing, is visible above the ear. The artist continues to refine the ear's contours and shadows, making precise adjustments to enhance its realism. As each second passes, subtle progressions in the ear's depiction are noticeable, highlighting the artist's technique and patience. The brush moves slowly and deliberately, adding depth and dimension to the painted ear. [0:05:20 - 0:05:39]: The artist eventually completes the ear, and the scene focuses on the finished painting, indicating the culmination of this segment's work.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the artist do just now?",
        "time_stamp": "00:05:38",
        "answer": "B",
        "options": [
          "A. Sketching the ear.",
          "B. Applying meticulous strokes to the ear area.",
          "C. Cleaning the paintbrush.",
          "D. Painting the background."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_126_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:05]: The video shows a close-up of a hand using a small paintbrush to add details to an illustration of a woman's side profile. The woman has dark hair adorned with a series of colorful leaves, primarily in shades of yellow and orange. The background is light grey. There are other brushes and pencils laying on the table around the artwork.  [0:08:06 - 0:08:12]: The camera zooms in closer to the part where the leaves meet the hair. The hand now swaps to a smaller brush, carefully painting intricate details on the orange leaves, ensuring each edge and vein of the leaves are well defined. [0:08:13 - 0:08:17]: The perspective zooms out slightly, showing the whole illustration from a somewhat wider view. Another brush with a long handle is used to refine the shading in the hair and leaves, making sure the blend between colors is seamless. [0:08:18 - 0:08:20]: The camera angle remains the same, focusing on the artist gently blending the leaves into the hair. The gentle strokes of the brush create a natural transition between the subject’s hair and the colorful leaves. The background remains consistent, maintaining the focus on the detailed work of the illustration.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the other brushes and pencils located in relation to the artwork?",
        "time_stamp": "00:08:10",
        "answer": "B",
        "options": [
          "A. On a shelf above the artwork.",
          "B. Scattered around the artwork on the table.",
          "C. Inside a drawer next to the table.",
          "D. Held in the artist's other hand."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the painting shown in the video?",
        "time_stamp": "00:08:20",
        "answer": "C",
        "options": [
          "A. A detailed painting of a woman with blonde hair on a dark blue background with a full moon and clouds.",
          "B. A landscape painting with mountains and rivers.",
          "C. A detailed illustration of a woman's profile with leaves in her hair being painted.",
          "D. A close-up portrait of an elderly man."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_126_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:09]: The video starts by showcasing an assortment of six boxed keychains laid out on a flat surface. Each keychain is housed in a distinct, pastel-colored box with a cut-out window design on the lid. From left to right on the top row, there is a pink box containing a pink keychain with the name \"Alli\" written in white script, black polka dots, and a white tassel. Next, a white box holds a silver-glittered keychain with a small RV design and a white tassel. The final box on the top row is light green, containing an orange round keychain with the name \"Peter\" written in white and a palm tree image in the background. On the bottom row, there is a yellow box with a yellow keychain bearing the name \"Wendy\" encircled by a floral pattern and a yellow tassel. Next, a purple box holds a purple keychain with \"bella\" written in white scripted text and decorated with a purple tassel. The last box on the right is light pink, featuring a pink heart-shaped keychain with the name \"Lisa\" written in white and a pink tassel. [0:00:10 - 0:00:14]: The scene changes to a close-up view of a pair of hands opening the pink box with the \"Alli\" keychain. One hand holds the box open while the other gently removes the keychain from the packaging, showing the intricate details of the decoration and the tassel attached to it. The hands carefully place the keychain back into the box, orienting it properly for display. [0:00:15 - 0:00:17]: The focus remains on the \"Alli\" keychain now resting neatly inside the open pink box. The soft lighting accentuates the shiny and smooth surface of the keychain, highlighting its decorative features and making the text and polka dots stand out distinctly. The video then transitions to a black screen. [0:00:18 - 0:00:19]: The words \"Keychain Display Box\" appear centered on the black screen in white text, providing a title or description for the display shown in the video. The title remains on the screen as the video concludes.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are the hands doing right now?",
        "time_stamp": "00:00:16",
        "answer": "D",
        "options": [
          "A. Writing the name \"Alli\" on a keychain.",
          "B. Holding the box closed.",
          "C. Removing the keychain from the packaging.",
          "D. Placing the keychain into the packaging."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_64_real.mp4"
  },
  {
    "time": "0:01:40 - 0:02:00",
    "captions": "[0:01:40 - 0:02:00] [0:01:40 - 0:01:41]: The video begins with a person holding a flat, rectangular, light pink cardboard piece with several flaps folded in different directions. Next to it, on a flat surface, is another cardboard piece with a small plastic sheet on top of it. The surface is a muted grey color. [0:01:42 - 0:01:46]: The person then picks up another piece of cardboard of the same color, this one having a big rectangular cutout in the middle. In one of their hands, they hold a cylindrical bottle of clear adhesive with a blue and white label. [0:01:47 - 0:01:53]: The person proceeds to apply the adhesive to the other flat cardboard piece with the large rectangular cutout. They are careful to apply it along the inner edges of the cutout, ensuring coverage around the entire border. [0:01:54 - 0:01:57]: After the adhesive is applied, the person positions a transparent plastic sheet over the cutout area, aligning it perfectly with the edges of the cardboard. They press it down to ensure it sticks firmly. [0:01:58 - 0:01:59]: The person then lifts the glued piece, giving the viewer a good look at the results. The transparent sheet stays flat and well-aligned with the cardboard.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "0:01:57",
        "answer": "C",
        "options": [
          "A. Picking up a light pink cardboard piece.",
          "B. Applying adhesive to a cardboard piece.",
          "C. Pressing down a transparent plastic sheet.",
          "D. Holding a cylindrical bottle."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_64_real.mp4"
  },
  {
    "time": "0:03:20 - 0:03:40",
    "captions": "[0:03:20 - 0:03:40] [0:03:24 - 0:03:25]: The video begins with a view of the Cricut Design Space user interface in a browser window. The screen displays a grid workspace with tools on the left side for operations such as \"New,\" \"Templates,\" and \"Projects,\" along with a top toolbar with option buttons like \"Undo\" and \"Image\" and a header showing the project name \"Untitled\". [0:03:25 - 0:03:27]: The view remains fixed on the Cricut Design Space interface. The grid workspace with its accompanying tools and options continues to be displayed. [0:03:27 - 0:03:28]: The screen changes to a file management window showing the contents of a folder named “DIY Craft Tutorials-KeychainBox-Set3”. The folder contains several items: three PDF documents named \"Design Space Instructions,\" \"License-Terms-and-Guide,\" and \"YouTube Tutorial\", four subfolders labeled \"DXF Files,\" \"PDF Files,\" \"PNG Files,\" and \"SVG Files,\" along with an additional blank document icon. The background of the file manager is white, with each item displayed at the top of the window. [0:03:28 - 0:03:29]: The contents of the folder remain the same. The cursor hovers over the icons, but no selection or opening action occurs yet. [0:03:29 - 0:03:30]: The cursor clicks and selects multiple icons, highlighting them in blue. The selected items remain the same. [0:03:30 - 0:03:31]: The selected items stay highlighted. The screen doesn't change, and no new actions occur. [0:03:31 - 0:03:32]: All selected items stay highlighted. No new action occurs yet. [0:03:32 - 0:03:33]: The view focuses back on the file management window. All selected items remain highlighted without any further interaction. [0:03:33 - 0:03:34]: The file management window still displays the same highlighted items. The cursor hovers over them without interacting. [0:03:34]: There is no change in the display. The file management window still shows the same view with the selected items. [0:03:35 - 0:03:36]: The screen transitions to a detailed instructional PDF document opened from the file management window. The document, titled \"Keychain Box Template,\" shows various images of keychain box templates in pastel colors at the top, followed by a paragraph describing how to access the YouTube tutorial and other details. The layout of the document includes headings, a text section, and visual guides. [0:03:36 - 0:03:37]: The PDF document remains on the screen, displaying further details about the keychain box template. It shows measurements and various parts that constitute the template. [0:03:37 - 0:03:38]: The PDF document continues to display more information about the keychain box template, including additional visual guides and specifications. [0:03:38 - 0:03:39]: The view stays fixed on the PDF document detailing the keychain box template, showing comprehensive instructional graphics and dimensions of the template parts. [0:03:39 - 0:03:40]: The PDF document shows further parts and measurements of the keychain box template. No interaction with the document is detected. [0:03:40 - 0:03:41]: Another section of the PDF document is shown, with more illustrations and size guides for various parts needed for the assembly, orienting predominantly horizontal and some vertical components. [0:03:41 - 0:03:42]: The document displays additional components with detailed guides and segments showing specific parts along with their exact dimensions and orientations, still mainly focused on horizontal layouts. [0:03:42 - 0:03:43]: More details about the keychain box template's sections, showing the necessary dimensions and assembly instructions in detail, complete with illustrations for each specific element. [0:03:43 - 0:03:44]: Displays extensive viewable instructions, including parts and images, dimensions, and notes on the keychain box template making process, all displayed between horizontal and vertical segregations. [0:03:44 - 0:03:45]: Another page of the PDF, focusing on more detailed segments of the template. More extensive guides illustrate the parts involved in crafting the keychain box model. [0:03:45 - 0:03:46]: An extended display section of various segments of the template in the PDF, each with their independent measurements and associated visuals, continuing the keychain box's detailed creation process. [0:03:46 - 0:03:47]: The",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the cursor doing right now?",
        "time_stamp": "0:03:44",
        "answer": "B",
        "options": [
          "A. Clicking the \"Undo\" button.",
          "B. Selecting multiple icons.",
          "C. Opening a subfolder.",
          "D. Deleting a document."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_64_real.mp4"
  },
  {
    "time": "0:05:00 - 0:05:20",
    "captions": "[0:05:00 - 0:05:20] [0:00:00 - 0:00:04]: The video starts by displaying a software interface, specifically the design space of Cricut Design Space on a computer screen. On the canvas, there are drawing outlines of various components of a keychain box; most of these shapes are pink except for one vertical rectangle, which is orange. The left rectangular shape appears to be the main body of the box, with distinct flap areas, and the other shapes are additional parts, including a tag and other sections. The time displayed on the screen indicates 05:49:53. [0:00:05 - 0:00:08]: The elements on the canvas have not changed, and the timestamp now reads 05:49:54. There are corresponding layers shown on the right side of the screen with keychain-box-1a-svg labels for each shape. The same label appears multiple times with the 'Basic Cut' operation described underneath each. [0:00:09 - 0:00:14]: The visual state remains consistent, with no new actions on the canvas. The timestamp continues to advance. The arrow pointing to various layers on the right shows the current selection within the software. [0:00:15 - 0:00:20]: The software interface maintains the same view, with the cursor visible at times, indicating user interaction or selection. The labeled layers on the right correspond to the shapes on the left, with several layers representing different parts of the keychain box design. The visual focus remains clear on the design elements of the keychain box.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the cursor indicate just now?",
        "time_stamp": "0:05:09",
        "answer": "C",
        "options": [
          "A. Selecting shapes on the canvas.",
          "B. Displaying a pop-up menu.",
          "C. Moving to the Layers panel on the right.",
          "D. Highlighting text input fields."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_64_real.mp4"
  },
  {
    "time": "0:06:40 - 0:07:00",
    "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:43]: The video begins with a computer screen displaying the Cricut Design Space application. The main window shows a design with two pink-colored shapes arranged on a grid mat. The larger shape is positioned at the top left, and the smaller one is at the bottom right. Both shapes have dashed lines inside them. To the left of the screen, under the \"Basic Cut\" and \"Score\" tabs, two rectangular icons depicting a cut and a score tool are visible. [0:06:44 - 0:06:48]: The screen remains static, still showing the design with the two pink shapes on the Cricut mat. The shapes are unchanged, and the tool options on the left side remain the same with no new actions occurring. [0:06:49 - 0:06:52]: The design on the screen updates. The positions of the shapes shift, and the design now includes an additional cut-out at the bottom left corner of the smaller pink shape. The icons on the left side remain the same as earlier, with the \"Basic Cut\" and \"Score\" options visible. [0:06:53 - 0:06:55]: The shapes on the mat appear larger, zooming in slightly. The positions of the shapes remain the same with the larger shape at the top and the smaller one at the bottom, still showing the detailed cut-out pattern. [0:06:56 - 0:06:58]: The screen transitions to a new interface in the Cricut Design Space application. The top shows a message: \"To continue, please connect your Maker 3.\" The rest of the screen is predominantly white, awaiting further user interaction. [0:06:59 - 0:07:01]: The screen remains static with the message about connecting the Maker 3. No additional changes are made in the interface, and it waits for the device connection. [0:07:02 - 0:07:04]: The screen remains unchanged, still showing the message to connect the Maker 3, with no further action taking place. [0:07:05 - 0:07:08]: The interface refreshes, showing an option to set the base material. Several colored tabs for different materials such as Popular, Favorites, and Browse All Materials are visible. Below these tabs, a list of material options appears, waiting for selection. [0:07:09 - 0:07:12]: The screen shows the \"All Materials\" tab selected, displaying a list of various materials on the Cricut Design Space. Materials like Chipboard, Corrugated Cardboard, and Foil Poster Board are listed in a scrollable menu. [0:07:13 - 0:07:16]: The view zooms in slightly on the materials list, focusing on items like Poster Board and Art Board. The search bar at the top allows for filtering among the materials listed. [0:07:17 - 0:07:21]: The perspective shifts again, focusing now on the \"Cardstock\" section. Multiple options for cardstock materials with various characteristics such as thickness and type are displayed. [0:07:22 - 0:07:27]: The list under the \"Cardstock\" section is detailed, showing different types like Deluxe Paper, Heavy Cardstock, and Light Cardstock. Each item has a checkbox for selection. [0:07:28 - 0:07:31]: The user scrolls down to find the \"Medium Cardstock - 80 lb (216 gsm)\" option, highlighted with a red arrow. This item is located towards the bottom of the list under the \"Cardstock\" section. [0:07:32 - 0:07:35]: The \"Medium Cardstock - 80 lb (216 gsm)\" option remains highlighted. The user prepares to select it, indicated by the cursor hovering over the checkbox next to it. [0:07:36 - 0:07:39]: The user selects the \"Medium Cardstock - 80 lb (216 gsm)\" option as the desired material. The checkbox next to it is ticked, confirming the selection. [0:07:40 - 0:07:42]: With the material selected, the screen updates to reflect the chosen \"Medium Cardstock - 80 lb (216 gsm)\" as the base material. The \"Set Base Material\" section at the top now includes this selected material. [0:07:43 - 0:07:46]: The bottom of the screen shows options to load tools and material, with icons depicting different tools needed for the selected task. The \"Press Go\" button is visible but not yet activated.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action did the user perform just now?",
        "time_stamp": "00:07:00",
        "answer": "C",
        "options": [
          "A. Selected the \"Score\" tool.",
          "B. Added a new shape to the design.",
          "C. Chose the \"Medium Cardstock - 80 lb (216 gsm)\" as the base material.",
          "D. Connected the Maker 3."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_64_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the hotel in relation to the cyclist right now?",
        "time_stamp": "00:00:19",
        "answer": "A",
        "options": [
          "A. On the right side.",
          "B. On the left side.",
          "C. Behind the cyclist.",
          "D. Directly ahead."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_297_real.mp4"
  },
  {
    "time": "[0:03:32 - 0:03:52]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the yellow vehicle located right now?",
        "time_stamp": "00:03:44",
        "answer": "A",
        "options": [
          "A. On the right side of the road.",
          "B. Directly behind the cyclist.",
          "C. Directly in front of the cyclist.",
          "D. On the left side of the cyclist."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_297_real.mp4"
  },
  {
    "time": "[0:07:04 - 0:07:24]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located on the left side, parallel to the cyclists right now?",
        "time_stamp": "00:07:18",
        "answer": "A",
        "options": [
          "A. A safety railing.",
          "B. A parking lot.",
          "C. A rest area.",
          "D. A grass field."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_297_real.mp4"
  },
  {
    "time": "[0:10:36 - 0:10:56]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located to the right side of the cyclist right now?",
        "time_stamp": "00:10:55",
        "answer": "A",
        "options": [
          "A. An open field.",
          "B. A dense forest.",
          "C. A lake.",
          "D. A mountain range."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_297_real.mp4"
  },
  {
    "time": "[0:14:08 - 0:14:28]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is on the right side of the cyclists right now?",
        "time_stamp": "00:14:07",
        "answer": "A",
        "options": [
          "A. A brick wall.",
          "B. A bush.",
          "C. A field.",
          "D. A dense forest."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_297_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: A circular ornament with the name \"Harlow\" written in white script is presented. The ornament is made of resin with a glittery gold and pink ombre effect. It hangs from a braided gold thread. The background includes wooden elements and a sprig of pink and white flowers on the left side. [0:00:03]: The screen transitions to black. [0:00:04 - 0:00:06]: The text \"Ombre Resin Ornament\" is displayed against a black background. [0:00:07]: The screen transitions to black again. [0:00:08 - 0:00:12]: The text \"Links to the materials I used are in the video description\" is displayed on a light pink background. [0:00:13]: The same text continues but with a change in background to a white hue. The words \"DIY CRAFT TUTORIALS\" appear in the bottom right corner. [0:00:14 - 0:00:15]: A pair of hands holds a grey respirator mask with two white filters on each side, with the brand \"KISCHERS\" visible in the center. [0:00:16 - 0:00:19]: The text \"A Respirator Mask\" appears on the left side while the hands continue to hold and slightly adjust the mask, showing close-up details of its components.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are the hands doing right now?",
        "time_stamp": "00:00:19",
        "answer": "D",
        "options": [
          "A. Painting the ornament.",
          "B. Hanging the ornament on a tree.",
          "C. Mixing resin components.",
          "D. Holding and adjusting a mask."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_54_real.mp4"
  },
  {
    "time": "0:01:40 - 0:02:00",
    "captions": "[0:01:40 - 0:02:00] [0:01:40 - 0:01:43]: A hand holding a transparent plastic cup containing several wooden sticks and a pair of tweezers is the main focus. The background is a plain, light-colored wall. The hand appears to belong to a person wearing a dark-colored sleeve. The text \"Cups, wood sticks and tweezers\" is displayed on the upper left side of the frame for these three seconds. [0:01:44 - 0:01:45]: The scene changes, showing a setup on a flat surface covered in a textured, white material. A hand wearing a black glove is pouring a liquid into a clear cup, which is held by another gloved hand. In the background, there is a beaker filled with a golden liquid and a wooden stick, alongside another small beaker. [0:01:46 - 0:01:47]: The pouring action continues, and a white circular object is placed horizontally on the surface. Another beaker and a small jar lid are visible on the right side. The setup remains consistent with the beaker of golden liquid and wooden stick. [0:01:48 - 0:01:51]: The cup being poured contains resin mixed with pink mica, as indicated by the text that appears in the top left corner reading \"Resin with Pink Mica.\" The pouring continues in a smooth, controlled manner. [0:01:52 - 0:01:57]: As the resin pours into the white circular object, the gloved hand uses a stick to help direct the flow. The golden liquid in the beaker and the arranged setup remain constant in the background. [0:01:58 - 0:01:59]: The remaining resin with pink mica in the cup is almost fully poured into the white object. The scene shows a close-up of the object as the resin spreads inside it, with both gloved hands maintaining their positions.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the gloved hand doing right now?",
        "time_stamp": "0:01:53",
        "answer": "D",
        "options": [
          "A. Shaking a beaker filled with golden liquid.",
          "B. Stirring the resin with a wooden stick.",
          "C. Holding a pair of tweezers.",
          "D. Pouring resin with pink mica into the white circular object."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_54_real.mp4"
  },
  {
    "time": "0:03:20 - 0:03:40",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:25]: A pair of hands, covered in black gloves, is seen holding a circular white ornament with a small hole at the top, positioned centrally over a metallic gold mat. The ornament is half pink and half gold, with a clear boundary line. The right hand holds a wooden stick at an angle, hovering over the golden section of the ornament, while the left hand holds the ornament securely. Two plastic cups, one filled with a pink liquid and the other with a golden liquid, are placed on the mat above the ornament. A white absorbent paper towel covers the surface beneath the mat. The pink liquid appears as droplets below the ornament on the mat. [0:03:25 - 0:03:32]: The right hand steadily moves the wooden stick, distributing the resin over the ornament’s surface. Gradually, the clear boundary starts to blur as the pink and gold resin mix subtly. The left hand remains steady, ensuring the ornament stays centered. The background remains consistent with the utensils and materials in their original positions. [0:03:32 - 0:03:37]: The manipulation continues with the right hand skillfully moving the wooden stick in smooth motions, occasionally dipping it back into the golden liquid in the cup. Several resin droplets accumulate below the ornament. The left hand adjusts the ornament slightly, ensuring an even application. The focus remains on achieving a seamless blend of the two colors on the ornament. [0:03:37 - 0:03:39]: As the blending proceeds, the right hand puts the wooden stick back into the cup containing the golden liquid. The left hand lifts the ornament slightly off the mat to inspect the smoothness of the resin layer. Both hands continue to handle the ornament carefully, striving for a smooth and even finish across the entire surface.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the right hand do just now?",
        "time_stamp": "00:03:29",
        "answer": "D",
        "options": [
          "A. Adjusted the position of the ornament.",
          "B. Lifted the ornament to inspect it.",
          "C. Held the ornament securely.",
          "D. Steadily moved the stick to distribute the resin."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_54_real.mp4"
  },
  {
    "time": "0:05:00 - 0:05:20",
    "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:19]: Two hands wearing black gloves are seen from a first-person perspective, working on a circular object placed on a brown mat. The background is a white, textured surface, possibly a padded table cover. The object appears to be a resin craft, displaying a pink and gold striped pattern. The left hand holds a small, clear plastic cup filled with a glittery substance. In the right hand, a wooden stick is used to spread and manipulate the substance over the surface of the circular object. There are a few small droplets on the brown mat around the main object, indicating potential drips during the crafting process. Nearby, two other cups with different substances are visible, one of which contains a pink liquid with a stick inside. The process of spreading the glittery material continues steadily throughout the sequence, with the left hand adjusting the cup's position and the right hand carefully applying the substance in a consistent manner. The object remains centered and the camera angle unchanged, maintaining a focus on the detailed work being performed.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "How is the glittery substance being applied to the circular object right now?",
        "time_stamp": "00:05:10",
        "answer": "D",
        "options": [
          "A. Using a paintbrush.",
          "B. Pouring it directly from the cup.",
          "C. Applying it with fingers.",
          "D. Spreading it with a wooden stick."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_54_real.mp4"
  },
  {
    "time": "0:06:40 - 0:07:00",
    "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:44]: A hand, dressed in a black sleeve, places a circular gold ornament on a grey, grid-patterned cutting mat. The ornament has the word \"Marlow\" written in white in the center. Another hand, also in a black sleeve, enters the frame and starts to peel off a piece of tape from the ornament. [0:06:45]: Both hands continue to work on peeling off the tape from the ornament. One hand holds the ornament steady, while the other carefully removes the tape. [0:06:46 - 0:06:48]: With the tape now removed, one hand stretches a piece of thin golden string in preparation. The ornament remains centered on the grey, grid-patterned cutting mat. [0:06:49 - 0:06:50]: The string is threaded through a small hole at the top of the ornament. One hand holds the string while the other hand guides it through the hole. [0:06:51]: The string is pulled through the hole, and one hand holds both ends of the string together. [0:06:52 - 0:06:53]: The person begins to tie the two ends of the string into a knot, creating a loop. [0:06:54 - 0:06:57]: The knot is tightened and secured by the person's hands, completing the attachment of the string to the ornament.  [0:06:58 - 0:06:59]: The person gives the string one final tug to ensure it is secure. The ornament is now ready to be hung, with the \"Marlow\" text clearly visible in the center.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "0:06:47",
        "answer": "D",
        "options": [
          "A. Placing the ornament on the mat.",
          "B. Peeling off tape from the ornament.",
          "C. Tying a knot in the string attached to the ornament.",
          "D. Giving the string one final tug to secure it."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_54_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:11]: Two Christmas ornaments are held up close to the camera from a first-person perspective. The one on the left is purple with streaks of darker and lighter shades, and has the name \"Avery\" written in white cursive letters, along with a small holly design beneath the text. The one on the right is teal with similar streaks and has the name \"Jackson,\" also in white cursive letters with a holly design. Both ornaments have silver tops with white ribbons tied into bows. The holder's hands move slightly, and at times, rotate the ornaments, displaying their colors and features from different angles. [0:00:12]: The screen goes black. [0:00:13]: The text \"D ORN\" appears centered on the black screen. [0:00:14 - 0:00:17]: The text \"PAINTED ORNAMENTS,\" centered on the black background, is displayed as the screen remains dark. [0:00:18 - 0:00:19]: The screen transitions to a light pink background with the text \"Templates and Materials used are listed in the description\" centered on the screen.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What was held up close to the camera right now?",
        "time_stamp": "00:00:09",
        "answer": "A",
        "options": [
          "A. Two Christmas ornaments.",
          "B. A pair of painted canvases.",
          "C. A set of colorful ribbons.",
          "D. Two Christmas cards."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_69_real.mp4"
  },
  {
    "time": "0:01:00 - 0:01:20",
    "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:04]: A pair of hands, visible in the lower part of the frame, are adding acrylic paint into a transparent plastic container with a clear lid. The container is positioned in the middle of the frame, situated on a white, textured surface which contrasts with a dark grid-patterned mat beneath. On the top left and middle parts of the white surface lies another transparent container, similar in design but with two white components. Off to the right side, two tubes of acrylic paint, one green and one blue, rest with their caps removed. Another paint tube, white in color, is being held in the left hand, and a green paint tube is held in the right.  [0:01:05]: The right hand brings the tube of green paint closer to the transparent plastic container in the middle; the left hand is slightly open, positioned above the white surface. The left hand is now holding the previously opened white paint tube. The clear plastic containers and paint tubes remain in their prior positions.  [0:01:06 - 0:01:11]: The right hand starts to squeeze and add the acrylic paint from the green tube into the transparent container, which already contains some blue paint. The left hand, still holding the white paint tube, is slightly relaxed and positioned near the edge of the white surface. The green and blue tubes, with their caps off, still rest on the right side of the white surface. Two small, white plastic caps lie near the right edge of the white surface.  [0:01:12 - 0:01:15]: The right hand places the green paint tube on the right side of the white textured surface, close to the blue paint tube. The white paint tube held by the left hand is now placed near the center of the white textured surface. The container with paint in the middle remains unchanged, and the surrounding items, including the two small white caps and the other paint container, are in their original positions.  [0:01:16 - 0:01:19]: The left hand picks up a transparent plastic piece with white components seen previously on the upper part of the white surface. The left hand positions the plastic piece directly over the container holding the blue and green paint, gently lowering it as the right hand stabilizes the container from the side. The right hand continues to hold the container in place while the left hand carefully lowers the additional plastic piece with white components into the paint mixture. The other items on the white surface maintain their positions.  [0:01:20]: The hand slightly swirls the container, causing the paints inside to mix subtly without blending completely. The container and surrounding tubes and caps are in the same positions as before.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the right hand do just now?",
        "time_stamp": "0:01:19",
        "answer": "B",
        "options": [
          "A. Picked up the blue paint tube.",
          "B. Picked up shaked and the transparent plastic sphere.",
          "C. Squeezed the white paint tube.",
          "D. Put down the green paint tube."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_69_real.mp4"
  },
  {
    "time": "0:02:00 - 0:02:20",
    "captions": "[0:02:00 - 0:02:20] [0:02:04 - 0:02:24]: A pair of hands, positioned at the center and oriented towards the camera, firmly grips a spherical object. The sphere, predominantly blue with hints of green and white, is being carefully wiped clean using a white cloth held between the thumb and fingers. The surface appears glossy, reflecting light with multiple high points of reflection. Meanwhile, on the gray grid-patterned work surface beneath the hands are three tubes of paint—one black, one green, and one blue— arranged in a slightly scattered manner on a white paper towel. Close by, part of a clear plastic container, thin and fragile-looking, occupies the top left corner. The right hand keeps holding the sphere while the cloth gently moves over the surface of the sphere’s opposite side. The repeated motion smooths out the painted surface, revealing a glossy finish while the left hand occasionally moves to adjust the hold. A few plastic caps associated with the paint tubes lie scattered and disorganized near the top and right sides of the workspace. The background remains consistently occupied by the grid-patterned surface, accentuating the organized environment of a craft activity.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:02:24",
        "answer": "B",
        "options": [
          "A. Painting the sphere.",
          "B. Wiping the sphere's surface.",
          "C. Mixing paint colors.",
          "D. Discarding the paint tubes."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_69_real.mp4"
  },
  {
    "time": "0:03:00 - 0:03:20",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:01]: A hand, most likely wearing a recording device, is placed over a white paper towel. The surface beneath the towel appears to be a grid-patterned worktable. Several small paint tubes are scattered around, including one with white paint and another with black paint situated nearer than the rest. The pink paint tube is held securely in the left hand, positioned over the transparent bauble. The bauble contains a mix of black and pink paint at the bottom. A small, blue-green globe-like object sits off to the side, contributing to the scene's creative ambiance. [0:03:01 - 0:03:05]: The hand squeezes a few drops of bright pink paint from the tube into the bauble, adding to the existing colors inside the bauble. The hand continues to firmly grasp the tube while angling it back toward the base. This action brings the tube approximately halfway into view. [0:03:05 - 0:03:06]: With the right hand still holding the pink paint tube upright, the left hand returns the tube to its designated spot on the paper towel. At the same time, another, larger purple paint tube, partially capped, lays on the paper towel alongside the white paint tube. [0:03:06 - 0:03:08]: The left hand picks up the white paint tube. This results in a slight camera shift as the focus centers on the white paint tube. Meanwhile, the discarded pink paint tube partially rolls on its side but stays on the towel. [0:03:08 - 0:03:11]: With the white paint tube in hand, a small stream of white paint is released into the bauble. The center is forming a small mound of colors mixing at the bottom. The hands continue to apply redirection and control of the color placement, ensuring white approximates other colors. [0:03:11 - 0:03:13]: After adding the white paint to the bauble, the left hand sets the white paint tube on the towel. This tube is angled, rolling slightly before halting. The left hand then reaches to grasp the bauble's base, holding it steady. [0:03:13 - 0:03:15]: The bauble is lifted, and the components inside are clearly visible through the transparent glass. This perspective offers a direct view inside the bauble, highlighting the contrasting hues of pink, white, and black paint blended while still retaining their unique shapes.  [0:03:15 - 0:03:19]: While holding the position, the camera shifts slightly. Both hands rotate the bauble gently. The white and pink paint swirl together in a mix with the black base, creating a marbled effect. The purple hues become more visible as the hands carefully shift the bauble to ensure a balanced, integrated blend. Careful handling ensures the paint layers remain distinct, avoiding overmixing.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:03:15",
        "answer": "C",
        "options": [
          "A. Add blue-green paint to the bauble.",
          "B. Squeeze more black paint into the bauble.",
          "C. Lift the bauble to show the paint inside.",
          "D. Clean the paint tubes."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_69_real.mp4"
  },
  {
    "time": "0:04:00 - 0:04:20",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:09]: In a first-person perspective, the camera captures a scene where two hands hold a spherical ornament covered in vibrant purple paint with irregular patches of white and black. The ornament rests on a white paper towel, which is slightly stained with purple paint. To the left of the ornament, there are two tubes of paint lying on the paper towel, one purple and one black, both uncapped. In the background, another blue ornament with white patches is visible on a grid-patterned surface. The video shows slight movements of the hands as the ornament is rotated, revealing additional perspectives of the painted surface. Meanwhile, the caption at the bottom reads, \"Depending on your temperature, it might take a little longer to dry.\" [0:04:10]: As the video progresses, the hands continue to rotate the ornament, exposing more of its colorful surface. The camera captures the details of the purple paint with white and black patches distinctly. The blue ornament remains stationary in the background, placed slightly to the right on the grid-patterned surface. [0:04:11 - 0:04:16]: The camera zooms in slightly on the purple ornament held by the hands, offering a closer look at the paint's texture and colors. The tubes of paint remain visible on the paper towel. The blue ornament in the background stays in its position, providing a contrast to the purple ornament in the foreground. [0:04:17 - 0:04:19]: The scene fades to black, and white text appears on the screen that reads, \"Personalizing the Ornaments.\" The text remains steady as the captions [0:04:17], [0:04:18], and [0:04:19] display sequentially with no other visual changes.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are the hands doing right now?",
        "time_stamp": "00:03:53",
        "answer": "B",
        "options": [
          "A. Applying paint to the ornament.",
          "B. Rotating the ornament to reveal different perspectives.",
          "C. Placing the ornament back on the towel.",
          "D. Cleaning the paint tubes."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_69_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The screen is completely black, with the timestamp \"00:00\" in white text in the top left corner; [0:00:01]: An abstract, colorful background with a mix of purple, pink, and black hues appears, along with horizontal glitch lines; [0:00:02 - 0:00:07]: A logo with the words \"GRAVITY THROTTLE RACING\" is displayed in white letters against a yellow, red, and blue triangular background; [0:00:08]: The scene transitions to a vibrant diorama of a Texaco gas station with several miniature cars, including a prominent red race car and a pink car labeled \"Charms.\" There are mountains in the background; [0:00:09 - 0:00:16]: Several miniature figurines are positioned around the gas station and cars. The scene shows various activities, including the cars getting refueled or serviced. The bright, sunny setting contrasts with the bright green landscape, and a large Texaco sign lists gas prices; [0:00:17]: The camera angle shifts slightly to reveal the full breadth of activity, with more emphasis on the cars and figurines interacting within the scene; [0:00:18 - 0:00:19]: The view transitions again to a field with green hills. There are miniature dinosaurs and figurines, also showing several cars and a helicopter within the scene.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What significant change happened just now?",
        "time_stamp": "0:00:19",
        "answer": "A",
        "options": [
          "A. The scene transitions to a field with green hills.",
          "B. The logo with \"GRAVITY THROTTLE RACING\" reappears.",
          "C. The abstract background becomes darker.",
          "D. The Texaco gas station disappears."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_493_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:01]: The video showcases a winding mountain road with a steep drop-off on one side, surrounded by snow-covered terrain and evergreen trees. A blue car is seen driving around a sharply curved incline. [0:04:01 - 0:04:03]: The camera follows the car's progress, showing it navigating the curves of the road. The background features rocky outcroppings and additional roadways traversing the mountainous region. [0:04:03 - 0:04:04]: As the car continues along the road, the landscape transitions to drier, more desert-like conditions. Various vehicles traverse different routes, with a black car dominating the scene. [0:04:04 - 0:04:06]: The black car is seen exiting a tunnel and continuing its path through a winding, elevated road. The road's design integrates with the rocky, arid terrain. [0:04:06 - 0:04:08]: An overhead view shows multiple cars, red and black, navigating the winding roadway. The roadways cross over one another, highlighting the complex, multi-level design. [0:04:08 - 0:04:10]: The camera angle shifts to emphasize a primary black car on the road while other cars maintain distance behind. The road weaves through diverse topographies, including rocky and desert-like environments. [0:04:10 - 0:04:11]: The viewpoint transitions to an overview of an airport, featuring hangars, a small plane, and a few vehicles parked or moving near the structures. A rocky outcrop stands prominently in the foreground. [0:04:11 - 0:04:13]: Vehicles, including two yellow cars, maneuver around a rocky structure near the airport. The road swerves and curves tightly around the terrain. [0:04:13 - 0:04:15]: The video focuses on a black car driving along a smoother section of the winding road. The background consists of barren, rocky hills and elevated tracks. [0:04:15 - 0:04:17]: The black car continues its route, passing under a sign and converging with other paths. Road surface variations are evident, adding texture to the visual experience. [0:04:17 - 0:04:19]: The video depicts a speed checkpoint displaying race information. The black car approaches and eventually passes the display, revealing time and tracking details. The surrounding environment consists of sparse vegetation and sandy terrain.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Which car won first place in this competition?",
        "time_stamp": "00:04:25",
        "answer": "D",
        "options": [
          "A. The red car.",
          "B. The orange car.",
          "C. The yellow car.",
          "D. The black car."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_493_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:02]: A few colored toy cars are racing on a beige terrain with tire tracks, resembling a snowy landscape with scattered toy trees. The camera follows the cars from a first-person perspective as they navigate the track.  [0:08:03 - 0:08:05]: The cars continue driving up a curved hill with more toy trees placed sporadically along the track. The terrain elevation changes as the cars move along the winding road. [0:08:06 - 0:08:07]: The cars make a sharp turn around a steep curve. The landscape appears rocky with elevated sections, and the toy trees are denser along the edge of the track. [0:08:08 - 0:08:09]: The track descends slightly as the cars race down the bending path, surrounded by a rocky and steep landscape. The toy trees continue to line the path, and some toy greenery is seen around the curves. [0:08:10]: One of the cars enters a small tunnel at the base of a rocky hill, while the background shows more of the rocky model terrain and a small bridge overhead. [0:08:11 - 0:08:13]: The scene transitions to an adjacent part of the track, where a model train passes behind a rocky backdrop, and toy vehicles are visible on another part of the track. The landscape resembles a desert area. [0:08:14 - 0:08:15]: Two toy cars continue to race alongside a model train on a parallel track. The cars pass a billboard, and the desert-themed landscape has model trees and shrubs. [0:08:16 - 0:08:18]: One car overtakes another car as they continue on a winding, slightly elevated road, with the desert backdrop featuring rail tracks and a colorful painted cliffside. [0:08:19]: The scene seamlessly transitions to a different setup, where different toy cars are positioned on separate tracks, ready for another race. The background showcases the starting line and racing details. [0:08:20]: The scene features a group of parked toy cars in different colors on a snowy terrain. The track is clearly defined with tire marks, and the terrain retains its white snowy appearance.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "How is the terrain described where the toy cars are racing at the beginning?",
        "time_stamp": "00:08:02",
        "answer": "A",
        "options": [
          "A. Snowy with scattered toy trees.",
          "B. Rocky and steep.",
          "C. Desert-themed with rail tracks.",
          "D. Grassy with ponds."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_493_real.mp4"
  },
  {
    "time": "[0:12:00 - 0:13:00]",
    "captions": "[0:12:00 - 0:12:20] [0:12:00 - 0:12:05]: The video begins with a view of a paved road curving to the right in a desert-like landscape. A red car leads the scene, followed by a black car, both driving along the road. Rail tracks run parallel to the road on the right side. In the distance, a tunnel is visible in the mountains, with two trains approaching from the left. The background features a clear, blue sky with white clouds and rocky mountain structures. [0:12:06 - 0:12:07]: The scene transitions to a close-up of a section of the road with a concrete sidewalk next to it, resembling a miniature setup. Two tripods are visible standing on the ground. The camera captures the white car's movement to the left. [0:12:08]: A further zoomed-in view shows a white car continuing its way across the miniature setup, with the backdrop of a grey wall. [0:12:09 - 0:12:12]: The video captures a list of racing teams and their ranking points. The prominent teams are “Larro,” “Jay Bo,” “Pacific Pirate,” and “Sisters of the Heavy.” The number of points for each round (R1-R7) along with totals are displayed. Larro leads with 23 points, followed by Jay Bo with 17, Pacific Pirate with 15, and Sisters of the Heavy scoring significantly lower. Different colors and fonts highlight each team’s name. [0:12:13 - 0:12:20]: The final frames show five toy cars lined up on a white, snowy track, ready for the race. The names \"Jay Bo,\" \"Larro,\" \"Sisters of the H.M.,\" and \"Pacific Pirate\" are visible, indicating the competitors.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which team is leading in the racing points?",
        "time_stamp": "00:12:14",
        "answer": "D",
        "options": [
          "A. Jay Bo.",
          "B. Pacific Pirate.",
          "C. Sisters of the Heavy.",
          "D. Larro."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_493_real.mp4"
  },
  {
    "time": "[0:15:00 - 0:15:43]",
    "captions": "[0:15:40 - 0:15:43] [0:15:40 - 0:15:43]: The video centers on a colorful logo that occupies the majority of the screen. The logo is shaped like an elongated, pointed oval, with a combination of yellow, blue, and red colors. The yellow background is prominent, with a blue triangular shape extending from the left into the center, overlaid by a smaller red triangle. The text \"GRAVITY THROTTLE RACING\" is written across the logo in three lines, with each word stacked vertically. The text is bold and white, standing out clearly against the colored background. The background of the entire frame is dark, possibly indicating a low-light environment or outer space theme, and faint horizontal lines are visible, creating a dynamic backdrop.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Which car won first place in this competition?",
        "time_stamp": "00:15:15",
        "answer": "D",
        "options": [
          "A. The red and bluecar.",
          "B. The black car.",
          "C. The yellow car.",
          "D. The brown and blue car."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_493_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video starts with a person sitting at a desk with a graphics card in front of them. The individual has a calm expression, wearing a grey t-shirt. The background is dimly lit with some modern furniture visible, including a dark cabinet and a monitor to the right. The desk is light-colored, contrasting with the darker background. The person gestures with their hands while talking, and a large \"$300\" appears on screen in bright yellow text, emphasizing the cost of the item being discussed. [0:00:04 - 0:00:06]: The individual continues discussing the graphics card, moving their hands expressively. The graphics card itself, mostly white with three large cooling fans, is positioned on the desk. At one point, the person holds up the graphics card to showcase it to the camera. [0:00:07 - 0:00:08]: The video transitions to a top-down view of the graphics card labeled \"GEFORCE RTX\". The card lies flat on the desk, with the person's hands on either side, providing a clear view of its design and branding. [0:00:09 - 0:00:12]: The person reaches to pick up another graphics card from the desk, holding two in hand for comparison. The second graphics card is dark in color with a different fan design. They place both cards next to each other on the desk for a side-by-side comparison, continuing to explain the differences. [0:00:13 - 0:00:17]: The focus shifts to a close-up of the darker graphics card being taken out of its packaging. The hands are seen carefully removing it from a black foam tray, with the branding \"INSPIRED BY GAMERS. BUILT BY NVIDIA.\" clearly visible on the packaging. The person examines the card, turning it around to show various angles and details. [0:00:18 - 0:00:20]: The final part of the video shows a web page featuring listings of different graphics cards for sale, including their prices and conditions. The screen displays a variety of GPUs with prices ranging from $224 to $300 and more, illustrating the options available for purchase. The user interface includes search filters, images of the products, and detailed information about each listing.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What text is prominently shown on the packaging of the darker graphics card right now?",
        "time_stamp": "00:00:15",
        "answer": "C",
        "options": [
          "A. DESIGNED BY GAMERS, BUILT BY NVIDIA.",
          "B. CREATED BY GAMERS, DEVELOPED BY NVIDIA.",
          "C. INSPIRED BY GAMERS. BUILT BY NVIDIA.",
          "D. ENGINEERED BY GAMERS, MADE BY NVIDIA."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_122_real.mp4"
  },
  {
    "time": "[0:01:20 - 0:01:40]",
    "captions": "[0:01:20 - 0:01:40] [0:01:20 - 0:01:24]: The video begins with a comparison chart showcasing average frames per second (FPS) for different titles under the label \"NVIDIA 3070 vs. 4060\" at \"1440p high/max.\" The chart shows various FPS values represented by green bars, with titles listed at the bottom, including \"DOOM Eternal,\" \"F1 22,\" \"Shadow of the Tomb Raider,\" \"Horizon Zero Dawn,\" \"Forza Horizon 5,\" \"COD: MW II,\" \"RDR2,\" \"God of War,\" \"Control,\" \"Dying Light 2,\" \"Hogwarts Legacy,\" and \"Cyberpunk 2077.\" The background is dark with a subtle gradient and some graphical detailing. [0:01:25 - 0:01:36]: As the video progresses, the chart remains on the screen, showing FPS values for the same titles mentioned earlier. The positions and values of the green bars do not change, indicating a static comparison without dynamic updates. The background continues to be dark with some stylistic grain or noise. [0:01:37 - 0:01:40]: From timestamp 0:01:37, the chart gradually fades out, and the scene transitions to a close-up view of a person seated at a desk. The individual is speaking and gesturing with their right hand. Two graphics card models are prominently displayed on the desk in front of them. The one on the left has a black design with dual fans, while the one on the right is white with triple fans. The environment is well-lit, modern, and clean, with background elements including a dark wall, a light source, and parts of the room.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What titles are not listed on the comparison chart shown right now?",
        "time_stamp": "00:01:36",
        "answer": "A",
        "options": [
          "A. GTA V.",
          "B. Shadow of the Tomb Raider.",
          "C. Forza Horizon 5.",
          "D. God of War."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_122_real.mp4"
  },
  {
    "time": "[0:02:40 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:43]: The video begins with a close-up shot of a graphics card on a white table. The card is black with red accents, and the brand name \"RADEON\" is visible. The lighting is soft with a dark background. Following this, a person holds a different graphics card, displaying both the front and the back. The card is black with dual fans, and the person's hands are prominently featured in the frame against a red and black background. [0:02:44 - 0:02:49]: The scene shifts to a medium shot of a person seated at a table, holding and pointing to the graphics card with dual fans. The person appears to be explaining something about the card. The room is well-lit with a modern, dark-themed decor. Another graphics card lies on the table beside the person, indicating a comparison might be taking place. The person then reaches toward the side to pick up another graphics card and brings it into the frame. [0:02:50 - 0:02:55]: The video transitions to a graphical display comparing the average frames per second (fps) of different games between the AMD Radeon card and another card. The graph shows data points for several games such as DOOM Eternal, Shadow of the Tomb Raider, Horizon Zero Dawn, and others. The data is displayed with varying fps numbers, with the AMD card showing higher performance. [0:02:56 - 0:02:59]: The performance comparison continues with the graph displaying detailed statistics that AMD Radeon’s fps is 16% faster than the average of another card. The list of games and their corresponding fps remain constant, showing the performance of the AMD card in various high-resolution scenarios. The background remains dark with a slight texture, highlighting the data points.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the brand name of the graphics card shown right now?",
        "time_stamp": "00:03:00",
        "answer": "C",
        "options": [
          "A. NVIDIA.",
          "B. GE FORCE.",
          "C. AMD.",
          "D. RADEON."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_122_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:04:20]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:11]: A man in a grey t-shirt is seated at a table with two large graphics cards placed in front of him. He is explaining something, using hand gestures to emphasize his points. The background is dimly lit with modern, minimalistic room decor, including a standing light to the left and a glimpse of a computer setup on the right. The left graphics card is black with dual fans, while the right graphics card has a distinct design with three fans and angular edges. The man points to the graphics cards, moving his hands in a descriptive manner. [0:04:12 - 0:04:13]: The view shifts to a close-up overhead shot of the two graphics cards on the table. The left card has a sleek design with two large fans, whereas the right card features three fans with a geometric, aggressive design. The man's hands briefly appear in the lower part of the frame, possibly indicating features on the cards. [0:04:14 - 0:04:19]: The video returns to the man as he continues to explain, periodically using hand gestures. He is now pressing his fist into his open palm and occasionally pointing towards one of the graphics cards on the table. His expressions are engaged, suggesting he is explaining something in detail about the graphical hardware in front of him.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the design feature of the graphics card on the right right now?",
        "time_stamp": "00:04:11",
        "answer": "B",
        "options": [
          "A. It has dual fans with a sleek design.",
          "B. It has three fans with a geometric, aggressive design.",
          "C. It features a single large fan in the center.",
          "D. It is completely fanless with a passive cooling system."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_122_real.mp4"
  },
  {
    "time": "[0:04:40 - 0:04:47]",
    "captions": "[0:04:40 - 0:04:47] [0:04:40 - 0:04:47]: The video features an individual in a well-lit room, sitting at a desk with two graphics cards displayed in front of them. The individual gestures with their hands while speaking, illustrating points about the graphics cards. The graphics card on the left is black with dual large fans, while the one on the right is white with triple fans. The person wears a grey shirt, and the background is a combination of dark and light areas, with some vertical lines and dark-colored equipment or furniture.",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many fans does the graphics card on the right have right now?",
        "time_stamp": "00:04:46",
        "answer": "C",
        "options": [
          "A. One.",
          "B. Two.",
          "C. Three.",
          "D. Four."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_122_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video begins with a close-up view of a painting that depicts a stylized, abstract figure in black with yellow color accents on what appear to be gloves. The background of the painting is a mix of white, gray, and patches of red and yellow. The painting is framed in black and hung on a white wall. [0:00:03 - 0:00:05]: The camera pans slightly to the right, revealing another painting next to the first one. This second painting also depicts an abstract figure, but this time the figure is in pale white, outlined with dark lines and featuring stripes on its neck and limbs. The figure is set against a dark background with vertical stripes and geometric shapes. [0:00:05 - 0:00:13]: As the camera continues to pan right, it captures more of the second painting, showing the entire figure, which is abstract and elongated. Another painting becomes visible to the right, showcasing two figures with red hair and pale bodies against a dark background. These figures are also abstract, with one featuring a spiral on the left arm. [0:00:13 - 0:00:16]: The video focuses on the third painting depicting two red-haired, pale figures. Both figures are stylized and abstract, with red and pink hues accentuating their bodies. The painting has a dark background and is framed in black. [0:00:16 - 0:00:19]: The camera moves further right, showing another abstract painting. This work features two black figures in a domestic scene set against a gray background. One figure is reclining on a couch or bed, while the other stands nearby. There's also a stark contrast with bright colors, including a pink section at the bottom of the painting. People are visible in the background as the video concludes.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the position of the third painting in relation to the second painting?",
        "time_stamp": "0:00:13",
        "answer": "B",
        "options": [
          "A. To the left.",
          "B. To the right.",
          "C. Above.",
          "D. Below."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What distinguishes the figures in the third painting?",
        "time_stamp": "0:00:16",
        "answer": "B",
        "options": [
          "A. They have blue hair and dark bodies.",
          "B. They have orange hair and pale bodies.",
          "C. They have green hair and yellow bodies.",
          "D. They have black hair and white bodies."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_471_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: The video begins showing an art display on a white wall, featuring various framed artworks. Prominently, there is a large square piece in the center with a purple abstract design. Surrounding it are smaller rectangular, square, and circular frames with different abstract and nature-themed artwork. To the right, there is an artwork depicting a child riding a turtle. [0:08:01 - 0:08:03]: The camera moves slightly to the right, providing a slightly better view of the painting of the child on the turtle. More people can be seen in the background, suggesting the setting is an art gallery. [0:08:03 - 0:08:04]: The camera pans down, giving a clearer focus on the lower part of the display, which includes more abstract art in both circular and rectangular frames. Flowers and greenery designs become visible on the left side. [0:08:04 - 0:08:05]: The view shifts to focus more on the four square pieces on the left side of the display, each showing colorful abstract depictions of flowers and plants. The artworks are vivid with rich greens, reds, and purples. [0:08:05 - 0:08:06]: The camera remains on the four square pieces, but moves slightly downward, concentrating on them more closely. The top two pieces have a prominent greenish-yellow hue, while the bottom two are primarily red and green. [0:08:06 - 0:08:08]: The camera continues moving slightly downward and to the right, maintaining focus on the four square floral artworks. The details of the artworks, such as individual brush strokes and textures, become more visible. [0:08:08 - 0:08:10]: The camera angle shifts rightwards to include additional smaller abstract pieces on a wooden background. These pieces feature white, cloud-like forms on wooden canvases ranging in size and shape. [0:08:10 - 0:08:12]: The camera pans right to reveal more of the wooden panel designs, each featuring minimalist white shapes. The background display’s arrangement seems deliberate, emphasizing the simplicity of the white forms against the natural wood. [0:08:12 - 0:08:14]: The view centers on an ensemble of six wooden panel pieces, highlighting their abstract and minimalistic nature. Each piece includes a solitary white shape, varying from oval to spherical. [0:08:14 - 0:08:16]: The camera continues to focus on the six wooden artworks, where the white shapes appear slightly raised, casting subtle shadows on the wood surface. The arrangement adds depth and texture to the display. [0:08:16 - 0:08:19]: The camera zooms in on the largest wooden panel piece at the top left of the display. This rectangular piece features a white, spherical shape subtly raised from the surface. The grainy texture of the wood and the delicate pattern surrounding the white shape become evident as the video concludes.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What characteristic is shared by the white shapes on the wooden panel pieces?",
        "time_stamp": "0:08:16",
        "answer": "B",
        "options": [
          "A. They are all triangular.",
          "B. They are all raised and cast shadows.",
          "C. They all have a blue background.",
          "D. They all include floral designs."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_471_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:10:43]",
    "captions": "[0:10:40 - 0:10:43] [0:10:40 - 0:10:42]: The video showcases an art exhibition. There are four pieces of artwork mounted on the white wall. The two on the left involve colorful strings creating patterns over a background of intricate black and white lace-like textures. These are rectangular and placed one above the other. To the right, there are two square black canvases. Each features a line of colorful spherical smiley-face objects arranged in rows. Some faces are arranged to form a pattern, while others appear more randomly placed. The right side of the frame shows part of another artwork featuring a painting with green foliage and white flowers on a blue background. In the background, visible parts of the gallery include artworks hanging on the walls and some visitors in the space. The area is well-lit, and the artworks are displayed neatly. [0:10:42 - 0:10:43]: The camera angle remains fairly consistent while slightly adjusting to provide a better view of the four artworks. The details in the background become clearer, showcasing additional paintings on the wall with varying colors and frames. The visitor in white pants and a darker top is partially visible to the left, suggesting an open gallery space. The lighting continues to highlight the artworks effectively.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Compared to the colorful string patterns, where are the black canvases with smiley decorations on the top of these two squares?",
        "time_stamp": "00:10:43",
        "answer": "C",
        "options": [
          "A. Directly above them.",
          "B. To the left of them.",
          "C. To the right of them.",
          "D. Below them."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Causal Reasoning",
        "question": "What can be inferred about the layout of the gallery space based on the video content?",
        "time_stamp": "00:10:43",
        "answer": "C",
        "options": [
          "A. The gallery is very crowded with limited space.",
          "B. The artworks are displayed haphazardly.",
          "C. The gallery is open with artworks neatly displayed and visible visitors.",
          "D. The lighting is dim and focused on certain artworks."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_471_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:00:10",
        "answer": "D",
        "options": [
          "A. The individual makes an espresso shot, adds water to it, and prepares it for serving.",
          "B. The individual heats water, measures coffee grounds, and prepares coffee in a French press.",
          "C. The individual collects various ingredients, chops them, and prepares a fresh smoothie.",
          "D. The individual picks up a milk jug, pours milk into a metal pitcher."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_372_real.mp4"
  },
  {
    "time": "[0:01:19 - 0:01:29]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:01:29",
        "answer": "D",
        "options": [
          "A. The individual washed the espresso machine parts and prepared fresh coffee grounds.",
          "B. The individual brewed tea leaves and poured tea into a cup.",
          "C. The individual mixed ingredients in a blender to prepare a smoothie.",
          "D. The individual steamed milk using an espresso machine's steam wand and wiped the steam wand clean."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_372_real.mp4"
  },
  {
    "time": "[0:02:38 - 0:02:48]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:02:45",
        "answer": "D",
        "options": [
          "A. The individual prepares espresso shots while organizing coffee cups and saucers.",
          "B. The individual brews tea, rinses teapots, and places them on a drying rack.",
          "C. The individual purchases new coffee supplies and stores them in the kitchen cabinets.",
          "D. The individual cleans several milk frothing pitchers, rinsing them under running water."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_372_real.mp4"
  },
  {
    "time": "[0:03:57 - 0:04:07]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:04:50",
        "answer": "D",
        "options": [
          "A. The individual arranges milk jugs and rinses them under running water, then cleans the countertop.",
          "B. The individual organizes kitchen utensils, prepares ingredients, and washes dishes in the sink.",
          "C. The individual identifies dirty items in the workspace, organizes them, and prepares for the next task by collecting utensils.",
          "D. The individual adjusted coffee machine settings, added coffee to cups while checking printed receipts."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_372_real.mp4"
  },
  {
    "time": "[0:05:16 - 0:05:26]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:05:26",
        "answer": "D",
        "options": [
          "A. The individual collects various bakery items, assembles them on a tray, and prepares to serve them to customers.",
          "B. The individual picks up a milk jug, prepares milk for steaming, and pours it into a cup for serving.",
          "C. The individual clears dirty dishes, washes them, and arranges them on a drying rack.",
          "D. The individual operates the espresso machine, prepares coffee, and serves the freshly brewed coffee to a customer."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_372_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a close-up shot of a wooden bowl filled with various fruits, including a pomegranate split open to reveal its seeds, green and purple grapes, and figs. The fruits are positioned neatly in the bowl, which is set against a backdrop of green leaves. The lighting is soft and natural. [0:00:02 - 0:00:06]: As the camera angle shifts downward, the hem of a red garment is visible on a wooden floor. The focus then changes to bare feet touching the floor, eventually revealing more of the red garment and another white piece of fabric nearby. [0:00:07 - 0:00:10]: The scene transitions to show a woman with curled hair, wearing a white dress, in the process of getting her hair styled by another person. The background is plain, with a focus on the woman’s face and the person styling her hair. [0:00:11 - 0:00:12]: The camera then pans out to display a wider view of the room. The woman is seated on a wooden chair, still getting her hair done. The room is decorated in a traditional style with patterned wallpaper and sconces holding lit candles. A small table with fruits and candles is visible in the background. [0:00:13 - 0:00:15]: There is a close-up shot of the woman’s face again, showing her smiling gently as her hair is being styled.  [0:00:16 - 0:00:19]: The camera view expands to show another person, dressed in a traditional outfit with a white cap and red dress, adjusting the seated woman’s gown and apron in the same traditionally decorated room. The interaction is nurturing and gentle.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the woman in the white dress doing when the scene transitions?",
        "time_stamp": "0:00:10",
        "answer": "C",
        "options": [
          "A. Dancing.",
          "B. Styling another person's hair.",
          "C. Getting her hair styled by another person.",
          "D. Eating."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_151_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:40 - 0:02:00] [0:01:40 - 0:01:46]: In a cozy, vintage-style room lit by several wall-mounted sconces, two women are engaged in the process of dressing. The woman on the left, wearing a white undershirt, stands facing the other woman, who appears to be helping her with a black outer garment. The woman on the left raises her arms while the woman on the right, dressed in a red and white outfit with a cap, secures the garment around her. The surroundings are modestly decorated with patterned wallpaper and a wooden bench. [0:01:47 - 0:01:54]: The dressing process continues as the woman on the right diligently adjusts the black dress over the white undershirt of the woman on the left. She lifts and arranges the garment, ensuring it fits snugly. The black dress has intricate patterns or embroidery visible on its surface. Throughout these actions, they remain in the same positions within the room. [0:01:55 - 0:01:56]: The focus shifts more closely as the woman in red works on fastening the dress, securing various adjustments to ensure a proper fit. The expressions of both women remain calm and concentrated on the task at hand. [0:01:57 - 0:01:59]: A close-up perspective captures the meticulous actions of fastening the black lace on the garment. The hands of the woman in red carefully work on the lacing, showing attention to detail. The other woman's contented expression is visible, indicating the completion of the dressing process.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the undershirt worn by the woman on the left?",
        "time_stamp": "00:01:55",
        "answer": "B",
        "options": [
          "A. Black.",
          "B. White.",
          "C. Red.",
          "D. Blue."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Event Understanding",
        "question": "What activity are the two women primarily engaged in?",
        "time_stamp": "00:02:00",
        "answer": "C",
        "options": [
          "A. Cooking.",
          "B. Cleaning.",
          "C. Dressing.",
          "D. Sewing."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_151_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:26]: A woman is standing still as another woman, dressed in period attire, assists her in adjusting a long, dark, flowing garment. The first woman has curled hair and is wearing a dark dress with golden accents, while the assistant, who has curly hair tied back, wears a red dress with a white apron and white headpiece. They are in a room with cream-colored walls, decorated with wall sconces and floral patterns near the ceiling and at chair rail height. Furniture like chairs and a dresser is present in the background. [0:03:27 - 0:03:36]: Close-up views focus on the flowing dark fabric of the dress, emphasizing its texture and movement as it is being adjusted. Another person’s arm, dressed in a light blue sleeve, occasionally comes into view, pulling and aligning the dress. [0:03:37 - 0:03:39]: Finally, a wide view shows the first woman standing poised while the assistant completes final adjustments on the front of the dress, maintaining her focused expression. The room's furnishings and decor remain consistent in the background.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the assistant's dress?",
        "time_stamp": "0:03:26",
        "answer": "B",
        "options": [
          "A. Blue and red.",
          "B. Red and white.",
          "C. Green and red.",
          "D. Purple and white."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_151_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:02]: The video opens with a close-up shot of a man with medium-length, curly, white hair. He has a full beard and is wearing a black top with a white neckline. The background is plain black. He initially has a neutral expression before gradually beginning to smile. [0:05:03 - 0:05:14]: The focus shifts to another person wearing a dark, possibly black robe with long, yellow-gold sleeves. Their hands are clasped together in front of their lap, with their fingers interlocked. The background here is simple and undecorated. [0:05:15 - 0:05:19]: The camera captures the upper part of a person's body, focusing on an ornately designed neckline of their dress. The dress is black with intricate gold stitching around the neckline. The person has curly hair that is partly visible. The background shows a window, through which blurred greenery can be seen.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the man with medium-length, curly, white hair wearing?",
        "time_stamp": "00:05:19",
        "answer": "B",
        "options": [
          "A. A white top with a black neckline.",
          "B. A black top with a white neckline.",
          "C. A dark robe with yellow-gold sleeves.",
          "D. A dress with intricate gold stitching."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_151_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:07]: In a cityscape featuring a bustling urban atmosphere, the video begins with a view of a cobblestone street. Tall, multi-story buildings in various shades of gray and beige line both sides of the street, reflecting European architectural styles with large windows and ornate balconies. The left side of the frame shows two people, one wearing a blue shirt and black shorts, and the other in a black top and yellow shorts, walking along the sidewalk. Street lamps and tram tracks are prominent features along the road. Various pedestrians are walking, engaging in casual activities, and some are heading towards the backdrop, where more buildings can be seen. [0:00:08 - 0:00:14]: As the footage continues, the camera captures more pedestrians crossing or navigating along the street. A street pole with graffiti appears in the middle of the frame, adding an urbane touch. The crosswalk is clearly visible with its distinctive white stripes against the dark cobblestone. On the right side of the frame, a woman in a patterned top and jeans, wearing a mask, approaches while carrying a blue bag. The background showcases storefronts with vibrant window displays and a moderate amount of foot traffic. [0:00:15 - 0:00:20]: The scene shifts focus slightly, panning from left to right. A crosswalk and a street corner with modern streetlights and signage come into view. Pedestrians continue to populate the area, moving in different directions. More historical buildings with large windows, some with balconies, occupy the background. Trees add a touch of greenery, and a kiosk stands at the corner. The final frames reveal a more expansive view of the square, with a statue and additional buildings in the distance, including one under construction. Three individuals walk close to the camera, adding a dynamic element to the concluding moments.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color shorts is the person wearing who is walking with someone in a blue shirt?",
        "time_stamp": "0:00:07",
        "answer": "B",
        "options": [
          "A. Black.",
          "B. Yellow.",
          "C. Red.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_331_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:01]: At the start, the video showcases a beautifully paved, expansive square with a central statue surrounded by buildings with classic European architecture. The sky above is bright and clear. [0:02:02 - 0:02:04]: As the video progresses, the camera angle shifts slightly to the right. A green bench with several people seated on it becomes more visible. Additionally, cars and buses are parked along the street to the left of the square. [0:02:05 - 0:02:08]: The camera continues to pan slightly to the right, enhancing the central positioning of the green bench. The bench has three people seated on it, engaging in what appears to be casual conversation. The street to the left is active with more vehicles and a visible pedestrian crossing. [0:02:09 - 0:02:12]: The orientation of the view is now almost directly centered on the square. The buildings on the left and right edges of the square frame the scene, with pedestrians walking towards the central area. The architecture of the buildings is ornate, featuring tall windows and elegant facades. [0:02:13 - 0:02:15]: As the panning continues, the camera slightly adjusts to include more details of the buildings on the left, with trees lining the street. The bus and car traffic remains visible, indicating an active urban setting. [0:02:16 - 0:02:18]: The focus shifts further to the right, revealing more of the right-side buildings. A cyclist rides through the square, heading towards the central area. The fountain and statue are still visible in the lower right corner. [0:02:19 - 0:02:20]: Finally, the camera settles to provide a comprehensive view of the square's expanse. Both sides of the square are framed by grand architectural buildings, and the central pathway is dotted with pedestrians, a cyclist, and arranged benches. The atmosphere is lively and bustling.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the cyclist heading as they ride through the square?",
        "time_stamp": "00:02:18",
        "answer": "C",
        "options": [
          "A. Towards the pedestrian crossing.",
          "B. Away from the central area.",
          "C. Towards the central area.",
          "D. Along the street to the left."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_331_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:05]: The video opens with a wide view of a large plaza paved with cobblestones. In the center background stands a grand, historical building with a tall clock tower topped with a green spire. The sky is clear and blue. The plaza is lined with neatly arranged trees, and some scaffolding can be seen on the left. A white van is parked on the right side of the plaza. The perspective is from a central point facing directly towards the building, and there are a few scattered pedestrians. [0:04:06 - 0:04:17]: As the seconds pass, the perspective remains focused on the grand building, but the camera slowly moves forward. Trees line both sides of the pathway leading up to the building. More details of the plaza become visible, including a circular brown area near the center and additional people walking around. The scaffolding on the left appears to be related to ongoing construction or an event setup. The white van is still parked, but there is slight movement towards the building in the background. Pedestrians are seen walking in both directions, and their positions change slightly as they move. [0:04:18 - 0:04:20]: Near the end of the video, more people are visible walking closer to the camera. Two people are walking on the left side of the path, and another person walks on the right. The historic building becomes more prominent, and details like the statues and carvings on its façade are clearer. The urban setting's activity is subtly dynamic with people strolling and some vehicles visible in peripheral areas. The weather remains clear and the video maintains a bright, sunny atmosphere.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is parked on the right side of the plaza?",
        "time_stamp": "00:04:05",
        "answer": "C",
        "options": [
          "A. A red bicycle.",
          "B. A blue car.",
          "C. A white van.",
          "D. A black motorcycle."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the scaffolding located in the plaza?",
        "time_stamp": "00:04:10",
        "answer": "C",
        "options": [
          "A. On the right side.",
          "B. Near the center.",
          "C. On the left side.",
          "D. In front of the clock tower."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_331_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:06]: The video showcases a street scene near a large ornate building with a classical architectural style. The building is on the right side of the visuals and features tall windows and intricate stonework. In the center of the frame, there are several large trees with lush green foliage providing shade over an adjacent parking and pedestrian area. Several vehicles are visible, including a white van that starts maneuvering from a stationary position, and a police car parked on the right side. Pedestrians are casually walking or standing near the white van, and other parked vehicles are visible under the trees.  [0:06:07 - 0:06:09]: The white van continues driving from left to right, reaching the pedestrian crossing area marked with white stripes on the road. Another dark green vehicle follows from the left. The pedestrians near the white van begin walking away from the scene. [0:06:10 - 0:06:12]: The white van has now left the scene while other vehicles continue moving in the central area. The dark green vehicle proceeds further from the left part of the frame towards the right along the crosswalk. Another silver car appears and is driving across the crossing. [0:06:13 - 0:06:16]: The silver car continues to move to the right, while additional pedestrians walk near the ornate building on the right side. Left of the screen, a van marked 'ATM' comes into view, moving down towards the crosswalk. [0:06:17 - 0:06:19]: The 'ATM' van continues its path towards the right across the pedestrian crossing. More people are seen walking on the sidewalk adjacent to the building steps. The scene is picturesque with well-maintained surroundings and a clear blue sky visible above all structures and trees. [0:06:20]: The scene encapsulates the final movement of the 'ATM' van as it veers slightly, with pedestrians continuing their activities around the picturesque location.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the vehicle that follows the white van?",
        "time_stamp": "00:06:10",
        "answer": "C",
        "options": [
          "A. Red.",
          "B. Black.",
          "C. Green.",
          "D. Blue."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_331_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located on the right side of the cyclist right now?",
        "time_stamp": "00:00:14",
        "answer": "B",
        "options": [
          "A. A supermarket entrance.",
          "B. A row of flags.",
          "C. A bus stop.",
          "D. A park."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_172_real.mp4"
  },
  {
    "time": "[0:02:04 - 0:02:24]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What structure is located straight ahead on the right side of the road, visible right now?",
        "time_stamp": "00:02:21",
        "answer": "C",
        "options": [
          "A. A bridge.",
          "B. A gym.",
          "C. A billboard.",
          "D. A roundabout."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_172_real.mp4"
  },
  {
    "time": "[0:04:08 - 0:04:28]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located on the right side of the road right now?",
        "time_stamp": "00:04:23",
        "answer": "A",
        "options": [
          "A. A series of road signs.",
          "B. A parked car.",
          "C. A bus stop.",
          "D. A pedestrian crossing."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_172_real.mp4"
  },
  {
    "time": "[0:06:12 - 0:06:32]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the cyclist right now?",
        "time_stamp": "00:06:14",
        "answer": "C",
        "options": [
          "A. Turning a corner.",
          "B. Going uphill.",
          "C. On a straight road.",
          "D. About to reach a roundabout."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_172_real.mp4"
  },
  {
    "time": "[0:08:16 - 0:08:36]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the big red truck?",
        "time_stamp": "00:09:20",
        "answer": "A",
        "options": [
          "A. On the left side of the cyclist.",
          "B. Behind the cyclist, on the right.",
          "C. On the right side of the main path.",
          "D. Ahead, to the left."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_172_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The scene begins on a flat, light-colored surface with a yellow wall in the background, featuring some marks. The top left corner shows a timestamp.  [0:00:01 - 0:00:04]: A hand appears from the right, holding a yellow rectangular block. A \"SUBSCRIBE\" button graphic with a red background is displayed in the top left corner of the screen. The hand places the block on the flat surface. Another logo in grey resembling the subscription notification bell appears next to the text. [0:00:04 - 0:00:07]: The hand adjusts and then releases the block on the surface. The graphics, including the subscribe button and bell, remain on screen as the hand starts to move away.  [0:00:07 - 0:00:09]: The hand is now out of frame, and the graphics disappear, leaving the block centered on the surface.  [0:00:09 - 0:00:13]: The scene remains mostly static until another hand appears from the right, holding a green block. The hand moves towards the already placed yellow block to position the green block on top. [0:00:13 - 0:00:15]: The hand successfully places the green block onto the yellow block and starts to move away. [0:00:15 - 0:00:17]: The hand now holds another green block and approaches the stacked blocks to add the second green block on top.  [0:00:17 - 0:00:20]: The hand firmly adjusts the blocks, ensuring they are stacked securely. The hand slowly moves away and finishes with a thumbs-up gesture.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What happens after the first green block is placed on the yellow block?",
        "time_stamp": "00:00:25",
        "answer": "D",
        "options": [
          "A. The hand removes the green block.",
          "B. The hand moves away and does nothing.",
          "C. The hand removes the yellow block.",
          "D. The hand adds another green block."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_209_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:40 - 0:02:00] [0:01:40 - 0:01:43]: A pair of hands is constructing a structure using brightly colored plastic building blocks on a white surface with a yellow wall in the background. The structure, primarily consisting of yellow and green pieces with some blue blocks, is being adjusted by the hands. [0:01:44 - 0:01:47]: One of the hands picks up and places another yellow block. The blocks appear to be interconnected and stacked on top of each other. [0:01:48 - 0:01:51]: The individual adjusts and adds a green block to the structure, ensuring the pieces fit tightly together. The structure's arrangement slowly becomes more stable and intricate. [0:01:52 - 0:01:55]: Both hands work on pressing down the blocks to ensure they are securely connected. The structure has an even distribution of yellow, green, and blue blocks. [0:01:56 - 0:01:58]: The hands briefly move away from the structure, showcasing the partially built assembly, which stands out prominently against the plain white and yellow background. [0:01:59]: The pair of hands then reappears, holding another blue block, ready to further add to the existing structure.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action do the hands perform just before moving away from the structure?",
        "time_stamp": "0:01:58",
        "answer": "D",
        "options": [
          "A. Adding a yellow block.",
          "B. Adding a blue block.",
          "C. Adjusting the blue block.",
          "D. Pressing down the green blocks."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_209_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:21]: A hand is reaching towards a colorful structure made of interlocking plastic blocks on a white table. This structure consists of blocks of various colors including blue, green, yellow, and orange. The hand appears to be adjusting or grabbing the blocks. The background is a plain yellow wall. [0:03:22]: The hand is no longer in the frame, and the structure is unchanged. [0:03:23 - 0:03:24]: The hand reappears to the right side of the frame, holding a green plastic block, and is moving it towards the table. [0:03:25]: The green plastic block is now positioned alone on the table in front of the original structure. The hand is no longer visible. [0:03:26 - 0:03:28]: A hand reappears on the right side of the frame, this time empty. It appears to be in a position as if to make another adjustment. [0:03:29]: The hand is in the process of moving back, seemingly completing a placement or an adjustment. [0:03:30 - 0:03:32]: A new yellow and green block has been placed on the table next to the original single green block. [0:03:33 - 0:03:35]: The hand picks up and arranges the new yellow block next to the existing green block. [0:03:36 - 0:03:37]: With the yellow and green blocks now together, the hand is no longer in the frame. [0:03:38 - 0:03:39]: The hand reappears, reaching towards the blocks on the table, and seems to be gathering additional blocks.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What sequence of actions occured with the green and yellow blocks just now?",
        "time_stamp": "00:03:30",
        "answer": "A",
        "options": [
          "A. The green block is placed first, followed by the yellow block.",
          "B. The yellow block is placed first, followed by the green block.",
          "C. The green block is removed, then the yellow block is placed.",
          "D. The yellow block is adjusted, then the green block is placed."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the new yellow and green block placed relative to the original structure?",
        "time_stamp": "00:03:32",
        "answer": "D",
        "options": [
          "A. Behind the original structure.",
          "B. Next to the original structure.",
          "C. On top of the original structure.",
          "D. In front of the original structure."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_209_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:01]: The video shows two hands working with interlocking plastic building blocks on a white surface. A stack of blue, yellow, and green blocks is in the background near a yellow wall, and a green block is in the foreground. [0:05:01 - 0:05:04]: The person’s hands move the green block towards the bottom of the frame, and they reach for additional blocks. [0:05:04 - 0:05:07]: The hands grasp another yellow block and place it beside the green block.  [0:05:07 - 0:05:09]: One hand continues to place and adjust yellow blocks while the other hand stabilizes the green block. [0:05:09 - 0:05:11]: The person’s hand secures the yellow blocks into the green block, creating a structure consisting of both yellow and green blocks. [0:05:11 - 0:05:15]: The person's right hand further secures the yellow blocks, pressing down to ensure they are tightly interlocked with the green block. [0:05:15 - 0:05:17]: They continue placing yellow blocks on the green base and adjusting the blocks to ensure alignment. [0:05:17 - 0:05:18]: The right hand makes minor adjustments to the yellow block structure, ensuring all pieces are correctly connected. [0:05:18 - 0:05:19]: The hand then pulls away, and the yellow and green block arrangement is completed, while the pre-constructed set remains in the background.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the person do with the yellow block just now?",
        "time_stamp": "0:05:15",
        "answer": "D",
        "options": [
          "A. Removed them from the structure.",
          "B. Arranged them on the blue block and adjusted for alignment.",
          "C. Stacked them on top of the blue blocks.",
          "D. Placed them above the green block and another yellow block."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_209_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:05]: The video captures a busy city street in the evening, showcasing a bustling atmosphere with various activities occurring simultaneously. The scene is set along a sidewalk where people are walking. The left side of the video frames features a tall building with a large storefront displaying a bright red and white 'Virgin' logo. The right side of the video frames reveals a row of bikes parked at a bike stand and several buildings that extend into the background. [0:00:06 - 0:00:10]: The video highlights the movement of pedestrians along the sidewalk. Some individuals are seen carrying umbrellas, hinting at recent or imminent rain. Streetlights and buildings are illuminated, contributing to the urban evening ambience. Vehicles, including taxis and cars, move along the street, with their headlights adding to the overall lighting of the scene. [0:00:11 - 0:00:15]: As the video progresses, more pedestrians come into view, some of whom are entering and exiting shops. A few people continue to walk with umbrellas open. The street remains busy with moving vehicles, and the background of tall buildings and vibrant lights create a dense metropolitan feel. [0:00:16 - 0:00:20]: The final segment continues to capture the livelihood of the city street. Pedestrians maintain their course on the sidewalk, some stopping momentarily. The buildings' illuminated signs and window displays remain vibrant, and the street stays active with flowing traffic. The overall scene encapsulates the urban life of a city street in the evening.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are some pedestrians seen doing as they walk along the sidewalk?",
        "time_stamp": "0:00:15",
        "answer": "C",
        "options": [
          "A. Riding bikes.",
          "B. Taking pictures.",
          "C. Carrying umbrellas.",
          "D. Jogging."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_306_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:23]: The video takes place on a city sidewalk at night, covered with scaffolding. The sidewalk is busy with people walking in both directions. To the left, there's a store with lit windows, while the right side has parked cars, including a black SUV and a food truck adorned with bright lights and a sign reading \"Shanif's Famous.\" [0:02:23 - 0:02:27]: A man wearing a yellow shirt walks toward the camera, while others in dark clothing walk in the opposite direction. The food truck continues to be prominently visible, parked along the street near the curb. [0:02:27 - 0:02:29]: More individuals, including a man in a beige shirt and another in a gray jacket, walk past the camera. The food truck's brightly lit front is also visible. Additional pedestrians are visible further down the sidewalk. [0:02:29 - 0:02:32]: The camera continues down the sidewalk, passing the food truck. Two women in patterned dresses and orange hijabs stand near the food truck. One of them points towards the truck’s brightly lit menu. [0:02:32 - 0:02:34]: As the camera moves past the truck, the focus shifts onto the bustling sidewalk ahead. The area is lit by a combination of street lights and the lights from various shops and vehicles. [0:02:34 - 0:02:37]: The path ahead continues under scaffolding, which is supported by metal beams. People walk toward the camera and in the distance, a mix of shops and illuminated signs are visible. [0:02:37 - 0:02:39]: The camera approaches the end of the scaffolding structure, revealing more pedestrians and street signs. Several vehicles are parked along the street, and the skyline of taller buildings is visible in the background.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the name on the food truck?",
        "time_stamp": "00:02:29",
        "answer": "B",
        "options": [
          "A. Marco's Delights.",
          "B. Shanif's Famous.",
          "C. Tasty Treats.",
          "D. City Eats."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_306_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:41]: The video shows a bustling city street during twilight, with tall buildings on either side and lots of lit windows. Several vehicles, including taxis, are on the road, and illuminated signs are visible, including one with a large star on it. [0:04:41 - 0:04:43]: As the camera moves forward, the cityscape remains consistently busy, with pedestrians on the sidewalk, streetlights illuminating the area, and people walking with umbrellas. Decorative plants in pots are positioned along the sidewalk. [0:04:43 - 0:04:45]: The video continues along the same street, with a clear view of both the busy vehicular traffic and the lively pedestrian activity. The camera captures another storefront with bright lighting. [0:04:45 - 0:04:47]: The scene remains lively with the continuous movement of traffic and people. A yellow taxi is prominent in the traffic, and the background reveals a mix of old and modern buildings. [0:04:47 - 0:04:49]: The street is crowded with both cars and people. The brightly lit storefront signs and window displays create a vibrant atmosphere. The sidewalk is lined with more potted plants. [0:04:49 - 0:04:51]: The camera still traverses the busy street, with taxis and other vehicles making up the traffic. Street signs and shop banners are visible, contributing to the city's energetic ambiance. [0:04:51 - 0:04:53]: The view continues along the busy street, capturing more vehicles and pedestrians. The large star sign becomes obscured as the camera moves forward. [0:04:53 - 0:04:55]: The footage shows continuous movement, with the street bustling with various activities. Pedestrians are visible near shop entrances, enjoying the evening. [0:04:55 - 0:04:57]: The video captures more of the lively street, including taxis, streetlights, and shop fronts. The pedestrians continue to move along the sidewalk. [0:04:57 - 0:04:58]: The street remains crowded with evening activity, with vehicles and pedestrians maintaining a constant presence in the video. Decorative potted plants line the sidewalk. [0:04:58 - 0:05:00]: More of the bustling evening street scene is visible, with bright lights from shop windows, moving traffic, and passing pedestrians. The camera continues its forward movement along the street.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the general atmosphere of the street right now?",
        "time_stamp": "0:04:54",
        "answer": "B",
        "options": [
          "A. Calm and quiet.",
          "B. Busy and vibrant.",
          "C. Deserted and eerie.",
          "D. Tense and chaotic."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_306_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:06]: The video begins on a busy city street during early evening, likely in New York based on recognizable signage. The sky is darkening, and there are numerous buildings, both tall and short, in the distance. In particular, an iconic, tall black skyscraper stands out among the lit-up buildings. The street is bustling with cars, some with headlights on, and numerous pedestrians are on the sidewalks. On the right side of the screen, there is a Macy's department store sign hanging from a large awning. Streetlights and storefronts are illuminated, reflecting off the wet streets. The camera moves forward smoothly, maintaining its focus straight down the street. [0:07:07 - 0:07:10]: As the camera continues moving forward, the perspective shifts slightly to show more of the people walking on the sidewalk. A person in a light gray hoodie and shorts walks ahead, and a variety of advertisements and signs are visible on storefronts. The camera passes by a Sunglass Hut store, featuring large poster advertisements of people wearing sunglasses. More pedestrians come into view, including a woman in a white shirt and cap walking briskly and a person in a black outfit looking at their phone. The environment buzzes with city activity as people move in different directions. [0:07:11 - 0:07:15]: The camera continues past the storefronts. One poster depicts a person in a red outfit with sunglasses. Pedestrians continue walking in both directions, and the scene is now closer to the entrance of a subway station named \"34th Street-Herald Sq Station.\" The entrance is marked with MTA signs indicating the subway lines running through this station. A woman is seen descending the stairs with a suitcase. Another brightly lit advertisement can be seen across from the station entrance. [0:07:16 - 0:07:18]: The camera now fully focuses on the subway entrance. Several people are going down the stairs into the station, including the woman with the suitcase and another person in a blue dress carrying a white tote bag. The station's fluorescent lights illuminate the descending stairs. A Burger King sign and caution sign are visible on the left wall of the entryway, indicating the presence of a fast-food restaurant nearby. The environment transitions from the bright, busy street to a more enclosed and calmer underground setting. [0:07:19 - 0:07:20]: Descending further, the camera captures more of the subway station, showing the tiled walls and steps more clearly. The atmosphere inside the station is lit with fluorescent lighting. As the camera tightly follows the descent, the people ahead continue walking down the stairs, some with bags, moving further into the subway station. The perspective provides a look into the city's underground transit system, blending the dynamic street-level activity with the structured calmness of the subway.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Which station entrance was shown just now?",
        "time_stamp": "0:07:18",
        "answer": "B",
        "options": [
          "A. Times Square Station.",
          "B. 34th Street-Herald Sq Station.",
          "C. Grand Central Station.",
          "D. Union Square Station."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_306_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:08:55]",
    "captions": "[0:08:40 - 0:08:55] [0:08:40 - 0:08:54]: A subway train is seen arriving at a station platform at 34th Street. The train, primarily silver with yellow accents, is visible from a first-person perspective standing on the platform. The station is illuminated with multiple fluorescent lights, and passengers can be seen inside the train as it approaches. The yellow tactile paving on the edge marks the boundary for safe standing. Gradually, more of the platform becomes visible, including a sign indicating the station name \"34 St\" and a garbage can with an anti-litter message. At 0:08:54, the rear end of the train is visible as it continues down the track, with its red tail lights illuminated and a 'Q' sign shown.  [0:08:55]: Another subway train, traveling in the opposite direction, is visible on the adjacent track.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the primary color of the subway train?",
        "time_stamp": "00:08:54",
        "answer": "C",
        "options": [
          "A. Blue.",
          "B. Green.",
          "C. Silver.",
          "D. Red."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_306_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the taxi visible right now?",
        "time_stamp": "00:00:38",
        "answer": "C",
        "options": [
          "A. Blue.",
          "B. Green.",
          "C. Yellow.",
          "D. Red."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_382_real.mp4"
  },
  {
    "time": "[0:02:05 - 0:02:10]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type of vehicle is shown in the foreground right now?",
        "time_stamp": "00:02:06",
        "answer": "B",
        "options": [
          "A. A truck.",
          "B. A taxi.",
          "C. A motorcycle.",
          "D. A bicycle."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_382_real.mp4"
  },
  {
    "time": "[0:04:10 - 0:04:15]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What type of lanes is indicated by the road markings right now?",
        "time_stamp": "00:04:14",
        "answer": "C",
        "options": [
          "A. Carpool lanes.",
          "B. HOV lanes.",
          "C. Bus lanes.",
          "D. Bike lanes."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_382_real.mp4"
  },
  {
    "time": "[0:06:15 - 0:06:20]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What time is displayed on the clock visible right now?",
        "time_stamp": "00:06:17",
        "answer": "D",
        "options": [
          "A. 12:50.",
          "B. 13:10.",
          "C. 12:00.",
          "D. 12:25."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_382_real.mp4"
  },
  {
    "time": "[0:08:20 - 0:08:25]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which brand's store is visible on the right side of the street right now?",
        "time_stamp": "00:08:22",
        "answer": "B",
        "options": [
          "A. Nike.",
          "B. GAP.",
          "C. Adidas.",
          "D. H&M."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_382_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video starts with a black screen. [0:00:01 - 0:00:04]: The scene shows a man with glasses and short, dark hair, wearing a black sweater, seated behind a white table. In front of him on the table is a black graphics card with three cooling fans. Behind him is a blue gradient background and two dark shelves on each side, holding items such as books, decorative figures, and a lamp. The man gestures with his hands, picking up and presenting a model of a futuristic object, possibly a component or a gadget, while he speaks directly to the camera. [0:00:05 - 0:00:19]: The video transitions to a black background with animated graphics showcasing various GPU models and specifications. Initially, two graphics cards are displayed: - The RTX 3080 Ti with 10240 CUDA cores, a boost clock of 1665 MHz, and 12GB of memory. - The RTX 3090 with 10496 CUDA cores, a boost clock of 1695 MHz, and 24GB of memory. After a few seconds, these graphics cards are replaced by newer models: - The RTX 4090 with 16384 CUDA cores, a boost clock of 2520 MHz, and 24GB of memory. - Another GPU, possibly a professional workstation model, with 18176 CUDA cores, a boost clock of 2505 MHz, and 48GB of memory. Prices for these GPUs are also shown, with the RTX 4090 listed at 15000 yuan and the professional model at 60000 yuan.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the boost clock speed of the RTX 4090 right now?",
        "time_stamp": "00:00:20",
        "answer": "C",
        "options": [
          "A. 1665 MHz.",
          "B. 1695 MHz.",
          "C. 2520 MHz.",
          "D. 2505 MHz."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_106_real.mp4"
  },
  {
    "time": "[0:03:20 - 0:03:40]",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:21]: The video begins with a comparison chart showing performance metrics for different graphics cards, specifically RTX 4080, RTX 4070 Ti SUPER, RTX 4070 Ti, and RTX 4070, in both 2K and 4K resolutions. The document displays two sections: \"3DMark Time Spy\" and \"3DMark Time Spy Extreme.\" [0:03:22 - 0:03:26]: The scene transitions to a person in a studio setting discussing the performance comparison. The person is seated at a white table with a large graphics card beside them. The background is blue, with shelves housing various electronic devices and decorations. [0:03:27]: The video switches to a first-person perspective in a video game. The on-screen information shows the frames per second (FPS), GPU details, and other performance stats. The player is navigating a narrow, city-like alley, with a red \"B\" painted on one of the walls. [0:03:28]: The player continues moving through the alleys, observing surroundings with detailed textures on the buildings. [0:03:29]: The player navigates towards an open area with a yellow building on the left and red building in the distance. A chicken is seen on the ground, adding a lively touch to the environment. [0:03:30]: The player advances deeper into the area, closely observing a yellow building with weathered textures on the right. The player's hand holding a weapon is visible. [0:03:31]: The player walks past a stone archway, with graffiti on walls and potted plants around, enhancing the urban setting. [0:03:32]: The perspective shifts to the player holding a pistol in a different alley. The ambient lighting and detailed architecture are evident. [0:03:33 - 0:03:34]: The player moves through the narrow alley, taking cover behind walls and cautious of potential threats. [0:03:35 - 0:03:36]: Smoke grenades create a smoky environment, reducing visibility. The player cautiously proceeds, keeping the pistol ready. [0:03:37 - 0:03:40]: The player engages in a confrontation within the smoke, with increased intensity and actions. The visibility is low due to the smoke cloud, adding tension to the gameplay.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which performance metrics are being shown right now?",
        "time_stamp": "00:03:40",
        "answer": "C",
        "options": [
          "A. CPU benchmarks for various processors.",
          "B. FPS results for different games.",
          "C. CPU clock, CPU Usage, VRAM Usage, GPU Power, GPU Temp, and FPS for 4070TiS.",
          "D. RAM speed metrics for different memory types."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_106_real.mp4"
  },
  {
    "time": "[0:06:40 - 0:07:00]",
    "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:44]: The scene depicts two individuals wearing dark jackets with large yellow \"FBI\" lettering on the back. They are walking briskly down a wet street. Various statistics regarding GPU and VRAM usage are displayed at the top of the frames. The background shows buildings with signs and some autumn trees. [0:06:44 - 0:06:52]: The scene shifts to a graphical performance comparison. The comparisons are for different graphics cards (RTX 4080, RTX 4070 Ti SUPER, etc.) playing a specific game (心灵杀手2). The bar graphs illustrate performance differences in frames per second (fps) at various graphics settings. [0:06:53 - 0:06:59]: The performance comparison continues with details on average fps and specific settings. More detailed breakdowns are shown, comparing the capability of each graphics card across different resolutions and settings. The data remains consistent, providing comprehensive information on graphical performance for each card displayed.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which graphics card is labeled in green in the performance comparison right now?",
        "time_stamp": "00:06:59",
        "answer": "C",
        "options": [
          "A. RTX 3060 Ti.",
          "B. RTX 4060 Ti.",
          "C. RTX 4070 Ti SUPER.",
          "D. RTX 2070."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_106_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:10:20]",
    "captions": "[0:10:00 - 0:10:20] [0:10:00 - 0:10:01]: The video begins with a first-person perspective interface of a computer screen displaying an image editing software. On the right side, a photo of a person wearing a white bear hat, blue scarf, and beige coat pointing upwards is shown on a bright blue background. Various editing tools and options are visible on the left side of the screen. [0:10:02]: The scene transitions to a darker setting, where a person is seated at a workstation in a dimly lit room, facing a collection of colorful screens. They appear to be working on a computer. [0:10:03 - 0:10:20]: The video now displays a static screen with a bar graph titled \"Stable Diffusion AI 图测试\". The graph compares the performance of different GPUs (RTX 4070, RTX 4070 Ti, RTX 4080) in terms of \"SD1.5(512x512)\" and \"SDXL(512x512)\" and \"SDXL(1024x1024)\" measured by \"TensorRT\". Numbers on the y-axis represent values (14, 15, 28, 32, 120, 112). This screen remains consistent with changes in some details, such as highlighting memory configurations (12GB for the RTX 4070 and RX 4070 Ti, 16GB for the RTX 4080).",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which GPU shows the best performance for \"SDXL(512x512)\" right now?",
        "time_stamp": "00:10:20",
        "answer": "C",
        "options": [
          "A. RTX 4070.",
          "B. RTX 4070 Ti.",
          "C. RTX 4080.",
          "D. SD1.5(512x512)."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_106_real.mp4"
  },
  {
    "time": "[0:12:40 - 0:12:53]",
    "captions": "[0:12:40 - 0:12:53] [0:12:40 - 0:12:45]: A man is seated behind a white table, facing the camera with a neutral expression. He is wearing glasses and a black shirt with an electronic component illustration on it. To his left, a large graphics card with three cooling fans is standing upright on the table. The background is a solid gradient of blue, with shelves on either side holding various objects, including gadgets and potted plants. As he speaks, icons representing \"like,\" \"comment,\" \"favorite,\" and \"share\" appear around him and the graphics card, indicating an interaction with the video content or a demonstration of features. [0:12:45 - 0:12:49]: A smartphone user interface overlay appears on the left side of the frame, showing an online shopping application. The display showcases various products, including clothing and electronics. The man continues to talk, occasionally gesturing towards the interface while maintaining eye contact with the camera. The interactive icons remain visible around him. [0:12:50 - 0:12:53]: The scene transitions to a black background filled with small white stars, giving a space-themed effect. A graphic logo with an electronic device icon appears on the left along with Chinese characters, likely representing the brand or video series title. The logo and text pulsate slightly with a glowing effect, set against the starry background.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What emojis are currently displayed around the man and the graphics card?",
        "time_stamp": "00:12:43",
        "answer": "C",
        "options": [
          "A. \"like,\" \"comment,\" \"follow,\" and \"share\".",
          "B. \"like,\" \"save,\" \"comment,\" and \"share\".",
          "C. \"like,\" \"insert coins,\" \"favorite,\" and \"share\".",
          "D. \"like,\" \"comment,\" \"subscribe,\" and \"share\"."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_106_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video starts inside a walk-in cold storage room with shelves on either side filled with dairy products. On the left, various milk cartons and beverage containers are arranged on metal shelves. On the right, there are boxes and crates stacked, holding more dairy items. The floor is metallic and appears clean. In the center, there is a trolley with crates of stacked yogurt cups. The lighting is bright, reflecting off the metallic surfaces, creating a well-lit environment. [0:00:04 - 0:00:06]: The camera moves closer to the stack of yogurt cups on the trolley, and the left shelving unit full of dairy products becomes more apparent. The focus is on these yogurt cups which are white with blue labels. Some of the products from the bottom shelves of the left-side shelving can also be seen more clearly, including milk cartons and various other dairy items. [0:00:06 - 0:00:17]: A hand, wearing black gloves, picks up a tray of yogurt cups from the top of the stack on the trolley. The person then moves towards the left shelving unit, reaching for an empty spot on the shelf. The hand places one of the yogurt cups on the shelf, next to a column of other similarly-stacked dairy products. The hand continually places more yogurt cups neatly in a row. The camera angle shows the upper shelves which hold additional dairy items like bottled beverages and yogurt packs. The items in the background include various cartons, bottles, and packages of dairy products, maintaining a consistent theme. [0:00:17 - 0:00:19]: The camera angle shifts slightly, showing more of the surrounding environment. The section on the left contains more milk cartons and other dairy goods. The person continues to place yogurt cups on the shelf, close to other similar products and directly beside more columns of stacked yogurt cups. [0:00:19 - 0:00:20]: The video concludes with a wider view of the left shelving unit filled with a variety of dairy products like different sizes of milk cartons and bottled beverages. On the right are stacks of crates and boxes, possibly for restocking the shelves. The action focuses on the final placement of yogurt cups, ensuring that the products are aligned and positioned correctly on the shelf.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What distinguishing feature is described about the yogurt cups?",
        "time_stamp": "0:00:06",
        "answer": "C",
        "options": [
          "A. They have a green label.",
          "B. They are stacked in red trays.",
          "C. They are white with blue labels.",
          "D. They are placed in a metal container."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Action Recognition",
        "question": "What action does the person perform with the yogurt cups?",
        "time_stamp": "0:00:17",
        "answer": "D",
        "options": [
          "A. The person sorts the cups by flavor.",
          "B. The person moves the yogurt cups to the right shelving unit.",
          "C. The person stacks the cups in a corner.",
          "D. The person places the yogurt cups neatly on the shelf."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_437_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:41]: The scene begins in a store aisle where a person wearing black gloves is holding a carton of milk and positioning it on an empty shelf among similarly shaped cartons. The setting includes shelves lined with various milk cartons, predominantly white with some green accents;  [0:02:42 - 0:02:43]: The individual's hand retrieves another milk carton from a nearby metal cart that is partially filled. There is a sense of repetitive action as multiple cartons are being restocked on the shelf;  [0:02:44 - 0:02:46]: Additional cartons are taken from the cart and methodically placed on the shelf. The individual continues to place the milk cartons on the empty shelf while visibly arranging them to align with the existing stock;  [0:02:47 - 0:02:49]: A broader view of the cart loaded with milk cartons is shown, positioned adjacent to the shelves. The person’s movements indicate a systematic approach to restocking the shelf;  [0:02:50 - 0:02:51]: Further cartons are picked from the shelf by the individual. The majority of the cartons show uniform packaging, and their consistent positioning emphasizes the methodical stocking process;  [0:02:52 - 0:02:55]: The person continues to arrange the milk cartons on the lower shelf. The camera captures gloved hands rearranging and placing the cartons carefully;  [0:02:56 - 0:02:59]: More cartons are picked from the cart and placed onto the shelf, maintaining the organized layout. The individual continues to fill the gaps on the shelf with the cartons from the metal cart. Пhroughout the frames, the organized process of restocking becomes more evident, showing the individual aligning and adjusting the cartons to fit neatly into the space.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person holding right now?",
        "time_stamp": "0:03:00",
        "answer": "B",
        "options": [
          "A. A bottle of juice.",
          "B. A carton of milk.",
          "C. A box of cereal.",
          "D. A can of soda."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_437_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:23]: In a storeroom with multiple metal shelves, the person wearing black gloves and a dark jacket places a tray of yogurt cups on the top shelf. The shelves hold various boxed and packaged items. [0:05:24 - 0:05:25]: The person continues arranging yogurt cups from the tray onto the shelf. More packaged goods are visible on the lower shelves. [0:05:26]: The tray of yogurt cups is now fully placed on the shelf. Boxes and other containers are neatly stacked around them. [0:05:27 - 0:05:29]: The person proceeds to the next tray of yogurt cups positioned on a wheeled cart. They lift the tray and begin arranging it on the shelf. [0:05:30 - 0:05:31]: The view shifts, showing the aisle with the wheeled cart full of yogurt trays. Shelves stocked with various dairy products and other groceries line both sides of the aisle. [0:05:32 - 0:05:33]: The perspective focuses on the cart and the surrounding stock of groceries. The scene is orderly, with different dairy products neatly arranged on the shelves. [0:05:34 - 0:05:35]: The person places another tray of yogurt cups from the cart onto the shelf, performing the same organizing action. [0:05:36 - 0:05:37]: The view once more shows the aisle with dairy products stocked neatly on both sides. The cart, now partially emptier, still has several trays of yogurt cups. [0:05:38 - 0:05:39]: The person completes positioning the last yogurt cups on the shelf, ensuring the items are organized. The storage area remains tidy, with goods systematically placed on the metal shelves.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What did the person pick up on the top shelf just now?",
        "time_stamp": "0:05:27",
        "answer": "C",
        "options": [
          "A. Boxes of cereal.",
          "B. Cans of soup.",
          "C. Yogurt cups.",
          "D. Bottled water."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_437_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:10:45]",
    "captions": "[0:10:40 - 0:10:45] [0:10:40 - 0:10:45]: The video shows a first-person perspective of someone stocking shelves in a refrigerated section of a grocery store. The shelves are full of various dairy products, including several stacks of round yogurt containers. The person's right hand is wearing a dark glove and a jacket. The hand reaches out to pick up and arrange yogurt containers, lifting them from a lower shelf and placing them onto an upper shelf. The refrigeration unit's lights provide a bright, even illumination over the products. The scene remains consistent with the person methodically organizing the products, ensuring they are stacked neatly and labeled correctly.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action is the person performing right now?",
        "time_stamp": "0:10:45",
        "answer": "C",
        "options": [
          "A. Cleaning the shelves.",
          "B. Taking inventory.",
          "C. Organizing yogurt containers.",
          "D. Restocking fresh vegetables."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_437_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the trees located right now?",
        "time_stamp": "00:00:20",
        "answer": "A",
        "options": [
          "A. On the both sides of the road.",
          "B. On the left side of the road.",
          "C. In the middle of the road.",
          "D. On the right side of the road."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_177_real.mp4"
  },
  {
    "time": "[0:01:59 - 0:02:19]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located on the right side of the path right now?",
        "time_stamp": "00:02:19",
        "answer": "A",
        "options": [
          "A. An open field.",
          "B. A dense forest.",
          "C. A farmhouse.",
          "D. A shopping plaza."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_177_real.mp4"
  },
  {
    "time": "[0:03:58 - 0:04:18]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the house with the brown roof located right now?",
        "time_stamp": "00:04:09",
        "answer": "A",
        "options": [
          "A. On the right side of the road.",
          "B. On the left side of the road.",
          "C. Directly behind the cyclist.",
          "D. In the middle of the forest."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_177_real.mp4"
  },
  {
    "time": "[0:05:57 - 0:06:17]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the trees located right now?",
        "time_stamp": "00:06:16",
        "answer": "A",
        "options": [
          "A. On the both sides of the road.",
          "B. On the left side of the road.",
          "C. In the middle of the road.",
          "D. On the right side of the road."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_177_real.mp4"
  },
  {
    "time": "[0:07:56 - 0:08:16]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the black car parked right now?",
        "time_stamp": "00:07:57",
        "answer": "A",
        "options": [
          "A. On the left side of the road.",
          "B. On the both sides of the road.",
          "C. On the right side of the road.",
          "D. No black car."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_177_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video begins with an entirely black frame. [0:00:01 - 0:00:05]: A person is seated at a white table in a studio setup with a blue background, black shelves, and various decorative items. On the table in front of them are two large graphics cards. The person is looking into the camera and occasionally at the cards, gesturing with their hands. [0:00:06 - 0:00:08]: The person continues speaking, making more hand gestures while looking at the graphics cards periodically. The light and decor on the shelves remain the same, providing a consistent background. [0:00:09]: The person appears to be smiling while still discussing the content. [0:00:10 - 0:00:12]: The focus zooms slightly closer. The person uses more expressive gestures with their hands, likely emphasizing certain points about the graphics cards. [0:00:13 - 0:00:15]: A graphic overlay appears on the screen, showing the text “9499元” along with a label indicating \"RTX 4080\". The person continues talking energetically, possibly explaining the price or features. [0:00:16]: Another graphic overlay appears, this time indicating \"RTX 3080\" with the price “5499元”. The individual maintains an engaging demeanor, looking directly at the camera. [0:00:17 - 0:00:19]: The person speaks in a conclusive tone, their hand movements suggesting a wrap-up of the explanation. The shelves and background decor remain unchanged, providing a consistent setting. [0:00:20]: The person looks slightly off-camera, transitioning into a more relaxed pose, possibly indicating the end of the video segment.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the price of the RTX 4080 shown right now?",
        "time_stamp": "00:00:15",
        "answer": "B",
        "options": [
          "A. 6499元.",
          "B. 9499元.",
          "C. 7499元.",
          "D. 5499元."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_108_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:04:20]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:06]: The video begins by showing a benchmark comparison for 3DMark Time Spy Extreme scores across three different graphics cards: RTX 4090, RTX 4080, and RTX 3090 Ti. The scores are displayed as horizontal bar graphs with the RTX 4090 achieving the highest score, followed by the RTX 4080, and then the RTX 3090 Ti. A percentage difference in score between the RTX 4080 and RTX 4090 is also indicated.   [0:04:07 - 0:04:09]: The scene changes to a first-person perspective where a person is seated behind a table with various items on it. The person is gesturing and appears to be explaining something about the items in front of him, which are two large graphics cards, each with multiple fans. The background is a neatly organized room with shelves holding tech gadgets and decor. [0:04:10 - 0:04:17]: The discussion continues, with the person consistently making gestures to emphasize points about the two graphics cards in front of them. The individual's facial expressions and actions suggest a detailed explanation is being provided, possibly comparing the features, performance, or other aspects of the graphics cards. [0:04:18 - 0:04:19]: The focus shifts back to a benchmark comparison, this time for 3DMark Port Royal scores, again comparing the three graphics cards: RTX 4090, RTX 4080, and RTX 3090 Ti. As before, the RTX 4090 leads, followed by the RTX 4080 and the RTX 3090 Ti, with numerical values displayed to highlight the differences.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is highlighted as the top-performing graphics card right now according to the 3DMark Time Spy Extreme scores?",
        "time_stamp": "00:04:20",
        "answer": "B",
        "options": [
          "A. RTX 3070.",
          "B. RTX 4090.",
          "C. RTX 4080.",
          "D. RTX 3090 Ti."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_108_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:08:20]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:04]: The video displays a benchmarking comparison between different GPUs: RTX 4090, RTX 4080, and RTX 3090 Ti. On the left side, a bar chart shows the average and 1% low FPS scores for each GPU at a resolution of 2560x1440. RTX 4090 shows an average FPS of 129, RTX 4080 has 118, and RTX 3090 Ti displays 119. On the right side, real-time statistics for clock, power, temperature, and VRAM usage of these GPUs are shown in separate columns. [0:08:05 - 0:08:09]: The chart on the left side changes to display new information for the resolution of 3840x2160. The data shows that the RTX 4090 has an average FPS of 124, RTX 4080 has 100, and RTX 3090 Ti shows 82. Real-time statistics of clock, power, temperature, and VRAM usage are still visible on the right side, revealing fluctuations among the GPUs. [0:08:10 - 0:08:14]: The video transitions to another benchmarking comparison featuring the game \"Cyberpunk 2077\". On the left side, a bar chart indicates performance metrics with the game at 2560x1440 resolution for RTX 4090, RTX 4080, and RTX 3090 Ti GPUs. RTX 4090 showcases an FPS of 147, RTX 4080 shows 131 FPS, and RTX 3090 Ti has 98 FPS. Real-time statistics for each GPU remain visible on the right side. [0:08:15 - 0:08:20]: The chart on the left continues to display performance metrics, with notable highlight boxes around the RTX 4080's FPS scores. Real-time statistics on the right continually update showing clock speeds, power consumption, temperatures, and VRAM usage for each GPU. The video consistently maintains comparative benchmarking information between these GPUs.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the average FPS score of the RTX 4080 at a resolution of 3840x2160 right now?",
        "time_stamp": "00:08:09",
        "answer": "C",
        "options": [
          "A. 124.",
          "B. 118.",
          "C. 100.",
          "D. 82."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_108_real.mp4"
  },
  {
    "time": "[0:12:00 - 0:12:20]",
    "captions": "[0:12:00 - 0:12:20] [0:12:00 - 0:12:07]: The video starts with a graph showing a performance comparison of RTX 4090, RTX 4080, and RTX 3090 Ti graphics cards. The graph has a black background with the title in white, and the performance bars are in green and white. The RTX 4090 is marked at 166%, the RTX 4080 at 121%, and the RTX 3090 Ti at 100%. [0:12:07 - 0:12:10]: The prices of the graphics cards are displayed on the graph. RTX 4090 is priced at 12999 yuan, RTX 4080 at 9499 yuan, and RTX 3090 Ti does not have a price listed. [0:12:10]: The graph maintains the same details as previously captured, emphasizing performance and price comparison of the three graphics cards. [0:12:11]: The text in the graph remains unchanged, continuing to show the comparison between RTX 4090, 4080, and 3090 Ti in terms of percentage performance and price in yuan. [0:12:12]: A person appears on screen, seated behind a white table with two graphics cards placed in front of him. He has glasses and wears a black shirt. There are two boxes with prices on either side of him—displaying prices of RTX 3080, 3090, 4080, and 4090. [0:12:13 - 0:12:16]: The person starts explaining the details of the graphics cards, gesturing with his hands. The RTX 3080 is priced at 5499 yuan, RTX 3090 at 11999 yuan, RTX 4080 at 9499 yuan, and RTX 4090 at 12999 yuan. [0:12:17]: The person continues talking and gesturing, with the graphics cards and their prices still prominently displayed on the screen beside him. [0:12:18]: The person adjusts his glasses and continues explaining the differences, occasionally pointing towards the cards on the table and the prices on the screen. [0:12:19 - 0:12:20]: The person holds one of the graphics cards in his hand while continuing to speak, explaining its features and benefits. The prices of the graphics cards are still displayed prominently on the screen.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the marked performance percentage of the RTX 4090 right now?",
        "time_stamp": "0:12:01",
        "answer": "D",
        "options": [
          "A. 100%.",
          "B. 121%.",
          "C. 129%.",
          "D. 166%."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the price of the RTX 4080 graphics card in yuan?",
        "time_stamp": "0:12:15",
        "answer": "B",
        "options": [
          "A. 5499 yuan.",
          "B. 9499 yuan.",
          "C. 11999 yuan.",
          "D. 12999 yuan."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_108_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:00:20",
        "answer": "A",
        "options": [
          "A. How to add mixed numbers.",
          "B. The subtraction of mixed numbers.",
          "C. The multiplication of fractions.",
          "D. The properties of whole numbers."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_212_real.mp4"
  },
  {
    "time": "[0:01:52 - 0:02:22]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:02:10",
        "answer": "A",
        "options": [
          "A. How to add those two mixed numbers.",
          "B. The process of converting mixed numbers to improper fractions.",
          "C. Subtracting mixed numbers.",
          "D. The definition of mixed numbers."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_212_real.mp4"
  },
  {
    "time": "[0:03:44 - 0:04:14]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker do next?",
        "time_stamp": "00:04:08",
        "answer": "A",
        "options": [
          "A. How to add those two mixed numbers.",
          "B. Subtract the fractions.",
          "C. Multiply the fractions.",
          "D. Divide the fractions."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_212_real.mp4"
  },
  {
    "time": "[0:05:36 - 0:06:06]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:06:36",
        "answer": "D",
        "options": [
          "A. How to add mixed numbers.",
          "B. How to convert fractions to decimals.",
          "C. The process of subtracting fractions.",
          "D. How to add those two fractions numbers."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_212_real.mp4"
  },
  {
    "time": "[0:07:28 - 0:07:58]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:07:58",
        "answer": "A",
        "options": [
          "A. Adding the fractions together.",
          "B. Converting the results back to whole numbers.",
          "C. Finding a different common denominator.",
          "D. Subtracting the fractions."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_212_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:10]: Two Christmas ornaments are prominently displayed against a background that transitions from white at the top to gray at the bottom. The ornament on the left is primarily blue with gold and white accents, and features the word \"Peace\" written in elegant white script. The ornament on the right has a more varied palette, predominantly showcasing pink and blue hues along with gold flecks, and is adorned with the word \"Joy\" in similar white script. Both ornaments have silver caps with a thin wire loop for hanging. The reflective surface of the ornaments creates highlights that change subtly as the perspective shifts. [0:00:11]: The screen dims, making the ornaments less visible. [0:00:12 - 0:00:14]: The phrase \"Alcohol Ink Ornaments\" appears in white, centered against a black background. [0:00:15]: The screen briefly turns black. [0:00:16 - 0:00:19]: Text appears on a pink background, stating, \"Links to the materials I used are in the video description.\" The text remains consistent until the end of the video.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What phrase appears right now?",
        "time_stamp": "00:00:14",
        "answer": "C",
        "options": [
          "A. \"Links to the materials I used are in the video description.\".",
          "B. \"Merry Christmas and Happy New Year.\".",
          "C. \"Alcohol Ink Ornaments.\".",
          "D. \"Handmade Ornaments by Design.\"."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_60_real.mp4"
  },
  {
    "time": "0:02:00 - 0:02:20",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:01]: Two black-gloved hands hold a clear glass ornament. A silver-colored top adorns the ornament, which is being positioned above a small white cup and a metal cap resembling a lid on a quilted white surface. [0:02:01 - 0:02:06]: The ornament is carefully detached from its silver top, which remains held by the right hand, while the translucent ornament body is in the left hand, both above a metal cap-like object and another small cup.  [0:02:07 - 0:02:15]: A clear plastic syringe appears in the frame and is positioned above the metal cap by the gloved hands. Fluid is gradually being dispensed into the metal cap from the syringe, as indicated by the precise alignment and controlled motion of the hands. [0:02:16 - 0:02:19]: The ornament body is repositioned above the metal cap while the syringe is still in use. Fluid fills the ornament through a hole at the top, dispensed from the syringe. The hands carefully adjust the ornament and syringe to ensure the fluid is correctly entering the ornament. Finally, the ornament is sealed with a white cap, ensuring no leaks or spills.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are the gloved hands doing just now?",
        "time_stamp": "0:02:19",
        "answer": "B",
        "options": [
          "A. Cutting the ornament into pieces.",
          "B. Dispensing fluid into the ornament.",
          "C. Painting the ornament.",
          "D. Measuring the ornament with a ruler."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_60_real.mp4"
  },
  {
    "time": "0:04:00 - 0:04:20",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:01]: The video opens with the viewer holding a clear round ornament in their left hand, which has areas filled with pink, purple, and blue ink, partially covered by gloves. The viewer holds a bottle of golden ink in their right hand and carefully applies it to the ornament. [0:04:01 - 0:04:04]: As the application continues, the golden ink spreads over the ornamental surface. The viewer adjusts their hold, turning the ornament to ensure an even distribution of the ink. The background consists of a textured white disposable mat beneath the working area and a transparent container with a small amount of liquid beside two bottles of purple and blue ink; all three are lying on the mat. [0:04:04 - 0:04:07]: The viewer’s hands maintain a steady grip while the golden ink continues to cover new areas on the ornament’s surface. The ornament is now shown with a golden streak that seamlessly connects the dynamic hues of blue, pink, purple, and gold. The viewer appears to be emphasizing the coverage evenly and systematically. [0:04:07 - 0:04:09]: With a gentle hand, the viewer continues to apply the ink, ensuring the ornament is thoroughly covered. The blending of the ink colors starts showing a more cohesive look, with the golden ink forming delicate lines over the other colors. [0:04:09 - 0:04:12]: The viewer puts down the golden ink bottle, now taking a purple ink bottle in their right hand. They begin applying the purple ink, blending it seamlessly into the existing colorful pattern on the ornament. [0:04:12 - 0:04:16]: The video progresses with the viewer carefully painting specific areas of the ornament, shaping the colors to create a vivid and fluid design. The pattern evolves with a mix of purple on various sections, merging with the surrounding blue and pink ink, giving the ornament a detailed and deliberate design.  [0:04:16 - 0:04:19]: The final frames show the viewer revisiting areas to apply additional golden ink, refining the pattern with structured movements that seamlessly blend all colors. The ornament, now vibrant and dynamic, exhibits a rich interplay of pink, blue, purple, and gold. The viewer showcases the completed piece, ensuring it is evenly coated and visually balanced.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:04:19",
        "answer": "B",
        "options": [
          "A. Holding a clear round ornament with golden streaks.",
          "B. Applying additional golden ink to the ornament.",
          "C. Mixing purple and blue ink in a container.",
          "D. Holding an empty transparent container in their right hand."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_60_real.mp4"
  },
  {
    "time": "0:06:00 - 0:06:20",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:04]: Wearing black gloves, the perspective shows both hands holding a spherical glass object, with a blue and gold marbled pattern. The object seems to be a bauble or ornament. In the right hand, there is a bottle of what appears to be gold paint with a small nozzle. The background includes a white, textured surface, likely a protective mat laid on a table or workstation. The hands hold the ornament delicately, and the gold paint is being applied in a careful, methodical manner to enhance the design. There are other bottles of paint nearby on the mat, indicating additional colors being used in the project. [0:06:05 - 0:06:09]: The process of applying gold paint to the ornament continues. The hands rotate the bauble slowly, ensuring an even distribution of the paint. The gold paint creates striking contrasts with the darker shades of blue, enhancing the visual appeal of the ornament. The nozzle of the paint bottle is used to add fine lines and details, contributing to the intricate design. The orientation of the bauble shifts slightly, providing a different angle to apply the paint and allowing different parts of the surface to be decorated. [0:06:10 - 0:06:14]: As the painting progresses, the ornament is turned and inspected to ensure the design is cohesive. The hands focus on different sections of the bauble, methodically adding more golden details against the patterned blue backdrop. The bottles of paint resting on the mat are occasionally glanced at, suggesting they will be used for additional touches. The mat underneath remains pristine, emphasizing the careful and precise application of paint without excess spilling. [0:06:15 - 0:06:19]: With the painting process nearing completion, the hands make final adjustments to the design. The ornament is held up slightly, inspected for any areas that might need further enhancement. The interplay of colors—vibrant blue hues blending seamlessly with shimmering gold—culminates in a visually striking piece. The bauble is then placed gently on the mat, signifying the end of the painting process. The surrounding bottles of paint offer a sense of the artistic endeavor undertaken, highlighting the careful craftsmanship involved in creating the ornate design.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "0:06:19",
        "answer": "B",
        "options": [
          "A. Applying a clear coat to the ornament.",
          "B. Inspecting the ornament for any areas that might need further enhancement.",
          "C. Holding the bauble unpainted.",
          "D. Removing paint from the mat."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_60_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:08:01 - 0:08:02]: A pair of hands in a dark purple long-sleeved garment holds a white rectangular piece of paper with printed grid lines. One hand is positioned near the bottom left corner of the paper, while the other hand, gripping a white tool, is drawing a straight line on the paper. Another white curved object is situated above the paper on a grey grid-patterned background. [0:08:02 - 0:08:03]: Both hands continue the drawing, with the left hand holding the paper steady while the right hand, holding the white tool, maintains the line along the edge. The white curved object remains unchanged in its position. [0:08:03 - 0:08:04]: The left hand adjusts its position, moving closer to the middle of the bottom edge of the paper, while the right hand still holds the tool. The line becomes more defined and prominent. [0:08:04 - 0:08:05]: The left hand begins to lift the paper slightly while the right hand continues to draw. The outline of the drawn figure becomes more visible. [0:08:05 - 0:08:06]: The left hand pulls the paper from the middle, while the white tool held by the right hand touches the drawn figure's edge. The curved white object on the top stays the same. [0:08:06 - 0:08:07]: The paper shows more intricate cuts, being peeled off gently by the left hand. The right hand remains in the same position, holding the white tool steady above the paper. [0:08:07 - 0:08:08]: The left hand further peels back the cut paper while the right hand slightly raises the white tool. Curves and pointed edges of cut sections become more visible. [0:08:08 - 0:08:09]: The left hand continues lifting the cut pieces, nearly removing a significant section of the paper. The white tool in the right hand hovers above the central area of the grid-lined paper.  [0:08:09 - 0:08:10]: The left hand finally removes a larger piece of the cut paper and shifts it towards the left. The right hand continues to hold the white tool above the remaining piece. [0:08:10]: A new scene unfolds with the right hand opening a cut piece of paper while the left hand steadies the remaining portion. Both hands are more centered in the frame, with the grey grid background consistent.  [0:08:11]: The right hand maintains hold of the cut piece, revealing more details of the intricate design, while the left hand slightly adjusts the other part of the paper. [0:08:12]: Both hands focus on separating a transparent sheet from the cut piece. The hands reveal more of the sheet, with the grey grid background providing a clear contrast to the translucent material. [0:08:13]: The transparent sheet held by the right hand is pulled away from the cut paper. The left hand remains in place, steadying the paper as more of the transparent sheet is revealed. [0:08:14]: With the transparent sheet almost fully separated, both hands hold respective parts, with the right fingers delicately handling the sheer material. The left hand appears to hold the remaining white piece. [0:08:15]: The fully separated transparent sheet held in the right hand is clearly visible with some cut designs now distinct. The left hand prepares to set down the remaining piece of paper. [0:08:16]: The right hand lowers the transparent sheet, bringing it closer for better handling. The paper's design continues to reveal itself, with delicate edges now clearly visible. [0:08:17]: A detailed view of the transparent sheet shows grid lines and subtle cut patterns. The right hand holds it centrally, while the left hand resumes its firm posture on the original piece of paper. [0:08:18]: The detailed grid lines and precise cuts on the transparent sheet come into full view as both hands adjust their positions. Right hand delicately places the sheet against the grey background. [0:08:19]: The transparent sheet is laid out fully against the grid background. Both hands make sure it is flat and aligned with the grid. The cut design seems intricate, revealed to its full extent. [0:08:20]: The hands return to the paper, smooth any curls or uneven edges on the transparent sheet ensuring it rests perfectly flat. [0:08:21]: The right hand picks up the white tool once again, carefully smoothing out the sheet. The left hand holds the paper in place ensuring stability for the precise task.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person using right now?",
        "time_stamp": "00:08:00",
        "answer": "B",
        "options": [
          "A. A pair of scissors.",
          "B. A white tool labeled 'cricut'.",
          "C. A ruler.",
          "D. A pen."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_60_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video begins with a black screen;  [0:00:01 - 0:00:04]: The text \"Sport Ball Keychains\" appears centered in the frame, white against the black background;  [0:00:05 - 0:00:06]: The layout changes to a design interface, featuring a grid in the main canvas area. A dark menu bar, with options such as \"File,\" \"Edit,\" and \"View,\" spans the top. Tool icons are aligned vertically on the left;  [0:00:07 - 0:00:08]: The interface transitions to an 'Upload' section. This area provides options for uploading images or selecting pattern fills. Thumbnails of recently uploaded images are visible at the bottom of the screen;  [0:00:09 - 0:00:11]: The cursor hovers over and clicks on the 'Upload Image' button. The middle part of the screen features a drag-and-drop area with the option to browse files;  [0:00:12 - 0:00:15]: A new window opens, showing the file directory. Several SVG files with sports themes, including footballs and basketballs, are listed. The user navigates to select the file named \"All-balls-svg.svg\";  [0:00:16 - 0:00:20]: The selected file is highlighted, and the user clicks 'Open.' The screen displays the SVG file in the design interface with a large preview. Below the image, there is red text stating, \"SVG cut file linked in description.\"",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the cursor do just now?",
        "time_stamp": "00:00:08",
        "answer": "B",
        "options": [
          "A. Click on the 'Edit' button.",
          "B. Click on the 'Upload Image' button.",
          "C. Drag a file into the upload area.",
          "D. Select a pattern fill option."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_67_real.mp4"
  },
  {
    "time": "0:02:00 - 0:02:20",
    "captions": "[0:02:00 - 0:02:20] [0:00:00 - 0:00:01]: The video starts with a collection of vinyl decals arranged neatly on a grid-patterned surface. There are various designs including the outlines of a volleyball, basketball, and soccer ball, arranged horizontally in a row at the top. Below these are assorted designs such as a plain orange circle, yellow tennis ball with bright green details, and several name tags in different font styles reading \"Macy,\" \"Nathan,\" \"Home,\" and \"Ryan\". The hands of the person wearing a ring on their left hand and holding a pink weeding tool are seen at the bottom of the frame, starting to weed the vinyl cut-out of the name \"Tracy\" written in pink cursive. [0:00:02 - 0:00:03]: The hands continue to weed the vinyl decal, working from right to left, removing the excess material around the name \"Tracy\".  [0:00:04 - 0:00:05]: The \"Tracy\" vinyl decal is now completely weeded. The hands begin to move away, with the pink weeding tool still in the right hand.  [0:00:06 - 0:00:07]: The person places the weeded \"Tracy\" decal to the left side and starts separating the other decals, beginning with picking up the \"Nathan\" name tag in blue script.  [0:00:08 - 0:00:09]: Holding the \"Nathan\" decal, the individual then moves some of the plain white backings of other decals to the top of the workspace, arranging them neatly.  [0:00:10 - 0:00:11]: With the \"Nathan\" decal now set aside, the hands reach for the \"Ryan\" name tag in bold black letters and start moving it.  [0:00:12 - 0:00:13]: The \"Ryan\" decal is placed alongside the \"Nathan\" decal, and the hand is now adjusting the positioning of a white backing next to a yellow tennis ball decal.  [0:00:14 - 0:00:15]: The hands concentrate on organizing the decals, ensuring the \"Macy\" name tag in pink cursive and the \"Home\" name tag in red script are visible in the lower part of the frame.  [0:00:16 - 0:00:17]: Attention is now on reorganizing the sports outlines. The circular volleyball outline is picked up and repositioned next to the soccer ball outline.  [0:00:18 - 0:00:19]: The person moves on to the basketball outline, placing it neatly beside the previously repositioned decals.  [0:00:20]: Finally, the hands hover over the decals, making some final readjustments. The entire selection of sports-themed decals and name tags is now visibly organized on the workspace.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:02:19",
        "answer": "C",
        "options": [
          "A. Adjusting the positioning of the soccer ball outline.",
          "B. Reorganizing the \"Home\" name tag.",
          "C. Tearing off the paper from the back of the white ring.",
          "D. Weeding the \"Tracy\" decal."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_67_real.mp4"
  },
  {
    "time": "0:04:00 - 0:04:20",
    "captions": "[0:04:00 - 0:04:20] [0:00:00 - 0:00:03]: A pair of hands is working on a gray grid mat, holding circular orange and black item with the name \"JOSH\" in white capital letters. Four paper envelopes lie in the upper part of the frame; two are partially open, one has \"Macy\" written in pink, and another \"Kailynn\" in blue. One hand is using a transparent tool to remove a part of the sticker from the item. [0:00:04 - 0:00:06]: The hands continue to remove the transparent covering from the circular item labeled \"JOSH.\" The progress of peeling is halfway complete while maintaining focus on the grid mat background and the arranged envelopes. [0:00:07 - 0:00:09]: The transparent covering is completely removed from the \"JOSH\" item, and the hands hold the circular item. The envelopes remain positioned at the top of the gray grid mat with the same arrangement and visibility. [0:00:10 - 0:00:15]: The right hand reaches across to the upper left envelope, then the hand moves back toward the grid mat's center. The hand now holds a plain circular white item. The previously removed transparent covering and the envelopes stay in their positions. [0:00:16 - 0:00:18]: A tool held in the right hand, with a pink-accented handle, is used to work on the plain white circular item. The hand carefully applies pressure to specific areas on the circle, gradually manipulating the item's surface. [0:00:19 - 0:00:21]: The right hand continues using the tool to make precise adjustments to the plain white circle. The focus remains on the dexterous manipulation of the item, while all other materials on the grid mat remain unchanged. [0:00:22 - 0:00:23]: The hand sets aside the tool and holds the white circular item aloft, displaying its completed aspect. The grip on the object suggests a thorough inspection. [0:00:24 - 0:00:25]: The white circle is placed back on the mat, and another piece of adhesive paper is picked up with a bright yellow and green design, resembling a tennis ball. [0:00:26 - 0:00:28]: Adjusting the yellow and green sticker, the hand positions the adhesive design centrally over the white circular item on the grid mat. The layout of the envelopes remains the same, providing a consistent background. [0:00:29 - 0:00:31]: Carefully aligning the yellow tennis ball design, it is applied and pressed down onto the white circular background. The adherence process is thorough, ensuring the design is properly affixed. The grid mat offers a consistent, even surface for this task. [0:00:32 - 0:00:33]: The hands begin to peel away the yellow and green sticker's adhesive backing, revealing a smooth, reflective surface beneath while maintaining alignment. The grid mat continues as a reliable working area. [0:00:34 - 0:00:37]: The full removal of the adhesive backing of the yellow and green design completes, leaving the tennis ball design perfectly on the white circular item. The hands gently smooth the top surface to secure the application. [0:00:38 - 0:00:39]: The newly completed item, featuring the tennis ball design, is examined by the held hands. The hands rotate the circle slightly, inspecting the design from varied angles to ensure proper application. [0:00:40 - 0:00:41]: The hands lower the completed yellow and green tennis ball item to the grid mat, placed next to the \"JOSH\" item. [0:00:42 - 0:00:43]: The focus returns to the hands reaching for another adhesive design. The steady sequence of removing and applying new items continues, ensuring meticulous attention to each object on the grid mat.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "0:04:36",
        "answer": "C",
        "options": [
          "A. Removing the transparent covering from the \"JOSH\" item.",
          "B. Holding the white circular item aloft for inspection.",
          "C. Removing the thin film from the surface of the green tennis ball icon.",
          "D. Reaching for another adhesive design."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_67_real.mp4"
  },
  {
    "time": "0:06:00 - 0:06:20",
    "captions": "[0:06:00 - 0:06:20] [0:00:00 - 0:00:03]: On a gray, grid-patterned cutting mat, several circular stickers and rectangular papers are carefully arranged. The circular stickers feature names like \"Ryan\" on a green background with a basketball design and \"JOSH\" on an orange background also resembling a basketball. Two small white rectangular cards with the names \"Nathan\" in blue and \"Macy\" in pink cursive are positioned nearby. A pair of hands, wearing a red long-sleeved top and a ring, is peeling off a white backing from one circular sticker featuring red stitches, reminiscent of a baseball. [0:00:04 - 0:00:07]: The hands continue to peel off the backing from the circular baseball sticker, revealing the sticky side. The peeled-off part is being held between the thumb and forefinger, while the other hand secures the sheet on the mat to prevent any shift. [0:00:08 - 0:00:09]: The backing is now completely removed, and the baseball sticker is fully exposed. The hands hover over the sticker as they reposition and prepare to move to the next step. [0:00:10 - 0:00:13]: One hand stays on the baseball sticker while the other reaches for the \"Nathan\" name card, lifting it from the mat. The \"Nathan\" name card is being held delicately between two fingers, preparing for a further step. [0:00:14 - 0:00:15]: The hands adjust the \"Nathan\" name card to face upward and examine it closely. The attention to detail is visible as the positioning is considered crucial for the coming steps. [0:00:16 - 0:00:17]: The \"Nathan\" card is flipped over to reveal an adhesive side covered by a transparent backing. The hands begin to peel away the transparent film from the edge. [0:00:18 - 0:00:19]: The removal of the transparent backing continues meticulously to ensure the underlying adhesive remains intact and does not crease or fold during the process. [0:00:20]: With the “Nathan” card's adhesive side now exposed, the hands prepare to position and apply it to the baseball sticker, optimizing the alignment and placement.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:06:16",
        "answer": "B",
        "options": [
          "A. Tearing the name \"JOSH\" sticker.",
          "B. Applying the \"Nathan\" name card to the baseball sticker.",
          "C. Cutting a new circular sticker.",
          "D. Reaching for the \"Ryan\" sticker."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_67_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:02]: The video begins with a first-person view of a set of hands holding a piece of art. The hands are placing the art gently onto a white paper sheet on a gray grid-patterned surface. On the top edge of the paper, five circular items, each designed differently with vibrant colors and unique names, are aligned neatly in a row. On the left, we see a tennis ball design with the name \"Ryan.\" Next, there is a basketball design with the name \"Josh,\" followed by a soccer ball design labeled \"Hana.\" To the right, there is a white baseball design labeled \"Callie,\" and farthest to the right, a baseball design with red seams labeled \"Nathan.\" A clear plastic cup is also placed on the paper, and the person is holding a black-lidded bottle upright with their right hand. [0:08:03]: The hands move to the center of the paper, focusing on the plastic cup and bottle. One hand holds the cup to the left, while the other holds the bottle to the right. [0:08:04 - 0:08:06]: The right hand lifts the bottle closer to the camera, showcasing it. The label on the bottle is green with readable text, but the exact text content is unclear from the imagery. The person keeps displaying the bottle. [0:08:07 - 0:08:13]: The hands bring the bottle back to the center of the paper, right above the plastic cup. The person unscrews the bottle cap and starts to prepare the contents. The movement is methodical, ensuring a precise process. [0:08:14 - 0:08:15]: The hands are shown clearly over the cup, indicating the person is possibly about to pour the liquid from the bottle into the cup. The motion is careful, suggesting the person is meticulous about the amount being poured. [0:08:16]: The person now holds the bottle and cap separately, one in each hand, showing readiness to close or change to another material. [0:08:17 - 0:08:19]: The person switches to a pink-capped bottle, demonstrating its preparation similarly to the first bottle. Hands rotate the bottle, perhaps to read instructions or to ensure proper mix before use. The video ends with the person handling this pink-capped bottle delicately.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:08:19",
        "answer": "C",
        "options": [
          "A. Pouring the liquid into the cup.",
          "B. Placing the cap back on the first bottle.",
          "C. Pouring the white granules from the pink bottle slowly into the cup.",
          "D. Holding the cup and bottle separately."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_67_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:16",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      },
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:45",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 1."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_92_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:32",
        "answer": "B",
        "options": [
          "A. 3.",
          "B. 5.",
          "C. 6.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_92_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:10",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 4.",
          "C. 3.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_92_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:17",
        "answer": "B",
        "options": [
          "A. 2.",
          "B. 9.",
          "C. 8.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_92_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why is the teddy bear seen flying in the toy plane in the sky?",
        "time_stamp": "00:24",
        "answer": "C",
        "options": [
          "A. Because someone threw the teddy bear.",
          "B. Because the wind carried the teddy bear.",
          "C. Because Mr. Bean uses a remote control to operate the toy plane.",
          "D. Because the teddy bear has wings."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_242_real.mp4"
  },
  {
    "time": "[0:02:09 - 0:02:39]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does Mr.Bean climb the tree?",
        "time_stamp": "0:02:24",
        "answer": "A",
        "options": [
          "A. Because Mr.Bean wants to retrieve the toy stuck in the tree.",
          "B. Because Mr.Bean wants to collect leaves from the tree.",
          "C. Because Mr.Bean wants a better view from the top of the tree.",
          "D. Because Mr.Bean is practicing climbing skills."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_242_real.mp4"
  },
  {
    "time": "[0:04:18 - 0:04:48]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why are the books stacked on the floor?",
        "time_stamp": "0:04:13",
        "answer": "B",
        "options": [
          "A. Because there was no more space on the shelves.",
          "B. As a step for the bird to walk down from the table.",
          "C. Because the books were being sorted for donation.",
          "D. Because they were recently used and not yet put away."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_242_real.mp4"
  },
  {
    "time": "[0:06:27 - 0:06:57]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the bird fly out of the window with an item?",
        "time_stamp": "0:06:53",
        "answer": "D",
        "options": [
          "A. Because the bird is building a nest and needs materials.",
          "B. Because the bird is curious and wants to explore outside.",
          "C. Because the bird is attracted to shiny objects.",
          "D. Because this item is very precious, the bird wants to steal it away."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_242_real.mp4"
  },
  {
    "time": "[0:08:36 - 0:09:06]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Where did Mr. Bean get all this gold from?",
        "time_stamp": "0:08:07",
        "answer": "C",
        "options": [
          "A. Mr. Bean won it in a lottery.",
          "B. Mr. Bean found it buried in his backyard.",
          "C. The bird kept by Mr. Bean stole from the old lady's house.",
          "D. Mr. Bean inherited it from a distant relative."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_242_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the content of the video just shown?",
        "time_stamp": "00:00:14",
        "answer": "C",
        "options": [
          "A. The vendor set up a beverage station with cups and straws, preparing for customers.",
          "B. The vendor organized a set of condiments and napkins at the counter to facilitate customer use.",
          "C. The vendor puts on glove, arranged hot dog buns, getting ready to serve customers.",
          "D. The vendor cleaned and restocked a salad bar, ensuring it is ready for use."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_377_real.mp4"
  },
  {
    "time": "[0:00:41 - 0:00:51]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions observed just now?",
        "time_stamp": "00:00:51",
        "answer": "C",
        "options": [
          "A. The individual prepared an omelet by whisking eggs and pouring them into a heated pan.",
          "B. The individual assembled a sandwich by layering ingredients onto a slice of bread, cut it, and then served it.",
          "C. The individual took a tortilla with a filling on it, placed it on a tray, and then moved away.",
          "D. The individual arranged cookies on a tray and put them in the oven for baking."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_377_real.mp4"
  },
  {
    "time": "[0:01:22 - 0:01:32]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the content of the video just shown?",
        "time_stamp": "00:01:32",
        "answer": "A",
        "options": [
          "A. The individual is preparing a sandwich by placing slices of meat onto a bun.",
          "B. The individual is making a pizza by spreading cheese and meat on dough.",
          "C. The individual is chopping vegetables for a salad close to a selection of various ingredients.",
          "D. The individual is serving prepared hot dogs along with various condiments."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_377_real.mp4"
  },
  {
    "time": "[0:02:03 - 0:02:13]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the food preparation process just now?",
        "time_stamp": "00:02:13",
        "answer": "C",
        "options": [
          "A. The preparer slices bread, adds mayonnaise, and tosses in fried fish.",
          "B. The preparer grills a hot dog, adds ketchup, and places it into a bun.",
          "C. The preparer slices bread, adds lettuce, and a variety of vegetables, and tops it with chicken pieces.",
          "D. The preparer makes a pizza, adding pepperoni and cheese before placing it into an oven."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_377_real.mp4"
  },
  {
    "time": "[0:02:44 - 0:02:54]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What action best summarizes the individual's task just now?",
        "time_stamp": "00:02:54",
        "answer": "B",
        "options": [
          "A. The individual prepared a salad using fresh vegetables and topped it with dressing.",
          "B. The individual assembled a sandwich by adding various ingredients and condiments.",
          "C. The individual grilled a hamburger patty and added cheese before serving it.",
          "D. The individual deep-fried snacks and served them with dipping sauces."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_377_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:00:10",
        "answer": "B",
        "options": [
          "A. The individual boiled pasta, mixed it with cheese sauce, and served it in a bowl.",
          "B. The individual selected package items, assembled a burger with cheese, and added a meat patty.",
          "C. The individual prepared a salad by washing vegetables, chopping them, and arranging them on a plate.",
          "D. The individual set the table with cutlery, placed a glass of water, and served a main course."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_341_real.mp4"
  },
  {
    "time": "[0:01:09 - 0:01:19]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:01:19",
        "answer": "C",
        "options": [
          "A. The individual boiled pasta, mixed it with cheese sauce, and served it in a bowl.",
          "B. The individual baked a cake, decorated it with icing, and placed it on display.",
          "C. The individual selected packaged items, assembled a breakfast burger with a meat patty, cheese, and a fried egg.",
          "D. The individual prepared a salad by washing vegetables, chopping them, and arranging them on a plate."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_341_real.mp4"
  },
  {
    "time": "[0:02:18 - 0:02:28]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:02:27",
        "answer": "B",
        "options": [
          "A. The individual prepared a pizza by spreading sauce, adding toppings, and placing it in the oven.",
          "B. The individual assembled a burger by retrieving sliced onions from refrigeration, adding them to the buns.",
          "C. The individual baked biscuits, buttered them, and served them with jam.",
          "D. The individual grilled chicken, seasoned it with herbs, and plated it with a side of vegetables."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_341_real.mp4"
  },
  {
    "time": "[0:03:27 - 0:03:37]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:03:37",
        "answer": "B",
        "options": [
          "A. The individual grilled chicken breasts, added seasoning, and placed them on a platter.",
          "B. The individual prepared a burger by retrieving a patty, adding cheese, and assembling it with a bun.",
          "C. The individual baked cookies, sprinkled them with sugar, and placed them on a rack to cool.",
          "D. The individual deep-fried french fries, salted them, and served them in a container."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_341_real.mp4"
  },
  {
    "time": "[0:04:36 - 0:04:46]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:04:46",
        "answer": "B",
        "options": [
          "A. The individual baked cookies, cooled them on a rack, and packaged them in boxes.",
          "B. The individual selected hash browns, packaged them in package.",
          "C. The individual grilled sandwiches, wrapped them, and packed them into boxes.",
          "D. The individual deep-fried chicken wings, added sauce, and arranged them on a plate."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_341_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:14]: The video begins with a first-person perspective showing a person pushing a metal shopping cart in a grocery store. The cart has a grey plastic crate on top, and the person's gloved hands are visible gripping the cart's handle. The area around the cart is tiled, and various grocery items are seen on shelves to the sides. As the cart moves, it passes by a section with fruits and vegetables, including oranges, bananas, cucumbers, and other produce neatly arranged in displays. Boxes and signs with prices are visible on the shelves. [0:00:15 - 0:00:17]: The view shifts slightly to the right, showing more of the produce area with more detail. The shelving displays cucumbers and lemons, which are organized in separate sections. There are also visible price labels for these items.  [0:00:18 - 0:00:19]: The camera focuses directly on the produce section, providing a closer look at the cucumbers and lemons. The cucumbers are wrapped in plastic, and all items are arranged neatly in their respective places. The video concludes with a clear view of the produce.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What object is being pushed at the beginning of the video?",
        "time_stamp": "0:00:20",
        "answer": "B",
        "options": [
          "A. A stroller.",
          "B. A metal shopping cart.",
          "C. A suitcase.",
          "D. A wheelbarrow."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_449_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:01]: At the beginning of the video, there is a display of red tomatoes crowded in a tray. The tomatoes are round and uniformly red, with slight variations in size. [0:04:02 - 0:04:03]: A person with gloved hands reaches into the tray of tomatoes, possibly to rearrange or pick up some of the produce. The gloved hands are visible from a first-person perspective. [0:04:04 - 0:04:05]: The camera pans slightly to the left, revealing more packaged tomatoes in plastic containers and bins. There are also other vegetables visible in different sections, positioned adjacent to each other. [0:04:06 - 0:04:07]: The tray of tomatoes is seen from a closer perspective. The person continues adjusting the tomatoes while still reaching into the tray. [0:04:08 - 0:04:09]: The hands keep adjusting the tomatoes. The background includes packaged tomatoes in plastic wrapping. Other vegetables, like green lettuce heads, are visible to the right. [0:04:10 - 0:04:11]: The movement of the hands continues, accompanied by slight shifts in the view. The surrounding area still shows the organized display of vegetables. [0:04:12 - 0:04:13]: The person’s hands continue handling the tomatoes. A closer look at the neighboring section of the vegetable stall shows green, wrapped lettuce heads and more tomatoes in clusters. [0:04:14 - 0:04:15]: The hands move towards the green, wrapped lettuce. The lettuce heads are organized neatly, and adjacent to them, another section of red tomatoes is visible. [0:04:16 - 0:04:17]: The camera captures a broader view of the vegetable display. Three sections are distinctly noticeable: a section of red tomatoes, a section of wrapped green lettuce heads, and another section of red tomatoes in clusters. [0:04:18 - 0:04:19]: The video ends with a steady frame showcasing all the vegetable sections side-by-side. The labels and prices for the different vegetables are visible at the bottom, indicating a well-organized display at a grocery store.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action is being performed by the person with gloved hands?",
        "time_stamp": "00:04:03",
        "answer": "B",
        "options": [
          "A. Washing the tomatoes.",
          "B. Rearranging and picking up tomatoes.",
          "C. Cutting the tomatoes.",
          "D. Weighing the tomatoes."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_449_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: The camera shows a first-person perspective in a grocery store, with a gloved hand holding a head of lettuce. In the background, there is a section for tomatoes and other vegetables on a display stand. [0:08:01]: The camera points downward, showing tomatoes that have fallen to the floor next to the shopper's feet and a shopping cart. [0:08:02]: A close-up view of the floor shows two tomatoes on the tile, between the feet of the person wearing black shoes. [0:08:03]: The person's gloved hand reaches down to pick up one of the tomatoes from the floor. [0:08:04]: The person has picked up both tomatoes and starts walking, with a shopping cart and more shelves visible in the background. [0:08:05]: The camera shows the person placing the tomatoes back into a shopping basket on the shopping cart. The basket contains various groceries including vegetables and other items. [0:08:06]: The gloved hand holds one tomato close to the display counter, showing a price tag and various other tomatoes and lettuce on the display. [0:08:07 - 0:08:08]: The person is seen placing the tomato back onto the display among other tomatoes, next to neatly stacked lettuce. [0:08:09]: The camera captures a close-up of the display, showing a pile of fresh tomatoes on the right and lettuce on the left.  [0:08:10]: The display shows a price label for the tomatoes and lettuce, with some items falling on the floor. [0:08:11 - 0:08:12]: A wider view shows more of the grocery store display for tomatoes, lettuce, and other vegetables, alongside the prices. [0:08:13]: The camera moves left, showing the neat arrangement of produce (various tomatoes, lettuce, and other vegetables) on the display. [0:08:14]: A gloved hand extends towards packed tomatoes sealed in plastic on the display shelf. [0:08:15 - 0:08:16]: The packed tomatoes are shown in more detail, with clear plastic packaging and labels on a metal display shelf. [0:08:17 - 0:08:18]: The packed tomatoes continue to be shown in close-up detail on the top shelves, with loose, unwrapped tomatoes on the bottom shelves. [0:08:19]: The display shows fresh tomatoes on the right and heads of lettuce on the left. Packages of smaller tomatoes are also visible on the left end of the display.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What does the person do with the tomatoes that fell on the floor?",
        "time_stamp": "0:08:10",
        "answer": "C",
        "options": [
          "A. Leaves them on the floor.",
          "B. Places them in a plastic bag.",
          "C. Puts them back in the shelf.",
          "D. Throws them away."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_449_real.mp4"
  },
  {
    "time": "[0:12:00 - 0:13:00]",
    "captions": "[0:12:00 - 0:12:20] [0:12:00 - 0:12:06]: The video starts by showing a first-person view of someone standing in a grocery store, in front of a display of green apples. They are wearing black clothing and gloves, holding a cardboard tray. The apples are positioned in the top half of the frame, while the tiled floor is visible below. The person begins to move their hands, positioning the tray over the apples. [0:12:07 - 0:12:10]: The view changes slightly as the person continues to pick up apples and place them in the tray. More sections of the fruit display are visible, including other types of produce like oranges and apples. Pricing labels are also visible on the shelves. [0:12:11 - 0:12:14]: The camera perspective shifts as the person moves away from the fruit section. The floor tiles remain consistent, but now a red basket with groceries appears in the view along with other produce sections, including green vegetables and various packaged goods. [0:12:15 - 0:12:19]: The view further adjusts as the person walks through the store. The red basket is seen again in the aisle. The person looks down at their feet, brief movements suggesting walking. The viewer can observe the consistent presence of neatly laid floor tiles and other grocery items lining the shelves. The scene concludes as the camera's focus remains on the floor tiles.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person holding in his hand right now?",
        "time_stamp": "00:12:05",
        "answer": "C",
        "options": [
          "A. A red basket.",
          "B. A plastic bag.",
          "C. A cardboard tray.",
          "D. A shopping list."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_449_real.mp4"
  },
  {
    "time": "[0:15:00 - 0:15:58]",
    "captions": "[0:15:40 - 0:15:58] [0:15:40 - 0:15:56]: In a grocery store, the camera shows a display of green and yellow pears in clear plastic containers stacked on a shelf. Above the shelf, a white sign displays the price “33.95.” The viewer's hands appear, wearing black gloves, interacting with the produce. The background contains more produce like avocados and other fruits in various colors. The gloved hands pick up and inspect several containers of pears. The camera angle is predominantly focused on the pears, with some views showing neighboring produce sections. A partial view of the floor is visible along with more signs displaying prices for various items. [0:15:57]: After inspecting the pears, the gloved hands pick up a paper flyer, focusing briefly on the other produce around. The flyer is held closer to the camera, suggesting a review of discounts or product information.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What price was displayed on the sign above the shelf with the pears?",
        "time_stamp": "00:15:58",
        "answer": "B",
        "options": [
          "A. 29.95.",
          "B. 33.95.",
          "C. 39.95.",
          "D. 35.95."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_449_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:44]: In this segment, we see a wall displaying several framed artworks, all depicting a character that looks like a robot boy. There are two rows of pictures, with four images in total in the top row and three images in the bottom row. The artworks are of various sizes and frames, with colors including black, orange, and metallic gray frames. The background of the wall is white, making the colorful pictures stand out more.  [0:02:45 - 0:02:49]: The camera focuses on a specific artwork in the center featuring the robot boy character with a cheerful expression, arms raised in the air. This piece is framed in a bright orange frame, contrasting with the white matting around the image.  [0:02:50 - 0:02:55]: Next, the camera shifts to another artwork next to the previous one, which showcases the robot boy with one side of his face peeled off, revealing mechanical components. This image has a metallic frame. Both pictures show a combination of artistic illustration with technological elements. [0:02:56 - 0:02:59]: The camera then pans upwards to the top row of images. These two images depict the robot boy in joyful, action-filled poses. Both frames are in black with gold trimming. The images have captions below them with some written information, likely describing the artworks.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the frame of the artwork featuring the robot boy with a cheerful expression and arms raised?",
        "time_stamp": "0:02:59",
        "answer": "B",
        "options": [
          "A. Black.",
          "B. Orange.",
          "C. Metallic gray.",
          "D. Blue."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Event Understanding",
        "question": "What was the general theme of the artworks displayed on the wall just now?",
        "time_stamp": "0:03:00",
        "answer": "C",
        "options": [
          "A. Nature landscapes.",
          "B. Abstract art.",
          "C. A robot boy character.",
          "D. Historical events."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_472_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:24]: Two colorful paintings are displayed on the wall. The left painting shows legs and feet standing over flowing, vibrant water with various colors such as blue, green, and purple. The right artwork features a person in red, embracing another figure under a moonlit sky with trees and stars in the background. Below these paintings, another smaller, colorful artwork is partially visible. [0:05:25 - 0:05:29]: The view shifts slightly to the right, revealing additional paintings. One is predominantly pink and red, showing abstract figures. Beside it, a large painting features a serene tiger and panda on a backdrop of green leaves against an orange sky. Another painting to its right depicts a tiger with a red flower on its head against a blue sky with clouds, standing over a green field. [0:05:30 - 0:05:35]: Further right, beneath the paintings, the name \"NAM JI HYUNG\" is on the wall. There are descriptions or plaques beneath the frames, and an additional framed piece showcases an illustration with leafy themes. Another painting displays a seated, abstract, gray-toned feline with colorful streaks over a black background labeled \"TTODOA.\" [0:05:36 - 0:05:37]: The focus shifts to more artwork on the wall. To the right of \"TTODOA\", there's a framed, minimalist illustration of black and white animals against a light backdrop while another vibrant piece below contrasts with its dark background and colorful elements. [0:05:38 - 0:05:39]: Finally, the rightmost range of paintings displays abstract designs with butterflies, concluding the showcased collection.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the spatial relationship between the tiger and panda painting and the illustration labeled \"TTODOA\"?",
        "time_stamp": "0:05:43",
        "answer": "A",
        "options": [
          "A. The tiger and panda painting is to the left of \"TTODOA\".",
          "B. The tiger and panda painting is to the right of \"TTODOA\".",
          "C. The tiger and panda painting is above \"TTODOA\".",
          "D. The tiger and panda painting is below \"TTODOA\"."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Text-Rich Understanding",
        "question": "What name is displayed beneath the paintings of a tiger with a red flower on its head?",
        "time_stamp": "0:05:35",
        "answer": "B",
        "options": [
          "A. TTODOA.",
          "B. NAM JI HYUNG.",
          "C. \"SERENE\".",
          "D. \"ARTIST\"."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_472_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: The video shows a white-walled gallery space. On the left wall, there is a colorful painting and a display of small objects in transparent boxes. A few people are visible in the background. [0:08:02 - 0:08:09]: The camera gradually zooms in on the transparent boxes mounted on the white wall. Each box contains a unique small object, and the display appears structured in vertical rows. A red dot is visible in the center of the arrangement of boxes. [0:08:10 - 0:08:12]: The focus narrows further on a single box containing a green object, positioned to the left of the red dot. The inside of the box is clear, and shadows are cast on the wall. [0:08:13 - 0:08:14]: The camera shifts slightly to show both the green and red objects in the transparent boxes, still centered around the red dot.  [0:08:15 - 0:08:18]: The perspective changes, and the camera moves to the left to reveal two paintings. The upper painting features a girl with balloons against a blue sky, and the lower painting shows a girl in a red dress next to a giraffe with balloons. [0:08:19 - 0:08:20]: The camera focuses on the upper painting, which displays a girl standing under a cluster of balloons floating in the sky. Hot air balloons appear in the background, adding depth to the scene.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action does the camera take after focusing on the green object?",
        "time_stamp": "0:08:29",
        "answer": "D",
        "options": [
          "A. Zooms out to show the entire gallery.",
          "B. Moves to the floor.",
          "C. Focuses on a different box.",
          "D. Shifts to show a painting of a girl standing under balloons floating in the sky."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_472_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:42]: A large square painting with a complex crisscrossing pattern of white lines on a dark blue background hangs on a wall. A smaller, similar square piece is partly visible beneath it on the left. Both pieces are framed and tightly arranged on the wall. [0:02:43 - 0:02:44]: The view expands slightly to the right, revealing more of the wall. Adjacent to the first piece is another artwork – a square piece with concentric, gradient blue circles creating an optical illusion. Below it are two smaller square artworks with colorful concentric circles in shades of green, blue, yellow, and red. [0:02:45 - 0:02:46]: As the view continues to move right, the large blue-and-white concentric circle piece is shown entirely, along with the two smaller pieces beneath it. Another vibrant circular artwork, this time with layers of colors including purple, green, and blue, shares the wall to the right. [0:02:47 - 0:02:51]: The camera shifts more to the right, exhibiting more colorful works. The vibrant circular piece remains in view, while next to it appears another artwork with a geometric design composed of overlapping colorful triangles. Below these, two additional smaller square pieces with concentric circle designs are seen, one with brighter colors. [0:02:52 - 0:02:55]: The focus moves rightward, featuring the geometrically designed artwork fully. The stark contrast between the dark and bright layers intertwined in triangles creates a vivid display. The circular artworks below this piece are consistently vibrant. [0:02:56 - 0:03:00]: As the camera continues to pass, more pieces with bold geometric and concentric circle designs are on display. The vibrant colors dominate the scene, displaying a modern and eclectic art collection. A white curtain appears, partially obscuring the last artworks while the video ends.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Which of the following best describes the sequence of artworks shown just now?",
        "time_stamp": "00:02:56",
        "answer": "C",
        "options": [
          "A. Large square painting, two smaller square artworks, another concentric circle piece.",
          "B. Concentric circles, geometric triangles, vibrant circular piece.",
          "C. Crisscrossing pattern, concentric circles, geometric triangles.",
          "D. Vibrant circular piece, geometric triangles, crisscrossing pattern."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_468_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:04]: The video begins with a view of a sculpture on a white pedestal against a white wall. The sculpture depicts a person holding a house, and the sculpture has an earthy color palette with shades of beige, brown, and white. As the camera pans slightly to the right, another painting starts to become visible on the adjacent wall. [0:08:04 - 0:08:07]: The view shifts more to the right, revealing a colorful painting beside the sculpture. The painting depicts a surreal landscape with green hills, a winding path, a person sitting in red robes next to a small table, and another person inside a house-like structure. The sky in the painting is depicted in shades of orange and yellow, indicating a sunset. [0:08:07 - 0:08:13]: The camera continues to move, fully unveiling the entire painting. The painting is set against a white gallery wall, and directly above the painting, the text \"Wonsook Kim\" is seen. To the left of the painting, another segment of the gallery wall comes into view, showing additional artworks. These include small drawings and sculptures placed at different heights. [0:08:13 - 0:08:18]: The camera angle shifts again, now focusing on the left wall. This wall is adorned with multiple artworks, including sculptural pieces and framed drawings. The sculptures appear dark and contrast against the white background of the wall. [0:08:18 - 0:08:20]: The video ends with a close-up of the wall displaying various artworks, emphasizing the intricate designs and textures. The artworks are spaced evenly, creating a visually appealing composition.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is depicted in the sculpture shown just now",
        "time_stamp": "00:08:08",
        "answer": "D",
        "options": [
          "A. A person holding a tree.",
          "B. A person holding a book.",
          "C. A person holding a bird.",
          "D. A person holding a house."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "What is located to the left of the sculpture?",
        "time_stamp": "00:08:07",
        "answer": "D",
        "options": [
          "A. A painting depicting people and a river.",
          "B. A doorway.",
          "C. Another sculpture.",
          "D. A painting depicting mountains and people."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Text-Rich Understanding",
        "question": "What text is seen directly above the colorful painting shown right now?",
        "time_stamp": "00:08:10",
        "answer": "D",
        "options": [
          "A. \"Gallery of Modern Art\".",
          "B. \"Sunset Landscape\".",
          "C. \"Art Exhibition\".",
          "D. \"Wonsook Kim\"."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_468_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:48]",
    "captions": "[0:09:40 - 0:09:48] [0:09:40 - 0:09:41]: The video starts in an indoor gallery with white walls. On the left side, there is a large display screen with a collage of images and the name \"Jin Hyu Lee\" displayed above it. There is a woman standing near the exhibit, looking at the display. [0:09:41 - 0:09:43]: The camera moves slightly to the right, focusing more on the display screen. The text \"Jin Hyu Lee\" remains prominent above the collage. There is a QR code and some descriptive text next to the display. [0:09:43 - 0:09:44]: The camera continues to move, revealing more of the gallery space to the right. The large screen with the image collage remains the focal point. [0:09:44 - 0:09:45]: The camera angle shifts further right, showing more exhibits and people in the background. The large screen with the collage is still centered in view. [0:09:45 - 0:09:46]: As the camera moves to the right, more of the gallery space and people walking around are visible. The focus remains on the exhibit of \"Jin Hyu Lee.\" [0:09:46 - 0:09:47]: The camera continues to pan right, capturing additional exhibits and attendees in the gallery. The primary focus remains on the exhibit screen displaying the collage. [0:09:47 - 0:09:48]: The video concludes with the camera still moving slightly to the right. The exhibit by \"Jin Hyu Lee\" remains in focus, and the gallery space and attendees are visible in the background.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What text is displayed above the screen currently shown in the video?",
        "time_stamp": "00:09:43",
        "answer": "D",
        "options": [
          "A. Gallery Name.",
          "B. Exhibit Title.",
          "C. Artist's Signature.",
          "D. Jin Hyu Lee."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_468_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: A person is holding a white plate with a colorful, vibrant dish that includes green vegetables and pasta. The dish is garnished with grated cheese. The background is blurry, showing a hint of a blue color. [0:00:02]: The wider scene reveals a kitchen with modern appliances and wooden cabinets. The person holding the plate is seen in a medium shot. On the countertop, there are several items including bowls, a potted herb, a bottle of olive oil, and some utensils. [0:00:03 - 0:00:05]: The scene transitions with a vibrant graphic display featuring the text \"NXT LVL KITCH N\" in bold, colorful letters. The background is a visually engaging mix of red, yellow, and green hues. [0:00:06]: The previous kitchen scene reappears with an effect making it look as if light is emitting from the countertop, creating a glowing effect. [0:00:07 - 0:00:19]: The person, in front of the kitchen setup, speaks directly to the camera while gesturing with their hands. Various cooking items can be seen clearly on the countertop, including a bottle of olive oil, a lemon, and other kitchen utensils. The lighting is bright, highlighting the well-organized and clean kitchen environment. During this segment, the person continues to engage the viewer with expressive hand movements and speech, occasionally looking down at the countertop, possibly referencing the ingredients and equipment laid out in front of them. The kitchen's modern aesthetic is emphasized with its stainless steel and wooden finishes.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What item is being held by a person at the beginning?",
        "time_stamp": "00:00:20",
        "answer": "A",
        "options": [
          "A. A white plate with pasta.",
          "B. A blue bowl with salad.",
          "C. A black pan with meat.",
          "D. A green cup with tea."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_43_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:21]: A man is standing in a kitchen, working on a wooden cutting board placed on a counter. He's facing forward, focused on chopping or preparing ingredients. The kitchen has a modern design with wooden and black accents, and various kitchen equipment and utensils are visible in the background. [0:03:22 - 0:03:23]: The camera shows a close-up of a frying pan on a stove. Inside the pan, there are small pieces of chopped meat and slices of garlic being sautéed. The ingredients are starting to cook and brown. [0:03:24 - 0:03:26]: The man lifts the frying pan slightly and shakes it to toss the ingredients. There is visible steam or smoke rising from the pan, indicating that the food is being cooked over a high flame. [0:03:27 - 0:03:28]: The man uses a wooden spoon to stir the contents of the frying pan, ensuring even cooking. The close-up shows the pieces of meat and garlic being mixed together. [0:03:29 - 0:03:39]: The cooking process continues with the man frequently stirring and tossing the ingredients in the pan using the wooden spoon. The contents are visibly browning and cooking evenly, with the pieces of meat becoming more golden and the garlic slices getting a caramelized color. The flame under the pan is consistently blue, indicating a controlled cooking environment. The kitchen's modern decor and layout remain visible in the background.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color are the slices of garlic as they are being cooked in the frying pan?",
        "time_stamp": "00:03:39",
        "answer": "A",
        "options": [
          "A. White.",
          "B. Green.",
          "C. Caramelized.",
          "D. Red."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_43_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:46]: A person is stirring a pan containing green peas and diced vegetables on a stove. They use a metal utensil to mix the contents with the flame visible underneath.  [0:06:47 - 0:06:49]: The person places the pan back on the stove and moves around the kitchen, reaching for various ingredients and utensils. The kitchen has wooden cabinets, a countertop with plates and bowls, and a stove with pots on top. [0:06:50 - 0:06:53]: The person turns their attention to a large pot of boiling water where they use tongs to stir the contents, appearing to be pasta. The steam is clearly visible above the pot. [0:06:54 - 0:06:56]: They return to the stovetop and lift the pasta out of the boiling water using tongs, checking its consistency. [0:06:57 - 0:06:59]: They shift focus from the boiling pot back to the countertop, where various ingredients and cooking utensils are arranged. Picking up a piece of lettuce, they begin to inspect it closely. [0:07:00]: The person continues to examine the lettuce, possibly preparing it for the next step in their cooking process. The kitchen environment remains consistent with neatly arranged shelves, cooking equipment, and ingredients in the background.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person using to stir the pot containing noodles?",
        "time_stamp": "00:06:46",
        "answer": "C",
        "options": [
          "A. Wooden spoon.",
          "B. Plastic spatula.",
          "C. Metal tongs.",
          "D. Silicone whisk."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_43_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:11:00]",
    "captions": "[0:10:00 - 0:10:20] [0:10:00]: The video starts with a close-up of a frying pan on a stove. Inside the pan, a mixture of vegetables, including green peas, diced tomatoes, and possibly some herbs, is being cooked. The pan is positioned centrally in the frame, with the visible blue flames of the stove burner beneath it. [0:10:01 - 0:10:03]: The scene shifts to feature a chef, who is standing in a modern kitchen interior with wooden walls and dark-colored shelves holding various kitchen utensils and appliances. The chef, who is wearing a dark T-shirt, lowers a lid onto a pot beside the frying pan, then focuses his attention on the frying pan. [0:10:04 - 0:10:07]: The chef uses a pair of tongs to lift some long pasta, possibly fettuccine, out of a pot of boiling water. The chef then holds the pasta above the pot, letting some water drip off. In the background, the sleek design of the kitchen with various equipment is visible. [0:10:08 - 0:10:11]: The chef transfers the pasta from the pot to the frying pan containing the vegetables. The pasta is placed over the vegetables, and it begins to mix with the contents of the pan. Steam rises from the frying pan, indicating that the ingredients are being cooked or reheated.  [0:10:12 - 0:10:14]: Now holding the frying pan by the handle, the chef uses tongs to stir and toss the pasta and vegetables together, ensuring they are well mixed. The chef appears to be explaining something while cooking, as his mouth is open and he is gesturing slightly. [0:10:15 - 0:10:18]: The camera angle changes to an overhead view, showing the chef's hands as he continues to mix the ingredients in the pan. The pasta is being combined with the vegetables, ensuring an even coating of sauce and ingredients. [0:10:19]: The view returns to focus on the chef. He holds the pan in one hand and gestures towards the camera, likely emphasizing a key point or instruction, as he continues to speak. The kitchen setup remains consistent in the background.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is inside the frying pan right now?",
        "time_stamp": "00:10:00",
        "answer": "A",
        "options": [
          "A. A mixture of vegetables, including green peas and diced tomatoes.",
          "B. Boiling water with pasta.",
          "C. Chicken and herbs.",
          "D. A steak and mushrooms."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Event Understanding",
        "question": "What happens after the chef uses tongs to lift pasta out of the pot?",
        "time_stamp": "00:10:11",
        "answer": "B",
        "options": [
          "A. The chef drains the pasta and sets it aside.",
          "B. The chef transfers the pasta to the frying pan containing vegetables.",
          "C. The chef adds seasoning to the boiling water.",
          "D. The chef puts the pasta back into the boiling water."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_43_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins with the emblem of a football club, featuring a shield design with several colors including red, yellow, and white. This emblem is set against a gradient blue background. At the one-second mark, a partially revealed \"SUBSCRIBE\" text appears beneath the emblem, eventually becoming fully visible in red with white text. [0:00:03 - 0:00:04]: The scene changes to a football pitch where a referee in a neon yellow shirt and black shorts, with various badges, is in the foreground. A football on a stand is centered. In the background, blurred teams are lining up, likely for a match start. [0:00:05 - 0:00:07]: The camera shifts to a handshake line where players from two teams, one in red and blue and the other in green and white, are greeting each other. Both teams wear jerseys and shorts, indicating the start of the game. [0:00:08]: On the sidelines, two coaches exchange a handshake and a brief hug in front of a dugout. Others, including an elderly man and another coach, watch nearby. They are against a background of fans seated in the stands. [0:00:09 - 0:00:10]: The focus is on two players from the same team in red and blue jerseys. Both players have serious expressions and are looking towards a point off-camera, likely during a match moment. [0:00:11]: A close-up shows one of the players in the red and blue jersey, who seems to be in motion or slightly turning. [0:00:12 - 0:00:16]: The action shifts to the gameplay. Several players, primarily in white jerseys with some in red and blue, are positioned around the penalty area, gearing up for an offensive play. The ball is in play, with players situating themselves for potential moves. Advertising boards and crowds are visible in the background. [0:00:17 - 0:00:19]: In another match sequence, players from both teams are actively engaged in chasing and dribbling the ball. The offensive team, in blue and red, makes a move toward the goal, culminating in a shot aimed at the goal. The defenders in green and white strive to intercept, while the goalkeeper and others react to the play in the backdrop. This segment captures the dynamic and tense nature of the attack.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What emblem appears at the beginning of the video?",
        "time_stamp": "00:00:20",
        "answer": "B",
        "options": [
          "A. An animal symbol.",
          "B. A football club shield.",
          "C. A national flag.",
          "D. A corporate logo."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      },
      {
        "task_type": "Causal Reasoning",
        "question": "Why are the players in green and white striving to intercept the ball?",
        "time_stamp": "00:00:19",
        "answer": "C",
        "options": [
          "A. They are warming up for the game.",
          "B. They are practicing their defensive moves.",
          "C. They are reacting to an offensive move toward the goal.",
          "D. They are performing a choreographed play."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_4_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:04]: The football match is taking place on a well-maintained green pitch. The timestamp shows 31:48 minutes in the first half with the score reading 2-0 in favor of the team in blue and red. The attacking team (in purple/red) is advancing towards the goal defended by the team in white. Notable elements include the goalpost at the center-bottom of the frames and several players in action near the penalty area; [0:01:05 - 0:01:09]: The scene changes to a different part of the pitch where the team in white is now closer to the goal, lining up for an attacking opportunity. There are fewer defenders in this sequence, and the player dressed in white is about to attempt a scoring shot. The subsequent frames show the striker managing to get a powerful shot off, while the goalkeeper in orange makes a desperate dive to stop the ball, but ultimately fails to do so; [0:01:10]: Close-up shot of the goalkeeper in orange, looking disappointed, possibly after conceding a goal. The background shows a crowded stadium with numerous fans observing the game; [0:01:11]: Close-up of a player from the team in white. He appears to be reacting to the previous play, his demeanor suggesting focus or determination. The surrounding crowd remains visible in the background, contributing to the lively atmosphere; [0:01:12 - 0:01:14]: The camera angle changes to show a goalpost from behind, providing a clear view of the defensive attempt by the goalkeeper in orange. The attacking team in white is in prominent positions, with one player directly challenging the defense. The approaching ball is captured just before it reaches the goal area; [0:01:15 - 0:01:19]: The play shifts once more, returning to an image of a white team attacking sequence. The forward momentum of the attacking team is evident, and the defenders in purple are retreating towards their goal. One player in white is advancing with the ball, setting up for either a pass or a shot as they approach the penalty box.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What was the score and the position of the teams right now?",
        "time_stamp": "00:01:04",
        "answer": "C",
        "options": [
          "A. 1-0, the team in blue and red is leading.",
          "B. 2-0, the team in white is leading.",
          "C. 2-0, the team in blue and red is leading.",
          "D. 1-1, the teams are tied."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_4_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00]: A close-up action shot of a soccer game, showing the goalkeeper in blue diving to his right attempting to save a shot. The ball is headed towards the goal from the left side of the image. The scoreboard shows the home team leading 3-0, and banners with advertisements are visible in the background. [0:02:01 - 0:02:07]: The perspective shifts to a broader view of the field. Players in purple and white are in action, with the ball being dribbled near the right corner of the goal area by a player in purple. Multiple players from both teams are inside the penalty area, some positioned defensively in anticipation of the attack. [0:02:08 - 0:02:11]: A player in purple is seen running towards the camera after scoring a goal. He is celebrating the goal, as evident from his facial expression and raised hands. The goal is confirmed by the updated score on the top left corner, now standing at 4-0. [0:02:12 - 0:02:15]: The player continues his celebration, heading towards the corner flag. He is joined by his teammates who are approaching him to celebrate together. The stadium is filled with spectators, some of whom are taking photographs or cheering. [0:02:16 - 0:02:18]: The game resumes, and the focus shifts back to another attack from the purple team. Players are moving quickly, with the ball being passed around inside the penalty area. The defensive players in white are trying to block the attacking players' movements. [0:02:19 - 0:02:20]: Another shot on goal is made by a player in purple. The goalkeeper in white dives to try to save the ball, but it is unclear if he succeeds. The defending team appears focused on preventing another goal.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the updated score after the player in number 11 scored a goal?",
        "time_stamp": "00:02:11",
        "answer": "D",
        "options": [
          "A. 2-0.",
          "B. 3-0.",
          "C. 5-0.",
          "D. 4-0."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_4_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:03:51]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:02]: The scene takes place in a soccer stadium during a match. Players in purple and white uniforms are on the field. The camera is positioned at a high angle, showing the attacking team in purple advancing towards the goal defended by players in white. The focus is on the player in purple running along the right side of the pitch, nearing the penalty area; [0:03:03 - 0:03:04]: The camera angle changes to a close-up of the goal area, showing the goalkeeper in blue and some players in white and purple hustling around the six-yard box. The ball is in mid-flight, heading towards the goal. The focus is on the action and the anticipation of a potential goal; [0:03:05 - 0:03:06]: The frame provides a closer view near the goalpost, showing intense competition for the ball between the attacking and defending players. The player in purple attempts to gain control while the player in white tries to clear the ball. The crowd in the background is visible; [0:03:07 - 0:03:09]: The camera switches back to a higher angle, showing the entire half of the pitch. Players are spread out, with the team in purple pushing forward in a coordinated attacking move, while the team in white positions itself for defense. The attacking players are passing the ball among themselves to create an opportunity; [0:03:10 - 0:03:12]: The frame captures the attacking team breaking through the defensive line, with players in purple sprinting towards the goal. A player on the left flank receives a pass and prepares to cross the ball into the penalty area; [0:03:13 - 0:03:16]: The attacking player crosses the ball from the left wing into the box. Players from both teams move into position. The ball is aimed towards the center, causing the defending team to react quickly; [0:03:17]: A close-up captures a player in purple entangled with the goal net, showing visible frustration. The player's emotions suggest a missed opportunity or a close moment during the attack; [0:03:18 - 0:03:19]: The player in purple disentangles from the net and turns to face the field. Another player in blue approaches, indicating a possible conversation or reaction to the recent play. The intensity and expressions reflect the competitive atmosphere of the match.\n[0:03:40 - 0:03:51] [0:03:40]: A group of soccer players are seen on the field, wearing different team uniforms. The player to the left is wearing a red and blue striped jersey with the name \"JOAO CANCELO\" and the number 21 on the back. The player on the right is wearing a similar jersey with the name \"MARCOSA\" and the number 17; [0:03:41 - 0:03:50]: The video transitions to a still image with bold text and graphics. The left side has a stylized \"FORCA BARCA\" logo in yellow and blue on a dark background. Below is a prominent \"SUBSCRIBE\" button in red and white. To the right, a red textured vertical stripe extends the length of the frame. In the middle, \"BEHIND THE SCENES,\" \"TRAINING,\" \"CHALLENGES,\" and \"SHOWS\" are listed repeatedly in alternating yellow and white text; [0:03:51]: The same still image featuring promotional text and the \"SUBSCRIBE\" button remains on the screen.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What uniform does the player named \"JOAO CANCELO\" wear?",
        "time_stamp": "00:03:40",
        "answer": "B",
        "options": [
          "A. Purple and white.",
          "B. Red and blue striped.",
          "C. Blue and yellow.",
          "D. Green and black."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_4_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "Right now, which brand's logo is prominently displayed on the building?",
        "time_stamp": "00:00:04",
        "answer": "D",
        "options": [
          "A. Starbucks.",
          "B. Burger King.",
          "C. KFC.",
          "D. McDonald's."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_386_real.mp4"
  },
  {
    "time": "[0:01:57 - 0:02:02]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "Right now, which type of vehicle is marked with the logo \"VIA Touch\"?",
        "time_stamp": "00:02:00",
        "answer": "D",
        "options": [
          "A. A taxi.",
          "B. A truck.",
          "C. A police car.",
          "D. A van."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_386_real.mp4"
  },
  {
    "time": "[0:03:54 - 0:03:59]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the awning next to the Duane Reade sign right now??",
        "time_stamp": "00:03:55",
        "answer": "A",
        "options": [
          "A. There's no awning.",
          "B. Red.",
          "C. White.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_386_real.mp4"
  },
  {
    "time": "[0:05:51 - 0:05:56]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is indicated by the green sign to the left of the street corner currently?",
        "time_stamp": "00:05:53",
        "answer": "A",
        "options": [
          "A. Bicycle route.",
          "B. Parking is prohibited.",
          "C. Pedestrian zone.",
          "D. One-way street."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_386_real.mp4"
  },
  {
    "time": "[0:07:48 - 0:07:53]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What type of sign is installed at the street corner right now?",
        "time_stamp": "00:07:48",
        "answer": "A",
        "options": [
          "A. One-Way Street.",
          "B. No Parking.",
          "C. Pedestrian Crossing.",
          "D. Speed Limit."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_386_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins with a landscape view of a miniature racing track set against a background painted with blue skies and white clouds. Several toy cars are parked in a fenced-off area with a small campground scene that includes a camper and some trees. Signs with different logos are mounted on the background hills.  [0:00:02 - 0:00:02]: The scene shows toy race cars, including an orange car, a white car, and a blue car, racing down a winding, elevated track. The background features grassy hills with trees and a sign with \"DD\" displayed prominently. [0:00:03 - 0:00:07]: The screen transitions to a title card against a black background, displaying glitchy, colorful text that reads \"NEXT GEN DIECAST RACING.\" Below, there is a subtitle saying, \"The Next Generation of Diecast Racing.\" [0:00:08 - 0:00:17]: The scene cuts to a close-up view of the race track with toy cars positioned at the starting line. The background includes a fenced area behind the track, populated with additional toy cars and various small buildings, such as what appears to be garage stations. Logos like Walmart and other racing-related signs are visible. The cars at the front of the race line include several colorful models: a dark blue car, an orange car with blue accents, a white car, and a red car. [0:00:17 - 0:00:19]: Overlaid text appears on the screen reading \"NEXT-GEN PISTON CUP Race 3 Round 1.\" The text stays on the screen while the cars remain poised at the starting line, ready to race. Some of the cars have moved slightly in preparation for the race.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What does the subtitle below the title card read right now?",
        "time_stamp": "0:00:05",
        "answer": "C",
        "options": [
          "A. \"The New Era of Toy Racing\".",
          "B. \"The Future of Car Racing\".",
          "C. \"The Next Generation of Diecast Racing\".",
          "D. \"Ultimate Toy Car Showdown\"."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_489_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:45]: The video opens with a view of four racing cars lined up on a track, preparing for the start of a race. The cars are positioned in a staggered formation, with two rows visible. The front row consists of cars with numbers #2.0 and #21, while the second row includes cars #123 and #94. The track is bordered by green grass and trees, and the backdrop is a painted sky with clouds. [0:02:46 - 0:02:49]: The scene remains focused on the racers at the start line, with the viewer getting a slightly angled perspective of the lineup. The backdrop and cars stay in the same positions, maintaining the anticipation of the upcoming race. [0:02:50 - 0:02:52]: The camera starts panning to the right, capturing the movement of the vehicles as they speed down the track. The landscape features rolling hills and includes several trees. The cars remain tightly grouped as they race. [0:02:53 - 0:02:55]: The scene transitions to a broader view of the racetrack, with the cars beginning to slightly spread out. The infield contains various colorful toy cars and a central pit area with a direct view of the multiple structures and advertisements. [0:02:56 - 0:02:59]: The camera angle shifts to follow the cars as they navigate through more of the track. The perspective emphasizes the track’s twists and turns, with the race advancing and cars speeding past. Key structures and branded areas come into view as the action intensifies, showing the cars rapidly moving along the course.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which car is in the front row along with car #2.0 at the start of the race?",
        "time_stamp": "00:02:45",
        "answer": "A",
        "options": [
          "A. Car #21.",
          "B. Car #123.",
          "C. Car #94.",
          "D. Car #3."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_489_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:24]: A green and purple race car with the number 68 and the name H.J. Hollis on the sides is shown on the racing track, with textual information displayed near the car: “Position: 2nd,” “Heat Wins: 8,” “Fastest Time: 8.535,” “Group 2,” and “Team: Krime Sindakit Racing.” The car has a sleek, aerodynamic shape and is positioned on the black racing track. In the background, a grid fence is visible with blurred colorful objects behind it. [0:05:25]: The screen transitions to an animated logo that reads \"NEXT GEN DIECAST RACING\" against a background of diagonal stripes in varying shades of green with black. [0:05:26 - 0:05:39]: A broader view of the race starting area shows four colorful race cars lined up on a model racetrack labeled \"Thunder Hollow Speedway.\" The background depicts a painted scene with clouds and trees, giving a miniature, scenic appearance. The first row features the purple and green car (#68 H.J. Hollis) and a blue and purple car (#54 Herb Curbler). Text on-screen indicates this is \"Group 2 - Heat 1.\" The second row includes a blue car (#19 Danny Swervez) and a pink car (#36 Rich Mixon). The track is bordered by green terrain, and small trees are positioned to the side of the track.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Which racing car won first place in this competition?",
        "time_stamp": "0:05:51",
        "answer": "B",
        "options": [
          "A. The pink car.",
          "B. The dark green car.",
          "C. The light green car.",
          "D. The blue car."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_489_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00-0:08:13]: A scoreboard showing the standings of Heat 3, detailing the positions, car numbers, and points of four racers. The background consists of a small, lush green hillside with some trees and a model residential setting featuring houses and roads. The board lists Herb Curbler in first place with 13 points, Rich Mixon in second place with 10 points, H.J. Hollis in third place with 6 points, and Danny Swervez in fourth place with 4 points. The perspective is over a racetrack. [0:08:14-0:08:20]: Four race cars lined up at the starting line of a racetrack, with a backdrop featuring green trees, grassy terrain, and a partly cloudy sky. The cars are brightly colored, including pink, blue, green, and another green one with distinct racing designs. The scene is labeled as \"Group 2 - Heat 4,\" with Danny Swervez and H.J. Hollis positioned in the front row, indicated by the text below the image.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What event is labeled \"Group 2 - Heat 4\" in the video?",
        "time_stamp": "00:08:27",
        "answer": "B",
        "options": [
          "A. The finish line celebration.",
          "B. The cars at the starting line.",
          "C. The awards ceremony.",
          "D. The scoreboard update."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      },
      {
        "task_type": "Counting",
        "question": "How many points does the second-place racer in Heat 3 have?",
        "time_stamp": "00:08:17",
        "answer": "C",
        "options": [
          "A. 4 points.",
          "B. 6 points.",
          "C. 10 points.",
          "D. 13 points."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_489_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:01:06",
        "answer": "A",
        "options": [
          "A. The addition of decimals.",
          "B. Converting fractions to decimals.",
          "C. The subtraction of decimals.",
          "D. The rules for multiplying decimals."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_227_real.mp4"
  },
  {
    "time": "[0:02:31 - 0:03:01]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What is the teacher likely to explain next?",
        "time_stamp": "00:03:00",
        "answer": "A",
        "options": [
          "A. How to correctly align the decimal points when adding.",
          "B. How to subtract decimal numbers.",
          "C. How to multiply decimal numbers.",
          "D. How to divide decimal numbers."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_227_real.mp4"
  },
  {
    "time": "[0:05:02 - 0:05:32]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:05:28",
        "answer": "C",
        "options": [
          "A. How to multiply whole numbers.",
          "B. The importance of carrying over digits.",
          "C. The process of correctly placing the decimal in the product.",
          "D. How to add the products from multiplication."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_227_real.mp4"
  },
  {
    "time": "[0:07:33 - 0:08:03]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What topic might the speaker explain next?",
        "time_stamp": "00:08:02",
        "answer": "C",
        "options": [
          "A. The steps for decimal addition.",
          "B. The steps for decimal subtraction.",
          "C. The principles of decimal division.",
          "D. The basics of whole number multiplication."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_227_real.mp4"
  },
  {
    "time": "[0:10:04 - 0:10:34]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:10:33",
        "answer": "C",
        "options": [
          "A. Simplifying square roots.",
          "B. Finding least common multiples.",
          "C. Shifting the decimal point in both numbers.",
          "D. Converting decimals to fractions."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_227_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video shows a person running towards a parked dark van on a city street at night. The person is wearing a high-visibility vest and is approaching the vehicle from the rear. The background reveals brightly lit storefronts with colorful neon signs and advertisements. [0:00:03 - 0:00:06]: The person, still in motion, reaches the side of the van and is about to open the driver’s side door. The streetlights illuminate the van, showing partial details of its body design, as shopfronts continue to light up the background. [0:00:07 - 0:00:08]: The person begins opening the van's driver-side door and stepping inside while the street and storefronts remain visible in the background. [0:00:09 - 0:00:11]: The person fully gets into the driver's seat of the van. The view then switches to an outside perspective, showing the van parked on the side of the street. The storefronts, particularly the shop named \"SUBURBAN,\" are clearly visible. [0:00:12 - 0:00:14]: The video maintains the external view of the dark van, with the person seen through the driver's side window. The \"SUBURBAN\" shop, with multiple light signs and window displays, continues to be a significant background feature. [0:00:15 - 0:00:17]: The camera angle remains steady, focused on the parked van in front of the \"SUBURBAN\" storefront, showing the person inside the vehicle. [0:00:18 - 0:00:20]: The video continues to display the parked van in the same position outside \"SUBURBAN.\" The surroundings maintain the vibrant night city ambiance with bright and colorful shop signs. [0:00:21 - 0:00:23]: The shot switches to a close-up view of the interior of the van, specifically focusing on the inventory management screen. The person is seen interacting with the interface, sorting items labeled as \"Paper,\" \"glowstick,\" and a \"backpack.\" [0:00:24 - 0:00:26]: The interaction continues with the inventory interface, showing more detailed options like \"Open glovebox.\" This segment focuses entirely on the interface. [0:00:27 and Onward]: The video shifts to a different screen displaying \"Trucking Progression,\" detailing various job options and locations such as \"Los Santos 24/7 Drop\" and \"Sandy Shores Diner Supply.\" The person is navigating these options on the screen.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:00:08",
        "answer": "A",
        "options": [
          "A. watching his phone.",
          "B. Turning on the headlights of the van.",
          "C. Closing the van's rear door.",
          "D. Opening the van's side door."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_282_real.mp4"
  },
  {
    "time": "0:02:40 - 0:03:00",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:50]: The video shows a person wearing a reflective vest and a mask standing next to a large dark-colored vehicle with the word “grime” visible on the side. They are holding a mobile phone and appear to be reading or typing on it. The scene is outdoors on a pavement. In the top-left corner, there is an inset of another person, possibly streaming or commenting live, with chat messages appearing on the right side. [0:02:51 - 0:02:58]: The person in the reflective vest continues to look at the phone as additional chat messages appear on screen. The inset still shows the live streamer's face and background. [0:02:59 - 0:03:06]: There is a subtle change in the surrounding light, suggesting time passing. The individual remains focused on the phone. The inset and the chat remain active. [0:03:07 - 0:03:10]: The person lowers the phone slightly but continues to hold it. Their attention remains on the screen, indicating they are focused on the contents. [0:03:11 - 0:03:14]: The camera shows the person interacting with the phone, possibly scrolling or selecting on the device. The environment stays constant with the same vehicle in the background. [0:03:15 - 0:03:18]: The person in the reflective vest slightly turns towards the vehicle but continues to keep their focus on the phone screen. [0:03:19 - 0:03:21]: Chat messages continue to appear on the right side of the screen, with the person maintaining their position next to the vehicle.  [0:03:22]: The person looks away from the phone momentarily. The surrounding environment, including the dark vehicle with the \"grime\" logo, remains unchanged. [0:03:23 - 0:03:27]: The person briefly glances around before returning their attention to the phone. The inset live stream and chat messages continue uninterrupted. [0:03:28 - 0:03:30]: The person in the reflective vest looks intently at the phone, possibly selecting or reading something important. The vehicle and pavement background stay consistent. [0:03:31]: The person turns more towards the camera, putting their phone away. The chat on the right shows more messages, and the inset still features the live streamer. [0:03:32 - 0:03:33]: As the person in the vest looks slightly away from the phone, they appear to be contemplating their next actions. The scene retains its exterior urban setting. [0:03:34 - 0:03:36]: The individual in the reflective vest is seen operating another screen, possibly accessing inventory or item interface. The inset with the streamer remains visible, and chat messages continue to update. [0:03:37 - 0:03:39]: A different interface is displayed on the screen, showing what seems to be a checklist or progression list. Labels such as \"Trucking Progression\" and several entries are visible. The person still appears to be focused on interacting with this digital interface. [0:03:40 - 0:03:42]: The progression list with various truck routes or tasks is clearly displayed on the screen. The person is likely reading the details or preparing to select an option. The inset and chat messages continue to show active engagement from the live streamer.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person in the reflective vest doing right now?",
        "time_stamp": "00:03:30",
        "answer": "A",
        "options": [
          "A. Checking the stock list.",
          "B. Inspecting the vehicle.",
          "C. Chatting with the live streamer.",
          "D. Walking away from the vehicle."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_282_real.mp4"
  },
  {
    "time": "0:05:20 - 0:05:40",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:22]: The video starts with a man wearing a green safety vest standing behind a box truck. He appears to be interacting with the vehicle, with a prompt displayed on the screen for opening the rear of the truck. [0:05:23 - 0:05:25]: The scene shifts to an inventory interface on a screen, showing various items categorized under \"Player,\" \"Cargo,\" \"Backpack,\" and \"Ground.\" The man seems to be transferring an item, as indicated by a menu that shows a stack of nine items being adjusted. [0:05:26 - 0:05:27]: The inventory interface remains on the screen, with slight changes. The man continues organizing items within the inventory. [0:05:28]: The inventory interface is still visible, with continued item management. The user interface details remain largely the same as previous frames. [0:05:29 - 0:05:31]: The scene switches back to the man standing behind the box truck. He is about to close the rear of the truck, as indicated by an on-screen prompt. [0:05:32 - 0:05:33]: The man walks toward a dark van labeled \"grime\" parked on the side of the street and approaches the driver’s side door. [0:05:34 - 0:05:36]: He reaches the driver's side door of the van, preparing to enter as displayed by his posture. [0:05:37 - 0:05:38]: The man is now seen opening the driver's side door and starting to get into the vehicle. [0:05:39 - 0:05:40]: The perspective changes to inside the van, showing a street view from the windshield as the man starts driving through a city street. The environment includes a dimly lit street, lined with buildings on both sides. [0:05:41 - 0:05:42]: The van is driving down the street, moving past parked cars and buildings. Street lights and a few illuminated signs are visible. [0:05:43 - 0:05:44]: The van continues moving, with slight turns indicating following the road. The surroundings remain urban with various storefronts and buildings. [0:05:45 - 0:05:46]: The van proceeds down the straight road, with buildings and advertisements visible in the background and along the sidewalks. [0:05:47 - 0:05:48]: The van is seen approaching an intersection. The surrounding area includes more buildings, street signs, and some parked cars. [0:05:49 - 0:05:50]: The van reaches the junction of two streets, preparing to make a turn, with visible traffic lights and crosswalks in view. The environment includes a gas station and several taller buildings.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the man doing right now?",
        "time_stamp": "00:05:40",
        "answer": "A",
        "options": [
          "A. Driving a van through a city street.",
          "B. Standing behind a box truck.",
          "C. Walking toward a dark van.",
          "D. Trying to open a box."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_282_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:02]: The video starts on a highway with multiple lanes in each direction. The vehicle, seen from a first-person perspective, is moving on the right side of the road. The road is flanked by rocky hillsides on either side. Several cars are visible driving in the opposite direction and a bridge is seen across the highway in the distance. [0:08:03 - 0:08:08]: The view shifts slightly, and the vehicle continues to move forward. The driver's hands are visible on the steering wheel with a green-lit dashboard. There is a constant visible chat overlay on the right side of the screen indicating a live stream. [0:08:09 - 0:08:11]: The vehicle proceeds towards the bridge in the distance. The surroundings remain largely unchanged with trees and power lines lining the road. [0:08:12 - 0:08:15]: The car passes under the bridge, and the focus remains on the dashboard and the road ahead. There is some chat activity visible on the overlay. [0:08:16 - 0:08:18]: Night vision activates, and the vision turns neon green, which changes the look of the environment significantly. The driver is seen interacting with the car controls. [0:08:19 - 0:08:21]: The night vision effect deactivates, returning to the usual view. The vehicle continues driving down the highway. The road signs and other vehicles are now visible again in their natural colors. [0:08:22 - 0:08:24]: A music player interface appears momentarily, and the driver seems to interact with it. The surroundings, mostly the roadside and nearby hills, stay the same. [0:08:25 - 0:08:27]: The video transitions back to regular driving footage, showing the car approaching a large sign hanging across the highway.  [0:08:28 - 0:08:30]: The vehicle continues down the highway while the scenery comprises trees, power lines, and continuing road views. The chat overlay continues to update with viewer messages.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "How did the environment change just now?",
        "time_stamp": "00:08:18",
        "answer": "A",
        "options": [
          "A. The vision turned neon green due to the activation of night vision.",
          "B. The vehicle stopped at a traffic light.",
          "C. The driver switched to a different lane.",
          "D. The car encountered a roadblock."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_282_real.mp4"
  },
  {
    "time": "0:10:00 - 0:10:20",
    "captions": "[0:10:00 - 0:10:20] [0:10:00 - 0:10:06]: A large van is navigating a curvy mountain road, with the view from the back left side. The van has visible branding and lettering on the sides. The background features a mountainous landscape with sparse vegetation and a mix of dirt and greenery. A pole is slightly obstructing part of the view as the van continues to turn. [0:10:07 - 0:10:10]: The van completes its turn onto a straight road. The perspective shifts to mostly behind the vehicle, with an expansive view of the road ahead lined with trees and mountains in the distance. Power lines and poles are also visible. [0:10:11 - 0:10:42]: The van drives down the straight road with trees and power lines lining the side. The surrounding scenery includes a mix of tall green trees and some additional structures like billboards and powerline towers. The road stretches straight ahead into the distance, leading towards mountains.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the van do just now?",
        "time_stamp": "00:10:07",
        "answer": "A",
        "options": [
          "A. Turned left onto a straight road.",
          "B. Pulled into a rest area.",
          "C. Stopped at a red light.",
          "D. Entered a tunnel."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_282_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the older woman appear annoyed or angry?",
        "time_stamp": "00:00:26",
        "answer": "D",
        "options": [
          "A. Because the character shouted loudly.",
          "B. Because the character knocked on the door.",
          "C. Because the character broke something.",
          "D. Because the character disturbed her."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_247_real.mp4"
  },
  {
    "time": "[0:02:15 - 0:02:45]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the man in the dirty car wave a money?",
        "time_stamp": "00:02:32",
        "answer": "A",
        "options": [
          "A. Because he wanted Mr. Bean to help him wash his car.",
          "B. Because he was offering to buy something from Mr. Bean.",
          "C. Because he wanted to pay Mr. Bean for directions.",
          "D. Because he was asking Mr. Bean for change."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_247_real.mp4"
  },
  {
    "time": "[0:04:30 - 0:05:00]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the person start pedaling the bicycle?",
        "time_stamp": "00:04:55",
        "answer": "C",
        "options": [
          "A. Because the person wants to generate electricity for a nearby device.",
          "B. Because the person is testing the bicycle for a friend.",
          "C. Because riding a bicycle can start the car wash machine.",
          "D. Because the person needs to warm up before a race."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_247_real.mp4"
  },
  {
    "time": "[0:06:45 - 0:07:15]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why do the woman and the driver sit down to have tea together?",
        "time_stamp": "00:06:58",
        "answer": "D",
        "options": [
          "A. Because the woman and the driver know each other.",
          "B. Because the man on the bicycle invited them to tea.",
          "C. Because the car broke down and they had nothing else to do.",
          "D. Because the driver's car is being cleaned."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_247_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:30]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why is Mr. Bean looking for something now?",
        "time_stamp": "00:08:35",
        "answer": "A",
        "options": [
          "A. Because he wanted to find something to clean the bird droppings that had fallen on the car.",
          "B. Because he lost his keys and needs to find them.",
          "C. Because he wants to find a tool to fix a flat tire.",
          "D. Because he is searching for his misplaced wallet."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_247_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the primary color of the lever located on the left side of the cockpit?",
        "time_stamp": "00:00:02",
        "answer": "C",
        "options": [
          "A. Red.",
          "B. Yellow.",
          "C. Blue.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_425_real.mp4"
  },
  {
    "time": "[0:01:15 - 0:01:20]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the predominant color of the device on the right side of the cockpit?",
        "time_stamp": "00:01:17",
        "answer": "D",
        "options": [
          "A. Green.",
          "B. Blue.",
          "C. Yellow.",
          "D. Black."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_425_real.mp4"
  },
  {
    "time": "[0:02:30 - 0:02:35]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is visible on the screen in the cockpit right now?",
        "time_stamp": "00:02:34",
        "answer": "B",
        "options": [
          "A. A speedometer.",
          "B. A map.",
          "C. An altimeter.",
          "D. A radar."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_425_real.mp4"
  },
  {
    "time": "[0:03:45 - 0:03:50]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the current weather condition outside the cockpit?",
        "time_stamp": "00:03:47",
        "answer": "B",
        "options": [
          "A. Clear skies.",
          "B. Cloudy skies with light rain.",
          "C. Snowfall.",
          "D. Heavy thunderstorms."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_425_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:05:05]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the pilot holding in his left hand right now?",
        "time_stamp": "00:05:07",
        "answer": "B",
        "options": [
          "A. A joystick.",
          "B. A blue handle.",
          "C. A map.",
          "D. A throttle lever."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_425_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What does the individual seem to be doing just now?",
        "time_stamp": "00:00:10",
        "answer": "A",
        "options": [
          "A. The individual is sorting order tickets and preparing them for meal preparation.",
          "B. The individual is cleaning the countertop and washing dishes.",
          "C. The individual is slicing vegetables and arranging them on plates.",
          "D. The individual is taking orders from customers and entering them into a computer."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_412_real.mp4"
  },
  {
    "time": "[0:02:07 - 0:02:17]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individuals taken just now?",
        "time_stamp": "0:02:17",
        "answer": "B",
        "options": [
          "A. The chef picks up plates and arranges them on the counter, then prepares meat for cooking.",
          "B. The chef did nothing, and colleagues selected fresh herbs to a container and others cook around the area.",
          "C. The chef sets the table with forks and knives, while colleagues prepare beverage orders.",
          "D. The chef chops vegetables and adds them to a soup, while colleagues focus on dessert preparation."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_412_real.mp4"
  },
  {
    "time": "[0:04:14 - 0:04:24]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken by the individual just now?",
        "time_stamp": "00:04:24",
        "answer": "C",
        "options": [
          "A. The individual served a finished dish to the customer and took a moment to clean the workspace.",
          "B. The individual cooked a meal, plated it, and served it to a fellow chef in the kitchen.",
          "C. The individual prepared and served several vegetables while coordinating with another chef in the busy kitchen.",
          "D. The individual took ingredients from storage, prepped them, and started cooking them on the stove."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_412_real.mp4"
  },
  {
    "time": "[0:06:21 - 0:06:31]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions just now?",
        "time_stamp": "00:06:31",
        "answer": "B",
        "options": [
          "A. The individual boiled water, added pasta, and prepared a seafood dish.",
          "B. The individual sautéed vegetables, seasoned them, and organized various ingredients on the counter.",
          "C. The individual chopped vegetables, grilled them, and set them aside for plating.",
          "D. The individual baked pastries, garnished them, and prepared them for serving."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_412_real.mp4"
  },
  {
    "time": "[0:08:28 - 0:08:38]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual just now?",
        "time_stamp": "00:08:37",
        "answer": "B",
        "options": [
          "A. The individual cleaned a table, set it with plates and cutlery, and prepared for serving customers.",
          "B. The individual grabbed tongs, plated cooked greens on a dish, added sauce, and moved the dish to the side.",
          "C. The individual grilled meat on a barbecue, basted it with sauce, and added it to a serving tray.",
          "D. The individual blended ingredients in a mixer, poured them into a baking dish, and placed the dish in an oven."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_412_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the traffic light now?",
        "time_stamp": "00:00:18",
        "answer": "B",
        "options": [
          "A. Yellow.",
          "B. Red.",
          "C. Green.",
          "D. Black."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_181_real.mp4"
  },
  {
    "time": "[0:02:02 - 0:02:22]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the position of the National Championships banner right now?",
        "time_stamp": "00:02:16",
        "answer": "C",
        "options": [
          "A. On the front of the truck.",
          "B. On the building to the left.",
          "C. Hanging above the road.",
          "D. On the ground below the cyclists."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_181_real.mp4"
  },
  {
    "time": "[0:04:04 - 0:04:24]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the cyclist with the number 845 positioned right now?",
        "time_stamp": "00:04:00",
        "answer": "B",
        "options": [
          "A. Directly ahead of the cyclist with the number 88.",
          "B. On the left side of the cyclist with the number 851.",
          "C. Behind the cyclist with the number 88.",
          "D. On the right side of the cyclist with the number 88."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_181_real.mp4"
  },
  {
    "time": "[0:06:06 - 0:06:26]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the position of the cyclist with race number \"#865\" right now?",
        "time_stamp": "00:06:12",
        "answer": "B",
        "options": [
          "A. At the front of the group.",
          "B. On the left side of the cyclist with the number 806.",
          "C. On the right side of the cyclist with the number 806.",
          "D. Leading a solo breakaway."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_181_real.mp4"
  },
  {
    "time": "[0:08:08 - 0:08:28]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the traffic light now?",
        "time_stamp": "00:08:19",
        "answer": "B",
        "options": [
          "A. Yellow.",
          "B. Red.",
          "C. Green.",
          "D. Black."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_181_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:01]: A hand points towards a grey baking dish filled with white flour. Other ingredients like cooking oil, sauces, and various bowls are scattered around the countertop. A larger glass mixing bowl with a whisk inside sits to the left. [0:03:02]: The scene shifts slightly, showing more of the prepared ingredients on the counter. The person is seen from the waist down, with a hand resting on the table and the other off-frame. Several more bottles and jars are visible. [0:03:03]: A man in a white t-shirt stands in a modern kitchen with grey cabinets. He gestures with his right hand while explaining something. In the background, there are cupboards with glass doors and a counter with various kitchen items. [0:03:04]: The man continues to talk, emphasizing his points with animated hand movements. A wooden cutting board and various ingredients are laid out on the counter in front of him, including what appears to be raw fish on a plate. [0:03:05]: The man rearranges some of the items on the countertop while continuing to speak. His gestures are lively and expressive. [0:03:06]: He holds a small mixing bowl with one hand and points toward it with the other. The kitchen setting remains the same, with various utensils and ingredients visible. [0:03:07]: The man is now showing another ingredient, holding it up for emphasis. His facial expression is intense as he explains the next step of the preparation. The stove and kitchen counter are visible in the foreground. [0:03:08]: He continues to explain, using his hands to illustrate his points more clearly. The plate with the fish remains in the forefront on the countertop. [0:03:09]: The man leans forward slightly, focusing on illustrating his point, possibly explaining the next action with his hand movements. [0:03:10]: He picks up a container of seasoning and a bottle of another ingredient, showing both to the camera. The backdrop remains the same, with a clean, modern kitchen setup. [0:03:11]: The man pours a particular ingredient from a tall white container into the mixing bowl. He has a focused look on his face, concentrating on getting the measurements right. [0:03:12]: Showing the close-up view, he explains the importance of the condiments he is using, holding them up to the camera. The mixing bowl lies on the counter below. [0:03:13]: He begins to add the spices directly into the mixing bowl, methodically pouring them while looking down, focusing on his task. [0:03:14]: He gestures again towards the camera, emphasizing the importance of the steps he's explaining. His right hand holds the container while the left hand makes a clear gesture. [0:03:15]: He picks up a small pinch of one of the ingredients, holding it up close to the camera while explaining its significance, ensuring the viewer can see the detail. [0:03:16]: The man resumes mixing the ingredients in the bowl, explaining further steps while looking down and concentrating on his actions. [0:03:17]: He pours what appears to be milk from a mug into the mixing bowl, demonstrating the next crucial step in the process. [0:03:18]: His hand places the now-empty mug on the counter, ensuring the camera focuses on the mixing bowl and its contents. Various other ingredients lie around, showing the preparation involved. [0:03:19]: The man’s face displays concentration as he continues to explain and perform the mixing process, ensuring each detail is carefully shown to the viewer.\n[0:03:20 - 0:03:40] \n[0:03:40 - 0:04:00] ",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the baking dish filled with flour?",
        "time_stamp": "0:03:01",
        "answer": "B",
        "options": [
          "A. Blue.",
          "B. Grey.",
          "C. White.",
          "D. Black."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_19_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:01]: A person wearing a white t-shirt is standing in the kitchen, holding a glass bowl with a mixture and a whisk. The background shows a modern kitchen setup with white tiles, dark cabinets, and various utensils and ingredients on the counter. [0:06:01 - 0:06:03]: The person continues to mix the contents of the bowl vigorously. A green bottle and other kitchen items are placed on the counter in front of them. [0:06:03 - 0:06:04]: The person briefly pauses and uses the whisk to scrape down the sides of the bowl, ensuring all the mixture is combined. [0:06:04 - 0:06:05]: The person resumes whisking, tilting the bowl to ensure thorough mixing. The kitchen counter now shows a mix of different ingredients, including bottles and bowls. [0:06:05 - 0:06:06]: The person continues the mixing process with focus, scraping the sides of the bowl with the whisk. The kitchen background remains consistent, with various utensils and items on the counter. [0:06:06 - 0:06:08]: The person continues whisking the mixture in the bowl. The kitchen environment shows clear glass cabinets, and a neatly organized space, including some stacked bowls and plates. [0:06:08 - 0:06:10]: The person's movements stay steady as they whisk. The camera focus remains on the whisking action, while the kitchen setup in the background shows more clearly stocked utensils and ingredients neatly arranged. [0:06:10 - 0:06:12]: The mixture in the bowl appears to be thickening slightly as whisking continues, with the person tilting the bowl for better mixing. The surrounding kitchen remains unchanged, maintaining a tidy appearance. [0:06:12 - 0:06:18]: The person then pours the mixture from the bowl into a pot on the stove. The camera angle shifts slightly to show a close-up of the mixture being transferred, with steam beginning to rise from the heated pot. A cup of coffee can be seen on the counter nearby, while the person adjusts the heat on the stove. [0:06:18 - 0:06:19]: The person places the bowl down and reaches for another utensil on the counter. The stove now has the pot with the mixture heating up, and the surrounding counter is filled with various kitchen items used in the preparation process.\n[0:06:20 - 0:06:40] \n[0:06:40 - 0:07:00] ",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What item does the person use to mix the contents of the bowl?",
        "time_stamp": "00:06:03",
        "answer": "C",
        "options": [
          "A. A spoon.",
          "B. A fork.",
          "C. A whisk.",
          "D. A spatula."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Object Recognition",
        "question": "What is the person wearing while standing in the kitchen?",
        "time_stamp": "00:06:01",
        "answer": "B",
        "options": [
          "A. A blue apron.",
          "B. A white t-shirt.",
          "C. A black jacket.",
          "D. A red sweater."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_19_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:03]: The video starts with a first-person perspective view of a kitchen. A man in a white t-shirt is standing at the counter. In front of him, on the countertop, there is a cutting board with sliced potatoes and a bottle of beer next to it. He picks up a paper towel from the counter and uses it to dry the potato slices. [0:09:04 - 0:09:05]: The man moves briskly to his right, reaching for something off-frame. Behind him, the kitchen counter has various items including a microwave, a mug rack, and some jars. He appears to grab a towel and turns back towards the stove.  [0:09:06 - 0:09:07]: The man focuses on the stove where there are two frying pans on the burners. One frying pan appears to have something frying in it. He uses a spatula to manipulate the food in the frying pan. [0:09:08 - 0:09:09]: He moves closer to the frying pan and begins to flip the golden-brown piece of food cooking in hot oil. The bubbling oil and sizzling sound suggest the food is being deep-fried. [0:09:10 - 0:09:12]: The man carefully continues to cook the food, using a spatula to ensure it's cooking evenly. The surface area of the stovetop includes a griddle and an unused burner. Utensils and a kitchen towel are visible to the side. [0:09:13 - 0:09:15]: He then flips the piece of food to reveal a crispy, evenly-browned side. He moves the spatula and checks the doneness of the food by lifting and peeking underneath. [0:09:16 - 0:09:19]: The video concludes with the man continuing to cook the food, occasionally flipping it and inspecting its color. The kitchen background remains unchanged with various cooking items and appliances neatly arranged.\n[0:09:20 - 0:09:40] \n[0:09:40 - 0:10:00] ",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is on the countertop in front of the man right now?",
        "time_stamp": "0:09:02",
        "answer": "C",
        "options": [
          "A. A loaf of bread and a bottle of wine.",
          "B. A bowl of fruit and a bottle of juice.",
          "C. A cutting board with sliced potatoes.",
          "D. A plate of vegetables and a bottle of water."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_19_real.mp4"
  },
  {
    "time": "[0:12:00 - 0:13:00]",
    "captions": "[0:12:00 - 0:12:20] [0:12:00 - 0:12:06]: The video begins with a person wearing a white t-shirt standing in a kitchen. The countertop in front of them is made of marble. They are stirring something in a small bowl with a spoon, other cooking items such as a lemon and a cup are visible nearby. The background shows a clean and organized kitchen with cabinets and a stove. [0:12:07 - 0:12:09]: The person, while holding a towel in one hand, moves toward the stove where a frying pan is positioned over a burner. The pan has something sizzling inside, likely cooking. [0:12:10 - 0:12:13]: The content in the frying pan continues to cook as the person uses a spoon to stir it, ensuring even cooking. The pan contents are turning golden brown.  [0:12:14 - 0:12:15]: A close-up shot of the frying pan reveals slices of food being cooked and stirred. The person uses a spoon to mix and move the slices around in the bubbling oil. [0:12:16 - 0:12:17]: The person continues stirring the food in the frying pan, appearing focused on ensuring they do not stick together. [0:12:18 - 0:12:19]: The food in the frying pan is becoming crispy as the person uses the spoon to gently stir and flip them for even cooking.\n[0:12:20 - 0:12:40] \n[0:12:40 - 0:13:00] ",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the countertop made of in the kitchen scene?",
        "time_stamp": "0:12:06",
        "answer": "C",
        "options": [
          "A. Granite.",
          "B. Wood.",
          "C. Marble.",
          "D. Stainless steel."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_19_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why did Mr. Bean turn off the TV?",
        "time_stamp": "00:00:51",
        "answer": "C",
        "options": [
          "A. Because he thought the show was too boring.",
          "B. Because he wanted to save electricity.",
          "C. Because he was afraid that the news about the theft of teddy bears on TV would scare his own teddy bear.",
          "D. Because he heard a noise outside and wanted to investigate."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_239_real.mp4"
  },
  {
    "time": "[0:02:12 - 0:02:42]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why Mr. Bean is shocked now.",
        "time_stamp": "0:02:45",
        "answer": "A",
        "options": [
          "A. Because he found that his little bear doll was missing.",
          "B. Because he just realized his wallet is missing.",
          "C. Because he saw something unexpected on TV.",
          "D. Because someone knocked on his door unexpectedly."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_239_real.mp4"
  },
  {
    "time": "[0:04:24 - 0:04:54]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why is Mr. Bean unhappy now?",
        "time_stamp": "00:04:51",
        "answer": "B",
        "options": [
          "A. Because his favorite TV show was canceled.",
          "B. Because he found that his teddy bear doll had changed its appearance.",
          "C. Because he lost an important document.",
          "D. Because he was unable to fix a broken appliance."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_239_real.mp4"
  },
  {
    "time": "[0:06:36 - 0:07:06]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does Mr. Bean decide to put up missing posters for his teddy bear?",
        "time_stamp": "0:06:47",
        "answer": "C",
        "options": [
          "A. Because he thought someone might have stolen his teddy bear.",
          "B. Because he wanted to warn others about a possible teddy bear thief.",
          "C. Because he realized the teddy bear he found earlier was not his original teddy bear.",
          "D. Because he believed his teddy bear had run away and needed help finding it."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_239_real.mp4"
  },
  {
    "time": "[0:08:48 - 0:09:18]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the purple bunny character decide to chase Mr. Bean?",
        "time_stamp": "0:09:08",
        "answer": "A",
        "options": [
          "A. Because Mr. Bean took away all of their dolls.",
          "B. Because Mr. Bean made fun of the bunny's appearance.",
          "C. Because Mr. Bean accidentally stepped on the bunny's foot.",
          "D. Because Mr. Bean broke the bunny's favorite toy."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_239_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video begins showing a first-person perspective view of a player on a football (soccer) field. The player is dribbling the ball towards another player dressed in a blue and yellow jersey. The field is well-lit and the ground is grassy. The other player is positioned slightly to the front and right of the camera's viewpoint, preparing to engage. [0:00:04 - 0:00:05]: As the player continues dribbling, they begin to move diagonally towards the left. The ball is being controlled with consistent touches, and the view shifts to include another player approaching from the right, wearing a similar attire. [0:00:06 - 0:00:09]: The player with the camera speeds up slightly, moving the ball with their feet. The opponent in the blue and yellow jersey begins to come closer, attempting to intercept the ball. The camera dips and rises, indicating potential evasive maneuvers being performed by the camera-wearer to avoid losing possession of the ball. [0:00:10 - 0:00:13]: The player makes a pass or strong kick as the viewpoint suddenly shifts forward, indicating the ball has been sent away. The camera captures the ball traveling across the field towards another opponent. The action is fast, with the view shifting quickly to track the ball's movement. [0:00:14 - 0:00:17]: The video continues with the player regaining control of the ball after it has been kicked or passed, dribbling it again while facing more opponents. The play maintains a dynamic pace, emphasizing the fluid nature of the game. The lights of the field can be seen in the background, enhancing the visibility of the action. [0:00:18 - 0:00:20]: Towards the end of the clip, the camera shows the player dribbling up the field, the view stabilized while the ball is closely controlled at their feet. The field ahead is open, with only a few players visible in the distance.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What color is the jersey of the approaching player?",
        "time_stamp": "0:00:03",
        "answer": "B",
        "options": [
          "A. Red and white.",
          "B. Blue and yellow.",
          "C. Green and black.",
          "D. White and black."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_258_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:02]: The video begins with a player in a blue and yellow jersey running across the field. The scene then shifts to a wide-open field with a few scattered players visible in the distance. [0:02:02 - 0:02:03]: The camera focuses on a soccer ball in the center of the field, indicating the player's intent to approach or interact with the ball. [0:02:03 - 0:02:05]: The ball is kicked and flies toward the camera. The camera then swivels quickly to the right, clearly following the ball's trajectory. More players and parts of the field come into view as the camera shifts its focus. [0:02:07 - 0:02:10]: The camera shifts back to the center of the field under bright floodlights, with visible player shadows on the ground. The scoreboard and distant players are visible in the background. The word \"Hey!\" appears, indicating someone is shouting or trying to get attention. [0:02:10 - 0:02:13]: The camera continues to follow a player in a dark blue jersey with the number 12 on the back, who runs toward the ball positioned near the center of the field. [0:02:14 - 0:02:15]: The camera shows an arm reaching out toward the soccer ball on the field, likely indicating the player's attempt to control or maneuver the ball. [0:02:16]: The focus is on the feet of two players as they approach the ball, showing close-up action and physical engagement for control. [0:02:17 - 0:02:19]: The camera follows the number 12 player in the dark blue jersey. Text at the bottom mentions the player's move and the opponent's successful block, indicating a specific failed soccer maneuver.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What object does the camera focus on in the center of the field right now?",
        "time_stamp": "0:02:03",
        "answer": "A",
        "options": [
          "A. A soccer ball.",
          "B. A water bottle.",
          "C. A player's shoe.",
          "D. A goalpost."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What number is on the back of the player in the blue jersey right now?",
        "time_stamp": "0:02:13",
        "answer": "C",
        "options": [
          "A. 10.",
          "B. 8.",
          "C. 12.",
          "D. 7."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_258_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:01]: The video starts with a first-person view on a soccer field during a night match. The field is brightly lit. Several players in different colored jerseys are seen scattered on the field. The camera is focused on one player in a blue jersey slightly to the right, in possession of the ball. Shadows of several people are cast on the grass. [0:04:01 - 0:04:02]: The camera moves forward, following the player in blue who is now running forward with the ball. The white lines marking the penalty area can be seen on the left. [0:04:02 - 0:04:03]: The ball is closer to the camera, moving towards the white line marking the edge of the penalty area. The goalpost and net are visible in the background, and a player, possibly a goalkeeper, is standing in front of the goal. [0:04:03 - 0:04:04]: The camera view is shaky and close to the ground, indicating a possible fall or slide. Legs and feet of players, one with black and another with white shoes, are visible in the foreground. [0:04:04]: The ball is in the net. One player is on the ground inside the goal area displaying excitement or exhaustion. Another player in a blue jersey stands nearby. The perspective indicates the goal was just scored. [0:04:05]: The camera captures the field and players moving back toward their positions after the goal. The word \"GOAL\" appears on the screen. [0:04:06]: Players are gradually moving back to their positions for the restart while the camera moves back. [0:04:07]: Players are seen regrouping. Some are congratulating each other. [0:04:08]: A replay of the pass leading to the goal begins. The camera focuses on a player in a yellow jersey passing the ball forward. The captions read, \"Min's sensuous pass.\" [0:04:09]: Continuation of the replay showing the ball moving towards the intended target. [0:04:10 - 0:04:11]: The ball reaches a player in a blue jersey who prepares to shoot at the goal. The captions still read, \"Min's sensuous pass.\" [0:04:11 - 0:04:12]: The player in the blue jersey takes the shot aimed at the goal. The captions read, \"Jay's finishing.\" [0:04:13 - 0:04:14]: The ball approaches the goal and a player can be seen advancing towards it. [0:04:15 - 0:04:19]: The camera captures the celebrating players from a distance as they return to their positions. Captions reflect post-goal commentary: \"The movement was so good! I had to pass it!\" The frame shows the broader view of the field under bright lights.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What can be seen in the background right now?",
        "time_stamp": "00:04:04",
        "answer": "B",
        "options": [
          "A. The penalty area.",
          "B. The goalpost and net.",
          "C. The midfield line.",
          "D. The audience stands."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_258_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:01]: The video begins showing a person from a first-person perspective on a soccer field. The camera is facing another player who is about to kick a soccer ball. Both players are on a well-lit green field with various advertisements visible in the background. [0:06:02]: The camera moves closer to the ball as the person controlling the camera appears to be running. [0:06:03]: The perspective shifts as the camera gets very close to the ball and another player is seen in the distance, possibly a defender. [0:06:04]: The person with the camera appears to be dribbling the ball, with their arms seen motioning out, and a defender approaches. [0:06:05]: The perspective shifts to show a broader view of the field, new players in the distance, and the ball being dribbled forward. [0:06:06 - 0:06:10]: The person successfully performs a dribbling skill, the “Ronaldo’s chop,” maneuvering past a defending player. Closer to the goal, another player appears to potentially pose a challenge. [0:06:11 - 0:06:12]: The person gets fouled, resulting in a tumble to the ground. The camera perspective shows the grass on the field up close while a player is seen running away. [0:06:13 - 0:06:14]: Repeats the earlier clip where \"Ronaldo's chop\" is successfully performed to dodge an opponent. [0:06:15]: Shows the broader view of the field again with the person dribbling the ball forward. [0:06:16 - 0:06:19]: The camera focuses on the ball being maneuvered past yet another opposing player, with the defenders actively attempting to intercept. The video maintains the first-person viewpoint, emphasizing the player's movements and the strategic dodging.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What skill did the person perform to get past the defender?",
        "time_stamp": "00:06:08",
        "answer": "C",
        "options": [
          "A. Step-over.",
          "B. Cruyff turn.",
          "C. Ronaldo’s chop.",
          "D. Maradona turn."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_258_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:01 - 0:00:07]: The video begins with a black screen that transitions into a visual with purple and black hues, accompanied by horizontal lines and static noise. A colorful, oval-shaped logo appears in the center of the screen, featuring a yellow background with blue and red triangles and the text \"GRAVITY THROTTLE RACING.\" [0:00:08 - 0:00:09]: The scene changes to a miniature diorama of a racetrack. Several colorful model train cars, including red, yellow, and white ones, are positioned around train tracks. The backdrop consists of a mountainous landscape with a blue sky. [0:00:10 - 0:00:13]: The view zooms out to reveal more of the racetrack and surrounding area. Numerous miniature vehicles are parked beside the track, including various race cars and toy trucks. The cars are in multiple colors, such as blue, red, black, and yellow. Several small figurines are also visible near the edge of the track. [0:00:14 - 0:00:17]: The camera angle shifts to show two toy dinosaur figures on a green field, surrounded by a few small people figurines. The dinosaurs, one brown and one green, stand in the center interacting with each other. Nearby, a few other diorama elements, such as a small tent and another toy truck, are seen. [0:00:18 - 0:00:19]: The final scene displays a busy diorama setup featuring multiple cars lined up in front of a miniature gas station. Several toy figurines are scattered around the scene, engaging in various activities. The backdrop includes a Texaco sign and a canopy tent, adding detail to the miniature environment.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What logo appears on the screen during the initial scene transition?",
        "time_stamp": "0:00:10",
        "answer": "A",
        "options": [
          "A. GRAVITY THROTTLE RACING.",
          "B. RACING THUNDER.",
          "C. SPEED CIRCUIT.",
          "D. TRACK RUNNERS."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Clips Summarize",
        "question": "What has been shown in the video so far?",
        "time_stamp": "0:00:21",
        "answer": "A",
        "options": [
          "A. A series of scenes featuring a miniature diorama with various toy elements.",
          "B. A documentary on real-life racing events.",
          "C. A detailed explanation of a new racing game.",
          "D. A tutorial on setting up a model train track."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_492_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:20 - 0:07:40] [0:07:20 - 0:07:21]: The video begins with a red car mid-jump over a green, water-like area between two rocky, snow-covered terrains. The background is primarily blue with hints of white. There is a black billboard with unreadable text ahead, alongside another sign on the left.  [0:07:21 - 0:07:24]: As the car lands, it drives up a snow-covered slope that starts to curve gently to the left. Sparse vegetation and small trees are scattered around the incline.  [0:07:24 - 0:07:27]: The car continues to drive along a winding snowy road surrounded by undulating hills and several coniferous trees. The background shows larger snow-covered hills or small mountains. [0:07:27 - 0:07:28]: The car navigates a sharp left curve on the road, which runs along the edge of a rocky cliff. Some green shrubs and grass are visible on the side of the road. [0:07:28 - 0:07:31]: The car continues around another tight bend, now transitioning to a downhill section. The surroundings become a mix of rocky terrain with sparse, dark-green trees and some bushes. [0:07:31 - 0:07:34]: The car completes the turn and briefly slows down as a green car and a white truck appear ahead on the same snowy slope. [0:07:34 - 0:07:37]: The car keeps moving on the winding road, passing the slower green car on a slight incline. Snow and rocky terrains remain consistent in the backdrop. [0:07:37 - 0:07:39]: The car continues to maneuver through the turns, moving past a camera crane, ascending and descending small slopes, and approaching a more open, sandy landscape with fewer trees.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What happens after the red car lands from the jump?",
        "time_stamp": "0:07:00",
        "answer": "B",
        "options": [
          "A. It drives into a dense forest.",
          "B. It continued to move forward on the sandy ground.",
          "C. It stops at a rocky cliff.",
          "D. It enters a tunnel."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_492_real.mp4"
  },
  {
    "time": "[0:11:00 - 0:12:00]",
    "captions": "[0:11:00 - 0:11:20] [0:11:00 - 0:11:07]: Four small toy race cars are positioned at the starting line of a snow-themed miniature racetrack. The cars have different colors and designs, including green, black, white, and grey. The track is made of white material resembling snow, with five lanes running parallel to each other. Nearby, a green sign reads “RACE 7,” listing the participants' names: Crazy Canuck, Maree Mapelli, Scooter, and Greg. The background features a section of blue wall and elevated snowy terrain; [0:11:08 - 0:11:09]: The race begins, and the cars rapidly accelerate down the initial straightaway. The green and black cars are seen slightly ahead of the others as they head towards a steeper slope that leads to a bridge over an icy river. The various colored cars stay close together as they move along the narrow lanes, now against a blue mountainous backdrop; [0:11:10 - 0:11:11]: The cars maneuver around the curve of the terrain, staying close to each other. The white car is leading, with the black and green cars closely trailing. The snowy landscape and steep slopes give the appearance of a challenging and complex track; [0:11:12 - 0:11:13]: Three of the cars continue on the track, navigating through a series of turns and snowy banks, while one car starts to lag slightly behind. The background now includes small model trees sparsely placed across the snowy terrain, adding to the wintery theme of the racetrack; [0:11:14 - 0:11:20]: As the race progresses, the cars navigate through sharp turns and elevation changes. The path curves sharply, and the toy cars follow the contours of the track closely. The grey and black cars consistently remain in the lead throughout the turns. The rocky and snowy landscape features more evergreen model trees, enhancing the scenic quality of the toy racetrack.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the track currently displayed in the video made up of?",
        "time_stamp": "00:11:07",
        "answer": "D",
        "options": [
          "A. Asphalt.",
          "B. Sand.",
          "C. Grass.",
          "D. Snow."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_492_real.mp4"
  },
  {
    "time": "[0:13:00 - 0:14:00]",
    "captions": "[0:13:40 - 0:14:00] [0:13:40 - 0:13:52]: The video shows a chaotic scene resembling a diorama featuring model cars, dinosaurs, and miniature human figures. Two large dinosaur models, one green and one orange, appear to be in a confrontation, with the green dinosaur holding a toy car in its mouth above the orange dinosaur. Numerous toy cars and trucks are arranged around the dinosaurs, some stacked on top of each other, creating a busy and crowded environment. Several miniature human figures are positioned near the dinosaurs and vehicles. In the foreground, toy railroad tracks cross from one side of the scene to the other. In the background, there is a scenic backdrop depicting green hills and a blue sky;  [0:13:53 - 0:13:54]: The frame transitions to a display screen with colorful, semi-abstract patterns resembling interference, indicating a possible transition or glitch effect in the video;  [0:13:55 - 0:13:59]: The screen now shows a logo for \"GRAVITY THROTTLE RACING\" on a yellow, blue, and red geometric design. It remains displayed until the end of the video.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the green dinosaur holding on its head?",
        "time_stamp": "0:14:00",
        "answer": "D",
        "options": [
          "A. A miniature human figure.",
          "B. A piece of railroad track.",
          "C. A toy truck.",
          "D. A toy car."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_492_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the pilot's headset right now?",
        "time_stamp": "00:00:03",
        "answer": "C",
        "options": [
          "A. Red.",
          "B. Blue.",
          "C. Black.",
          "D. White."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_424_real.mp4"
  },
  {
    "time": "[0:02:02 - 0:02:07]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is in the pilot's left hand right now?",
        "time_stamp": "00:02:05",
        "answer": "B",
        "options": [
          "A. A map.",
          "B. A control stick.",
          "C. A headset.",
          "D. A smartphone."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_424_real.mp4"
  },
  {
    "time": "[0:04:04 - 0:04:09]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the top of the control stick handle right now?",
        "time_stamp": "00:04:06",
        "answer": "C",
        "options": [
          "A. Red.",
          "B. Blue.",
          "C. Yellow.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_424_real.mp4"
  },
  {
    "time": "[0:06:06 - 0:06:11]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the scene below the helicopter now?",
        "time_stamp": "00:06:09",
        "answer": "B",
        "options": [
          "A. A lake.",
          "B. A forest.",
          "C. A mountain.",
          "D. A city."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_424_real.mp4"
  },
  {
    "time": "[0:08:08 - 0:08:13]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What text is displayed on the screen right now?",
        "time_stamp": "00:08:10",
        "answer": "C",
        "options": [
          "A. \"I apologize for the inconvenience. The footage contains private information.\".",
          "B. \"The video has been temporarily removed.\".",
          "C. \"Unfortunately I had to remove the footage because there was private information on an aircraft that was shown.\".",
          "D. \"I lost some footage, so I'll record more in another video. Sorry!\"."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_424_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual taken just now?",
        "time_stamp": "00:00:10",
        "answer": "C",
        "options": [
          "A. The individual arranged dishes and served them to a customer in a kitchen setting.",
          "B. The individual wiped down the countertops and sanitized the kitchen utensils.",
          "C. The individual selected a glass, added ice from the ice maker.",
          "D. The individual took various tools from a drawer and began assembling a device."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_354_real.mp4"
  },
  {
    "time": "[0:02:14 - 0:02:24]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual taken just now?",
        "time_stamp": "00:02:24",
        "answer": "B",
        "options": [
          "A. The individual made a sandwich using fresh ingredients and served it on a plate.",
          "B. The individual filled a pitcher with milk and steamed it.",
          "C. The individual organized cutlery in a drawer and cleaned the kitchen area.",
          "D. The individual cleaned the steaming wand and organized cups on a shelf."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_354_real.mp4"
  },
  {
    "time": "[0:04:28 - 0:04:38]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual taken just now?",
        "time_stamp": "00:04:39",
        "answer": "B",
        "options": [
          "A. The individual cleaned the countertop and put away kitchen utensils.",
          "B. This person cleaned the kitchen utensils used for making coffee.",
          "C. The individual prepared a sandwich, wrapped it up, and served it to a customer.",
          "D. The individual restocked supplies in a pantry and washed dishes."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_354_real.mp4"
  },
  {
    "time": "[0:06:42 - 0:06:52]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual taken just now?",
        "time_stamp": "00:06:52",
        "answer": "B",
        "options": [
          "A. The individual arranged dishes on a table and served a customer.",
          "B. The individual measured and cleaned the portafilter, weighed handle filter, and prepared it for brewing.",
          "C. The individual wiped down the counters and organized kitchen utensils.",
          "D. The individual prepared a sandwich, wrapped it, and handed it to a customer."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_354_real.mp4"
  },
  {
    "time": "[0:08:56 - 0:09:06]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual taken just now?",
        "time_stamp": "00:09:06",
        "answer": "B",
        "options": [
          "A. The individual poured a latte into a to-go cup and handed it to a customer.",
          "B. The individual set up the handle filter,and prepared an espresso shot.",
          "C. The individual measured coffee grounds and cleaned the grinder.",
          "D. The individual operated the espresso machine, steamed milk, and cleaned the milk frother."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_354_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the position of the lake right now?",
        "time_stamp": "00:00:06",
        "answer": "D",
        "options": [
          "A. On the left side.",
          "B. Behind the cyclist.",
          "C. Parallel to the road.",
          "D. At the foot of the mountains ahead.."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_298_real.mp4"
  },
  {
    "time": "[0:02:55 - 0:03:15]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the cyclist heading right now?",
        "time_stamp": "00:03:14",
        "answer": "D",
        "options": [
          "A. Back to the starting point.",
          "B. Towards an open field.",
          "C. Down a paved road.",
          "D. Towards a dense forest."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_298_real.mp4"
  },
  {
    "time": "[0:05:50 - 0:06:10]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the yellow caution sign located right now?",
        "time_stamp": "00:05:52",
        "answer": "D",
        "options": [
          "A. On a tree to the right.",
          "B. Ahead on the ground.",
          "C. Behind the cyclist.",
          "D. On a tree to the left."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_298_real.mp4"
  },
  {
    "time": "[0:08:45 - 0:09:05]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the cyclist heading right now?",
        "time_stamp": "00:08:55",
        "answer": "D",
        "options": [
          "A. Up a steep hill.",
          "B. Across a wooden bridge.",
          "C. Through an open field.",
          "D. Down a narrow trail."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_298_real.mp4"
  },
  {
    "time": "[0:11:40 - 0:12:00]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the cyclist approaching right now?",
        "time_stamp": "0:11:49",
        "answer": "D",
        "options": [
          "A. A rocky path.",
          "B. A steep uphill.",
          "C. A narrow bridge.",
          "D. Some fallen trees."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_298_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video starts with a black screen.  [0:00:01 - 0:00:07]: A man with short dark hair and a light olive green t-shirt is sitting at a white table against a plain gray background. He appears to be talking and gesturing with his hands. On the right side of the table, there is a black object that resembles a computer component. [0:00:08]: A hand is lifting a black mesh panel. Below the mesh panel, there seems to be an open computer case. [0:00:09]: Two hands are holding a graphics card and positioning it inside the open computer case. [0:00:10]: One hand is placing a cooler master component into the computer case. [0:00:11]: A hand is making adjustments to components within the computer case, ensuring they are secure. [0:00:12 - 0:00:13]: The perspective changes to the bottom of the computer case. A hand is holding some cables and positioning them as if to manage the cable layout. [0:00:14 - 0:00:17]: The perspective shifts back to the top of the case where hands are positioning the graphics card, ensuring it fits properly inside the case.  [0:00:18 - 0:00:19]: The video returns to the man at the white table, where he continues to gesture and talk, maintaining the same seated position as earlier.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action are the hands performing right now?",
        "time_stamp": "00:00:15",
        "answer": "A",
        "options": [
          "A. Positioning the graphics card inside the case.",
          "B. Lifting a black mesh panel.",
          "C. Installing a light olive green t-shirt.",
          "D. Talking at a white table."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_116_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:03:20]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:03]: The video starts with a close-up view of a computer case's internals. The open case reveals two large cooling fans mounted side by side on a component at the top. The fans have black blades with a circular, metallic center. The interior of the case and the fans appear to be mainly black, with some minor details and text visible on the components. [0:03:03 - 0:03:04]: A close-up shot of a dark mesh panel on the computer case is shown, with a \"GEFORCE GTX\" logo illuminated in white on the right side and some small, red-lit indicators on the left side. [0:03:04 - 0:03:07]: The view then transitions to a person’s hands assembling or handling computer components. They hold a metal bracket and inspect it while on a dark surface that contains additional computer parts, some of which are in plastic bags. [0:03:07 - 0:03:10]: The individual places the bracket onto the back of a small motherboard. Various components and circuits are visible on the motherboard, which is primarily black with some silver and copper details. The person fits the bracket precisely over the CPU slot and connectors. [0:03:10 - 0:03:13]: The focus returns to the computer case. The individual holds and examines two metal mounting brackets. Next, they lower the motherboard into the case, carefully positioning it into place. [0:03:13 - 0:03:17]: The individual installs a large graphics card into the case. The graphics card, marked with the \"ROG\" logo, is black with three cooling fans. After positioning it correctly, they secure it into the case's slot. [0:03:18 - 0:03:19]: The video concludes with a close-up view of a screen displaying GPU thermals data for various cooling configurations. The data includes temperature, noise level, and various cooler models and speeds.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which logo is illuminated in white on the dark mesh panel right now?",
        "time_stamp": "0:03:03",
        "answer": "C",
        "options": [
          "A. GEFORCE GT.",
          "B. GTX ROG.",
          "C. GEFORCE RTX.",
          "D. GEFORCE ROG."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_116_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:06:20]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:01]: The video starts with a close-up of the interior of a computer case from a top-down perspective. The case is open, revealing components such as cooling pipes, a cable, and a power supply. The cables are black, and there is a glowing blue light emanating from one of the components;  [0:06:02 - 0:06:04]: The view switches to a different angle, showing the computer case standing on a flat surface. The case has a matte black finish, and it is illuminated with a soft blue light. The background is blurred, indicating a shallow depth of field;  [0:06:05 - 0:06:07]: The camera returns to a top-down view of the computer case's interior, similar to the opening shots. The glow from the blue light is slightly more prominent, and the arrangement of components remains unchanged;  [0:06:08 - 0:06:19]: The scene changes to a man seated at a table with the computer case to his left. He is casually dressed in a green shirt and appears to be discussing or explaining something about the computer case, gesturing with his hands and changing his expressions throughout. The background is plain and white, focusing all attention on the man and the computer case.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the light glowing from one of the components right now?",
        "time_stamp": "00:06:25",
        "answer": "C",
        "options": [
          "A. Red.",
          "B. Green.",
          "C. Blue.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_116_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:20]",
    "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:05]: The video begins with hands holding a graphics card above the motherboard of an open computer case on a purple background. The graphics card, large and black with complex patterns and a logo, is being positioned above the motherboard slot. The case is empty apart from a few scattered screws. The person's hands are steady as they align the card with the slot. [0:09:06 - 0:09:09]: The scene shifts to show the back of a black computer case on a white desk. The user connects multiple cables to the ports at the bottom rear of the case. A keyboard with a single highlighted purple key is partially visible on the left side of the screen, lying on the desk. [0:09:10 - 0:09:12]: The focus returns to the inside of the computer case, revealing the installed graphics card. The interior is well-lit with RGB lighting, showcasing a detailed look at various components, including cooling fans and the motherboard. The colors from the lighting shift from blue to pink, adding visual flair to the hardware. [0:09:13 - 0:09:15]: The camera zooms in on the illuminated side of the graphics card, focusing on the spinning fans. The card's RGB lighting accentuates the branding and details, with purple and blue hues prominently featured. The fans are in motion, indicating the system is powered on. [0:09:16]: The video cuts to another close-up shot of a different section of the internal components featuring a \"PHANTEKS\" branded part with RGB lighting. The lighting adds blue and green accents to the grey component, emphasizing its sleek design. [0:09:17]: The following frame shows a blurred view of a computer case in the background, accentuated by RGB lighting. The lighting setup around the case bathes it in various colors, although the subject is not clearly visible. [0:09:18 - 0:09:19]: Lastly, the camera concentrates on another internal component branded \"NZXT.\" This section of the hardware shows intricate cabling and other interconnected parts. The RGB lighting provides colorful highlights, focusing on a strip of multi-colored lights attached to the internal setup.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What brand name is shown on the GPU right now?",
        "time_stamp": "00:09:13",
        "answer": "C",
        "options": [
          "A. ASUS.",
          "B. CORSAIR.",
          "C. MSI.",
          "D. EVGA."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_116_real.mp4"
  },
  {
    "time": "[0:11:00 - 0:11:20]",
    "captions": "[0:11:00 - 0:11:20] [0:11:00 - 0:11:01]: The video shows a computer setup on a black desk. The computer case is silver with a mesh top, and its side panel is removed, exposing the interior components, including RGB-lit fans. Beside the computer case, a mouse sits on the left side of the desk, and a few screws and a screwdriver lie on the right side of the desk. The background has a gradient of blue and purple hues, giving a futuristic ambiance. [0:11:02 - 0:11:19]: A person with short dark hair wearing a green T-shirt is seated at a white table. On the left side of the table, there is a large black rectangular object with vents on the sides and front, possibly another computer case or an electronic device. The individual is talking and using hand gestures while smiling occasionally. The background is plain and light grey, making the person and the black object stand out. The person continues to speak and make various hand gestures, sometimes placing their right arm on the table or pointing with the left hand.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What items are beside the computer case right now?",
        "time_stamp": "00:11:01",
        "answer": "A",
        "options": [
          "A. A screwdriver and mouse.",
          "B. A pair of pliers and a keyboard.",
          "C. A keyboard.",
          "D. A set of cables."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_116_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:20]: The video shows a first-person perspective of a settings menu within a game, which seems to be set up for vehicle control configurations. The background of the settings menu has a slightly blurred greenish hue with what appears to be the top of a vehicle, possibly a car, partially visible. The menu itself is organized into columns and rows. On the top, there are several tabs, with the \"SETTINGS\" tab active. Various functions related to vehicle controls are listed, such as \"Vehicle Turn On/Off\" and \"Vehicle Toggle Light Indicator,\" alongside buttons to which they are mapped. On the left side are additional categories like \"GENERAL\" and \"VEHICLES\" in a sidebar menu. To the top-left, part of a video stream window is visible, showing a person in a room sitting in front of a computer setup with multiple monitors, keyboards, and other equipment. The person is engaged in speaking, as indicated by their open mouth and varying facial expressions. To the right of the settings menu, several chat messages are visible, indicating interaction from viewers. The overall action is taking place within the context of a gaming environment, with the player adjusting settings while being observed by an audience.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the character controlled by the player now?",
        "time_stamp": "00:00:20",
        "answer": "A",
        "options": [
          "A. In the blue car.",
          "B. In the black car.",
          "C. In the red car.",
          "D. In the white car."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_273_real.mp4"
  },
  {
    "time": "0:02:40 - 0:03:00",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:44]: The video starts inside a car, viewed from a first-person perspective. The car is driving through a tunnel. The view showcases hands on the steering wheel, the dashboard, and the tunnel's interior. The driver is following the yellow center line on the road. [0:02:45 - 0:02:53]: The car continues through the tunnel. The tunnel structure is consistent, lit by artificial lights, and the vehicle is shown maintaining its path in the middle of the lane, indicating steady driving. In the driver's peripheral view, adjacent buildings and streetlights are visible. [0:02:54 - 0:03:00]: The car exits the tunnel, transitioning to an open road, revealing a cityscape with buildings and palm trees ahead. The vehicle starts to veer left, preparing for a turn, indicating a change in direction. The scene shifts to a third-person perspective, showing the vehicle from behind as it turns left onto another road. The car is blue, and the street appears to be in an urban area, with modern buildings and some vegetation. The vehicle navigates around a bend and enters a parking area with multiple levels. It drives into an open space surrounded by parked cars and buildings, finally coming to a stop.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the car doing right now?",
        "time_stamp": "00:02:54",
        "answer": "C",
        "options": [
          "A. It exited a tunnel and stopped abruptly.",
          "B. It drove straight through the tunnel without turning.",
          "C. It veered left and entered a parking area.",
          "D. It turned right and sped up on an open road."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_273_real.mp4"
  },
  {
    "time": "0:05:20 - 0:05:40",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:25]: A man wearing a striped shirt stands facing another person in a military-style outfit with a helmet. The man with the striped shirt appears to be interacting with a device in his hand while the other person looks at him with a serious expression. Behind them, there's a gray wall, and a small inset video shows a person speaking in the top left corner, along with a chat overlay on the right; [0:05:26 - 0:05:30]: The man in the striped shirt continues to engage with the person in the military outfit, who remains in the same position. The inset video and chat overlay remain consistent; [0:05:31 - 0:05:32]: The man in the striped shirt begins to walk away from the person in the military outfit. The background shows a parking area with several service vehicles parked, labeled \"gruppex\"; [0:05:33 - 0:05:37]: The man moves further into the parking area, walking past more service vehicles. The setting appears to be underground, with pillars and dim lighting; [0:05:38 - 0:05:40]: The man continues walking, glancing around his surroundings. He stops momentarily, and an on-screen map appears briefly, which then disappears, revealing the parking area again. He resumes walking but appears to be focused on the device in his hand.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the man wearing a striped shirt do just now?",
        "time_stamp": "00:05:32",
        "answer": "B",
        "options": [
          "A. Engaged with the person in the military outfit.",
          "B. Walked away from the person in the military outfit.",
          "C. Looked at the gray wall behind him.",
          "D. Spoke to the inset video."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_273_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:03]: A light blue car, possibly an older model hatchback, is parked near a sidewalk lined with a tree. The surrounding area appears to be an urban location with large buildings, some under construction or unfinished, evidenced by barriers and construction signs. The car begins to move forward into an open, barricaded area. [0:08:04 - 0:08:13]: The car drives slowly towards a large building labeled \"Union Depository.\" The road markings direct the vehicle towards an entrance, and various posters and signs can be seen on the walls and construction barriers. The setting continues to present an urban and somewhat industrial environment, with the car approaching an entrance marked by yellow clearance caution signs. [0:08:14 - 0:08:20]: The car navigates inside a dimly lit underground garage with concrete columns and parked vehicles. Multiple trucks and vans are stationed around. Moving cautiously, the car maneuvers through the garage, eventually stopping near a white and green van marked \"Gruppe 6.\" The driver exits the car, revealing a blond man dressed in a striped shirt and light-colored pants. He begins walking towards the white van, appearing to target it for some purpose.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the driver do just now after parking the car in the garage?",
        "time_stamp": "00:08:20",
        "answer": "B",
        "options": [
          "A. He opened the car trunk.",
          "B. He walked towards the white van.",
          "C. He checked the car's tires.",
          "D. He spoke to someone on the phone."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_273_real.mp4"
  },
  {
    "time": "0:09:40 - 0:09:58",
    "captions": "[0:09:40 - 0:09:58] [0:09:40 - 0:09:42]: The video starts with a view from a first-person perspective. The person is inside a building, standing near a wall with a light gray surface. Near the wall, there is a fellow wearing a striped shirt and another character wearing a uniform that appears to be security personnel. The scene is well-lit, with the corridor extending both in front and behind the characters. [0:09:43 - 0:09:45]: The person in the striped shirt is seen standing close to the wall-mounted red box. The background still shows a hallway with multiple bays on the right side. [0:09:46 - 0:09:51]: As the sequence progresses, the perspective mostly remains the same. The camera angle slightly shifts, showing more interaction between the person in the striped shirt and the security personnel, who is standing with crossed arms. The person in the striped shirt appears to be looking at the security personnel and turning slightly to the right. [0:09:52 - 0:09:55]: The character in the striped shirt takes out a mobile device and seems to be interacting with it. The security personnel remains standing, with his arms crossed, attentively watching the person in the striped shirt. [0:09:56 - 0:09:58]: The video remains focused on the interaction between the two characters. The person with the striped shirt continues to use the mobile device, while the security personnel stands close to the wall, maintaining the same pose. The background corridor continues to stretch back, dimly lit but displaying structural elements. [0:09:42 - 0:09:45]: The view slightly shifts to show intention and body language as the character in the striped shirt continues moving and interacting. The perspective maintains focus on the immediate interactions.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the spatial relationship between the security personnel and the person in the striped shirt right now?",
        "time_stamp": "00:09:54",
        "answer": "C",
        "options": [
          "A. The person in the striped shirt is standing behind the security personnel.",
          "B. The security personnel is standing above the person in the striped shirt.",
          "C. The person in the striped shirt is standing near a wall-mounted red box, facing the security personnel.",
          "D. The security personnel is walking away from the person in the striped shirt."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_273_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:19]: The video is set on a bustling urban street corner during the evening, evidenced by the soft lighting and the prominent sunset hues in the sky. The scene features various buildings of different architectural styles lining both sides of the street. Notably, there is a tall, ornate building with a tower-like structure on the right side of the frame, while on the left, there are several shorter commercial buildings with large storefront windows glowing with internal light. The street and sidewalks are filled with people walking in different directions; some individuals are seen carrying bags, while others appear to be engaged in conversations. Additionally, there are a few cyclists navigating the road. Orange traffic barrels and signs indicate some construction or road work taking place in the vicinity. Two large green street signs are visible, displaying the names \"Union Square W\" and \"University Pl,\" marking the intersection. The trees are lush with green leaves, suggesting that it might be late spring or summer. Several illuminated signs and traffic lights contribute to the overall bustling and vibrant urban atmosphere of the scene.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is prominently displayed on the large green street signs?",
        "time_stamp": "0:00:20",
        "answer": "D",
        "options": [
          "A. \"Broadway\" and \"5th Ave\" and \"Broadway\".",
          "B. \"Main St\" and \"1st Ave\".",
          "C. \"Central Park\" and \"7th Ave\".",
          "D. \"Union Square West\" and \"E 17 st \" and \"Broadway\"."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_304_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:23]: The video begins with a view of a busy city street from a first-person perspective. The street is flanked by tall buildings, with a red-brick building on the left and a mixture of modern and classic architectural styles visible further down the street. Both sides of the street are lined with trees and shopfronts. Pedestrians, some carrying bags, are walking along the sidewalks. One pedestrian wearing a blue hoodie and holding a red and white patterned bag is prominent on the left side of the screen. The lighting suggests it is either early evening or late afternoon, as the sky has a soft pink hue. [0:02:24 - 0:02:27]: The viewpoint begins moving forward along the sidewalk, giving the impression of walking. Additional pedestrians are visible, including a person in a white hoodie and another in a black t-shirt with a graphic on the back. Several cars and a motorcycle are seen traveling down the street, with their headlights and taillights on, contributing to the bustling evening atmosphere. [0:02:28 - 0:02:33]: As the viewpoint moves further down the street, more details of the buildings and street signs become visible. A green street sign indicates \"E 19 St,\" and a \"One Way\" sign is visible. The red-brick building on the left appears to be a corner building with large windows displaying products or brightly lit interiors. People continue crossing the streets and walking along the sidewalks. [0:02:34 - 0:02:37]: The street remains lively as more pedestrians and vehicles move through the area. A cyclist wearing a black hoodie rides past on the right side. Pedestrians are seen interacting and engaging in conversations as they continue their activities. The sky's hue deepens, and the city lights become more pronounced. [0:02:38 - 0:02:40]: The video shows a closer view of a crosswalk, with multiple people about to cross the street. The corner store has a well-lit entrance, and additional pedestrians are visible on the other side of the intersection, highlighting the continuous movement and activity in this urban environment. The overall scene emphasizes a typical evening in a bustling city environment, filled with lights, movement, and a vibrant ambiance.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the pedestrian on the left side of the screen holding?",
        "time_stamp": "0:02:23",
        "answer": "D",
        "options": [
          "A. A black bag.",
          "B. A green backpack.",
          "C. A yellow tote bag.",
          "D. A red and white patterned bag."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_304_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:42]: The video shows a street scene during the evening, with the sky displaying a pinkish hue near the horizon. The buildings on both sides of the street are tall, with one building prominently displaying the number \"913\" in large font above an entrance. A group of people, including a person in a red outfit, walk under a white-covered passageway. On the street, a cyclist rides a bike in the direction of the observer, and a red car is parked;  [0:04:43 - 0:04:47]: The illuminated passageway continues to be the focus, with groups of pedestrians walking along its path. A black car drives by, and a parked car's headlights are visible. Flowerpots with pink flowers adorn the foreground on tall white poles, contrasting with the cooler tones of the structures; [0:04:48 - 0:04:50]: Pedestrians, including a couple holding hands and someone in a red shirt, walk closer towards the observer. The street remains busy with people and vehicles, and the flowerpots provide a vibrant element to the urban concrete setting. A white pole in the foreground aligns with the covered passageway; [0:04:51 - 0:04:55]: As the video progresses, various pedestrians walk along the passageway, while others converse and stroll by. Different flowerpots and street elements add depth to the scene. Streetlights and building interior lights create a warm ambiance. Bicycles are seen parked along the street; [0:04:56 - 0:04:59]: More pedestrians become apparent, including a group of young people chatting and looking at their phones. The view continues to shift between the white-covered passageway and the adjacent street, showing different angles of the same urban area. The video ends with a clear view of buildings extending into the distance and people moving through the well-lit passageway.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What number is prominently displayed above an entrance just now?",
        "time_stamp": "00:05:00",
        "answer": "D",
        "options": [
          "A. 819.",
          "B. 731.",
          "C. 129.",
          "D. 913."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_304_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:04]: The scene takes place in a bustling city environment during dusk with a pink and purple sky. Buildings are illuminated, with several visible on the left side of the frame, their windows aglow with yellow light. There is a lit-up T-Mobile storefront and yellow umbrellas with people sitting underneath them. A prominent feature is a rectangular booth labeled \"BEER WINE COFFEE,\" brightly lit from the inside. People, including a worker in a lime green vest, are milling around, adding to the lively atmosphere. [0:07:05 - 0:07:08]: The worker in the lime green vest is seen walking from left to right. The sky still exhibits beautiful hues of pink and purple. Several blue chairs and tables are arranged in the scene. In the background, a large circular structure glowing with white light is visible, adding to the urban ambiance. [0:07:09 - 0:07:13]: More people are seen walking around and engaging with each other near the \"BEER WINE COFFEE\" booth and the blue tables. The circular light structure becomes more prominent and is now central in the frame. The T-Mobile store and adjacent illuminated buildings continue to provide a lively backdrop. [0:07:14 - 0:07:19]: The frame shifts focus toward the large circular structure, which has \"NEW YORK CITY PORTAL\" banners in front of it. The \"BEER WINE COFFEE\" booth is still visible but now off to the right. Various QR codes and text are visible on the banners. The urban environment remains vibrant with a mixture of pedestrians and soft, pastel skies filling the background.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What colors are the sky exhibiting now?",
        "time_stamp": "00:07:08",
        "answer": "D",
        "options": [
          "A. Blue and yellow.",
          "B. Orange and red.",
          "C. Green and blue.",
          "D. Pink and blue."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the T-Mobile storefront located in relation to the white circular thing?",
        "time_stamp": "00:07:13",
        "answer": "C",
        "options": [
          "A. Center of the white circular thing.",
          "B. Right side of the white circular thing.",
          "C. Left side of the white circular thing.",
          "D. Not visible in the white circular thing."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_304_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video begins with a black screen. [0:00:01 - 0:00:05]: The scene shows a man wearing a black shirt standing at an indoor location with green backlighting. He is holding a large, rectangular object, which appears to be a piece of computer hardware, specifically an RTX 4090 graphics card. The man looks at the object and rotates it to show different angles. In the background, there are various objects including a large black box labeled “GEFORCE RTX 4080” on a shelf. [0:00:06 - 0:00:11]: The man continues to show the graphics card, holding it up and tilting it to various angles while looking at it. His expressions are varied, as if he is explaining something about the device. The green backlighting remains consistent in the background, and the setting appears to be a room designed for presenting or reviewing technological items. [0:00:12 - 0:00:19]: The camera angle switches to a close-up shot of the graphics card. The man holds the card with both hands, displaying different parts of it, including the fan and the side where “GEFORCE RTX” is written. The background shifts to a blue hue, and the man's focus remains on showcasing the details of the graphics card.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What label on the graphics card is visible right now?",
        "time_stamp": "00:00:11",
        "answer": "A",
        "options": [
          "A. RTX 4090.",
          "B. AMD RADEON.",
          "C. INTEL ARC.",
          "D. NVIDIA TESLA."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_121_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:03:20]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:20]: Two cars are seen from a rear, first-person perspective as they drive along a country road with clear, sunny weather. The road stretches straight and then curves gently to the left. Both cars are driving side by side; the left car's license plate reads \"OPT1MUM.\" The scenery includes fields, trees, and distant mountains. Each car's performance is tracked, with the left screen displaying \"RTX 3080\" and the right \"RTX 4080,\" showing fluctuating frames per second (FPS). Initially, the road's right shoulder is lined with grass and the left side with trees. As the vehicles proceed, they pass by varying objects like small bushes, road signs, another vehicle, and electrical towers. The video emphasizes the difference in performance between the two cars, highlighting the visuals and fluidity of the driving experience, with FPS rates prominently displayed throughout the video.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What license plate is displayed on the left car's rear right now?",
        "time_stamp": "00:03:20",
        "answer": "B",
        "options": [
          "A. RTX1MUM.",
          "B. OPT1MUM.",
          "C. DLX3080.",
          "D. CAR4080."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_121_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:06:20]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:01]: The video begins with a person running in a dark, narrow corridor illuminated by dim, artificial lighting on the left side. Above are three graphical overlays showing different computing performances (RTX 2080 S, RTX 3080, and RTX 4080). [0:06:01 - 0:06:03]: The person continues running down the corridor, with their hands visible and moving rhythmically as they sprint. The setting remains unchanged, including the graphical overlays. [0:06:03 - 0:06:04]: The person is now closer to a wider part of the corridor, where the path splits and more lighting fixtures are visible on the ceiling. [0:06:04 - 0:06:05]: As the person progresses further, hanging objects and occasional rubbish on the floor can be seen in the corridor. The overall lighting is slightly improved. [0:06:05 - 0:06:06]: The corridor shows an increasing number of scattered items on the floor, with the person approaching these objects. The environment remains a concrete-themed, industrial design. [0:06:06 - 0:06:07]: Continuing down the corridor, the person encounters more suspended objects, some levitating mid-air. The lighting brightens slightly as they near the end of the corridor. [0:06:07 - 0:06:08]: As the person gets closer, a noticeable bright spot at the corridor’s end appears, hinting at an exit or a different section. Suspended items remain overhead. [0:06:08 - 0:06:09]: The layout shows a fork or a split ahead, with more detailed concrete structures visible. The presence of floating objects continues. [0:06:09 - 0:06:10]: The person approaches a point where people, possibly security or employees, are seen in the corridor, hanging from the ceiling. Some are moving while others appear stationary. [0:06:10 - 0:06:11]: As the video progresses, different suspended individuals become more distinct, with details such as clothing visible. The end of the corridor is approaching. [0:06:11 - 0:06:12]: The corridor continues to feature identical design elements, with the person advancing towards more hanging individuals and some scattered office furniture. [0:06:12 - 0:06:13]: The focus remains on the corridor's end, showing clearer views of the hanging individuals and approaching a bright area potentially leading to another room. [0:06:13 - 0:06:14]: The person nears what appears to be an office space or open area, indicated by clearer lighting and visible structures such as staircases and partitions. [0:06:14 - 0:06:15]: Approaching a staircase, the video reveals a signboard on the wall indicating directions to different facilities like a mail room and cafeteria. [0:06:15 - 0:06:16]: The corridor environment transitions to a concrete staircase. The individual ascends the stairs, with the end of the corridor fully illuminated. [0:06:16 - 0:06:17]: As the person runs up the stairs, other structural elements become visible, suggesting an industrial or office setting. The area becomes more defined. [0:06:17 - 0:06:18]: Upon reaching the top of the stairs, the environment transitions to an open-plan office space, with different levels and office furniture. [0:06:18 - 0:06:19]: The person navigates through cubicles and open areas, with the lighting conditions varying as they move towards a dimly lit corner or corridor. [0:06:19 - 0:06:20]: The person continues their movement through the office space, approaching another stairwell or descending area, in consistent dim lighting.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which graphical overlay is NOT shown right now?",
        "time_stamp": "00:06:20",
        "answer": "C",
        "options": [
          "A. RTX 2080 S.",
          "B. RTX 3080.",
          "C. RTX 3070.",
          "D. RTX 4080."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_121_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:20]",
    "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:14]: The video starts with a black background displaying a performance comparison graph for various GPUs using Blender 3.3, specifically the Scanlands demo at a resolution of 3840x1500. The y-axis lists different GPUs: RTX 4090, RTX 4080, RTX 3090, RTX 3080, RTX 2080 Ti, and RTX 2080 Super from top to bottom. Beside each GPU name are horizontal bars indicating the time taken (in seconds) for each GPU to complete the task. The RTX 4080 bar is highlighted in bright green, showing 62 seconds. From top to bottom, the times are 46 seconds (RTX 4090), 82 seconds (RTX 3090), 110 seconds (RTX 3080), 176 seconds (RTX 2080 Ti), and 223 seconds (RTX 2080 Super). The title \"Blender 3.3\" is at the top center, and \"GPU Only - Scanlands demo, 3840x1500\" is beneath it. [0:09:15 - 0:09:19]: At this point, the display changes to another performance comparison graph for the V-Ray 5 benchmark, specifically the GPU RTX performance. The background is still black, but the title at the top center has changed to \"V-Ray 5 benchmark\" followed by \"GPU RTX\" underneath. The GPUs compared remain the same: RTX 4090, RTX 4080, RTX 3090, RTX 3080, RTX 2080 Ti, and RTX 2080 Super. Horizontal bars extend next to each GPU name, indicating scores instead of time. The RTX 4080 bar is highlighted in bright green, showing a score of 3972. The other scores are 5762 (RTX 4090), 2691 (RTX 3090), 2134 (RTX 3080), 1308 (RTX 2080 Ti), and 975 (RTX 2080 Super).",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the V-Ray 5 benchmark score for the RTX 3080 right now?",
        "time_stamp": "00:09:19",
        "answer": "A",
        "options": [
          "A. 2314.",
          "B. 2691.",
          "C. 1308.",
          "D. 3972."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_121_real.mp4"
  },
  {
    "time": "[0:11:40 - 0:11:45]",
    "captions": "[0:11:40 - 0:11:45] [0:11:40 - 0:11:45]: In a small office space with green-dominant lighting and decor, there is a young man holding a graphics card. The room features a visible GeForce RTX 4080 box on a shelf to the man's left. He is attired in a black t-shirt and is speaking while steadily showcasing the graphics card, moving it slightly for better viewing. A computer setup is noticeable behind him, along with vertical green lights and sleek, modern furnishings. The frames capture various angles and moments of his interaction, maintaining the graphics card's presence as the focal point. The man alternates his gaze between the camera and the graphics card, displaying comfort and familiarity with the product.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What brand and model of the graphics card box is visible on the man's hand right now?",
        "time_stamp": "00:11:45",
        "answer": "C",
        "options": [
          "A. Radeon RX 590.",
          "B. GeForce GTX 1080.",
          "C. GeForce RTX 4080.",
          "D. Radeon RX 6700."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_121_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:01]: A person, wearing a white sleeveless top, is pouring a clear liquid from a green-bottled container into a silver pot placed on a stove. The person's left hand holds the bottle while the person unscrews the green cap with their right hand. [0:01:02]: The person turns to the left, placing the bottle on a counter that has a green container, a tall brown pepper mill, and a mug, next to which stands another pot with a ladle in it. [0:01:03]: The person, still looking to their left, reaches for the lid of the green container while holding the green bottle with their right hand. [0:01:04]: Facing forward, the person pours a small amount of a white substance from a glass bowl into the silver pot on the stove. [0:01:05 - 0:01:07]: The person remains focused on the task of adding the substance into the pot while holding the bowl by its rim with both hands. In the background, a white-tiled wall, a white cabinet, and a window with a green leafy plant can be seen. [0:01:08]: Close-up of the person's hand stirring the contents of the pot with a spoon. A black pan and another silver pot with a white ladle are placed nearby. [0:01:09]: The person continues stirring the contents of the pot, moving the spoon in a clockwise direction. [0:01:10 - 0:01:11]: The person shakes the spoon slightly, possibly to mix or dissolve the contents better. [0:01:12 - 0:01:13]: The person looks content while stirring the pot, and a smile forms on their face as they continue cooking. The person's surroundings include a pepper mill, green container, and another pot on the stove. [0:01:14 - 0:01:17]: The person stirs the pot with increased vigor, smiling and appearing to enjoy the activity. The background remains consistent, featuring a white-tiled wall, cabinets, and a window with greenery. [0:01:18 - 0:01:19]: The person continues stirring the pot, maintaining an expressive and joyful demeanor. Their focus remains on the pot as they engage in cooking.\n[0:01:20 - 0:01:40] \n[0:01:40 - 0:02:00] ",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the woman holding in her right hand right now?",
        "time_stamp": "00:01:02",
        "answer": "D",
        "options": [
          "A. A silver pot.",
          "B. A green-bottled container.",
          "C. A glass bowl.",
          "D. A silver spoon."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Action Recognition",
        "question": "What does the woman do after pouring a small amount of a white substance into the pot?",
        "time_stamp": "00:01:15",
        "answer": "D",
        "options": [
          "A. Places the bowl on the counter.",
          "B. Looks out the window.",
          "C. Shakes the spoon.",
          "D. Stirs the contents of the pot with a spoon."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_22_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:03]: A close-up shows hands chopping a piece of raw red meat on a wooden cutting board. The person uses a sharp knife, making precise cuts. There is a ring on the person's left hand, and pale nail polish is noticeable. The meat is being diced into small chunks, with pieces in various stages of being chopped and some already cut lying on the cutting board. [0:03:04 - 0:03:06]: Zooming out, the scene reveals a clean, modern kitchen setting. The person chopping the meat wears a white sleeveless top with a textured surface. On the counter are several kitchen items, including a green cup, a pepper grinder, a bottle of sauce, and a black avocado. A ceramic bowl and plate are also visible near the cutting board. [0:03:07 - 0:03:09]: The focus returns to a close-up of the chopping process. The person continues to dice the meat, and the knife blade is visible making swift, smooth motions. Their fingers are positioned carefully to avoid the blade. [0:03:10 - 0:03:12]: The person reaches for a white bowl and starts to transfer the diced meat into it using their hands. The bowl, now partially filled with diced meat, is placed on the cutting board adjacent to the chopping activity. [0:03:13 - 0:03:14]: The camera shows the white bowl with the diced meat sitting on the wooden cutting board. Nearby are the ceramic plate and knife, indicating a pause in the chopping activity. [0:03:15 - 0:03:17]: The perspective includes the person in the modern kitchen setting, now holding a jar with a spoon. The person scoops a white substance from the jar into the bowl containing the diced meat. The green cup and pepper grinder remain on the counter, and the person's actions suggest they are preparing a dish. [0:03:18 - 0:03:19]: With a close-up focus on the bowl, the person adds the white substance on top of the meat. The person uses a spoon to scoop from the jar, indicating a deliberate and measured addition to the ingredients in the bowl. The nearby plate on the cutting board shows readiness for the next preparation step.\n[0:03:20 - 0:03:40] \n[0:03:40 - 0:04:00] ",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the woman's nail polish?",
        "time_stamp": "0:03:03",
        "answer": "D",
        "options": [
          "A. Red.",
          "B. Pink.",
          "C. Blue.",
          "D. Pale."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_22_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:04]: A frying pan is placed on a stove with a lid partially covering it. The hand holding the pan is moving it into position. Gradually, the hand begins to firmly secure the lid atop the pan, ensuring it is tightly in place over the bubbling contents within. [0:04:05 - 0:04:07]: The scene shifts to a kitchen where a woman in a white sleeveless dress is engaging in some cooking tasks. She lifts the pan and balances it while holding a white plate, positioning it to transfer the contents from the pan onto the plate. [0:04:08 - 0:04:10]: After successfully transferring the food, the woman partially turns her body, reaching for nearby condiments on the counter. The large stainless steel refrigerator and various kitchen tools on the counter, including a green mug and a pepper grinder, provide the kitchen backdrop. [0:04:11 - 0:04:14]: She proceeds to fetch a spoon and other preparing tools from the counter and adds a portion of an ingredient into a large pot on the stove while stirring it occasionally. She multitasks between the stove and the counter, ensuring everything is blended perfectly. [0:04:15 - 0:04:16]: The woman continues her cooking process as she turns back to the pot on the stove, adding more ingredients and checking the progress of her dish. The background remains consistent with the modern kitchen design, featuring a large window with visible greenery. [0:04:17 - 0:04:19]: She resumes focusing on the pot, using a spatula to stir and mix the contents. Her expression indicates concentration as she ensures everything is evenly cooked. The kitchen environment provides a clean and organized backdrop to her cooking activity.\n[0:04:20 - 0:04:40] \n[0:04:40 - 0:05:00] ",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is one of the items visible on the kitchen counter along with the condiments?",
        "time_stamp": "0:04:10",
        "answer": "D",
        "options": [
          "A. A red mug.",
          "B. A blue cup.",
          "C. A yellow bowl.",
          "D. A green mug."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Action Recognition",
        "question": "What has this woman just done?",
        "time_stamp": "0:04:36",
        "answer": "D",
        "options": [
          "A. She starts cleaning the pan.",
          "B. She begins to wash the plate.",
          "C. She opens the refrigerator.",
          "D. She chop the green onions on the cutting board."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_22_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with an overhead view of a round wooden cutting board placed on a white surface. The cutting board has a detailed engraved design featuring the word \"JOHNSON\" prominently in the center. The design, symmetrical and intricate, includes floral and geometric patterns; [0:00:02 - 0:00:17]: The perspective changes, revealing hands holding the cutting board. The lower part of the board features lighter wood grain, while the upper part is slightly darker. The handle to the right has a rounded shape with a hole near its end. The background now consists of a gray grid mat, indicating a workspace or craft station. [0:00:18 - 0:00:20]: The hands place the cutting board back onto the grid mat, releasing it at the edges. The detailed engraving remains clearly visible, and the handle and wood grain colors are consistent throughout the sequence.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:00:20",
        "answer": "A",
        "options": [
          "A. Displaying the round wooden board engraved with \"Johnson\" on it.",
          "B. Engraving the word \"JOHNSON\" on the board.",
          "C. Changing the perspective of the view.",
          "D. Cleaning the cutting board with a cloth."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_70_real.mp4"
  },
  {
    "time": "0:02:00 - 0:02:20",
    "captions": "[0:02:00 - 0:02:20] [0:02:01 - 0:02:09]: The video shows a computer screen displaying detailed black and white mandala artwork on a grid background. The mandala consists of intricate, repeating patterns including petal shapes, circular designs, and finely-drawn lines. There is a horizontal black bar overlaying the central portion of the mandala. The screen is part of a design software interface, with toolbars and menus visible at the top and sides, containing various icons, text options, and measurement tools. The canvas is titled \"Repurpose Coloring Card\" as seen in the top middle of the screen. [0:02:10 - 0:02:18]: The perspective remains constant, focused on the mandala artwork, which stays centered on the screen. The design interface and its features, including the gridlines and black bar, do not change. The layout and intricacy of the mandala's patterns are continuously visible, emphasizing the detailed and symmetrical nature of the design elements. Throughout this segment, the design window's elements, such as the rulers along the top and left, remain unchanged. [0:02:19 - 0:02:20]: The lower portion of the mandala begins to be obscured as the black bar slightly shifts downwards on the screen. The symmetrical design of the mandala still remains visible above the bar, with the intricate details continuing to be presented clearly. The surrounding interface and tools remain static and consistent with the previous frames.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is happening to the computer screen design interface right now?",
        "time_stamp": "00:02:41",
        "answer": "A",
        "options": [
          "A. The two black horizontal lines turned red.",
          "B. The design interface is closing suddenly.",
          "C. New icons are appearing in the toolbar.",
          "D. The mandala artwork is being deleted."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_70_real.mp4"
  },
  {
    "time": "0:04:00 - 0:04:20",
    "captions": "[0:04:00 - 0:04:20] [0:04:05 - 0:04:11]: The video begins with the viewer navigating a digital design interface. The main focus is on an intricate black and white mandala design, which features the word \"JOHNSON\" prominently in bold, uppercase letters at its center. The background consists of a grid layout, with various tool options listed at the top of the screen. The viewer moves the cursor to the “Align” menu, and a dropdown list of alignment options appears. The grid is uniform, delineated with both horizontal and vertical lines, and the mandala design is centered in the grid. [0:04:11 - 0:04:19]: The view shifts slightly but remains focused on the same intricate mandala design centered on the grid. The dropdown menu with alignment options closes, and the interface remains stable, displaying the identical tools and layout options at the top. The grid lines remain unwavering in the background, maintaining an organized appearance. The sidebar on the right part of the screen shows a \"Results\" tab. [0:04:19 - 0:04:21]: The screen transitions to a new phase, displaying a loading animation accompanied by the text “Sorting project into mats by color.” The background is grey, and a circular icon indicates the sorting process in progress. [0:04:21 - 0:04:28]: The sorting screen changes slightly, now with a translucent view of the mandala design overlaid on a dark grid background. The text “Sorting project into mats by color” remains centered on the screen, alongside the progress indicator in the form of a rotating circle. [0:04:28 - 0:04:46]: The mandala design is now fully revealed against a black rectangular backdrop within the design mat interface of a crafting machine. The grid layout is prominent, and the design is perfectly aligned according to the grid. Tool options and project details appear on the left sidebar, which includes a mini design preview and settings for material type and cutting preparation. The overall layout indicates the project is nearly ready for execution, with the final positioning checks taking place.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action is the viewer taking right now?",
        "time_stamp": "00:03:43",
        "answer": "A",
        "options": [
          "A. Adjusting the font of the center.",
          "B. Adjusting the grid layout.",
          "C. Selecting a new tool.",
          "D. Zooming in on the mandala design."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_70_real.mp4"
  },
  {
    "time": "0:06:00 - 0:06:20",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:02]: The recording shows a pair of hands using a pen-like tool to trace around a large mandala stencil on a piece of white paper. The paper is secured with gold-colored tape at the top and sides. A lint roller with a blue handle rests on the upper right corner of the workspace, which is covered by a grey grid-pattern cutting mat. The hands are wearing a pink knitted sweater. [0:06:03 - 0:06:08]: The focus continues on the hands tracing various intricate lines of the mandala stencil using the same pen-like tool. Occasionally, the right hand lifts the lint roller slightly off the mat before placing it back down. [0:06:09 - 0:06:18]: The hands adjust and reattach the tape to better secure the stencil, then proceed to trace additional lines. The stencil features the word 'JOHNSON' prominently in the center, surrounded by a detailed floral or mandala design. The hands meticulously follow each contour of the stencil with precision. [0:06:19]: The left hand continues tracing while the right hand, holding the pen-like tool, moves steadily to ensure all details of the stencil are replicated on the paper underneath. Additional sections of the detailed floral design become visible as the tracing progresses.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "0:06:19",
        "answer": "A",
        "options": [
          "A. Peeling off the stickers on the picture one by one.",
          "B. Adjusting tape.",
          "C. Using a lint roller.",
          "D. Cutting the paper."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_70_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:03:00 - 0:03:05]: The video starts with a view of a person’s hands from a first-person perspective. The person wearing a long-sleeved purple sweater is holding a piece of white paper with a complex, symmetrical black design on it. The design appears to be a geometric mandala-like pattern with intricate details. The right hand holds a ruler horizontally across the paper, aligning it with the midsection of the design. [0:03:05 - 0:03:07]: The person adjusts the positioning of the paper while keeping the ruler steady, ensuring it aligns properly with the grid lines on the background cutting mat. The cutting mat is green-gray with a white grid. [0:03:07 - 0:03:08]: The person continues to adjust the top part of the white paper and the design, still keeping the ruler in place and ensuring the alignment is precise with the grid in the background. [0:03:08 - 0:03:11]: The person begins to lift the top section of the paper upward, revealing the same complex design below. They hold a pair of black-handled scissors in the right hand, ready to make precise cuts. [0:03:11 - 0:03:14]: The person places the left hand near the top of the paper, ensuring it remains steady while they cut along the edge. The scissors move slowly and carefully along the horizontal edge, cutting through the white paper. [0:03:14 - 0:03:15]: The view focuses on the cutting mat and the intricate design on the remaining section of paper as the top portion is completely cut off and set aside. The person’s hand adjusts and smooths down the remaining piece of paper on the wooden board underneath. [0:03:15 - 0:03:17]: The paper, which has been mostly cut, is now being adjusted for precise placement. The person's hands work to align the design with the grid and the edges of the wooden board. [0:03:17 - 0:03:20]: The person lifts a transparent sheet with the same geometric pattern drawn on it. The hands then start to position this transparent sheet over the remaining white paper, ensuring it aligns perfectly with the design. [0:03:20]: With focused attention, the person moves the transparent sheet carefully, ensuring the detailed alignment of the mandala-like pattern with the white paper underneath. The video concludes with the person finalizing the placement of the transparent sheet.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:08:13",
        "answer": "A",
        "options": [
          "A. Cutting off the top portion of the paper.",
          "B. Folding the paper along the edge of the design.",
          "C. Lifting a pair of black-handled scissors.",
          "D. Aligning the ruler with the grid lines on the cutting mat."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_70_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: A man wearing a green t-shirt stands in a well-organized kitchen. Behind him, there are white brick walls, wooden shelves with neatly arranged dishes and cups, and a countertop with various items. Several knives are mounted on a magnetic strip on the right side. There are also glass jars containing various ingredients on the shelves. A red “COOK” sign decorates the upper left portion of the wall. [0:00:02]: The man raises his hands in front of him, spreading his fingers and gesturing slightly outward. He appears to be explaining or emphasizing something. [0:00:03]: The man points directly towards the camera with both hands, adding emphasis to the points he is making. His expression is engaged and animated. [0:00:04]: With his right hand touching his head, the man closes his eyes and makes a slightly exaggerated facial expression, as if he is trying to convey a strong point or emotion. [0:00:05]: The man stands with both hands clasped in front of his chest, gazing directly towards the camera with a neutral expression. [0:00:06]: He raises both hands again, fingers extended, continuing to speak passionately, emphasizing his speech with gestures. [0:00:07]: The man presses his palms together in front of him as if he is pleading or making a heartfelt point, maintaining eye contact with the camera. [0:00:08]: The camera angle shifts to a side view, showing the left side of the man's face. He continues speaking, hands still clasped together. There is a blue cabinet behind him. [0:00:09]: The man gestures with his hands again, palms upturned, as he continues to speak. His facial expression conveys enthusiasm. [0:00:10 - 0:00:11]: He holds his hands out with palms facing slightly upwards, continuing his speech. His facial expression is earnest as he engages with the viewer. [0:00:12]: The man’s mouth is open as he speaks forcefully, maintaining the same hand gesture. His head tilts slightly to the right. [0:00:13]: He gazes slightly upwards, appearing to conclude his statement. His hands remain extended outward. [0:00:14]: The camera returns to a front view. The man claps his hands together once as if signaling the conclusion or transitioning to a new point. [0:00:15]: He continues clapping his hands gently as he speaks, looking directly at the camera with an earnest expression. [0:00:16]: The man clasps his hands together again, holding them in front of his chest. He continues to speak with a serious expression. [0:00:17]: His right hand briefly moves away from the clasped position and he gestures outward with it. His expression remains focused. [0:00:18]: The man’s hands return to a clasped position in front of his chest. He looks down briefly, appearing thoughtful as he speaks slowly. [0:00:19]: The man flings his arms wide open, with palms facing the camera, and his expression shifts to a more welcoming demeanor. He stands confidently as he continues to engage the viewer.\n[0:00:20 - 0:00:40] \n[0:00:40 - 0:01:00] ",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What sign is decorating the upper left portion of the wall behind the man?",
        "time_stamp": "00:00:40",
        "answer": "C",
        "options": [
          "A. A blue \"CHEF\" sign.",
          "B. A yellow \"COOK\" sign.",
          "C. A red \"COOK\" sign.",
          "D. A green \"CHEF\" sign."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_18_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:00 - 0:01:20] \n[0:01:20 - 0:01:40] \n[0:01:40 - 0:02:00] [0:01:40 - 0:01:43]: A pair of hands is seen chopping garlic cloves on a wooden cutting board positioned in the center of a kitchen countertop. The left hand holds the garlic in place while the right hand maneuvers a knife to slice the cloves into small, even pieces. To the right, a white skillet is visible on the stove with a mix of melted butter and oil, creating a translucent sheen. Various ingredients such as leafy greens, jars with spices, and bowls are organized around the chopping board and pan. [0:01:44 - 0:01:46]: The hands continue chopping the garlic. The position of the knife and garlic remains consistent but is more clearly isolated on the board. In the background, more of the stovetop and the sizzling pan can be seen. [0:01:47]: The person uses the flat side of the knife to scoop up the chopped garlic from the board. [0:01:48 - 0:01:49]: The garlic is added to the white skillet on the stove, visibly mixing with the butter and oil, which starts to bubble gently. [0:01:50 - 0:01:52]: The remaining garlic on the cutting board is being gathered, and the hand starts making its way back to the skillet to add the rest. [0:01:53 - 0:01:56]: The garlic is stirred into the mixture using a wooden spoon, dispersing it evenly. Gentle, intentional movements suggest careful cooking, ensuring the garlic is not burned but lightly cooked. [0:01:57]: The camera angle shifts to show a man at the stove, wearing a dark T-shirt, reaching over to the left of the stove to grab another ingredient. [0:01:58]: The man sprinkles a pinch of salt into the bubbling mixture in the white skillet below. [0:01:59]: The man looks up and speaks while continuing to stir the contents of the skillet. The background shows a well-organized kitchen with a brick wall, shelves holding plates, jars, and various other kitchen items.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the man at the stove wearing?",
        "time_stamp": "00:01:57",
        "answer": "C",
        "options": [
          "A. A green T-shirt.",
          "B. A striped shirt.",
          "C. A dark T-shirt.",
          "D. A chef's hat."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_18_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] \n[0:03:20 - 0:03:40] [0:03:20]: The video opens with a close-up view of a wooden spoon stirring leafy greens in a white pan. The pan sits on a stovetop burner, heating the greens, while to the left, a stainless-steel pot is partially visible.  [0:03:21 - 0:03:23]: Next, the camera captures a broader scene showing the chef in the kitchen. He is stirring the contents of the pan with the wooden spoon. A person holding a camera rig stands to the right, recording the chef's actions. The backdrop features a white brick wall with shelves holding various kitchen items and the word “COOK” in large, red letters. [0:03:23 - 0:03:25]: The chef continues cooking, briefly looking towards the camera operator. Behind the chef, a variety of items are placed on the shelves, including plates, jars, and colorful bottles, giving a vibrant feel to the kitchen.  [0:03:25 - 0:03:26]: The chef momentarily looks down at his workspace and then reaches to his right to grab an item off-camera, while the pan is now settled on the stovetop. [0:03:26 - 0:03:27]: Now, the chef grabs a lid, preparing to cover the pot next to the pan on the stovetop.  [0:03:27 - 0:03:28]: The view transitions to an overhead shot of the workspace. The pan with leafy greens is on the left, and the stainless-steel pot, filled with a yellowish liquid, is on the right. Several seasonings are nearby. [0:03:28 - 0:03:31]: The overhead shot continues as the chef uses a ladle to pour liquid from the pot into the pan with leafy greens. The mixture appears to be heating up, with steam rising. [0:03:32 - 0:03:33]: The camera returns to a close-up side view of the chef as he lifts the lid and continues transferring the hot liquid into the pan, with steam continually rising. [0:03:34 - 0:03:35]: The chef's face is captured closer as he focuses on transferring the liquid. Steam and moisture from the hot concoctions are visible. [0:03:35 - 0:03:37]: The chef continues to pour with precision. As more steam rises, he exchanges a few words, appearing to be in a discussion with an off-screen individual. [0:03:37 - 0:03:39]: The chef moves back towards the counter space to adjust items on the counter. His expressions hint at concentration and urgency as he continues his culinary tasks.\n[0:03:40 - 0:04:00] ",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the chef holding while stirring the leafy greens in the pan?",
        "time_stamp": "00:03:20",
        "answer": "C",
        "options": [
          "A. A metal spoon.",
          "B. A plastic spatula.",
          "C. A wooden spoon.",
          "D. A whisk."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_18_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] \n[0:06:20 - 0:06:40] \n[0:06:40 - 0:07:00] [0:06:40 - 0:06:41]: A man with short, light brown hair is seen in a kitchen setting, wearing a dark gray shirt. He is holding a small object in his hands, looking down at it while speaking. Behind him, there is a white tiled wall with a wooden shelf holding jars and plates. [0:06:41 - 0:06:43]: The man continues speaking and gesturing with his right hand. The background remains consistent, with a blue cupboard and a black oven visible on the left. [0:06:43 - 0:06:45]: He turns slightly to his right, reaching for something on the countertop. A large stainless steel pot with steam rising is situated on the stove in front of him, adjacent to a white bowl filled with green vegetables. [0:06:45 - 0:06:46]: The man uses tongs to lift yellow pasta from the pot, steam still rising. The setting remains the same, with the blue cupboard and white tiles prominent in the background. [0:06:46 - 0:06:47]: He transfers the pasta from the pot into the white bowl of vegetables on the stove. The stovetop arrangement with the pots and bowls remains unchanged. [0:06:47 - 0:06:48]: Turning slightly to his left, he continues adding pasta to the bowl of vegetables, still using the tongs. The background features remain constant, the shelf behind him displaying various jars and crockery. [0:06:48 - 0:06:50]: The man is seen gesticulating with his left hand while speaking, facing slightly towards the camera. The blue cupboard, oven, and other kitchen elements are clearly visible. [0:06:50 - 0:06:52]: He uses a slotted spoon to stir the contents of the pot on the stove, maintaining his focus on the cooking process. [0:06:52 - 0:06:54]: More pasta is being scooped with the slotted spoon and added into the bowl of vegetables. The countertop still displays various kitchen items, including a wooden cutting board to the left. [0:06:54 - 0:06:55]: He transitions from using the tongs to the slotted spoon, swirling the pasta in the pot. The same kitchen setup is depicted behind him. [0:06:55 - 0:06:56]: The man stirs the pasta and greens mixture in the bowl once more. Items on the countertop remain in their respective positions, including jars of spices and other condiments. [0:06:56 - 0:06:57]: He packages the cooked mixture, still narrating or explaining the process, looking into the pot as he works. The kitchen's clean and arranged setup stays evident in the background. [0:06:57 - 0:06:59]: The man, slightly angled to his right, appears focused on seasoning or adjusting the food, continuing his explanation. The backdrop continues to show the well-organized kitchen elements like cutting boards, crockery, and kitchen tools hanging on the wall.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the man doing right now?",
        "time_stamp": "00:06:46",
        "answer": "B",
        "options": [
          "A. He chops vegetables.",
          "B. He uses spoon to lift yellow pasta from the pot.",
          "C. He pours sauce into the pot.",
          "D. He sets the table."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_18_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:40]",
    "captions": "[0:09:00 - 0:09:20] \n[0:09:20 - 0:09:40] ",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "How much time does this person have left now??",
        "time_stamp": "0:09:05",
        "answer": "B",
        "options": [
          "A. 01:53.",
          "B. 01:52.",
          "C. 01:51.",
          "D. 01:54."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_18_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is currently written on the sign visible in the video?",
        "time_stamp": "00:00:03",
        "answer": "D",
        "options": [
          "A. START.",
          "B. GO!.",
          "C. MICRO.",
          "D. WHOOP."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_254_real.mp4"
  },
  {
    "time": "[0:00:48 - 0:00:53]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is written on the yellow brand visible right now?",
        "time_stamp": "00:00:52",
        "answer": "D",
        "options": [
          "A. FINISH.",
          "B. START.",
          "C. ALERT.",
          "D. TIMING."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_254_real.mp4"
  },
  {
    "time": "[0:01:36 - 0:01:41]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color predominantly describes the bike's front suspension that is visible right now?",
        "time_stamp": "00:01:37",
        "answer": "A",
        "options": [
          "A. Orange.",
          "B. Green.",
          "C. Blue.",
          "D. Red."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_254_real.mp4"
  },
  {
    "time": "[0:02:24 - 0:02:29]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the barrier predominantly visible on the sides of the track right now?",
        "time_stamp": "00:02:27",
        "answer": "A",
        "options": [
          "A. Yellow.",
          "B. Red.",
          "C. Blue.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_254_real.mp4"
  },
  {
    "time": "[0:03:12 - 0:03:17]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the name displayed on the white banners on the sides of the track right now?",
        "time_stamp": "00:03:13",
        "answer": "D",
        "options": [
          "A. BikeLand.",
          "B. Whoop.",
          "C. Timing.",
          "D. Val di Sole."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_254_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video begins in a well-lit storage area with shelves lined with various cartons of drinks. The shelves are organized, with different drinks like milk, juice, and other beverages placed in an orderly fashion. The perspective is from a first-person viewpoint, and the individual is wearing black gloves. The individual reaches out and begins to rearrange or retrieve some drink cartons from the shelves, specifically focusing on a row of identical cartons of vanilla drink. [0:00:04 - 0:00:05]: The viewpoint shifts to show more shelves filled with similar drink cartons and several carts loaded with boxes in the center aisle. The walls and floor are clean, and the area looks organized. Multiple shelves hold various items, while carts with boxes indicate that restocking might be ongoing. [0:00:06 - 0:00:08]: The individual returns to the shelves with cartons, this time picking up a different type of beverage carton. They begin to realign the cartons on the shelf, ensuring they are positioned correctly. [0:00:09 - 0:00:10]: The camera angle shifts again to show a long aisle with multiple shelves on either side, filled with different types of drink cartons. Some boxes and carts are still visible in the aisle, and there are no other people present in this section. [0:00:11 - 0:00:16]: The individual starts working on a different section of the shelves, now focusing on another type of drink carton. They pick up and place several cartons, aligning and organizing them as they go. The shelves are labeled with product names and prices. [0:00:17 - 0:00:19]: The viewpoint changes once more, showing an area at the end of the aisle. It includes more carts loaded with boxes and shelves filled with drink cartons. The individual begins to handle some of the boxes on the cart, adjusting them possibly in preparation for more organizing or restocking tasks.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type of drink cartons did the individual rearranging first?",
        "time_stamp": "0:00:03",
        "answer": "B",
        "options": [
          "A. Orange juice.",
          "B. Yoghurt.",
          "C. Chocolate milk.",
          "D. Apple juice."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_440_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:21]: The video starts with a close-up of a store shelf displaying various products in boxes and cartons. The shelves are organized, with prices visible next to the products. The viewer's hands are seen reaching for a product on the bottom shelf. [0:02:22 - 0:02:23]: The camera angle shifts as the viewer moves along the aisle, revealing additional shelves filled with more products. A cart filled with items is visible further down the aisle. [0:02:24 - 0:02:31]: The scene transitions to another storage area featuring metal shelves stocked with various goods. The shelves on the left contain different products compared to those on the right. Boxes and containers of multiple sizes and shapes are neatly arranged on these metal racks. The lighting is bright, and the flooring appears to be a smooth, clean surface. [0:02:32 - 0:02:34]: The video continues in the storage area, showcasing an orderly arrangement. The camera moves along the aisle, displaying the items on the shelves more clearly. [0:02:35 - 0:02:40]: A close-up reveals specific products on a particular shelf, including a box labeled \"Arla Mild Yoghurt.\" The viewer's hand is seen interacting with the items on the shelves, possibly checking or adjusting the boxes.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What specific product label is visible in the close-up at the end of the video?",
        "time_stamp": "00:02:40",
        "answer": "B",
        "options": [
          "A. Danone Greek Yogurt.",
          "B. Arla Mild Yoghurt.",
          "C. Yoplait Fruit Yogurt.",
          "D. Activia Natural Yogurt."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_440_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:42]: The video starts in a storage room with shelves filled with various cartons, mostly drinks. A metal trolley filled with cardboard pieces is visible in the center;  [0:04:42 - 0:04:45]: Moving forward, the viewpoint passes through the storage area lined with shelves on both sides. The shelves on the left are stocked with products, while on the right, there are boxes and cartons stacked on metal trolleys; [0:04:45 - 0:04:48]: The video continues as the camera traces the path along the aisle. The person appears to handle a cardboard box, placing it on top of others in the metal trolley; [0:04:48 - 0:04:51]: The camera view continues along the aisle, providing a view of more shelves stocked with products. The path opens up to reveal more storage racks and stacks of cartons; [0:04:51 - 0:04:55]: The person navigates the trolley further into the room, where more metal shelves and stacks of red crates filled with products are visible; [0:04:55 - 0:04:57]: The viewpoint is now closer to the back area of the storage room, focusing on the red crates stacked against the right wall. The person appears to be inspecting or adjusting the crates; [0:04:57 - 0:04:59]: The viewpoint pans to the floor area of the storage room, showing stacks of crates, and cardboard pieces on the trolley. The video concludes with the camera directed downward.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the video primarily showing in the storage room right now?",
        "time_stamp": "00:04:54",
        "answer": "C",
        "options": [
          "A. Inspecting crates.",
          "B. Stocking shelves.",
          "C. Moving through aisles.",
          "D. Organizing products."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_440_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:03]: A person wearing black gloves is seen in a supermarket aisle with various milk cartons on shelves. The person is holding a milk carton and placing it on a lower shelf. There are different milk cartons with different prices displayed on the shelf. The floor is grey, and the shelves are white metal racks;  [0:07:04 - 0:07:06]: The scene changes to a different part of the aisle. The person is standing in an area with more shelves filled with various dairy products. The cart on the floor next to them is red, and a large metal rack is seen at the back of the room;  [0:07:07 - 0:07:10]: The person continues to place milk cartons on the lower shelf. They pick up another carton from the red cart on the floor and place it on the shelf marked with a price tag of 26.95. The shelves are labeled with various price tags, and there are multiple milk cartons stored neatly; [0:07:11 - 0:07:16]: The person is organizing the milk cartons on the shelf, placing them one by one. They are picking up milk cartons from the red cart and arranging them on the shelf, occasionally crouching down to place the cartons on the lower shelves. The carton brands and prices vary; [0:07:17 - 0:07:20]: The person continues this activity, ensuring the milk cartons are properly placed and aligned on the lower shelf. The cartons are continuously picked from the red cart and positioned on the shelf priced at 26.95.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type of products is the person handling in the supermarket aisle?",
        "time_stamp": "00:07:20",
        "answer": "C",
        "options": [
          "A. Bread loaves.",
          "B. Canned goods.",
          "C. Milk cartons.",
          "D. Fruit packages."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the cart used by the person in the supermarket right now?",
        "time_stamp": "00:07:06",
        "answer": "B",
        "options": [
          "A. Blue.",
          "B. Red.",
          "C. Green.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_440_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why did the old lady fall?",
        "time_stamp": "00:01:06",
        "answer": "A",
        "options": [
          "A. Because she stepped on a toy racing car on the ground.",
          "B. Because she tripped over a loose rug in the hallway.",
          "C. Because she missed a step while walking down the stairs.",
          "D. Because she lost her balance while reaching for something on a high shelf."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_236_real.mp4"
  },
  {
    "time": "[0:02:01 - 0:02:31]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why is the old lady lying in the hospital bed now?",
        "time_stamp": "0:02:09",
        "answer": "A",
        "options": [
          "A. Because she stepped on a toy car and fell down before.",
          "B. Because she had a sudden illness and needed immediate medical attention.",
          "C. Because she was involved in a minor car accident.",
          "D. Because she fainted from low blood pressure while shopping."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_236_real.mp4"
  },
  {
    "time": "[0:04:02 - 0:04:32]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why is the floor of this room completely covered with water now?",
        "time_stamp": "0:04:35",
        "answer": "B",
        "options": [
          "A. Because a pipe burst under the sink.",
          "B. Because the faucet was left on, the water overflowed.",
          "C. Because someone accidentally spilled a large container of water.",
          "D. Because the washing machine malfunctioned and leaked."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_236_real.mp4"
  },
  {
    "time": "[0:06:03 - 0:06:33]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does Mr. Bean hit the table with a mop?",
        "time_stamp": "0:06:30",
        "answer": "A",
        "options": [
          "A. Because he wants to catch that cat.",
          "B. Because he is trying to kill a bug crawling on the table.",
          "C. Because he accidentally spilled something and is trying to clean it up.",
          "D. Because he is frustrated and taking out his anger on the table."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_236_real.mp4"
  },
  {
    "time": "[0:08:04 - 0:08:34]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why did the character saw a piece of wood?",
        "time_stamp": "00:08:23",
        "answer": "B",
        "options": [
          "A. Because the character needed firewood for the fireplace.",
          "B. As materials for a convertible sofa.",
          "C. Because the character was building a new shelf for the kitchen.",
          "D. Because the character wanted to repair a broken chair."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_236_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a close-up shot of a pencil sketch on a textured white canvas. The sketch is of a woman's face, focusing on her eyes, nose, and lips. [0:00:01 - 0:00:06]: A hand holding a thin paintbrush starts to paint the right eye of the sketch. The brush applies black paint to the iris and pupil area with precise strokes. [0:00:06 - 0:00:12]: The brush continues to paint the right eye, filling in the iris and pupil smoothly while adding definition and depth to the eye.  [0:00:12 - 0:00:16]: The hand ensures even coverage and starts to outline the upper eyelid, creating a more detailed and complete look for the eye. [0:00:16 - 0:00:19]: The brush adds final touches to the eye, including the upper eyelid line, making the eye appear more realistic and giving it a finished look. The painting action stops, and the final frame showcases the detailed and complete right eye.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the painting just shown in the video?",
        "time_stamp": "0:00:19",
        "answer": "B",
        "options": [
          "A. A partially painted left eye with preliminary details.",
          "B. A complete and detailed right eye, including the upper eyelid line.",
          "C. A detailed sketch of the entire face with no paint.",
          "D. An unfinished sketch with some paint on the lips."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_135_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:01]: A realistic portrait of a woman's face is being painted on a white canvas. The face outline is sketched in thin lines. The eyes are painted with dark colors, including black and brown, and have a strong gaze with defined eyebrows. The lips are painted in shades of red and pink. A paintbrush with a pointed tip is applying a pink hue to the lower lip. The paintbrush is held at a slight angle, indicating the painter's fine control. [0:03:02 - 0:03:03]: The paintbrush moves slightly lower on the lips, blending the pink color further. The facial features remain the same, with the upper parts of the eyes detailed with dark and reddish-brown shades, giving them depth and contrast against the white canvas. [0:03:04 - 0:03:05]: The paintbrush shifts focus to the nose area, applying a lighter shade, possibly to highlight the contours of the nose. The placement and movement indicate careful detailing. The lips maintain their rich red and pink shades, standing out prominently against the white canvas. [0:03:06 - 0:03:07]: The paintbrush continues to add details around the nose, applying a mix of colors to create shadows and highlights, enhancing the nose's definition. Meanwhile, the areas around the cheeks and the outer regions of the face remain largely unpainted. [0:03:08 - 0:03:10]: Additional layers of color are applied to the cheek area beneath the eye, introducing greenish tones, suggesting an underpainting technique to build the portrait's depth. The use of different tones is evident as the artist goes over some earlier painted regions. [0:03:11 - 0:03:13]: The paintbrush applies reddish and pinkish tones on the left cheek, blending with the previous underpainting. This blending adds more depth and complexity to the facial shades, indicating subtle transitions of color on the skin.  [0:03:14 - 0:03:15]: The artist continues working on the cheek, brushing more nuanced colors. Careful shading and subtle lines enhance the contouring around the cheekbone area. The eye and lip areas remain defining features with dark and vibrant hues. [0:03:16 - 0:03:20]: The final touches on the cheek incorporate soft blending, creating a smooth gradation of colors that complement the detailed eyes and lips. The portrait shows distinct facial features with well-defined eyes and lips, while the cheek areas transition into more realistic skin tones. The areas around the nose, eyes, and mouth reflect careful attention to nuanced color application, enhancing the realism of the portrait.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Which area of the face remains largely unpainted so far?",
        "time_stamp": "00:03:07",
        "answer": "D",
        "options": [
          "A. The eyes.",
          "B. The lips.",
          "C. The eyebrows.",
          "D. The cheeks and hair."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_135_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:20]: The video showcases a close-up of a painting in progress, depicting a woman's face with vibrant pink hair. The artwork is on a canvas, and throughout the video, a paintbrush intermittently appears, adding fine details to the portrait. The woman’s facial features, including her dark eyes, straight nose, and slightly pursed lips, are detailed realistically, with variations of color showing contour and dimension. The painting is mostly complete, with the artist focusing on final touches, particularly around the nose and eyes areas. The background remains out of focus and mostly plain, emphasizing the central figure of the painting.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the painting proess just shown in the video?",
        "time_stamp": "00:06:20",
        "answer": "B",
        "options": [
          "A. The artist is preparing a new canvas for painting.",
          "B. This artist is adding details to the face of the portrait.",
          "C. The artist is drawing a rough sketch of a landscape.",
          "D. The artist is cleaning up after completing a painting."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_135_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:20]: The video shows a close-up view of a painting on a canvas featuring a portrait of a woman with pink hair. The painting is in progress, as indicated by visible brush strokes and the presence of a hand holding a paintbrush that is continuously working on the portrait. The scene is set indoors, indicated by the background elements such as a green plant on the left, and various tools and materials positioned around the canvas. The painter focuses on the sheen on the woman’s hair, occasionally adding details to the facial features and background. The brush movements are deliberate and targeted, suggesting careful attention to detail. The environment appears calm and methodical as the artist works on the piece. The canvas is positioned upright on an easel, providing a steady surface for the painter to work on.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is positioned to the left of the canvas?",
        "time_stamp": "00:09:20",
        "answer": "B",
        "options": [
          "A. A window.",
          "B. A green plant.",
          "C. A cup of coffee.",
          "D. A lamp."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Clips Summarize",
        "question": "Which statement best summarizes the painting process just shown in the video?",
        "time_stamp": "00:09:18",
        "answer": "C",
        "options": [
          "A. The artist is rapidly applying layers of paint with a wide brush.",
          "B. The painter is outdoors, capturing a landscape with vivid colors.",
          "C. The artist is working methodically on a woman's portrait, focusing on hair details indoors.",
          "D. The canvas depicts an abstract scene with no distinguishable features."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_135_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What is the best summary of the actions performed just now?",
        "time_stamp": "00:00:10",
        "answer": "C",
        "options": [
          "A. The individual cleaned the counter, set up station supplies, and took a coffee break.",
          "B. The individual collected wrapping papers, retrieved buns, and placed them on the counter.",
          "C. The individual took wrapping papers, handled buns by hands, and placed the buns on the counter.",
          "D. The individual stocked the shelves, wiped down surfaces, and organized condiments."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_342_real.mp4"
  },
  {
    "time": "[0:00:43 - 0:00:53]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What summarizes the actions performed just now?",
        "time_stamp": "00:00:47",
        "answer": "C",
        "options": [
          "A. The individual cleaned the station, organized utensils, and disposed of waste materials.",
          "B. The individual picked and packed food items, marked them for delivery, and cleaned the station afterward.",
          "C. The individual selected a yellow wrapping paper, placed an order ticket on it.",
          "D. The individual placed food items in a container, labeled it, and set it aside for pickup."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_342_real.mp4"
  },
  {
    "time": "[0:01:26 - 0:01:36]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What is the best summary of the actions performed just now?",
        "time_stamp": "00:01:36",
        "answer": "C",
        "options": [
          "A. The individual cleaned the counter, organized ingredients, and prepared a salad.",
          "B. The individual prepared a sandwich, added condiments, and wrapped it for delivery.",
          "C. The individual cooked bacon, meat pie, and scrambled eggs, then placed them in a container.",
          "D. The individual prepared a fruit platter, arranged it neatly, and handed it to a customer."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_342_real.mp4"
  },
  {
    "time": "[0:02:09 - 0:02:19]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What is the best summary of the actions performed just now?",
        "time_stamp": "00:02:21",
        "answer": "C",
        "options": [
          "A. The individual prepared a batch of fries, added salt, and placed them in a serving container.",
          "B. The individual organized several orders, packed different items, and completed the preparation station setup.",
          "C. The individual took pancakes, added meat pie from the warmer, and placed scrambled eggs on the same tray.",
          "D. The individual assembled a salad with fresh ingredients, topped it with dressing, and delivered it to the counter."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_342_real.mp4"
  },
  {
    "time": "[0:02:52 - 0:03:02]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What is the best summary of the actions performed just now?",
        "time_stamp": "00:03:02",
        "answer": "C",
        "options": [
          "A. The individual toasted buns, added condiments, and wrapped sandwiches for delivery.",
          "B. The individual organized cooking utensils, took inventory, and sanitized the station.",
          "C. The individual prepared a breakfast burger, added egg and cheese, and neatly wrapped it.",
          "D. The individual made a cup of coffee, added cream, and handed it to a customer."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_342_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:04]: The video begins with a first-person perspective, driving a large, dark-colored van down a city street. The view is centered on the road ahead, flanked by palm trees and buildings, including a tall, white, rectangular building in the distance. The sky is clear and blue. On the left side of the screen, a smaller window displays a person in a room with multiple computer screens, looking focused. [0:00:05 - 0:00:09]: The vehicle continues straight down the road. There are several billboards and streetlights visible. Pedestrians and small businesses line the sides of the street. The smaller window still shows the person concentrating on their screens. [0:00:10 - 0:00:14]: The vehicle keeps moving forward. An intersection with traffic lights approaches, with some lights indicating red and green. Surrounding buildings are a mix of commercial and residential properties with palm trees and other smaller vegetation. [0:00:15 - 0:00:18]: The van drives towards and through the intersection, which appears clear without other moving vehicles obstructing the path. The dense arrangement of buildings continues, as well as the palm trees lining the sidewalks. [0:00:19 - 0:00:20]: The vehicle moves past another intersection, now beginning to approach a parking area. The video shows a parking lot entrance with some signage. An open space appears where other parked vehicles are slightly visible in the distance.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person in the smaller window doing right now?",
        "time_stamp": "00:00:11",
        "answer": "B",
        "options": [
          "A. Talking on the phone.",
          "B. Concentrating on his screens.",
          "C. Writing notes on a paper.",
          "D. Drinking coffee."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_287_real.mp4"
  },
  {
    "time": "0:02:40 - 0:03:00",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:43]: The video starts with a first-person perspective inside a game, facing the front windshield of a gray delivery van. The van is stopped at an intersection with another truck ahead and various buildings and billboards visible in the background. Traffic lights and street signs are also present. [0:02:44 - 0:02:46]: The perspective switches to a close-up menu titled \"Trucking Progression,\" displaying various job options like \"Los Santos Courthouse\" and \"Sandy Shores Liquor Ace.\" Prices and other mission details are listed under each job. [0:02:47 - 0:02:50]: The menu view continues as the options on the screen remain the same, and no additional interactions seem to be happening at this moment. [0:02:51]: The view switches back to the outside scene with the delivery van now moving forward slightly in the intersection. [0:02:52 - 0:02:54]: The gray delivery van is seen from an outside perspective, waiting at the next intersection. A large truck passes by closely on the left side of the van. [0:02:55]: The van is stationary again, now with the large truck fully passing by and continuing on its route. [0:02:56 - 0:02:59]: The van proceeds to drive forward through the intersection, passing a building complex and small park area on the right side. The road appears clear with no immediate obstacles. [0:03:00]: The van continues driving down the street as the video ends.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the delivery van doing right now?",
        "time_stamp": "00:03:08",
        "answer": "C",
        "options": [
          "A. Stopped at an intersection.",
          "B. Waiting for a truck to pass.",
          "C. Moving forward through the intersection.",
          "D. Turning right at the intersection."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_287_real.mp4"
  },
  {
    "time": "0:05:20 - 0:05:40",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:24]: A grey delivery van with the word \"grime\" in blue is parked in an industrial area near white and grey buildings and other parked vehicles. A person is seen through the driver's window. In the background, someone is walking towards the van. [0:05:25 - 0:05:33]: The van moves forward slightly and then comes to a stop. Two people are visible near the van, one standing close to the driver side window and the other running towards the front of the van. Another large vehicle approaches from the opposite direction, passing by the van. [0:05:34 - 0:05:37]: The scene shows more activity, with people walking around the parked cars and another truck moving past. The person standing near the driver's side window remains, while another person walks away from the van. [0:05:38 - 0:05:40]: The camera angle shifts slightly, showing more details of the setting, including shipping containers and industrial machinery in the background. The person near the driver's side continues standing beside the van, engaging with the driver. Another individual also appears to be getting into the passenger seat of the van. [0:05:41 - 0:05:45]: The scene continues with more movement from the people around the van. The large truck that was approaching earlier is now moving past the van, heading towards the open lot. The person at the driver's window maintains their position. [0:05:46 - 0:05:53]: The activity around the industrial area continues, with various individuals walking around, and vehicles maneuvering near the buildings. The person near the van's driver's side continues interacting with the driver while another person stands near the back of the van. [0:05:54 - 0:05:58]: The video shows a side view of the van as it starts to move again. The individual standing by the driver's door begins to follow the van as it rolls forward.  [0:05:59 - 0:06:04]: As the van continues to drive through the industrial area, it passes several other parked and moving vehicles. The person who was standing by the driver's side now walks away from the van and out of the camera's view. [0:06:05 - 0:06:10]: The van drives through a different part of the industrial area, passing large cranes and shipping containers in the background. Another person approaches the van, standing near the passenger side this time. [0:06:11 - 0:06:19]: The van stops near a different set of buildings. The person near the passenger side window converses with someone in the van, then starts moving away. The van subsequently resumes motion, driving further into the industrial area. [0:06:20]: The entire scene typifies a busy industrial area with continuous movement, conversations, and vehicle activity, all centered around the grey delivery van interacting with different individuals.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person who is near the black van doing right now?",
        "time_stamp": "00:05:52",
        "answer": "C",
        "options": [
          "A. Walking away from the van.",
          "B. Running towards the front of the van.",
          "C. Moving to the car's door.",
          "D. Getting into the passenger seat of the van."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_287_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:15]: A black delivery van stops at a red traffic light at an urban intersection. In the background, there are palm trees and tall buildings under a clear blue sky. The light turns green, and the van proceeds straight down the road. As the van moves forward, traffic lights, palm trees, and various buildings line both sides of the street. [0:08:15 - 0:08:20]: The van turns right at an intersection and enters a parking lot. It continues forward, heading towards a booth labeled \"24hr Parking.\" The driver parks the van near the booth, exits the vehicle, and approaches a security officer inside the booth. They engage in a conversation, and the interface shows the name \"Harry Miller\" along with options for the player to interact, such as \"Tracking Delivery,\" \"Open Stock,\" and \"Leave Conversation.\" The interface reveals inventory items such as packages and personal information, indicating interaction within a game.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the black delivery van do just now?",
        "time_stamp": "00:08:14",
        "answer": "C",
        "options": [
          "A. Stopped at a red traffic light.",
          "B. Turned right at an intersection.",
          "C. Parked near a booth labeled \"24hr Parking\".",
          "D. Entered the booth to talk to a security officer."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_287_real.mp4"
  },
  {
    "time": "0:10:40 - 0:10:51",
    "captions": "[0:10:40 - 0:10:51] [0:10:40 - 0:10:43]: The video starts with a first-person perspective of a person driving a van. The van is at a parking gate, with red barriers and a checkpoint visible ahead. The driver's view includes a dashboard camera feed. [0:10:44 - 0:10:46]: The van continues to move forward slightly, aligned with the red barriers at the checkpoint. The surrounding area includes concrete pavement and a minimal urban setup with security gates. [0:10:47 - 0:10:49]: The van advances further into the parking area, approaching some green bins. Details of the surrounding infrastructure and buildings become clearer, seen through the front windshield. [0:10:50 - 0:10:51]: As the van continues to park, the scene shifts, with varying background colors indicating different zones or sections. The motion of the van is controlled and deliberate.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the van doing right now?",
        "time_stamp": "00:10:39",
        "answer": "B",
        "options": [
          "A. Moving in reverse.",
          "B. Parking in a designated area.",
          "C. Waiting at a checkpoint.",
          "D. Loading items from green bins."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_287_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] The 20-second video begins with a close-up shot from a first-person perspective of a person holding a fan of playing cards in their hands, which includes an ace, king, queen, five, and eight of various suits. The setting is indoors, with both players seated at a rectangular table covered with a smooth, light-colored surface. Directly across the table, an individual sits facing the camera. They wear a patterned, short-sleeved shirt and are holding their own set of playing cards. This person is positioned slightly to the left of the frame. Behind them, part of the wall, a window, and a section of a chair with a dark piece of clothing draped over it are visible. On the table between the two players, there is a neat stack of face-down cards to the right, alongside two face-up cards: a three of hearts and a four of clubs. The face-up cards are more centered on the table. Throughout the video, the player holding the camera shuffles the cards in their hand multiple times and occasionally adjusts their grip. The other player remains largely stationary, occasionally lifting their cards slightly as if contemplating their next move. In the initial stages, the camera holder is seen spreading their cards into a fan and shuffling them, revealing small movements of their fingers and quick shifts in the arrangement of the cards. As the video progresses, the perspective slightly changes, indicating the camera holder is moving their hands up and down. The other player continues to maintain their focus on their own hand, occasionally looking at the center pile or adjusting their card arrangement. At one point, they gesture slightly towards the middle of the table, possibly indicating their intention to play a card or signaling something related to the game. During these interactions, the camera holder's cards change position frequently, showing combinations of numbered cards and face cards, all with different suits. The background remains consistent, maintaining a focus on the table, the two players, and the playing cards. The video concludes with the same relative arrangement of the players and cards. The camera holder still has a fan of cards in hand, and the game appears to be ongoing, with both players engaged in their next moves.\n[0:00:20 - 0:00:40] The video captures a scene of two individuals engaged in a card game, filmed from a first-person perspective. The location appears to be an indoor setting, with the players seated across from each other at a rectangular table.  In the background, a large window on the left side allows natural light to enter the room, casting bright illumination on the left side of the table. The window has some blinds partially covering it. There is also a large white pillar to the left and an office chair with a jacket draped over it on the right. Near the center of the room beyond the table, another desk or area appears to be visible. In the foreground, the first-person perspective gives a clear view of the cards held by the individual wearing the recording device, revealing a hand consisting of various cards: prominently featuring hearts, diamonds, and clubs. The hands are steadily dealing and organizing cards.  In the middle of the table, a pile of face-up cards is evident, including several face cards like kings and queens from different suits. A separate deck of face-down cards is positioned to the right of this pile. The other player is directly across the table, wearing a patterned short-sleeve shirt. This player is partially leaning forward with their forearms resting on the table and appears to be actively playing their turn, holding a fan of cards close to their chest. Throughout the video, various actions of dealing, picking up, and rearranging cards can be observed. The player across occasionally changes their posture, either leaning forward to take a card or pulling back to sort through their hand. The table itself is grey and uncluttered aside from the cards and a black object resembling a mousepad or a computing device to the right. Overall, this video captures the meticulous and concentrated atmosphere of a card game as the two players thoughtfully engage in their turns with detailed actions involving the cards they handle. The natural light, combined with the indoor office setting, provides a clear and unobstructed view of the game in progress.\n[0:00:40 - 0:01:00] The video depicts a card game being played between two individuals from a first-person perspective. The scene takes place in a well-lit room with neutral-colored walls and a large window on the left side, allowing natural light to filter in. A modern office chair and desk setup is evident.  At the start of the video, the player using the recording device holds a hand of cards, which includes the 5, K, A, 8, and 2 of hearts. Across the rectangular table sits the opponent, a figure wearing a short-sleeved, patterned shirt. The opponent is also holding a hand of cards and appears to be concentrating on their hand, with their arms resting on the table. As the game progresses, the cards on the table in the middle appear to be part of a discard pile, with various face-up cards, including a King and multiple suits of cards spread out. The player wearing the recording device moves their hand several times. First, they seem to draw a card from the deck positioned at the center-right of the table adjacent to the discard pile. With each movement, the viewer sees the player's hand changing slightly, indicating cards being drawn or played. The opponent frequently reaches forward to either place a card on the discard pile or to draw a card themselves. There is a constant, deliberate exchange of cards as both players contemplate their next moves. Notably, the opponent maintains a consistent body posture, only moving their hands while occasionally adjusting their seated position to lean slightly forward or back. Throughout the video, the recording player's hand reveals a changing set of hearts ranging from low numbers to face cards and aces. The pace of the movements is steady, maintaining a casual yet focused atmosphere indicative of a strategic card game. In summary, the video thoroughly captures a strategic exchange in a card game between two players, with clear indications of card movements and game mechanics taking place in a brightly lit, modern office environment.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What card is this person playing now?",
        "time_stamp": "00:00:11",
        "answer": "A",
        "options": [
          "A. Nine of Spades.",
          "B. Six of Spades.",
          "C. Nine of Hearts.",
          "D. Seven of Diamonds."
        ],
        "rekv": " C"
      },
      {
        "task_type": "Clips Summarize",
        "question": "What does the video primarily capture?",
        "time_stamp": "00:00:19",
        "answer": "A",
        "options": [
          "A. A card game in progress between two individuals.",
          "B. A cooking tutorial.",
          "C. A conversation between two people.",
          "D. A travel vlog."
        ],
        "rekv": " A"
      },
      {
        "task_type": "Object Recognition",
        "question": "What else is on the table besides the playing cards and a black object?",
        "time_stamp": "00:00:54",
        "answer": "A",
        "options": [
          "A. A pair of glasses.",
          "B. A computer.",
          "C. A pile of books.",
          "D. A stack of notebooks."
        ],
        "rekv": " B"
      }
    ],
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_9_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:01:31]",
    "captions": "[0:01:00 - 0:01:20] The video begins by showing a view from the first-person perspective of someone playing a card game. The setting is a well-lit room with light gray walls and a large window to the left, allowing natural light to illuminate the area. The video captures a tabletop with a smooth gray surface.  Opposite the camera is another player, wearing a short-sleeved, multi-colored patterned shirt. The opponent sits with their arms resting on the table and is holding a set of cards in their hands. A deck of cards lies in the middle of the table, with a visible mix of both face-up and face-down cards spread out chaotically. To the right of the deck is a neatly stacked pile of cards placed face-down, and further to the right is a dark-colored mouse, suggesting that this might be a multi-purpose desk. Directly in front of the camera, one can see the hands of the individual recording, holding a fan of cards that include a combination of hearts, clubs, and spades. The cards visible in the fan show numbers and faces such as 5, 6, 8, and K of hearts, and 2 and 4 of clubs, indicating that the person is in the middle of a card game. As the video progresses, the camera holder shifts the cards slightly between their hands, seemingly organizing or deciding on a move. The opponent periodically places cards on the table, adding to the pile in the middle, while the person holding the camera responds by continuing to adjust their own cards. The camera captures the quick, fluent motions of drawing and discarding cards, typical of a fast-paced card game. The opponent's movements are deliberate; they extend their arm forward to place cards onto the central pile, showing a mix of both face cards and numbered cards. The hands of the person filming often come into focus, shifting and manipulating their cards, which demonstrate a range of suits and numbers predominantly focused on the game strategy. Throughout the entire video, the scene remains focused on the competitive yet cordial environment of the card game. The predictable interval of play actions between the two players creates an engaging rhythm, with rapid card handling motions indicating a blend of seriousness and enjoyment in the gameplay. The setting remains static, ensuring the concentration stays cohesively on the card game. The engaging dynamics of the game and the detailed view from the player's perspective create an immersive experience for the viewer.\n[0:01:20 - 0:01:31] The video opens with a clear view of a person sitting across a table in a well-lit room. The person wears a patterned shirt with a predominantly grey and black design. On the table between the person and the camera is a pile of playing cards, some face up and some face down. The cards feature a mix of suits, with both red and black readily visible in the heap. To the right of the pile, a neatly stacked deck of cards sits, its face down. The person across the table is holding several cards in their hands, fanning them out and seeming to contemplate their next move in the card game. As the video progresses, it’s evident that the camera captures the hands of the person from a first-person perspective, revealing that they hold a spread of cards, predominantly hearts, with values including 5, 6, and Ace. The camera holder’s hands frequently move, interacting with the cards on the table, shuffling through their hand of cards, or reaching down to pick up new ones or place cards into play. The person across the table occasionally picks cards from the pile and adds them to their hand, their focus shifting between the cards they hold and the pile on the table. The opposing player’s facial expression changes subtly, sometimes with a slight smile, indicating their engagement in the game. Throughout the video, the room’s background remains consistent, featuring light-colored walls and large windows on the left side, allowing natural light to fill the space. Additionally, there's a visible part of a whiteboard or similar writing surface behind the person across the table, as well as a black jacket slung over the back of a chair beside them. This video captures the dynamic of a card game in progress from a first-person viewpoint, emphasizing the interaction between the players and the use of strategic thinking, indicated by the careful examination and play of the cards.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What type of shirt is the opponent wearing?",
        "time_stamp": "00:01:11",
        "answer": "A",
        "options": [
          "A. A blue and white patterned shirt.",
          "B. A plain white shir.",
          "C. A yellow patterned shirt.",
          "D. A red checkered shirt."
        ],
        "rekv": " A"
      },
      {
        "task_type": "Prospective Reasoning",
        "question": "What will the camera holder most likely do next?",
        "time_stamp": "00:01:15",
        "answer": "A",
        "options": [
          "A. Place Four of hearts onto the central pile.",
          "B. Place the Ace of Spades onto the discard pile.",
          "C. Put the Seven of Diamonds onto the foundation stack.",
          "D. Move the King of Clubs to the empty tableau column."
        ],
        "rekv": " D"
      }
    ],
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_9_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video begins with an image of a white plate, fork, and knife arranged on a light blue surface. The knife and fork are positioned to mimic the hands of a clock, set to ten o'clock. On the left of the plate, there is a vertical rectangular shape, also white. The scene transitions to a man wearing a white short-sleeved coat standing in front of a blue background with the text “RAMSAY in 10” in large bold letters. The man has his arms crossed and is looking directly at the camera. [0:00:04 - 0:00:07]: The scene changes to a modern kitchen featuring white and dark cabinets, a light-colored countertop, and hanging glass-front cabinets with various dishes and glasses. A large black clock with white numerals is on the left wall. A man in a dark t-shirt and light-colored pants walks into the scene from the left, with people visible in the background through a doorway leading to another room.  [0:00:08 - 0:00:14]: The man moves towards a kitchen island with several stovetop burners and begins to speak animatedly. He faces the camera directly, later switching to a vertical video format while continuing his presentation. The text \"Previously Recorded Live\" appears at the top right. Several pots and a pan are on the burners, and a metal spoon and other cooking utensils lie on the countertop. The background shows more kitchen equipment and ingredients neatly arranged on the counter. [0:00:15 - 0:00:19]: As the video progresses, the man leans on the counter and gazes towards the camera, continuously engaging in his dialogue. The \"RAMSAY in 10 LIVE\" logo appears on the screen in the bottom left corner. The kitchen remains the central focus, with the man's actions centered around the kitchen island and the cookware displayed.\n[0:00:20 - 0:00:40] \n[0:00:40 - 0:01:00] ",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the coat worn by the man standing in front of the blue background with the text “RAMSAY in 10”?",
        "time_stamp": "00:00:03",
        "answer": "D",
        "options": [
          "A. Black.",
          "B. Dark blue.",
          "C. Light grey.",
          "D. White."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_25_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] \n[0:02:20 - 0:02:40] \n[0:02:40 - 0:03:00] [0:02:40 - 0:02:42]: The recording starts in a bright kitchen. A person, wearing a dark navy blue shirt and dark pants, stands at a kitchen island. They are standing to the left of the stovetop, extending their right arm to hold the handle of a pot with a lid. The kitchen has white walls, and a well-lit window with a view of greenery is seen in the background. Several bottles and containers are arranged on the counter. [0:02:43 - 0:02:44]: The scene changes to a vertical frame showing the same person, now closer to the camera. They are holding the handle of a frying pan with their left hand and slightly lifting it from the stovetop. Behind them are white cabinets with glass doors displaying cups and bowls. [0:02:45 - 0:02:46]: The camera focuses on the person who lifts the frying pan to show it to the camera. The stovetop is visible with a pot on one burner and an empty burner next to it. The background continues to display the kitchen cabinets and countertops. [0:02:47 - 0:02:51]: The person places the frying pan back on the stovetop, reaches down to grab a utensil, and straightens up again. The camera captures the pots and pans on the stove, and the counter space behind them, which has a microwave, a fruit bowl, and a few kitchen utensils in holders. [0:02:52 - 0:02:55]: The person moves to the right side of the kitchen island, leaning over a wooden cutting board that holds various ingredients including a bowl of green peas, raw meat, and other chopped vegetables. Their right hand hovers above the ingredients as they appear to explain or demonstrate something to the camera. [0:02:56 - 0:02:58]: Close-up shots show more details of the ingredients on the cutting board. Explicitly, there are two bowls with green peas, a plate with what looks like raw meat slices, and a small bowl with an orange liquid, possibly beaten eggs. The person continues to move their hands and gestures as they talk. [0:02:59]: The recording ends with the person still standing by the cutting board, looking towards the camera while explaining or emphasizing a point, using hand gestures to indicate their message. The background maintains a consistent appearance with kitchen items and appliances in view.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color shirt is the person wearing right now?",
        "time_stamp": "0:02:10",
        "answer": "D",
        "options": [
          "A. Light blue.",
          "B. White.",
          "C. Green.",
          "D. Dark navy blue."
        ],
        "required_ability": "working memory",
        "rekv": " D."
      },
      {
        "task_type": "Object Recognition",
        "question": "What is the person holding with his left hand right now?",
        "time_stamp": "0:02:44",
        "answer": "D",
        "options": [
          "A. A pot lid.",
          "B. A wooden spoon.",
          "C. A bowl of peas.",
          "D. A frying pan handle."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_25_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:00 - 0:05:20] \n[0:05:20 - 0:05:40] [0:05:21 - 0:05:22]: A stovetop is visible with two black pans on an active gas burners. The left pan has water boiling over a high flame, with pasta being cooked inside, while the right pan contains some oil. The stovetop is set in a well-lit kitchen with dark cabinetry and a marble countertop. A person’s torso and arms in a blue shirt stand near the stove, preparing food;  [0:05:23 - 0:05:27]: The focus shifts to a man adding diced meat to the pan with oil, using his hands to drop the pieces in. The meat begins to sizzle as it contacts the heated oil. The man is working carefully, ensuring all the pieces are separated as they are added to the pan;  [0:05:28 - 0:05:31]: The man continues to brown the diced meat in the pan. His motions are deliberate, ensuring an even spread of the meat pieces for consistent cooking. The contents of the pan gradually become more prominent as the boiling pot of pasta is in the background;  [0:05:32 - 0:05:35]: The scene changes, revealing more of the kitchen. A wooden cutting board with some herbs in a bowl is visible on the marble counter. In the background, there is a dining area with a window letting in natural light. The man places the cutting board aside and picks up a white towel to wipe his hands;  [0:05:36]: The man, still holding the towel, appears to be checking or reaching for something on the stovetop near the boiling pot. The kitchen remains well-organized, with various kitchen items neatly arranged;  [0:05:37 - 0:05:38]: The man’s attention returns to the meat in the pan, stirring it with a wooden spoon to ensure even cooking. The sizzling sound of the browning meat and bubbling of the boiling water create a bustling atmosphere indicative of active cooking in progress.\n[0:05:40 - 0:06:00] ",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the man adding to the pan with oil?",
        "time_stamp": "0:05:29",
        "answer": "D",
        "options": [
          "A. Diced vegetables.",
          "B. Pasta.",
          "C. Herbs.",
          "D. Diced meat."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_25_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:04]: The video starts with a close-up shot of a person cooking, showing a stovetop with two pans. One pan, placed on the left burner, contains boiling liquid with steam rising. To its right, a frying pan is filled with ingredients being cooked—chunks of ham, mushroom slices, and bits of tomato. The person has a checked kitchen towel draped over their right shoulder and is stirring the contents of the frying pan with a spatula. The camera angle initially provides a perspective from above the pans, and then it zooms in to focus more on the food cooking in the frying pan. [0:08:05]: The scene transitions, and now the video shows the cook at a countertop, preparing additional ingredients. The wooden cutting board has a bowl placed atop it, and other ingredients like green herbs and additional uncooked meat slices are arranged nearby. The cook, with the kitchen towel still resting over their shoulder, is using a cheese grater to grate cheese into the bowl directly. [0:08:06 - 0:08:19]: The cook grates cheese continuously over the bowl, and the scene maintains an over-the-shoulder view, showing the process in detail. The grated cheese flakes fall into the bowl, which already contains egg yolks that are slightly visible at the bottom. The camera angle shifts slightly to focus on the exact motions of the cook's hands, demonstrating the grating technique. The setup remains consistent, highlighting the meticulous cooking process. Occasionally, the scene shifts to show the cook’s facial expressions and upper body, further emphasizing the attention to detail in the preparation of the dish.\n[0:08:20 - 0:08:40] \n[0:08:40 - 0:09:00] ",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What ingredients are being cooked in the frying pan?",
        "time_stamp": "0:08:04",
        "answer": "D",
        "options": [
          "A. Chunks of beef, onion slices, and bits of pepper.",
          "B. Slices of chicken, garlic cloves, and bits of zucchini.",
          "C. Pieces of fish, broccoli florets, and bits of carrot.",
          "D. Chunks of ham, mushroom slices, and bits of pepper."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_25_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the current weather like outside?",
        "time_stamp": "00:00:03",
        "answer": "C",
        "options": [
          "A. Rainy and cloudy.",
          "B. Snowy with heavy clouds.",
          "C. Sunny with clear skies.",
          "D. Windy and overcast."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_452_real.mp4"
  },
  {
    "time": "[0:01:37 - 0:01:42]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the suv that is visible ahead?",
        "time_stamp": "00:01:41",
        "answer": "C",
        "options": [
          "A. Orange.",
          "B. Blue.",
          "C. Black.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_452_real.mp4"
  },
  {
    "time": "[0:03:14 - 0:03:19]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What brand name is visible on the large sign on the left side of the road?",
        "time_stamp": "00:03:18",
        "answer": "B",
        "options": [
          "A. WESTIE.",
          "B. SYNAPSE.",
          "C. FASHION.",
          "D. COFFEE."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_452_real.mp4"
  },
  {
    "time": "[0:04:51 - 0:04:56]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What brand name is visible on the building to the right?",
        "time_stamp": "00:04:54",
        "answer": "A",
        "options": [
          "A. 3HB.",
          "B. 3HT.",
          "C. 5HB.",
          "D. 5HT."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_452_real.mp4"
  },
  {
    "time": "[0:06:28 - 0:06:33]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the current color of the nearest car parked on the left side of the road?",
        "time_stamp": "00:06:30",
        "answer": "C",
        "options": [
          "A. Black.",
          "B. Red.",
          "C. White.",
          "D. Blue."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_452_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is visible on the right side of the road right now?",
        "time_stamp": "00:00:15",
        "answer": "B",
        "options": [
          "A. A fence.",
          "B. A small white sign.",
          "C. A utility pole.",
          "D. A parked vehicle."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_173_real.mp4"
  },
  {
    "time": "[0:02:08 - 0:02:28]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "On which side are the two walkers located right now?",
        "time_stamp": "00:02:13",
        "answer": "A",
        "options": [
          "A. On the right side.",
          "B. In the middle of the path.",
          "C. On the left side.",
          "D. Not visible."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_173_real.mp4"
  },
  {
    "time": "[0:04:16 - 0:04:36]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located on the left side of the road right now?",
        "time_stamp": "00:04:22",
        "answer": "B",
        "options": [
          "A. A row of houses.",
          "B. A field.",
          "C. A dense forest.",
          "D. A river."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_173_real.mp4"
  },
  {
    "time": "[0:06:24 - 0:06:44]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located to the right side of the cyclist right now?",
        "time_stamp": "00:06:47",
        "answer": "C",
        "options": [
          "A. A parked black car.",
          "B. An open field.",
          "C. A white building.",
          "D. A wooded area."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_173_real.mp4"
  },
  {
    "time": "[0:08:32 - 0:08:52]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the tree shadow falling on the road right now?",
        "time_stamp": "00:08:50",
        "answer": "B",
        "options": [
          "A. On the left side of the road.",
          "B. On the right side of the road.",
          "C. In the center of the road.",
          "D. No shadows are visible."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_173_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a completely black frame.  [0:00:01 - 0:00:04]: The scene shifts to the interior of an electronic device, with many illuminated components in various colors including green and blue. The lighting changes from green to blue, illuminating the intricate design of the device. [0:00:05 - 0:00:06]: The inside view shows different colorful lights glowing, with components visible against a backdrop of purple and pink hues. A mesh-like pattern, part of the device chassis, reads “G*FIRE RTX.” [0:00:07 - 0:00:08]: The perspective changes to the external monitor showing a first-person shooter game. The player is navigating through different environments in the game on a sleek black monitor. [0:00:09 - 0:00:10]: The shot now includes a sleek, white computer case positioned next to the monitor. The setup includes a black mouse mat on a wooden surface. [0:00:11 - 0:00:17]: A man in a black shirt is presenting several computer parts on a desk. He holds a blue box labeled “Intel.” The scene shows him explaining the components, with a PSU box and another smaller box next to him. [0:00:18 - 0:00:19]: The final frames feature a close-up of the side of the computer case, which has a perforated metal design, probably for ventilation. A hand appears near the device, suggesting further interaction.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What label is visible on the computer component inside the electronic device right now?",
        "time_stamp": "00:00:06",
        "answer": "A",
        "options": [
          "A. GEFORCE RTX.",
          "B. FIRE GTX.",
          "C. GEFORCE GTX.",
          "D. G*FORCE GTX."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_118_real.mp4"
  },
  {
    "time": "[0:02:40 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:44]: Two hands work on a motherboard placed on a light wooden surface. The left hand holds the motherboard steady, while the right hand uses a thermal paste applicator to spread thermal compound on the CPU. The motherboard has multiple ports and components, including visible RAM slots, heatsinks, and capacitors. [0:02:45 - 0:02:47]: The right hand installs a Cooler Master heatsink onto the motherboard's CPU. The hand adjusts the position and secures the heatsink onto the CPU. The motherboard continues to sit on the light wooden surface. [0:02:48 - 0:02:51]: The hands then install a cooling fan onto the mounted heatsink. The right hand secures the fan in place while the left hand holds the heatsink for stability. The fan has a label on it and appears to be black in color. The motherboard's various ports and components remain in view. [0:02:52 - 0:02:54]: The person lifts the motherboard with the newly installed heatsink and fan. They hold it up, showing it from different angles. The background includes a large space with some furniture, indicating an indoor environment. Nearby, a screw and a screwdriver lie on the surface. [0:02:55 - 0:02:59]: The sequence transitions to unboxing and setting up a small, white computer case. The person places the case on the same light wooden surface, removes the protective plastic packaging, and reveals the case's perforated design and ports. The case has multiple vents and appears compact.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What brand of heatsink is being installed onto the motherboard's CPU right now?",
        "time_stamp": "0:02:47",
        "answer": "B",
        "options": [
          "A. Noctua.",
          "B. Cooler Master.",
          "C. Corsair.",
          "D. Be Quiet!."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_118_real.mp4"
  },
  {
    "time": "[0:05:20 - 0:05:40]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:25]: The video begins with a webpage featuring a product image of a black fan cooler on the center of the screen. The background of the webpage is black, and there are navigation links at the top labeled “PRODUCTS,” “ABOUT ARCTIC,” and “COMMUNITY.” There is a white logo in the upper left corner. Specific texts highlight \"P12 PWM PST\" and options for the fan's color selection.  [0:05:26 - 0:05:31]: The view shifts to a top-down angle of a computer build. A person with short hair is working on the components of a compact computer case. The case is open, and multiple cables are visible. The individual is connecting or adjusting cables inside the case. There are visible components such as a power supply and a cooler. [0:05:32 - 0:05:33]: The person continues to adjust the components and cables of the computer case.  [0:05:34 - 0:05:34]: The perspective switches to a side view, showcasing the back of the computer case. The person is moving the case to align it and continues managing the internal components. [0:05:35 - 0:05:37]: The person is seen positioning and organizing wires inside the case, while maintaining a detailed focus on the PC build’s assembly process. [0:05:38 - 0:05:40]: The person is handling the case with both hands adjusting the inner and outer parts, preparing to either close it or further arrange the internal setup. There are multiple fans visible through the side ventilation of the case.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What product image is shown on the webpage right now?",
        "time_stamp": "00:05:20",
        "answer": "B",
        "options": [
          "A. White CPU cooler.",
          "B. Black fan cooler.",
          "C. RGB lighting.",
          "D. Transparent case fan."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_118_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:08:20]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00]: The video begins with a display of an MSI installation window on a computer screen. The window is partially transparent, showing the progress of an installation process. The background consists of another application in white color with a menu on the left-hand side. [0:08:01]: The scene transitions to a different window displaying an MSI interface on a computer screen. The interface includes various controls and indicators related to computer performance. The background features a vivid, gradient display with shades of blue and purple. [0:08:02]: Another window appears, showing a \"Voltage/Frequency curve editor\" with a graph plotting frequency versus voltage. The x-axis represents frequency in MHz and the y-axis represents voltage in mV. A slight curvature is visible in the graph. [0:08:03 - 0:08:11]: Both the MSI interface and the Voltage/Frequency curve editor are displayed side by side. The graph in the voltage/frequency editor shows modifications, with the curve displaying adjusted points. The MSI interface continues showing performance metrics such as temperature and clock speed. There's a focus on the changes in the metrics on the MSI interface as different settings may be adjusted. [0:08:12 - 0:08:14]: A close-up of another computer application focuses on settings related to CPU voltage. The interface lists several settings for CPU Core/Cache Voltage with options such as \"Auto,\" \"Offset Mode,\" and \"Fixed Mode.\" Descriptions for input voltage and system stability are visible on the right side of the window. [0:08:15 - 0:08:18]: The same voltage settings interface shows a selection being made for \"Offset Mode,\" with further calibration settings visible. Each setting indicates different voltage levels, with accompanying options set to \"Auto.\" Descriptions about system damage risk when overvoltage occurs remain visible. [0:08:19]: Finally, the video closes with a view of a hand holding a computer processor on a neutral background. The processor's details are partially readable, displaying some text and its pin layout.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What setting is being selected in the voltage settings interface right now?",
        "time_stamp": "00:08:18",
        "answer": "C",
        "options": [
          "A. Auto Mode.",
          "B. Fixed Mode.",
          "C. Offset Mode.",
          "D. Manual Mode."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_118_real.mp4"
  },
  {
    "time": "[0:09:40 - 0:09:59]",
    "captions": "[0:09:40 - 0:09:59] [0:09:40 - 0:09:41]: The video features a scene featuring a racing simulation game displayed on a monitor. The top portion of the screen includes performance metrics such as graphics card (RTX 3080) temperature and performance. Details of drivers like Sargeant and Leclerc are visible along the bottom portion of the screen.  [0:09:42 - 0:09:47]: The scene transitions to a first-person view of an individual setting up a computer station. A close-up of their hand interacting with the desk setup is seen, involving a white computer case and a monitor displaying a first-person perspective racing game. The setup is on a wooden desk, and the person seems to be adjusting components. A microphone arm is visible on the left side of the frame. [0:09:48 - 0:09:52]: The individual, now fully visible, is explaining and demonstrating something about the computer case. They remove the side panel of the case to show internal components or specific features of the hardware. [0:09:53 - 0:09:57]: The person continues to describe aspects of the computer's hardware while pointing to various parts. Their facial expressions and body orientation suggest they are engaged in providing detailed explanations. [0:09:58]: The video concludes with a focused shot of the white computer case. The case has a perforated side panel through which illuminated internal components can be seen. The scene emphasizes the aesthetic and functional design of the computer setup.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What hardware performance metric is visible at the top of the screen right now?",
        "time_stamp": "00:09:41",
        "answer": "B",
        "options": [
          "A. CPU clock speed.",
          "B. Temperature, power, framerate for cpu and gpu.",
          "C. Network speed.",
          "D. Memory usage."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_118_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: A structure made of colorful interlocking blocks is shown, forming an archway. Underneath the archway, a toy car is positioned. In the top left corner, an inset image displays a small parking lot with several toy cars. The text \"Car Parking\" is visible on the right side. [0:00:04]: The scene changes to a plain, light-gray surface. [0:00:05 - 0:00:06]: A hand appears, placing a yellow, rectangular block with protruding studs onto the surface. [0:00:07 - 0:00:08]: The yellow block remains on the surface while the hand is no longer in view. [0:00:09 - 0:00:11]: The hand reappears, holding another similar yellow block, and places it adjacent to the first block, connecting them side by side. [0:00:12 - 0:00:13]: Both connected yellow blocks remain on the surface, with the hand once again out of view. [0:00:14 - 0:00:15]: The hand reappears with another yellow block and proceeds to attach it to the previous two blocks, extending the connected row. [0:00:16]: The hand slightly hovers as if a slight adjustment is being made to the arrangement of the blocks. [0:00:17]: The four connected yellow blocks remain on the surface in a line. [0:00:18 - 0:00:19]: The hand appears again, this time placing yet another yellow block, extending the previously connected line of blocks.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is positioned underneath the archway made of colorful interlocking blocks?",
        "time_stamp": "00:00:03",
        "answer": "D",
        "options": [
          "A. A small tree.",
          "B. A toy truck.",
          "C. A small building.",
          "D. A toy car."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Event Understanding",
        "question": "How many yellow blocks are connected in a line?",
        "time_stamp": "00:00:19",
        "answer": "C",
        "options": [
          "A. Two.",
          "B. Three.",
          "C. Four.",
          "D. Five."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_207_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:02]: A hand is placing a rectangular, orange brick piece on top of a formation of yellow and blue brick pieces on a flat, white surface. The yellow pieces are forming the base, and blue pieces are atop some of the yellow pieces. The position of the hand indicates the action of adding pieces to the left side of the formation. [0:01:03 - 0:01:04]: After the orange brick is placed, the hand is removed, leaving a structure that includes yellow, blue, and orange blocks. The orange piece is attached on top of the yellow base on the left side. [0:01:05 - 0:01:06]: The same hand reappears, holding a yellow brick piece, and positions it to attach to the existing formation. This hand seems to be adjusting or adding more components to the left side of the structure. [0:01:07 - 0:01:09]: A green brick piece is picked up and placed onto the left side, extending the formation. The structure is symmetrical, with alternating colors of yellow, orange, and green on one side, and blue bricks in the middle. [0:01:10 - 0:01:11]: The hand moves to the right side of the formation and interacts with the bricks, perhaps adjusting or preparing to add more pieces. One orange-colored piece is already placed on this side. [0:01:12 - 0:01:14]: On the right side, the hand adjusts or adds more bricks to balance both sides of the structure. This includes more orange bricks to match the other side's design. [0:01:15 - 0:01:17]: The hand carefully adjusts the rightmost side yellow pieces, ensuring the design's symmetry, aligning them with precision as the structure becomes more extended on both sides. [0:01:18 - 0:01:20]: More pieces are added on the right side to complete the symmetry with the left. The formation now features a balanced structure with yellow, blue, orange, and green bricks, spread evenly. The hand then exits the frame, leaving the structure complete.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the green brick placed relative to the yellow bricks in the structure?",
        "time_stamp": "0:01:09",
        "answer": "D",
        "options": [
          "A. In the middle.",
          "B. On the right side.",
          "C. On top of the blue bricks.",
          "D. On top of the yellow bricks."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_207_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:08]: The video shows a first-person perspective of someone creating a structure using colorful toy bricks on a light-colored surface. Various brick colors, including blue, yellow, green, and orange, are visible. The individual is holding and placing bricks, adding pieces to a partially completed structure that has two main levels.  [0:03:09 - 0:03:12]: The person continues to add more yellow bricks on top of the existing multicolored structure, using one hand to place the bricks. [0:03:13 - 0:03:16]: The individual adjusts the position of the yellow bricks to secure their placement on the structure. The hand is mostly visible, focusing on ensuring the stability of the added pieces. [0:03:17]: The person's hand is removed, and the completed structure is shown. Various connected sections of colorful bricks align horizontally with different themes of yellow, green, and orange layers. [0:03:18 - 0:03:19]: The scene changes slightly to showcase a new set of connected bricks arranged in a bordered rectangular shape on the surface. A hand is seen moving towards the arrangement, likely about to make adjustments or additions.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What does the person do after adding more yellow bricks?",
        "time_stamp": "00:03:16",
        "answer": "C",
        "options": [
          "A. Removes the yellow bricks.",
          "B. Adds green bricks.",
          "C. Adjusts the yellow bricks.",
          "D. Adds blue bricks."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Causal Reasoning",
        "question": "What is the person likely to do next after moving their hand towards the new set of connected bricks?",
        "time_stamp": "00:03:19",
        "answer": "B",
        "options": [
          "A. Remove the connected bricks.",
          "B. Add more bricks to the arrangement.",
          "C. Paint the bricks.",
          "D. Disassemble the structure."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_207_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:00:13",
        "answer": "B",
        "options": [
          "A. How to calculate the probability of an event.",
          "B. The definition of basic probability.",
          "C. An example involving multiple dice rolls.",
          "D. The combination formula in probability."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_215_real.mp4"
  },
  {
    "time": "[0:02:22 - 0:02:52]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:02:53",
        "answer": "C",
        "options": [
          "A. The concept of mutually exclusive events.",
          "B. Introduction to experimental probability.",
          "C. Explanation of rolling a six-sided die.",
          "D. Definition of independent events."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_215_real.mp4"
  },
  {
    "time": "[0:04:44 - 0:05:14]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What concept might the speaker explain next?",
        "time_stamp": "00:05:05",
        "answer": "D",
        "options": [
          "A. How to find the sum of rolled dice values.",
          "B. How to design an experiment with coins.",
          "C. The process of drawing bar graphs for data.",
          "D. The probability of rolling a specific number with a die."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_215_real.mp4"
  },
  {
    "time": "[0:07:06 - 0:07:36]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:08:01",
        "answer": "B",
        "options": [
          "A. Addition of probabilities.",
          "B. The probability of this disc rotating to the blue area.",
          "C. Expected value calculation.",
          "D. Subtraction of probabilities."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_215_real.mp4"
  },
  {
    "time": "[0:09:28 - 0:09:58]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:10:00",
        "answer": "B",
        "options": [
          "A. The concept of probability distributions.",
          "B. Calculating the probability for yellow colors.",
          "C. How to convert decimals to fractions.",
          "D. Applying the formula to find expected value."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_215_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:05]: The video begins with a night-time setting, showing the exterior of a building with a brightly lit shop window displaying various electronics, including multiple monitors displaying the same 'Expedition' logo on a brick wall inside. Several cameras on tripods and other tech equipment are positioned within the window display. The shop’s sign reads \"Electronics\" and a door with a \"207\" plaque is visible to the right, while part of an ajar red door can be seen to the left. [0:00:06 - 0:00:10]: The perspective moves to the right, revealing a small street or alleyway lined with several storefronts, including \"Katner's Deli\" featuring a vibrant red door and another display window advertising sandwiches, breads, and snacks. Nearby shop signs and a few planters with flowers are also visible under warm lighting that decorates the façade. To the right, shop fronts continue down the street with more details becoming visible, such as bright lights and signs. [0:00:11 - 0:00:15]: The camera continues further along, now showing a themed environment that includes a large animatronic dog hanging from a crane over the street, which is part of a ride or attraction. There are cardboard boxes with various cartoon-style graphics, such as an image of a car, and characters from a popular animated franchise, enhancing a fun, playful atmosphere. The structures are reminiscent of a miniature town or theme park, with several small streets and detailed building faces. [0:00:16 - 0:00:20]: The view moves alongside a large yellow vehicle, likely a construction machine, which follows the track along the small streets filled with colorful decorations and thematic objects. The backdrop includes detailed murals of a skyline at dusk, and vibrant foliage adorning the scene, contributing to an immersive, well-crafted environment. The movement seems fluid and integrates seamlessly with the setting, ensuring engagement with various interactive elements consistent with a theme park attraction.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the door of Katner's Deli?",
        "time_stamp": "00:00:10",
        "answer": "C",
        "options": [
          "A. Blue.",
          "B. Green.",
          "C. Red.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Event Understanding",
        "question": "What appears to be part of the theme park attraction?",
        "time_stamp": "00:00:11",
        "answer": "A",
        "options": [
          "A. A large animatronic dog hanging from a crane.",
          "B. A live music performance.",
          "C. A magic show.",
          "D. A water fountain display."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_309_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:42]: The scene is set in an indoor area resembling a pet store or a themed attraction. The room is well-lit with a chandelier hanging from the ceiling and has a modern design. There is a large screen displaying a lion, positioned centrally on the wall. Beneath the screen, there are packages lined up neatly, possibly pet food or products. To the right, there are shelves holding various items such as bowls, pillows, and toys in different colors. To the left, there is a section designed like a ride or an attraction with benches where people are seated. [0:02:43 - 0:02:45]: The large screen now shows a pug instead of a lion. The setup of the room remains the same, with shelves on the right housing more pet products and people sitting on a ride-like structure on the left. The blue-lit area beyond the ride contains large domes and decor elements that contribute to a whimsical atmosphere. [0:02:46 - 0:02:48]: The ride vehicle appears to be moving, revealing more of the blue-lit section on the left side with large, spherical decorations reminiscent of bubbles. A shiny, metallic figure stands in the background. The shelves on the right are stocked with pet food and accessories, including toys and bowls. [0:02:49 - 0:02:50]: The ride vehicle continues to move forward, providing a clearer view of the shelves loaded with various products and another part of the blue-lit area containing more thematic elements such as large bubbles and a clock showing the face of a cat. [0:02:51 - 0:02:52]: The focus shifts towards a white, fluffy cat figure on a pedestal near the blue-lit area. The shelves next to the cat figure display more pet food packages and toys. On the wall behind the cat figure, there is a poster labeled \"pet wash,\" suggesting a themed pet washing area. [0:02:53 - 0:02:55]: The camera position seems stationary for a moment, showing the cat figure upright on the pedestal with the \"pet wash\" sign clearly legible in the background. Adjacent shelves slightly change their visibility, still showing various pet products. [0:02:56 - 0:02:57]: The blue-lit area in the background shows animated elements like bubbles and machinery parts, enhancing the thematic feel. The centerpiece of the blue-lit area appears to be a pet washing station, consistent with the sign nearby. [0:02:58 - 0:02:59]: The ride seems to have moved a bit forward, revealing more of the theme. Bright, soft lighting illuminates carefully placed decorations in the blue area. The clock is still visible in the background, contributing further to the playful and immersive atmosphere.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is displayed on the screen now?",
        "time_stamp": "0:02:40",
        "answer": "A",
        "options": [
          "A. A lion.",
          "B. A tiger.",
          "C. A pug.",
          "D. A cat."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_309_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:28]: Characters in oversized costumes, resembling Minions and a person in a hat, are standing in front of a brightly colored entrance with the sign \"Home for Girls\" above it. Some adults and children are posing for photos. There are people standing on either side, including an individual taking photos and other visitors observing the scene. Decorative topiary bushes are present on either side of the entrance.  [0:05:29 - 0:05:34]: The scene expands, showing more visitors lined up along a barrier. The people in line appear to be waiting their turn to approach the character area. The background includes detailed brick architecture, colorful wall designs, and various themed decorations that suggest this is part of an amusement park. [0:05:35 - 0:05:39]: The camera angle shifts slightly, revealing an archway with the title \"Fun Land\" in the background. The crowd density increases, displaying visitors walking around and observing the area. Amusement park attractions and booths become visible, indicating this is a populated, lively space in the park.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is written above the brightly colored entrance now?",
        "time_stamp": "00:05:26",
        "answer": "C",
        "options": [
          "A. Home for Children.",
          "B. Welcome to the Park.",
          "C. Home for Girls.",
          "D. Fun Land."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_309_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:04]: The video begins with a first-person view of a colorful amusement park or carnival. A wide walkway extends into the distance, flanked by a variety of vibrant attractions and booths. On the left, a tall green tower with spiral decorations looms, reminiscent of a whimsical fantasy structure. To the right, there are booths adorned with striped awnings selling toys and food items, with one of them labeled \"MINI TOSS.\" The center background showcases a purple castle-like building with teal rooftops and flags, contributing to the playful and festive atmosphere.  [0:08:05 - 0:08:11]: As the scene progresses, the viewpoint shifts slightly to the right, approaching a section with more attractions. The focus moves towards a peculiar ride on the right side, featuring three costumed characters inside a futuristic rocket-like vehicle with a transparent dome. Nearby, vibrant decorations and merchandise stalls add to the lively setting, while a few people are observed walking by, enjoying their day.  [0:08:12 - 0:08:15]: The camera continues to pan slowly towards the ride, highlighting the details of the characters inside the rocket vehicle. They exhibit cheerfully exaggerated expressions, adding to the whimsical nature of the scene. The background remains consistent with colorful booths and buildings. [0:08:16 - 0:08:18]: A closer view of the rocket ride and the area around it is shown. The nearby flora and decorative elements are clearer, revealing a well-maintained garden with clipped bushes and flowers. Further to the right, more details of the attractions and crowd are visible. [0:08:19 - 0:08:20]: The final frames focus back on a nearby stall brightly painted in blue and green, with plush toys and colorful merchandise on display. The general lively and cheerful environment of the amusement park persists.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the green tower with spiral decorations located relative to the walkway?",
        "time_stamp": "0:08:04",
        "answer": "A",
        "options": [
          "A. To the left.",
          "B. To the right.",
          "C. In the center.",
          "D. In the background."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_309_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The frame shows a completely black screen with a timestamp in the top left corner. [0:00:01 - 0:00:02]: A structure made of colorful interlocking plastic building blocks is visible. The structure resembles a helicopter, with blue pieces forming the rotor blades, and other pieces in yellow, green, and red colors. The word \"Helicopter\" is written in large blue font on the right side. The structure is placed on a light-colored textured surface. [0:00:03 - 0:00:05]: The focus is on a light-colored textured surface, and various bright-colored pieces of interlocking blocks are present on the left and right sides. A hand appears from the top of the frame, placing a yellow block in the center of the frame. [0:00:06 - 0:00:07]: The hand moves away after placing the yellow block at the center. Another hand appears with a green block, moving it towards the center of the frame. [0:00:08 - 0:00:10]: The hand places the green block adjacent to the yellow block. The two blocks are aligned next to each other on the light-colored surface. [0:00:11]: The frame shows the yellow and green blocks placed next to each other. The hand has moved out of the frame. [0:00:12 - 0:00:14]: The hand appears again, this time adding another green block adjacent to the first green block. The hand then holds both green blocks together, aligning them properly. [0:00:15 - 0:00:16]: The hand adjusts the green blocks next to the yellow block, ensuring they are aligned correctly. [0:00:17 - 0:00:19]: The hand picks up a blue block and places it beside the two green blocks. The three blocks are now aligned next to the yellow block, forming a rectangular structure. [0:00:20]: The frame shows the final arrangement of the blocks on the light-colored surface.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What colors are the blocks used to form the helicopter rotor blades?",
        "time_stamp": "00:00:02",
        "answer": "A",
        "options": [
          "A. Blue and Green.",
          "B. Yellow and Blue.",
          "C. Green and Red.",
          "D. Red and Orange."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Event Understanding",
        "question": "What is the sequence of actions performed with the blocks just now?",
        "time_stamp": "00:00:23",
        "answer": "A",
        "options": [
          "A. Placing a yellow block, then two green blocks, and finally a blue block.",
          "B. Placing a blue block, then two green blocks, and finally a yellow block.",
          "C. Placing a yellow block, then a blue block, and finally two green blocks.",
          "D. Placing a yellow block, then one green block, and finally two blue blocks."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_206_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:03]: A hand is seen placing a yellow building block, which has studs on top, onto another block structure on a flat surface. The structure contains blue and green blocks underneath the yellow one. Some disorganized blocks are lying around. [0:01:04 - 0:01:07]: The hand adjusts the yellow block, ensuring it fits properly onto the structure below it. The hand uses fingers to press the yellow block securely in place. [0:01:08 - 0:01:10]: The hand adjusts position, checking the stability of the block. It appears to ensure the block is firmly attached by pressing it further down. [0:01:11 - 0:01:12]: The hand briefly leaves the block structure, showing it standing firmly on its own. More blocks of various colors are scattered around the main structure. [0:01:13 - 0:01:15]: The hand returns, grabbing the top yellow block and temporarily removing it from the structure. It lifts the block at an upward angle. [0:01:16 - 0:01:17]: The hand places the yellow block back onto the structure, reattaching it carefully. It seems to realign it for proper fitting. [0:01:18 - 0:01:20]: The hand releases the yellow block, showing the reassembled structure standing steadily. The hand retreats from the scene slowly.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the hand doing after placing three yellow blocks onto the top of the structure?",
        "time_stamp": "00:01:11",
        "answer": "D",
        "options": [
          "A. Adds another block on top.",
          "B. Removes the yellow block.",
          "C. Leaves the block structure.",
          "D. Adjusts the yellow block."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_206_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:06]: A hand is seen moving towards a structure made of colorful interlocking building blocks, which are primarily yellow, orange, green, and blue. The hand picks up a green block from the right side and attaches it to the existing structure. [0:02:07 - 0:02:11]: The hand adjusts the green block, ensuring it is properly connected. The structure now has an extended green piece. [0:02:12 - 0:02:15]: The hand picks up another yellow block and places it on top of the green extension, continuing to build the structure. [0:02:16 - 0:02:19]: The hand moves towards a red block on the left side, picks it up, and starts positioning it near the structure, possibly to attach it next.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What was the sequence of events the hand followed when building the structure just now?",
        "time_stamp": "00:02:15",
        "answer": "B",
        "options": [
          "A. Picks up a yellow block, adjusts it, then picks up a red block.",
          "B. Picks up a green block, adjusts it, then picks up a yellow block.",
          "C. Picks up a red block, adjusts it, then picks up a yellow block.",
          "D. Picks up a blue block, adjusts it, then picks up a green block."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_206_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:03:55]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:03]: A person's hands are constructing a structure using colorful plastic building blocks on a light-colored surface. The structure features various blocks in green, blue, yellow, orange, and red hues, arranged in a somewhat cross-like pattern. The person's hands are actively placing new blocks onto the structure.   [0:03:04 - 0:03:08]: The person continues to add green blocks to the structure, and then adds an orange block. The blocks are firmly attached in place. The structure is taking shape with layered and interconnected blocks.   [0:03:09 - 0:03:15]: The person begins adding blue blocks to the top of the structure, placing them horizontally across the green and orange blocks. The hands adjust the placement to ensure the blocks are secured together.   [0:03:16 - 0:03:19]: Another blue block is added to the far end of the structure near a previously added red block. The hands gently press the blocks together, ensuring a tight fit within the construction, giving the structure a more expansive spread.\n[0:03:40 - 0:03:55] [0:03:40 - 0:03:45]: A multi-colored LEGO structure featuring a cross-shaped design with overlapping blue and green beams. Additional orange and yellow LEGO bricks are integrated into the structure. A person's hand is seen adding another brick to the structure, carefully placing it on top. [0:03:46 - 0:03:50]: The scene transitions to an orange background with the text \"Thanks for watching! Don't forget to subscribe!\" written in white cursive and block letters. [0:03:51 - 0:03:54]: The video returns to the LEGO structure, displaying the final build labeled \"Helicopter\" in yellow text at the bottom. The structure remains unchanged.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action is the person performing with the blue blocks?",
        "time_stamp": "00:03:15",
        "answer": "D",
        "options": [
          "A. Adds blue blocks to the bottom of the structure.",
          "B. Adds blue blocks to the side of the structure.",
          "C. Removes blue blocks from the structure.",
          "D. Adds blue blocks to the top of the structure."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_206_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the black car parked right now?",
        "time_stamp": "00:00:36",
        "answer": "A",
        "options": [
          "A. On the right side of the road.",
          "B. On the left side of the road.",
          "C. Directly in front of the road.",
          "D. Behind the cyclist."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_167_real.mp4"
  },
  {
    "time": "[0:02:05 - 0:02:25]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the current value of 'distanz' displayed in the video?",
        "time_stamp": "00:02:25",
        "answer": "A",
        "options": [
          "A. 6.0KM.",
          "B. 6.1KM.",
          "C. 6.2KM.",
          "D. 5.9KM."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_167_real.mp4"
  },
  {
    "time": "[0:04:10 - 0:04:30]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the object seen parallel and closest to the right side of the cyclist right now?",
        "time_stamp": "00:04:30",
        "answer": "B",
        "options": [
          "A. A river.",
          "B. A line of trees.",
          "C. Another cyclist.",
          "D. A fenceline."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_167_real.mp4"
  },
  {
    "time": "[0:06:15 - 0:06:35]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the white railing located right now?",
        "time_stamp": "00:06:33",
        "answer": "A",
        "options": [
          "A. On the right side of the path.",
          "B. On the left side of the path.",
          "C. Overhead, forming an arch.",
          "D. Directly in front of the cyclist."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_167_real.mp4"
  },
  {
    "time": "[0:08:20 - 0:08:40]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the power lines located right now?",
        "time_stamp": "00:08:33",
        "answer": "C",
        "options": [
          "A. To the left of the road.",
          "B. Overhead, crossing the road.",
          "C. To the right side of the road.",
          "D. Directly in front of the cyclist."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_167_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:04]: The video begins with a close-up view of a detailed pencil drawing on a white canvas. The drawing is of a person's face adorned with floral elements in the hair. This face is incomplete, featuring the outlines of the eyes, nose, and mouth along with the floral headpiece.  [0:00:04 - 0:00:08]: A paintbrush with a fine tip is introduced to the scene, moving towards the left eye of the drawing. It starts to carefully outline the top eyelid with precision, adding more details to the drawing. [0:00:08 - 0:00:12]: The brush continues to refine the left eye, completing the top lid’s outline and then subtly moving away from the eye.  [0:00:10 - 0:00:14]: The brush shifts focus to the right eye, beginning to outline this area with similar careful strokes. [0:00:14 - 0:00:18]: The video shows the brush defining the right eye, completing the eyelid’s outline, and then starting to fill in the pupil with black paint. Both eyes’ outlines are now distinctly more visible. [0:00:18 - 0:00:20]: The final frames show the right eye’s pupil darkened completely as the brush finishes the detailed work, resulting in both eyes having significantly more definition compared to the beginning of the video.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Which part of the drawing is being detailed by the brush in the first few seconds?",
        "time_stamp": "00:00:20",
        "answer": "A",
        "options": [
          "A. The left eye.",
          "B. The right eye.",
          "C. The nose.",
          "D. The mouth."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      },
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the content just shown in the video?",
        "time_stamp": "00:00:20",
        "answer": "D",
        "options": [
          "A. A paintbrush outlines and details floral elements.",
          "B. A detailed drawing of a face is created from scratch.",
          "C. A paintbrush adds color to the floral headpiece.",
          "D. A paintbrush outlines and fills in the eyes of a detailed drawing."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_134_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:08]: A painting in progress is shown on a canvas, depicting a woman's face. Her hair is pulled back, with loose strands drawn at the top, and she's wearing earrings. Her skin is painted with a mix of light and dark tones, primarily focusing on shading around the nose and cheeks. The background is a plain off-white color. An artist's brush is visible within the frames, applying more shading and details to the woman's face. The head and face are nearly fully detailed, but parts of the hair and neck remain in sketch form. [0:03:09 - 0:03:11]: The artist continues to make detailed adjustments to the painting, focusing on the woman's lips. The brush applies more red paint to the lips, enhancing the color and definition. [0:03:12 - 0:03:14]: The artist's brush moves to different parts of the face, adding subtle shades of pink and red on the cheeks and edges of the nose. These actions provide more depth and dimension to the face. The right side of the face (from the viewer's perspective) starts reflecting a more lifelike appearance due to the new shades. [0:03:15 - 0:03:19]: The painting continues to receive minor touch-ups, especially around the eyes, nose, and lips. Each stroke of the brush adjusts and intensifies the hues, making the woman's facial features more pronounced and vibrant. The artist's hand is often visible, showing precise and deliberate brushwork. The painting gradually inches towards completion in these final moments.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the painting process just shown in the video?",
        "time_stamp": "00:03:20",
        "answer": "B",
        "options": [
          "A. An artist begins sketching a woman's face on a new canvas.",
          "B. An artist is detailing a woman's face, focusing on her eyes, cheeks, and nose.",
          "C. An artist adds a detailed background to a painting of a woman.",
          "D. An artist is drawing abstract patterns around a woman's face."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_134_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:20]: In this video, there is a close-up view of a painting on a white canvas. The painting is a portrait of a woman with prominent eyebrows and red lips, wearing earrings and flowers in her hair. The background of the scene is blurred, with some green leaves and other objects visible behind the canvas. Throughout the video, different brushes appear, making contact with the painting. At [0:06:00], a paintbrush is applied under the right eye of the portrait, then it moves upwards towards the eyebrow at [0:06:05], and the hand adjusts the strokes around the eye and the edge of the face till [0:06:13]. At [0:06:18], a wider brush is used to make strokes under the eye again, until [0:06:19], where the finishing touches are applied around the cheek area. The hand and brushes are seen making careful adjustments to the painting, adding details and final touches to the portrait.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the painting process just shown in the video?",
        "time_stamp": "0:06:20",
        "answer": "B",
        "options": [
          "A. A landscape being painted with different shades of green.",
          "B. A portrait of a woman being detailed with various brushes.",
          "C. An abstract painting with bright colors.",
          "D. A still life of fruits being created with careful strokes."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_134_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:03]: The video shows an artist painting on a vibrant green canvas using a small paintbrush. The painting is a detailed portrait of a woman with bold, dark eyebrows and bright red lips. She has flowers adorning her hair. The artist's hand moves the brush across various parts of the painting, adding strokes of color. The background includes some greenery and other objects that are slightly blurred. [0:09:04 - 0:09:08]: The artist continues with delicate touches, focusing on the floral elements in the woman’s hair. The brush is primarily used at the top center part of the painting where the flowers are. The background appears consistent, showing a mix of soft-focused elements like plants and perhaps other studio equipment or paintings. [0:09:09 - 0:09:12]: The artist's hand moves down to the lower part of the painting, adding fine details. The portrait steadily comes to life with every precise brushstroke. The woman’s expression remains serene and lifelike, enhanced by the artistic detailing. [0:09:13 - 0:09:16]: The brush adds finishing touches to the woman’s hair and forehead. The artist uses a steady and deliberate approach, ensuring each stroke enhances the already detailed features. The light in the room remains constant, showcasing the textures and colors on the canvas vividly. [0:09:17 - 0:09:19]: The artist continues to refine the portrait, particularly focusing on the forehead and the surrounding hair area. The woman's gaze in the painting is direct and engaging, giving a sense of depth and realism. The background elements stay blurred, keeping the attention on the artwork.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the content of the video?",
        "time_stamp": "0:09:19",
        "answer": "B",
        "options": [
          "A. An artist painting a landscape.",
          "B. An artist creating a portrait with vibrant colors and detailed features.",
          "C. A woman decorating her hair with flowers.",
          "D. An artist discussing painting techniques."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_134_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:40 - 0:02:00] [0:01:40 - 0:01:45]: The scene is set in a well-equipped kitchen with shelves containing various kitchenware and decor. In the foreground, there is a countertop with various cooking ingredients and utensils spread out, including a bowl of cauliflower, a chopping board with diced ingredients, butter, and a lime. A person wearing a blue T-shirt is pouring a golden liquid, likely oil, into a pan that is placed on a stovetop. The pan is held in their left hand while the right hand is pouring the oil.  [0:01:46 - 0:01:49]: The person is standing in front of the stovetop, holding a small piece of food above the pan with both hands. They are focused on the task, preparing to place the food into the hot pan. Various utensils and ingredients are visible around the stovetop.  [0:01:50 - 0:01:53]: The camera angle shifts to a close-up of the pan from the side, showing the person’s hands placing pieces of ingredients into the heated pan. The stove burners are visible, and the person's movements are precise as they add the ingredients one by one. [0:01:54 - 0:01:59]: The overhead perspective shows the person continuing to add small pieces of food into the pan with both hands. The layout of the stovetop and surrounding ingredients is clear, and the focus remains on the delicate task of placing ingredients into the pan. Eventually, the view zooms out slightly, presenting the person continuing their cooking process in a broader view of the kitchen setting.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person in the blue T-shirt pouring into the pan?",
        "time_stamp": "00:01:45",
        "answer": "A",
        "options": [
          "A. Oil.",
          "B. Water.",
          "C. Milk.",
          "D. Vinegar."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Object Recognition",
        "question": "Which of the following items is NOT mentioned as being on the countertop?",
        "time_stamp": "00:01:45",
        "answer": "A",
        "options": [
          "A. A blender.",
          "B. A bowl of cauliflower.",
          "C. A chopping board with diced ingredients.",
          "D. Butter."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_39_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:25]: The video begins with a man in a blue shirt standing behind a kitchen counter, facing a stove with multiple burners. To his left, there is a yellow bottle and a block of knives. On the countertop, several cooking utensils and ingredients are arranged. He is holding a frying pan in his left hand, and it contains chopped vegetables and other ingredients. He starts by tossing the contents in the pan while cooking over the flames. [0:03:26 - 0:03:27]: The perspective shifts slightly, giving a wider view of the kitchen. In the background, several shelves on the wall hold different dishes and kitchenware. The man gestures with his right hand as if explaining something. [0:03:28 - 0:03:29]: The scene continues with him attending to something on the counter. He picks up a red container and continues to look engaged in what he is doing. [0:03:30 - 0:03:31]: He is seen holding the red container over the frying pan and inspecting its contents carefully. Additional utensils and ingredients remain visible on the countertop, depicting a well-prepared cooking setup. [0:03:32 - 0:03:34]: The camera focuses more closely on the man's hands as he uses a spoon to scoop out some spices from the red container and add them to the frying pan. The pan continues to sizzle with the vegetables and possibly other ingredients inside it. [0:03:35 - 0:03:37]: The camera then zooms out slightly to show the man standing centered behind the stove, continuing to add the spices from the red container into the frying pan. The kitchen shelves in the background prominently display various kitchen items such as jars, cups, and cutting boards. [0:03:38 - 0:03:39]: Finally, an aerial view shows the contents of the frying pan, allowing viewers to see the mix of vegetables and possibly meat being cooked. The man’s right hand is seen as he continues to add ingredients, while the kitchen stove with multiple burners is in full view.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the man holding in his right hand right now?",
        "time_stamp": "0:03:20",
        "answer": "A",
        "options": [
          "A. A frying pan.",
          "B. A spoon.",
          "C. A knife.",
          "D. A red container."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_39_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:01]: A man in a blue T-shirt is seen in a modern kitchen with blue cabinets and white subway tiles. He is standing near the oven, pulling out a tray with a cloth. The counter in the foreground is filled with kitchen utensils, a frying pan, and some bowls with ingredients. [0:05:02 - 0:05:03]: The man places the tray on the counter and looks at the frying pan on the stove. In the background, shelves display various kitchen items, including plates, jars, and a large red \"COOK\" sign. [0:05:04 - 0:05:05]: The view shifts to a top-down perspective of the stove. The frying pan contains a mixture of food items, while a slice of bread is placed on a tray beside it. Various ingredients and utensils are scattered around the counter. [0:05:06 - 0:05:07]: The man is holding a slice of bread with both hands, inspecting it closely. He is still standing near the counter and stove with various kitchen utensils and ingredients visible. [0:05:08 - 0:05:09]: The man begins to prepare the ingredients on the stove, adjusting the frying pan and other utensils. The kitchen setup remains consistent with colorful cabinets, a large red \"COOK\" sign, and various kitchen items on the shelves. [0:05:10 - 0:05:11]: The man adjusts the frying pan on the stove. Ingredients are visible in bowls on the counter, and the setup remains organized with various kitchen tools. [0:05:12 - 0:05:13]: The man lifts the frying pan, tilting it slightly to stir the food inside. The counter and kitchen items are clearly displayed in the background, maintaining a vibrant and well-organized environment. [0:05:14 - 0:05:15]: The man uses a spoon to stir the contents of the frying pan. Slices of bread are visible on a tray beside the stove, and the counter is filled with various ingredients. [0:05:16 - 0:05:17]: The top-down view shows the man actively stirring the frying pan using a spoon and another utensil. The food appears to be well-cooked and mixed, with bread slices placed beside the stove on a tray. [0:05:18 - 0:05:19]: The man continues to stir the food in the frying pan, focusing on evenly cooking the mixture. The organized kitchen setup with utensils and ingredients remains consistent in the background.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the man in the blue T-shirt pulling out from the oven?",
        "time_stamp": "0:05:01",
        "answer": "A",
        "options": [
          "A. two bread.",
          "B. A pie.",
          "C. A cake.",
          "D. A pot."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_39_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:43]: A person, viewed from a first-person perspective, is standing in a kitchen equipped with modern appliances and a counter filled with cooking utensils, including a knife block and various tools. The person is using a fork to arrange food on a baking tray filled with browned and seasoned pieces. The tray is positioned on a stovetop. [0:06:43 - 0:06:44]: The tray now sits on the counter without being moved. The person turns their attention to a frying pan on the stove, beginning to work with its contents using a cooking utensil.  [0:06:44]: The person makes a gesture with their hand while leaning slightly to the right, indicating an action or a measurement related to cooking. [0:06:45 - 0:06:46]: The focus returns to the baking tray as more food is added to it, carefully arranging the pieces. The background remains consistent with kitchen decor, including cabinets and various cooking tools. [0:06:46 - 0:06:48]: The person shifts their attention back to the frying pan, stirring its contents with a utensil while monitoring the food in the baking tray. [0:06:48 - 0:06:49]: Again, focus returns to the baking tray, with the person adding a final touch or ingredient.  [0:06:49 - 0:06:52]: The camera angle now provides a wider view of the kitchen, including shelves with dishes and other kitchen items. The person continues to work at the stove area, adjusting and preparing the food. [0:06:52 - 0:06:54]: The person is seen taking something from a counter, possibly a cloth or similar item, before returning focus to the tray.  [0:06:54 - 0:06:55]: Using a cloth, the person carefully picks up the baking tray from the counter. [0:06:55 - 0:06:56]: A closer view of the tray being held by a cloth-covered hand, showing finely grated cheese or toppings on the food. [0:06:56 - 0:06:57]: The camera pans slightly to the side showing bits of utensils and food on the counter. Text from a producer appears asking for a time check. [0:06:57 - 0:06:58]: The person moves towards an oven, preparing to put the baking tray inside while still holding the tray carefully with the cloth. [0:06:58 - 0:06:59]: Finally, the person opens the oven door and begins to place the tray inside, showing a glimpse of the oven interior, while holding the door open with one hand and maneuvering the tray inside with the other.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person using to arrange food on the baking tray right now?",
        "time_stamp": "0:06:43",
        "answer": "A",
        "options": [
          "A. his hands.",
          "B. Spoon.",
          "C. Tongs.",
          "D. Spatula."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_39_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which brand's advertisement is visible right now on the digital billboard at the forefront?",
        "time_stamp": "00:00:10",
        "answer": "A",
        "options": [
          "A. Toshiba.",
          "B. Coca-Cola.",
          "C. Samsung.",
          "D. Apple."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_388_real.mp4"
  },
  {
    "time": "[0:01:22 - 0:01:27]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which apparel brand's sign is visible on the right side of the street?",
        "time_stamp": "00:01:24",
        "answer": "A",
        "options": [
          "A. Levi's.",
          "B. Gap.",
          "C. Puma.",
          "D. H&M."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_388_real.mp4"
  },
  {
    "time": "[0:02:44 - 0:02:49]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What are the primary colors visible on the tall vertical screen to the left?",
        "time_stamp": "00:02:45",
        "answer": "D",
        "options": [
          "A. Blue and white stripes.",
          "B. Red and yellow stripes.",
          "C. Blue and yellow stripes.",
          "D. Red and white stripes."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_388_real.mp4"
  },
  {
    "time": "[0:04:06 - 0:04:11]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the current color of the taxi right now that is at the forefront?",
        "time_stamp": "00:04:08",
        "answer": "D",
        "options": [
          "A. Black.",
          "B. White.",
          "C. Red.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_388_real.mp4"
  },
  {
    "time": "[0:05:28 - 0:05:33]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which bank's logo is visible on the red and white storefront on the left right now?",
        "time_stamp": "00:05:31",
        "answer": "A",
        "options": [
          "A. Bank of America.",
          "B. Chase.",
          "C. Wells Fargo.",
          "D. Citibank."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_388_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions occurring just now?",
        "time_stamp": "00:00:10",
        "answer": "D",
        "options": [
          "A. A barista prepares a hot beverage, adds sugar, stirs, and serves it with a lid.",
          "B. A barista retrieves an ingredient from a drawer, prepares a cold drink, adds a straw, and serves it.",
          "C. A barista prepares a cold drink, adds whipped cream, and hands it to a customer without a lid.",
          "D. A barista prepares a hot beverage, retrieves an lid, adds it, and passed it to a customer."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_368_real.mp4"
  },
  {
    "time": "[0:01:47 - 0:01:57]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions occurring just now?",
        "time_stamp": "00:01:57",
        "answer": "D",
        "options": [
          "A. An individual fills a cup with water, places it in a microwave, and heats it.",
          "B. An individual measures out flour, sifts it into a bowl, and prepares to bake bread.",
          "C. An individual cuts vegetables, arranges them on a plate, and garnishes with herbs.",
          "D. An individual prepares a cup of coffee by gathering supplies and opening a coffee bag with scissors."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_368_real.mp4"
  },
  {
    "time": "[0:03:34 - 0:03:44]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taking place just now?",
        "time_stamp": "00:03:44",
        "answer": "D",
        "options": [
          "A. An individual measures coffee beans, grinds them, and prepares the ground coffee in a portafilter.",
          "B. An individual weighs ground coffee, adjusts the grinder settings, and grinds more beans.",
          "C. An individual selects coffee beans, roasts them, and prepares them for grinding.",
          "D. An individual adds coffee grounds to a portafilter, weighted it, and used spoon to adjust it."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_368_real.mp4"
  },
  {
    "time": "[0:05:21 - 0:05:31]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions occurring just now?",
        "time_stamp": "00:05:30",
        "answer": "D",
        "options": [
          "A. An individual grinds coffee beans, pours them into a filter, and makes a pour-over coffee.",
          "B. An individual selects a coffee pod, inserts it into a machine, and prepares a cup of coffee.",
          "C. An individual brews a pot of coffee, pours it into a carafe, and serves it to customers.",
          "D. An individual measures coffee grounds, tamps them, and prepares an espresso."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_368_real.mp4"
  },
  {
    "time": "[0:07:08 - 0:07:18]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions occurring just now?",
        "time_stamp": "00:07:18",
        "answer": "D",
        "options": [
          "A. An individual measured coffee beans, ground them, and filled a cup with the ground coffee.",
          "B. An individual steamed milk, brewed an espresso, and combined them to make a latte.",
          "C. An individual brewed coffee, added sugar, and served it with a stir stick.",
          "D. An individual brewed an espresso, poured milk into it, and adorned it with a latte art heart."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_368_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a close-up view inside a retail store, showcasing a wooden shelf on the left. The shelf holds clothing, including white T-shirts and colorful button-up shirts with a tropical print. A green shirt with a small logo on the chest is hanging next to other similarly styled apparel. The shelves have potted plants placed on top. [0:00:01 - 0:00:02]: The camera moves slightly to the right, keeping the wooden shelf in the frame. The focus is still on the clothing, and more hats and shirts become visible. A mirror is seen at the end of the aisle, reflecting part of the store. [0:00:02 - 0:00:03]: The view shifts further to the right, revealing a large stone pillar in the center of the frame. The background still shows shelves filled with various clothing items, including a section with graphic T-shirts. [0:00:03 - 0:00:06]: Moving past the stone pillar, the store opens up, revealing more of its interior. Multiple display stands with merchandise are placed in the middle of the floor. The floor is patterned with hexagonal tiles. There is a display of a small, kiddie ride-on vehicle, designed like a Jurassic Park jeep in the center of the store with a screen above showing footage from the movie. [0:00:06 - 0:00:10]: The camera continues to move through the store, passing the ride-on vehicle. On the right, another shopper is visible browsing items. The walls are adorned with various products, and the lighting is bright. There is a mix of natural and artificial light, with visible sunlight coming from large windows or doors toward the back. [0:00:10 - 0:00:15]: As the camera approaches the back of the store, the display becomes clearer. More merchandise, such as toys and themed apparel, is organized meticulously on the shelves. Counters and racks display products orderly, and the decor involves a theme consistent with the Jurassic Park branding. [0:00:15 - 0:00:20]: Near the back entrance or exit, large windows or glass doors allow daylight to illuminate the area. The store’s layout leads customers toward this exit, passing by stands with various souvenirs and clothing items. The colors and organization maintain a vibrant and welcoming atmosphere throughout.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What item is prominently displayed in the center of the store?",
        "time_stamp": "0:00:08",
        "answer": "D",
        "options": [
          "A. A large stone pillar.",
          "B. A wooden shelf with potted plants.",
          "C. A mirror reflecting part of the store.",
          "D. A kiddie ride-on vehicle."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_319_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:25]: The video opens with a first-person view inside a dimly lit hallway with metallic railings forming a queue path. The floor appears to be made of glossy black material, and the walls are a combination of tan and metallic finishes. On the left wall, several panels and screens are visible, displaying various technological graphics. Signs and a trash bin are also visible. The lighting is minimal but strategically placed, emitting from the ceiling and small fixtures on the walls. [0:02:26 - 0:02:28]: Continuing down the hallway, the focus remains on the row of screens along the left wall, each showing different information. Wiring and conduits run along the walls, adding to the industrial feel. The lighting adjusts slightly, creating a more immersive and focused environment as the path leads forward toward a glowing blue door at the end of the corridor. [0:02:29 - 0:02:34]: As the person moves further along the hallway, the right wall becomes more visible, with additional signage and lighting features. The hallway opens slightly ahead to reveal more of the facility, including another set of doors with similar blue illumination. The pathway is guided by metallic handrails, and more details become apparent, such as a sign with warning instructions near the glowing door at the end. [0:02:35 - 0:02:36]: The person approaches a transition space leading into a larger, more central room. The walls become more structured and reinforced, and the glowing blue light intensifies, highlighting a significant cylindrical structure in the middle of the upcoming room. This structure is surrounded by handrails, indicating its importance or potential hazard. [0:02:37 - 0:02:39]: Entering the central room, the glowing cylindrical object becomes the focal point, emitting a radiant blue light. The room is circular, and the walls are lined with various control panels and doors marked with identifiers such as \"E3.\" The lighting in this area is more vivid, enhancing the futuristic, high-tech atmosphere. The cylindrical structure appears to be a core component, possibly a reactor or central hub in this facility.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What material is the hallway floor made of?",
        "time_stamp": "00:02:25",
        "answer": "D",
        "options": [
          "A. Carpet.",
          "B. Wood.",
          "C. silver.",
          "D. Glossy black material."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_319_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:44]: The video begins with a futuristic and dynamic scene featuring large, metallic robots engaging in combat. Bright and vibrant colors like orange and blue dominate the visuals. The setting appears to be an industrial or urban area, illuminated with multiple bright lights. [0:04:45 - 0:04:47]: The perspective shifts slightly to reveal more of the surrounding environment. The area is filled with industrial equipment and buildings with bright lights shining from various angles, creating a sense of depth and intensity. A close-up view of the walls and architecture is visible as the camera moves. [0:04:48 - 0:04:49]: The camera appears to move further into the surrounding structure, revealing intricate details of the walls and light fixtures. The background transitions to show a nighttime cityscape with illuminated buildings and a busy, industrial atmosphere enhanced by bright spotlights and dynamic lighting effects. [0:04:50 - 0:04:56]: The focus shifts back to the combat between the robots, which are now more distinguishable with their detailed features and vibrant colors. The larger robot predominantly has blue and red parts and is standing tall in a powerful stance. The intense action scene continues with rapid movements and dramatic lighting creating a striking visual impact. [0:04:57 - 0:04:59]: The camera zooms out to show a wider view of the cityscape. The robots continue their battle amidst the industrial background. Neon lights, reflective surfaces, and the detailed city setting contribute to a futuristic and intense atmosphere. The scene is dynamic, with high-speed movements and flashing lights that convey the fast-paced action.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What objects are primarily featured in the video scene right now?",
        "time_stamp": "00:04:55",
        "answer": "D",
        "options": [
          "A. Cars.",
          "B. Humans.",
          "C. Animals.",
          "D. Robots."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What are the predominant colors in the visuals of the video?",
        "time_stamp": "00:05:00",
        "answer": "D",
        "options": [
          "A. Red and yellow.",
          "B. Green and purple.",
          "C. Black and white.",
          "D. Orange and blue."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_319_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:01]: The video begins with a first-person perspective. A large, yellow and silver robot stands prominently to the left of the screen, with a background of dimly lit stone architecture and blue ambient light.  [0:07:01 - 0:07:03]: Then, the scene transitions to focus on a road with yellow lines and structural elements, such as beams and cables, overhead. The camera seems to move quickly, giving the impression of rapid motion. [0:07:03 - 0:07:06]: In the following frames, the focus shifts to another robot, larger and more detailed, showing red, blue, and silver components. It appears in a brightly lit area which contrasts with the surrounding dark environment. The background is filled with industrial elements. [0:07:06 - 0:07:12]: The camera continues to focus on this robot, which seems to be moving or coming closer to the viewer. The lighting emphasizes its metallic surfaces and the complex machinery within its body. [0:07:12 - 0:07:14]: The scene shows the robot in blue light, with more details of its formidable structure becoming visible. The background shows cityscape elements and industrial infrastructure. [0:07:14 - 0:07:15]: Attention then shifts again to another dark environment, where various mechanical and structural elements with red and orange lighting effects create a chaotic scene. It suggests destruction or action taking place. [0:07:15 - 0:07:19]: This segment focuses on this chaotic environment with structural damages and intense lighting effects, including red and orange hues highlighting the wreckage and debris scattered around. [0:07:19 - 0:07:20]: The scene maintains the same dark and chaotic environment, emphasizing further the detailed wreckage and mechanical elements within the frame.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is prominently featured on the screen right now?",
        "time_stamp": "0:07:00",
        "answer": "D",
        "options": [
          "A. A large tree.",
          "B. A blue vehicle.",
          "C. A group of people.",
          "D. A yellow and silver robot."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_319_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual taken just now?",
        "time_stamp": "00:00:18",
        "answer": "C",
        "options": [
          "A. The individual set the table with dishes and silverware before preparing a salad.",
          "B. The individual gathered ingredients for a dessert and started mixing them in a bowl.",
          "C. The individual prepared a drink by opening a bottle, removing the cork, and setting up glassware.",
          "D. The individual cooked a main course by grilling meat and adding spices to it."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_405_real.mp4"
  },
  {
    "time": "[0:02:01 - 0:02:11]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual taken just now?",
        "time_stamp": "0:02:11",
        "answer": "C",
        "options": [
          "A. The individual chopped vegetables, added them to a bowl, and served a salad.",
          "B. The individual brewed coffee, added sugar and milk, and served it to the customer.",
          "C. The individual prepared a cocktail by adding ice to a shaker, pouring in ingredients.",
          "D. The individual poured wine into glasses and served them to the customers."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_405_real.mp4"
  },
  {
    "time": "[0:04:02 - 0:04:12]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the content of the video just shown?",
        "time_stamp": "0:04:18",
        "answer": "C",
        "options": [
          "A. The bartender prepared several coffee orders for a group of customers.",
          "B. The bartender checked customer IDs and handed out drink menus.",
          "C. The bartender placed two cocktails on the counter.",
          "D. The bartender restocked the bar with empty glass bottles while cleaning dishes."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_405_real.mp4"
  },
  {
    "time": "[0:06:03 - 0:06:13]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual taken just now?",
        "time_stamp": "00:06:13",
        "answer": "C",
        "options": [
          "A. The individual garnished a cocktail with a cherry and served it.",
          "B. The individual prepared a cocktail by mixing ingredients and shaking them.",
          "C. The individual added ice and lime to a drink, garnished it, and placed it on the bar counter.",
          "D. The individual was preparing a hot drink by heating it on the stove."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_405_real.mp4"
  },
  {
    "time": "[0:08:04 - 0:08:14]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual taken just now?",
        "time_stamp": "00:08:14",
        "answer": "C",
        "options": [
          "A. The individual prepared a hot beverage by boiling water and steeping tea.",
          "B. The individual garnished a cocktail with a cherry and served it to a customer.",
          "C. The individual filled several glasses with ice cubes from a container in preparation for drinks.",
          "D. The individual prepared a meal by grilling various ingredients and plating them."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_405_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:18",
        "answer": "A",
        "options": [
          "A. 0.",
          "B. 2.",
          "C. 3.",
          "D. 4."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_84_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:07",
        "answer": "A",
        "options": [
          "A. 4.",
          "B. 5.",
          "C. 6.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_84_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:02",
        "answer": "A",
        "options": [
          "A. 5.",
          "B. 2.",
          "C. 3.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_84_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:01",
        "answer": "A",
        "options": [
          "A. 5.",
          "B. 2.",
          "C. 7.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      },
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:06:00",
        "answer": "A",
        "options": [
          "A. 7.",
          "B. 5.",
          "C. 8.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_84_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:47]: Several paintings are displayed on a white wall. The paintings have ornate gold and wooden frames. The leftmost painting depicts a swan. Adjacent to it, there is a portrait with a detailed decorative frame. The largest painting on the right shows a close-up of white and peach roses against a gentle sky. [0:02:48 - 0:02:56]: The camera moves vertically and horizontally, displaying more of the paintings. Additional floral paintings are visible, maintaining a similar color scheme and ornate framing. A black nameplate with white text \"Angelina Holmbrinska\" is seen above a large painting of pale roses on a dark background. [0:02:57 - 0:03:00]: The camera continues to move, focusing on a painting of a woman sitting among roses, with a serene landscape in the background. The color palette includes pastel pinks, reds, and greens, creating a soft, harmonious scene.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the nameplate seen above the painting?",
        "time_stamp": "0:02:52",
        "answer": "B",
        "options": [
          "A. White with black text.",
          "B. Black with white text.",
          "C. Gold with black text.",
          "D. Wooden with white text."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Clips Summarize",
        "question": "What type of paintings are mainly displayed right now?",
        "time_stamp": "0:03:00",
        "answer": "C",
        "options": [
          "A. Abstract paintings.",
          "B. Portraits.",
          "C. Floral paintings.",
          "D. Landscape paintings."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_463_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:22]: The video begins inside an art gallery with various colorful artworks adorning the walls. A person dressed casually in a blazer and jeans, carrying a black bag, walks past the camera from the right-hand side of the frame to the left. The artworks on the walls are varied, some featuring abstract designs with vibrant colors and different shapes. [0:05:23 - 0:05:24]: As the person continues walking, a small table with a stack of books topped with an orange-colored item is visible in the foreground. On the walls, close-up shots show two large paintings with intricate designs that resemble blooming branches with flowers in a variety of colors, including green, yellow, purple, and pink. [0:05:25 - 0:05:27]: The artworks on the wall are further highlighted, revealing the fine details of the branches and flowers. The table with books remains in the same position, emphasizing the constant focus on the artwork. The color palette of the paintings includes greens and yellows, giving a spring-like atmosphere to the scene. [0:05:28]: The camera gradually rotates to reveal an additional artwork in the background, slightly obscured by the images of branches and flowers. [0:05:29]: The camera then focuses on a new piece of art, a large framed photograph of what appears to be a mannequin head with long blonde hair. The mannequin is blowing a large bubble with bubblegum, which is centered in the frame. The background remains the white gallery wall, and other people can be seen walking and viewing the art in the distance. [0:05:30 - 0:05:34]: This photograph remains the center of focus while subtle changes occur around it, such as the movements of gallery visitors. The mannequin's expression and the bubblegum remain prominent. [0:05:35]: The camera slightly pans right, revealing more of the gallery space, with a wide corridor and several visitors observing various art pieces. [0:05:36]: As the camera continues to pan right, it captures additional artwork on the far wall, featuring a variety of abstract and conceptual designs. One painting portrays a humanoid figure with striking facial features, while another presents a more traditional portrait. [0:05:37 - 0:05:39]: The video concludes with close-up shots of two large, framed paintings featuring abstract humanoid figures with distinctive features. The backgrounds of these artworks include neutral tones with distinguishing elements, such as red and green accents which provide contrast. The gallery space overall has a modern and minimalist design, with white walls and polished concrete floors, providing a clean backdrop for the vivid artworks.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is on top of the stack of books leftside on the small table?",
        "time_stamp": "00:05:24",
        "answer": "D",
        "options": [
          "A. A blue-colored book.",
          "B. A green-colored book.",
          "C. A red-colored book.",
          "D. An orange-colored book."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_463_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:06]: The video starts by focusing on a white wall in an art gallery displaying a collection of black-and-white photographs. The largest photograph, positioned in the center at the top, depicts two palm trees near a body of water. Below it, there are four horizontally oriented photographs arranged two by two, showcasing various landscape scenes with rocks, water, and beaches. The surrounding walls also display other framed pieces, and a few visitors can be seen walking through the gallery. [0:08:07 - 0:08:09]: The camera begins moving to the right, revealing more artworks hung on adjacent walls. The gallery is spacious, with white walls and ceiling lights illuminating the exhibits. Two people are seen conversing near a table with information or brochures on it, further enhancing the gallery's lively atmosphere. [0:08:10 - 0:08:12]: Focus shifts to another set of framed photographs on a white dividing wall. These photographs feature a variety of subjects, including a landscape at sunset, a close-up of flowers, and other abstract compositions. As the camera moves closer, details of the artworks become more distinct. [0:08:13 - 0:08:15]: The camera lingers on the new set of artworks, capturing the details and textures of the photographs. The sunset photo with mountains and reflected light in the water draws significant attention, occupying the topmost position. Below it, one photograph displays an old, weathered door frame surrounded by black and white contrasts, while another exhibits flowers set against a rusted metallic background. [0:08:16 - 0:08:19]: The camera continues to survey the photographs, focusing on the one with the rusted background and flowers. A new photograph at the bottom section of the wall displays a seemingly abandoned entrance sign at twilight, enhancing the exhibit's eclectic collection. An individual pauses to observe the photographs more closely, adding to the gallery’s interactive experience.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the photograph of the sunset with mountains positioned in the new set of artworks?",
        "time_stamp": "00:08:19",
        "answer": "B",
        "options": [
          "A. At the bottom section of the wall.",
          "B. At the topmost position.",
          "C. Below the old, weathered door frame.",
          "D. Next to the close-up of flowers."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_463_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:59]",
    "captions": "[0:09:40 - 0:09:59] [0:09:40 - 0:09:44]: A sequence of photographs mounted on a white wall, each in black frames. The top two photographs show art pieces with dark backgrounds. The left photo features a can-like object with scattered items around it, while the right photo depicts various small objects arranged. [0:09:44 - 0:09:47]: The bottom left photo shows an assortment of items, including books and cards, arranged on a table against a dark background. The bottom right photo shows a globe on a stand. [0:09:48 - 0:09:49]: The left photograph on the bottom row now shows a pile of scattered items such as books on a dark background. The right photograph displays a pedestal with various fruits. [0:09:50 - 0:09:52]: The images on the wall shift slightly to reveal the names \"ERICA KELLY MARTIN\" and \"FERNANDO GUERRERO\" written below the photographs. Adjacent to the right, two more framed photographs appear, showing a detailed cityscape. One depicts the Eiffel Tower with surrounding buildings and streets. [0:09:53 - 0:09:54]: Further perspective into the exhibit shows the name \"FERNANDO GUERRERO\" clearly displayed under the cityscape images. [0:09:55 - 0:09:59]: The camera zooms in on the detailed cityscape photograph of the Eiffel Tower and a statue in front of a building. Background views in this art exhibit are well-lit with visitors visible.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which names are revealed below the photographs right now?",
        "time_stamp": "00:09:56",
        "answer": "C",
        "options": [
          "A. ERICA KELLY MARTIN.",
          "B. FERNANDO GUERRERO.",
          "C. ERICA KELLY MARTIN.",
          "D. JOHN DOE."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_463_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:00:16",
        "answer": "A",
        "options": [
          "A. Graphing on the Coordinate Plane.",
          "B. Calculus functions.",
          "C. Trigonometric ratios.",
          "D. Differential equations."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_211_real.mp4"
  },
  {
    "time": "[0:02:06 - 0:02:26]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:02:22",
        "answer": "B",
        "options": [
          "A. How to label the axes.",
          "B. How to connect the plotted points.",
          "C. How to identify coordinates.",
          "D. How to scale the graph."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_211_real.mp4"
  },
  {
    "time": "[0:04:12 - 0:04:32]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:04:20",
        "answer": "D",
        "options": [
          "A. How to plot points on the axes.",
          "B. The importance of the x-axis.",
          "C. The significance of the y-axis.",
          "D. What ordered pairs represent."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_211_real.mp4"
  },
  {
    "time": "[0:06:18 - 0:06:38]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:06:37",
        "answer": "A",
        "options": [
          "A. How to plot another point.",
          "B. How to label the y-axis.",
          "C. The importance of the y-intercept.",
          "D. How to draw a quadratic curve."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_211_real.mp4"
  },
  {
    "time": "[0:08:24 - 0:08:44]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:08:31",
        "answer": "C",
        "options": [
          "A. How to plot a line graph.",
          "B. The significance of the axes intersections.",
          "C. How to differentiate the quadrants.",
          "D. The concept of linear equations."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_211_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the vendor's actions just now?",
        "time_stamp": "00:00:10",
        "answer": "D",
        "options": [
          "A. The vendor picked up multiple croissants, placed them into a box, and handed it to a customer.",
          "B. The vendor rearranged pastries on the tray, cleaned the display glass, and took an order from a customer.",
          "C. The vendor picked up a muffin, added nuts, and placed it onto a display dish.",
          "D. The vendor selected a pastry using tongs, placed it into a paper bag."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_351_real.mp4"
  },
  {
    "time": "[0:02:01 - 0:02:11]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the barista's actions just now?",
        "time_stamp": "00:02:10",
        "answer": "D",
        "options": [
          "A. The barista prepared a small cappuccino, added honey, and gave it to the customer.",
          "B. The barista brewed a large black coffee, added sugar, and placed it on the counter for pickup.",
          "C. The barista made an espresso shot, added milk, and served it with a croissant.",
          "D. The barista poured a large latte, sealed the cup with a lid, and handed it over to the customer."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_351_real.mp4"
  },
  {
    "time": "[0:04:02 - 0:04:12]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the barista's actions just now?",
        "time_stamp": "00:04:12",
        "answer": "D",
        "options": [
          "A. The barista picked up fruits to prepare a smoothie, blended them, and served it to a customer.",
          "B. The barista prepared a double espresso, added cream, and served it with a cookie.",
          "C. The barista brewed a pot of tea, added honey, and served it in a teapot to a customer.",
          "D. The barista steamed milk, prepared orders of oat latte and soy flat white."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_351_real.mp4"
  },
  {
    "time": "[0:06:03 - 0:06:13]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the barista's actions just now?",
        "time_stamp": "00:06:13",
        "answer": "D",
        "options": [
          "A. The barista arranged mugs on the counter, brought milk from the fridge, and cleaned the workspace.",
          "B. The barista refilled the milk container, steamed milk, and cleaned the counter.",
          "C. The barista cleaned the espresso machine, brewed espresso, and arranged cups for serving.",
          "D. The barista grabbed milk cartons into the fridge, organized them, and prepared the workspace for further tasks."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_351_real.mp4"
  },
  {
    "time": "[0:08:04 - 0:08:14]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions just now?",
        "time_stamp": "00:08:19",
        "answer": "D",
        "options": [
          "A. The cashier counted money in the register, sorted coins and bills, and printed the receipt.",
          "B. The cashier scanned a barcode, weighed the item, and asked the customer for payment.",
          "C. The cashier arranged items on the counter, packed them into a bag, and handed the bag to the customer.",
          "D. The cashier picked items on the touchscreen, confirmed the order, and selected a payment option."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_351_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: Initial frames are completely black, indicating the video is starting;   [0:00:03 - 0:00:10]: The scene transitions to a vibrant, pixelated environment resembling a Minecraft landscape. The camera is facing a path that leads towards a large, multi-story house made of bricks and wood. There are neatly arranged, lush green crops on either side of the path, featuring red and yellow flowers. Several white birch trees with black spots are scattered around. In the foreground, a character dressed in a yellow and pink outfit, possibly an avatar, is facing the camera, standing still on the path;   [0:00:10 - 0:00:18]: The camera maintains its position as it captures the static scene. The character remains in the same position while the background stays consistent, presenting an extended view of the detailed landscape and structures;   [0:00:19]: The view shifts to a blurred image of a computer screen displaying a Minecraft gameplay screenshot. The screenshot shows a pixelated mountainous landscape with a distinctive red and white pattern in the background. It appears that the video is showing an in-game overlay or reference.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the character's outfit standing on the path?",
        "time_stamp": "0:00:17",
        "answer": "B",
        "options": [
          "A. Blue and white.",
          "B. Yellow and pink.",
          "C. Red and green.",
          "D. Black and yellow."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_189_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:42]: The video starts with a first-person perspective view facing downward at the top of a gray cobblestone structure. To the left is a lower platform made of the same cobblestone material, and green grassy ground with scattered yellow flowers is seen below. [0:04:43]: The viewpoint shifts upwards, showing more of the cobblestone building. A portion of a white bed with a red cover and a brown crafting table are visible on the grassy area below. [0:04:44 - 0:04:45]: The camera angle moves to show an upward view of the gray cobblestone structure. The sky is clear blue with a few white clouds scattered around. [0:04:46 - 0:04:49]: The view remains upward, emphasizing the height of the building and more of the blue sky. The white clouds appear to be at various distances in the sky, creating depth. [0:04:50 - 0:04:51]: The camera shifts back down, focusing more on the entire building structure on the grassy field. The crafting table and bed are more clearly seen. The background shows a snowy hillside to the right. [0:04:52 - 0:04:53]: The angle shifts upward again but remains centered on the cobblestone structure and the sky above it. [0:04:54]: The view changes back to the lower perspective, prominently displaying the cobblestone building, bed, crafting table, and surrounding green grassy area. [0:04:55 - 0:04:56]: The first-person perspective moves closer to the snow-covered hillside on the right while still capturing parts of the cobblestone structure and the crafting area below. [0:04:57 - 0:05:00]: The camera angle moves to the top of the cobblestone building, showing the layout of the roof area. There is a square cobblestone block prominently placed, and parts of the building's walls and inner section are visible, with more of the surrounding snowy landscape coming into view.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What objects are seen on the grassy area below the stone structure?",
        "time_stamp": "00:05:00",
        "answer": "A",
        "options": [
          "A. A white and red bed and a crafting table.",
          "B. A brown chair and a red table.",
          "C. A yellow bed and a crafting table.",
          "D. A green chair and a red table."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_189_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:20 - 0:09:40] [0:09:20 - 0:09:24]: The scene shows a large tree with a thick trunk and vines growing on its surface. The player is holding a tool, likely an axe, and is standing close to the tree. In the background, there is dense foliage with tall bamboo stalks and more trees, all set against a clear blue sky with scattered clouds. [0:09:25 - 0:09:28]: The camera angle shifts upward, showing the player looking up at the tall tree they are interacting with. The tree is surrounded by other tall bamboo plants and lush greenery. The player appears to be cutting through the tree trunk as visible from the blocks breaking and disappearing. [0:09:29 - 0:09:33]: After looking up, the player lowers their view back to the base of the tree and continues to hack at the tree's trunk. The scene captures the player methodically working around the tree, cutting through the trunk blocks sequentially and clearing out the surrounding vines. [0:09:34 - 0:09:37]: The player resumes chopping upward toward the higher parts of the tree. The canopy of the tree and sky is visible again, emphasizing the player's continued efforts to chop down the large trunk section by section. [0:09:38 - 0:09:40]: The player moves around the tree, continuing to chop the tree from different angles to efficiently harvest its blocks. The focus remains on the interaction with the tree and the dense forest environment. The background remains filled with green vegetation and scattered bamboo stalks.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What tool is the player holding in his hand right now?",
        "time_stamp": "0:09:41",
        "answer": "B",
        "options": [
          "A. A shovel.",
          "B. A pixel-style scissors.",
          "C. A saw.",
          "D. A pickaxe."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Action Recognition",
        "question": "What action is the player performing in the video right now?",
        "time_stamp": "0:09:37",
        "answer": "B",
        "options": [
          "A. Planting seeds.",
          "B. Pruning the vines on the tree.",
          "C. Climbing the tree.",
          "D. Collecting bamboo stalks."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_189_real.mp4"
  },
  {
    "time": "[0:14:00 - 0:15:00]",
    "captions": "[0:14:00 - 0:14:20] [0:14:00 - 0:14:01]: The video starts with a first-person perspective of a grassy and dirt area adjacent to a stone wall. The user appears to be holding a block of dirt. [0:14:02 - 0:14:03]: The user removes a block of dirt in front of the stone wall. Bits of dirt can be seen flying away as the block is removed. [0:14:04 - 0:14:05]: The camera moves away from the stone wall, showing more grass and dirt, and then transitions to an open pathway with cobblestones and dirt. [0:14:06 - 0:14:07]: The user continues to walk along the pathway, revealing a stone staircase next to a stone wall structure. [0:14:08 - 0:14:09]: A tree comes into view on the right side of the pathway. [0:14:10 - 0:14:12]: The user turns to face the tree directly, getting closer to it. The tree stands alone on a small grassy elevation. [0:14:13 - 0:14:14]: The user looks upwards, revealing a house structure built into the side of a hill, with some wooden beams and a partially visible open area. [0:14:15 - 0:14:16]: The view shifts back to the tree, focusing on its thick trunk and dense green leaves that create a shadowed area beneath it. [0:14:17 - 0:14:18]: The user appears to select a cobblestone slab from the inventory at the bottom of the screen while looking at the ground. [0:14:19 - 0:14:20]: The user places the cobblestone slab on the ground near some grass and dirt, finalizing the structure next to the tree and the stone building in the background.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What can be seen flying away when the user removes a block of dirt?",
        "time_stamp": "0:14:03",
        "answer": "B",
        "options": [
          "A. Bits of stone.",
          "B. Bits of dirt.",
          "C. Grass particles.",
          "D. Wood fragments."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_189_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person outside the car holding in their hand right now?",
        "time_stamp": "00:00:02",
        "answer": "D",
        "options": [
          "A. A notepad.",
          "B. A radio.",
          "C. A stopwatch.",
          "D. A digital device."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_268_real.mp4"
  },
  {
    "time": "[0:01:34 - 0:01:39]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the driver's gloves right now?",
        "time_stamp": "00:01:38",
        "answer": "C",
        "options": [
          "A. Blue.",
          "B. Green.",
          "C. Red.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_268_real.mp4"
  },
  {
    "time": "[0:03:08 - 0:03:13]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What can be seen through the windshield right now?",
        "time_stamp": "00:03:12",
        "answer": "C",
        "options": [
          "A. A racetrack.",
          "B. A dirt path.",
          "C. A field with trees.",
          "D. A forest."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_268_real.mp4"
  },
  {
    "time": "[0:04:42 - 0:04:47]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What can be seen through the windshield right now?",
        "time_stamp": "00:04:42",
        "answer": "C",
        "options": [
          "A. A dense forest.",
          "B. City buildings.",
          "C. A field with rows of poles.",
          "D. A sandy desert."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_268_real.mp4"
  },
  {
    "time": "[0:06:16 - 0:06:21]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is visible through the windshield right now?",
        "time_stamp": "00:06:22",
        "answer": "A",
        "options": [
          "A. A field with rows of poles.",
          "B. A lake.",
          "C. A forest path.",
          "D. A residential area."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_268_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: In a sunny outdoor setting, there's a white sign with \"UNIVERSAL STUDIOS\" inscribed, flanked by two large, beige planters with green foliage. These planters are on either side of the walkway. To the right, there are structures providing shade, and partly visible to the far right is a yellow vintage car parked next to a red vehicle under a canopy. [0:00:03 - 0:00:05]: The camera moves past the sign and planters. The yellow vintage car becomes more visible, positioned in front of a black wall with a tree visible in the background. The path beside the cars is partially shaded. [0:00:06 - 0:00:08]: The camera continues moving forward, with the yellow vintage car prominently in view, along with a red sports car next to it. The black wall behind them features minimal text, and there are more cars in a row under the shade. [0:00:09 - 0:00:11]: Now past the planters, the pathway in front of the camera shows more of the car exhibit. A few other cars are seen lined up in parallel. The yellow vintage car has large wheels and polished chrome parts, with the red sports car parked to its right. [0:00:12 - 0:00:13]: The red sports car directly ahead appears sleek and low to the ground, with a black convertible next to it. Further down the line, additional vehicles, including a dark blue car which can be barely seen, are parked under the black canopy providing shade. [0:00:14 - 0:00:15]: The black convertible car aligns directly in front of the camera, flanked by a red vehicle to the left and a maroon, futuristic-looking vehicle to the right. The canopy and surrounding greenery are evident in the background. [0:00:16 - 0:00:17]: Moving past the black convertible, the maroon, aerodynamic car with a pointed nose takes focus. Behind it, a shiny blue sports car with aqua tones is visible, and the line of diverse cars continues. [0:00:18 - 0:00:19]: The viewpoint progresses, showing a closer look at the maroon vehicle, alongside the sleek, white and blue car. Further down the row, the hood of a red classic car starts to become visible. [0:00:20]: Approaching the end of the pathway, the final segment captures the detailed differences in the cars, further illustrating the diverse collection under the canopy, concluding with a clear view of the red and white vintage car in the far end.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What color is the vintage car parked right side to the red vehicle under a canopy?",
        "time_stamp": "0:00:11",
        "answer": "B",
        "options": [
          "A. Blue.",
          "B. Yellow.",
          "C. Green.",
          "D. Black."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_314_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:42]: The video begins with a first-person perspective showing a rustic street scene. On the left side of the frame is a building with a white facade that reads \"MERCADO.\" The building appears old, with weathered walls and a door and window visible. To the right of the building, several smaller adjacent structures can be seen, all built similarly. The street is uneven, covered with rocks, puddles, and patches of mud. In the foreground, there is an old wooden cart with wheels. In the background, tall trees with dense foliage filter sunlight.  [0:02:43 - 0:02:46]: The camera moves forward slightly, keeping the buildings on the left and the wooden cart in the center of the frame. There are minimal changes in the overall scene composition. Details of the structures and surroundings remain consistent, with dense greenery visible among the trees in the background. [0:02:47 - 0:02:50]: Continuing forward, the left-hand buildings remain static, while the wooden cart becomes more prominent, centrally positioned in the frame. The road’s uneven surface with rocks and mud is clearly visible. The trees in the background cast shadows on the ground, creating a play of light and dark areas. [0:02:51 - 0:02:55]: As the video progresses, the structures on the left continue to remain in view, though the perspective slightly shifts. The building labeled \"MERCADO\" stays partly in the frame. The wooden cart remains central, and the overall environment with trees and rocky surface persists. [0:02:56 - 0:02:59]: Towards the end, the scene remains consistent. The camera moves subtly ahead, maintaining focus on the street with the rustic buildings on the left and the wooden cart centrally positioned. The overall environment, with rocks, puddles, and trees, continues to be depicted in detail.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is written on the building with a white facade?",
        "time_stamp": "00:03:00",
        "answer": "A",
        "options": [
          "A. MERCADO.",
          "B. LIBRERÍA.",
          "C. CAFETERÍA.",
          "D. BANCO."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_314_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:30]: The video starts with a view of a rustic wooden building labeled \"DILLERS BOARDING HOUSE\". The building has two stories, with a balcony on the second floor and a porch on the first floor. Wooden chairs are positioned on the left side of the porch. As the video progresses, the camera moves to the right, revealing more of the building and the street. There are multiple buildings along the street, with one building painted green and the other in brick, each with windows and various architectural details. [0:05:30 - 0:05:37]: Continuing to pan right, the camera shows a brick building with a staircase leading up to an entrance. Nearby, a large number \"33\" is painted on the side of a distant warehouse-like structure. The scene includes some well-maintained vegetation and tall palm trees, with clear blue skies overhead. The camera continues to move smoothly, revealing more of the street and surrounding environment. [0:05:37 - 0:05:39]: Towards the end, the view shifts to an open area with a small fountain on the right. There is a wooden fence surrounding the area, and a few trees are visible in the background. The road is well-paved, and an area with a sign and yellow fire hydrant comes into view, indicating a more open space ahead.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type of building is labeled \"DILLERS BOARDING HOUSE\"?",
        "time_stamp": "0:05:20",
        "answer": "B",
        "options": [
          "A. A single-story house.",
          "B. A two-story wooden building.",
          "C. A warehouse.",
          "D. A three-story apartment."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_314_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:02]: The scene showcases an open area with a wide road curving to the right. In the distance, various buildings, trees, and a broad blue sky with scattered clouds are visible. On the left, a large geometric-shaped building with a white and grey facade has a sign reading \"50.\" Surrounding it are parked vehicles and construction materials. A stretch of green grass lines the road on the left side. [0:08:03 - 0:08:06]: The perspective moves forward toward the large building with the \"50\" sign. The white pickup truck remains parked, with other vehicles and construction materials still visible. The grassy area narrows slightly, and the paved road and adjacent structures become more prominent. [0:08:07 - 0:08:10]: As the camera moves further, additional details of the large building, such as the entrance and surrounding objects, become clearer. A white car is seen turning to face the building, and the tarp-covered material on the right becomes more discernible. The road becomes more central, leading directly toward the building. [0:08:11 - 0:08:12]: The camera continues moving closer to the building, offering a clearer view of its entrance. The left side reveals more parked vehicles and construction items. The right side shows a notable decrease in the grassy area while more paved surface and tarp-covered materials come into view. [0:08:13 - 0:08:15]: As the camera progresses further towards the building, the entrance and surroundings, including parked vehicles and various construction items, become more defined. The blue tarp on the right side is now prominent, and more details of the building and its facade are visible. [0:08:16 - 0:08:18]: Closing in on the building, the camera captures a clearer view of parked vehicles and construction-related items near the entrance. The facade details and the structure’s layout become more apparent. The leading path directly toward the building and its immediate surroundings continue to be in focus. [0:08:19]: The camera reaches near the building's entrance, presenting an unobstructed view of the entrance and the layout of parked vehicles and other items. The white truck parked on the left is more distinguished, along with the close-up view of the building's details.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the sign reading on the large geometric-shaped buildin right now?",
        "time_stamp": "00:08:05",
        "answer": "C",
        "options": [
          "A. 25.",
          "B. 30.",
          "C. 50.",
          "D. 75."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_314_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:56]",
    "captions": "[0:09:40 - 0:09:56] [0:09:40 - 0:09:42]: The video shows a quiet and deserted roadside area during the daytime. The road is cracked and well-worn, leading to multiple buildings made of stone and wood. These buildings, with aged facades, are in a pull-off along the street. Trees with green foliage surround the area, and a blue sky with some clouds is visible. A building with a corrugated red roof and a faded sign are in the distance, while the one in the center has a pitched roof with a chimney. [0:09:43]: The scene transitions to an area with a reflective overlay of two separate locations. One part shows the view of vehicles, including a black SUV, a white van, and a large truck parked in a lot with trees on a grassy hill. The other part remains fixed on the original buildings. [0:09:44 - 0:09:46]: The frame fully transitions, revealing a different area with a parking lot to the left. Various vehicles, including vans and trucks, are parked near some green trees and grassy land. The road ascends toward the right where the landscape becomes greener with overhanging trees. [0:09:47 - 0:09:50]: The road curves to the right and ascends, passing by a fenced area with a sign that has an illustration of a person swimming in blue water. The text on the sign says \"BEACH CLOSED\" with an arrow pointing to the right. This area has numerous tall trees, and a slight incline can be seen leading to what appears to be some residential buildings in the background. [0:09:51 - 0:09:53]: A closer view of the sign indicates that it is mounted on wooden posts beside the road. The \"BEACH CLOSED\" text is clearer, with the sign positioned next to other traffic and construction cones, suggesting restricted access. [0:09:54 - 0:09:56]: The scene transitions back to a view of the \"Amity Hotel\" sign on a gray, wooden-sided building next to a body of water. The reflection in the water shows the nearby structures and surrounding trees. On the left, a guided tram tour is visible, indicating this place as part of an attraction or set. The water is calm, with the area appearing serene and green with lush foliage.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What does the sign with an illustration of a person swimming in blue water indicate?",
        "time_stamp": "0:09:53",
        "answer": "C",
        "options": [
          "A. \"Pool Open\".",
          "B. \"No Swimming\".",
          "C. \"Beach Closed\".",
          "D. \"Swim at Your Own Risk\"."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_314_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:11]: A scene unfolds in what appears to be a gun shop with two people conversing in the center. The space is well-lit by ceiling lights, highlighting the extensive array of firearms mounted on the walls behind them. The person on the left wears a high-visibility vest, dark shirt, and headphones, with short blonde hair. The individual on the right, who has dark hair and sunglasses, is engaged in conversation. Various display cases filled with accessories are present, including one between the two individuals. The background includes framed certificates and signs, notably a prominent sign saying \"LOS SANTOS GUN CLUB\". [0:00:12 - 0:00:20]: The perspective shifts slightly while the two individuals continue their discussion. The person on the left occasionally looks towards the camera, as if addressing it or responding to prompts. The view then transitions to a broader perspective of the room. An American flag, additional shelves, and a vending machine come into view, with the final frames capturing a door with a sign labeled \"THANKS FOR EXIT\". The overall atmosphere remains the same, focused on the interaction between the two people in the gun shop setting.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the both persons doing right now?",
        "time_stamp": "00:00:20",
        "answer": "B",
        "options": [
          "A. Looking at the display cases.",
          "B. Having a face-to-face conversation while standing.",
          "C. Walking towards the exit.",
          "D. Talking on the phone."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_293_real.mp4"
  },
  {
    "time": "0:03:20 - 0:03:40",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:23]: The video begins with a first-person perspective of driving a blue car on a road at night. City lights illuminate the street, and there are markings and signs along the road. The car is heading straight with a clear view of an intersection ahead. [0:03:24 - 0:03:27]: As the car moves forward, the perspective shows the road more clearly lined with red and yellow street markings. The driver approaches the stop sign, and the surroundings include streetlights and buildings. [0:03:28 - 0:03:36]: The vehicle turns to the right, driving past a small green grass section with a red border and a palm tree. The road seems mostly empty except for some parked cars and a few buildings with lighted windows in the background. [0:03:37 - 0:03:40]: The car continues along the road, showing more items to its side like a signpost and streetlights. The vehicle aligns towards a minor intersection with clearer visibility of nearby buildings and parked vehicles.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the car doing right now?",
        "time_stamp": "00:03:30",
        "answer": "C",
        "options": [
          "A. Driving straight through a major intersection.",
          "B. Turning left onto a side road.",
          "C. Keep going straight on the road.",
          "D. Stopping at a stop sign."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_293_real.mp4"
  },
  {
    "time": "0:06:40 - 0:07:00",
    "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:42]: A person in a reflective vest stands next to a small blue car parked on the side of a street. The background features palm trees and a house with pink neon lights along the roofline. [0:06:43 - 0:06:44]: The person opens the car door and begins to get inside the blue car. The street is well-lit with artificial lights, and there are various neon lights visible in the distance. [0:06:45 - 0:06:47]: The person is now fully inside the blue car, which is still parked on the side of the street. The neon light decorations of nearby houses are visible. [0:06:48]: The blue car’s brake lights are illuminated. The car starts to move along the street with a person visible inside, driving. [0:06:49]: The blue car continues down the street. Another person is seen walking towards the vehicle on the passenger side. [0:06:50]: The walker raises their hand, likely to signal or communicate with the driver. [0:06:51 - 0:06:52]: The walker approaches the car and opens the passenger door while the blue car is still on the street. [0:06:53 - 0:06:54]: The person gets into the car, and the vehicle is stationary on the road. [0:06:55]: The car door closes, and the blue car appears ready to resume moving. [0:06:56 - 0:06:58]: The car starts moving forward down the street, passing houses decorated with lights. [0:06:59 - 0:07:01]: As the car progresses along the street, it passes several decorated houses on both sides of the road. [0:07:02]: The car continues to drive forward, reaching an intersection. [0:07:03 - 0:07:04]: The blue car slows down as it approaches a stop sign at the intersection. [0:07:05]: The car comes to a complete stop at the intersection before turning left. [0:07:06]: The blue car makes a left turn at the intersection onto a cross street. [0:07:07 - 0:07:08]: The car continues straight down the new street, which is lined with more houses and some parked vehicles. [0:07:09 - 0:07:10]: The car proceeds down the road, driving past additional houses and streetlights. [0:07:11]: The blue car drives straight, moving past another intersection with no traffic. [0:07:12 - 0:07:14]: The car continues down a dimly lit road in a residential area, with various street and house lights illuminating the path. [0:07:15]: The blue car keeps driving straight, maintaining a consistent speed through the residential neighborhood.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the walker do just now?",
        "time_stamp": "00:06:54",
        "answer": "C",
        "options": [
          "A. Adjusted the car's mirror.",
          "B. Walked past the blue car.",
          "C. Opened the passenger door and entered the car.",
          "D. Signaled another car to stop."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_293_real.mp4"
  },
  {
    "time": "0:10:00 - 0:10:20",
    "captions": "[0:10:00 - 0:10:20] [0:10:00 - 0:10:04]: The scene is set at night in an industrial area with several people standing and talking. The video is from a first-person perspective showing a group of three individuals. The person in the foreground has a reflective vest on. The background features parked cars, trailers, and lit buildings.  [0:10:05 - 0:10:08]: The focus remains on the group of three. One person, dressed in bright pink attire, looks downwards while the others seem engaged in conversation. A blue car is parked nearby.  [0:10:09 - 0:10:11]: The person in the reflective vest continues to face the group. The person in black and white attire is the center of attention. They are wearing a hood and sunglasses. [0:10:12 - 0:10:14]: The person in the black and white attire makes a gesture, possibly continuing the conversation, and moves slightly. The person in pink stands still. The background reveals more of the parked cars. [0:10:15 - 0:10:16]: The scene remains largely unchanged with the focus on the conversation amongst the group. There is movement from the person in black and white, suggesting interaction or animated discussion. [0:10:17 - 0:10:18]: The chat on the stream updates rapidly while the group continues their interaction. The background shows the industrial setting, including machinery and storage units. [0:10:19 - 0:10:20]: The scenes transition smoothly with continued dialogue among the individuals. The person in pink is focused on the conversation, and the video provides a coherent view of the industrial night setting.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person in the pink attire doing right now?",
        "time_stamp": "00:10:20",
        "answer": "D",
        "options": [
          "A. Looking downwards.",
          "B. Making a gesture.",
          "C. Moving slightly.",
          "D. Standing still."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_293_real.mp4"
  },
  {
    "time": "0:13:20 - 0:13:23",
    "captions": "[0:13:20 - 0:13:23] [0:13:20]: A person is walking towards another individual standing against a brick wall in a dark setting. The scene includes a backdrop featuring a large brick wall with a building's corner visible; [0:13:21]: The individual nears the person standing against the brick wall, who appears to be engaged in conversation. Interactive icons are displayed above the head of the person against the wall; [0:13:22]: The camera is oriented towards the standing person, and an interaction menu titled \"Frank Miller\" appears, offering options for further engagement. The scene remains dark with the brick wall as the main background.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:13:23",
        "answer": "B",
        "options": [
          "A. Walking away from the brick wall.",
          "B. Engaging with the interaction menu.",
          "C. Standing and observing the environment.",
          "D. Adjusting their position against the wall."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_293_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:04]: The video begins with a first-person perspective in a kitchen setting. A man standing in front of a white brick wall is speaking. The wall has two shelves: the upper shelf holds green and clear jars on the right side; the lower shelf holds plates, bowls, and cups, neatly arranged. The man wears a green shirt and appears engaged in conversation. [0:00:05 - 0:00:09]: The man maintains eye contact with the camera, alternating between holding his hands together and using expressive hand gestures to emphasize his points. Various kitchen tools, including knives and cutting boards, are seen on the right side of the background. [0:00:10 - 0:00:13]: The man continues talking, occasionally looking thoughtful. His expressions convey that the topic is interesting and requiring explanation. The camera angle remains steady, keeping the focus on him and the neatly arranged kitchen background. [0:00:14]: The scene shifts to a close-up of the number \"10\" and a stylized depiction of a kitchen utensil, slightly tilted to the right. The overall tone is sleek and modern. [0:00:15 - 0:00:17]: The next frames feature a man wearing a darker shirt than initially, standing with a broad smile. Behind him, the words \"RAMSAY in 10\" are prominently displayed in blue against a white and gray gradient background. The transition suggests a shift in focus from detailed discussion to a more formal introduction. [0:00:18]: A graphic slides across the screen, partially covering the kitchen from earlier. The word \"LOOK\" made of large red letters is visible on a high shelf. [0:00:19]: The final scene returns to the man, now in a gray shirt, continuing his conversation. The background remains consistent with the earlier frames, featuring the same organized kitchen setting. The camera captures the man's final gestures and expressions.\n[0:00:20 - 0:00:40] [0:00:20 - 0:00:23]: A man in a dark gray shirt stands in front of a kitchen counter with a wide stance, his arms outstretched. The background shows a white brick wall with wooden shelves holding various kitchen items, including dishes, glassware, and utensils. On the top shelf on the left, large red letters spell out \"COOK\". The man then lowers his arms and begins to speak, his facial expression serious and engaged. [0:00:24 - 0:00:28]: The man turns his body slightly to the left while continuing to speak, his attention directed toward something off-camera. His movements include subtle hand gestures emphasizing his speech. The shelves behind him contain jars of ingredients and kitchen tools, arranged neatly. [0:00:29 - 0:00:33]: The man returns to face the camera directly, bringing his hands together in a pleading gesture while continuing his dialogue. His expression is earnest, and the background remains consistent with the white brick wall and organized kitchen shelves. [0:00:34 - 0:00:36]: Shifting his body again to his left, the man extends his right arm forward, his hand open as if making a point. He continues to speak animatedly, conveying a sense of urgency or importance. [0:00:37 - 0:00:39]: The man faces the camera again, bringing both hands close to his body as he continues speaking. His facial expression intensifies, underscoring his message. The kitchen in the background remains a consistent element, providing a well-organized and clean setting.\n[0:00:40 - 0:01:00] [0:00:40 - 0:00:42]: In a kitchen environment, a person wearing a dark grey shirt holds a white plate with a piece of raw steak. The background reveals white bricks, kitchen shelves with various dishes and utensils, and a countertop with different cooking items. [0:00:43 - 0:00:45]: The person continues to hold the steak on the plate, showing its features in more detail. The steak is marbled and red. The kitchen backsplash, cupboards, and some kitchen utensils remain visible in the background. [0:00:46 - 0:00:50]: The person rotates the steak slightly, revealing different angles and features. The individual also seems to be explaining or discussing the steak, using their other hand to gesture or point at specific parts of the steak. A shelf in the background has various colorful items, including bottles and boxes. [0:00:51 - 0:00:55]: The person keeps the steak prominent, holding it near the center of the frame while continuing to make hand gestures. The dark shirt contrasts with the white plate and red steak, emphasizing the visual features of the steak. The utensils and other kitchen items are consistently visible in the background. [0:00:56]: The person looks down at the steak while holding the plate closer to the counter. The background continues to show the white brick wall and kitchen shelving. [0:00:57 - 0:00:58]: The person extends their right arm towards the counter, pointing at several limes and other kitchen items. The focus shifts slightly towards the counter’s various items, including bowls and a grater. The person’s left hand still holds the plate with the steak. [0:00:59]: The focus shifts to a bowl of rice as the person pointedly gestures towards it with a fork. There are several other items around the bowl, including a jar of spices, pieces of vegetables, and a mortar and pestle. The background remains a bit blurred, keeping the attention on the countertop items.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color shirt is the man wearing when he holds the white plate with a piece of raw steak?",
        "time_stamp": "00:00:57",
        "answer": "A",
        "options": [
          "A. Dark gray.",
          "B. Green.",
          "C. Blue.",
          "D. White."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_12_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:03]: Various ingredients are neatly arranged on a countertop. A jar of chili flakes sits on a white cutting board, surrounded by a bowl of white rice, limes, and other small bowls of ingredients. A hand is pointing near the jar, indicating towards the prepared items. [0:01:03 - 0:01:09]: With a top-down view, the camera captures a bowl of whole red radishes, alongside bowls containing sauces, seasoning, and other ingredients essential for cooking. The hand transitions across the frame, indicating different elements on the table. [0:01:09 - 0:01:16]: The frame shifts to show a person standing in a modern kitchen, with white brick walls and kitchen items arranged on shelves. The person is actively gesturing and explaining something while occasionally interacting with frying pans on the countertop. [0:01:16 - 0:01:19]: Transitioning focus back to the cooking area, the person picks up a frying pan and holds it out while continuing to speak, suggesting a demonstration or explanation of cooking techniques taking place. The background includes detailed kitchen elements like shelves, cookware, and counter space.\n[0:01:20 - 0:01:40] [0:01:20 - 0:01:21]: The video opens with a person standing in a modern kitchen, facing the camera. The kitchen has a blue and white color palette with wooden countertops and white subway tile backsplash. Various kitchen utensils and ingredients are neatly arranged on the counter. The person is wearing a dark grey t-shirt and is looking directly at the camera, appearing to be in the midst of a cooking demonstration.   [0:01:21 - 0:01:22]: The person continues to speak while gesturing with their hands. They then turns their head slightly to the left, possibly looking at something off-camera. [0:01:22 - 0:01:23]: A wider shot of the kitchen shows more details of the surroundings. The word \"COOK\" is prominently displayed in large, red letters on a shelf behind the person. Several bottles, jars, and other kitchen items are neatly organized on the shelves. [0:01:23 - 0:01:24]: The person turns their body to the right and reaches toward the counter, picking up a plate with their left hand. [0:01:24 - 0:01:25]: They hold up a plate containing a piece of raw meat, holding it in front of their chest with their left hand and gesturing towards it with their right hand as they continue to speak. [0:01:25 - 0:01:26]: A closer view shows the person focusing on the piece of meat, explaining something in detail while making hand motions over it. [0:01:26 - 0:01:27]: The person continues speaking, still holding the plate of meat, and making more small gestures with their right hand. [0:01:27 - 0:01:28]: The view shifts slightly downwards, focusing on the countertop area where the person is about to put the plate down. The person's torso is in the frame, and partially obscured by the countertop. [0:01:28 - 0:01:29]: The person places the plate of meat on the wooden cutting board in front of them. The kitchen counter features several bowls, bottles, and other ingredients. [0:01:29 - 0:01:30]: The person raises their hands slightly and gestures towards the camera, explaining the next steps of the cooking process. The kitchen in the background displays more details such as cutting boards, knives on a magnetic strip, and various kitchen tools. [0:01:30 - 0:01:31]: The person continues to gesture with their hands while explaining the cooking process, looking down at the counter and occasionally glancing at the camera. [0:01:31 - 0:01:32]: They reach for a bottle of olive oil from the counter and start pouring it over the raw meat on the plate, holding the bottle with their right hand. [0:01:32 - 0:01:33]: The person looks intently at the plate as they continue to pour olive oil over the meat, ensuring it is thoroughly coated. [0:01:33 - 0:01:34]: The person continues to pour the oil, covering the meat evenly. The plate is positioned on a wooden cutting board, with various kitchen utensils and ingredients nearby. [0:01:34 - 0:01:35]: The person moves the bottle away from the plate, finishing the task of coating the meat with olive oil, and places the bottle back on the counter. [0:01:35 - 0:01:36]: They grab a small bowl of salt and begin to sprinkle it over the oiled meat using their right hand. [0:01:36 - 0:01:37]: The top-down view provides a closer look at the meat now coated with olive oil, and the person continues to sprinkle salt over it. [0:01:37 - 0:01:38]: The person continues to season the meat, ensuring it is evenly coated with salt while still maintaining the focus on the demonstration. [0:01:38 - 0:01:39]: The view remains centered on the meat and the person's hands as they complete the seasoning step, preparing the meat for the next stage of the cooking process.\n[0:01:40 - 0:02:00] [0:01:40 - 0:01:44]: The video opens with a view of a kitchen countertop from a first-person perspective. On the wooden cutting board, a hand is seen placing a piece of steak in a large white plate to the center right of the frame. To the left, there is another white bowl containing some food remnants. In the background, a stovetop with four burners is visible on the right. Behind the cutting board, several ingredients are neatly arranged, including a bowl with three eggs and bottles containing oil and seasoning. [0:01:45 - 0:01:46]: Transitioning to another angle, the viewer now sees a person in a gray shirt holding the piece of steak over the plate with one hand. We're provided with a wider view of the countertop, which includes a skillet on the stovetop burner to the left and an assortment of ingredients spread out on the counter, including bottles, bowls, and chopped items. The stovetop and adjacent parts of the counter are positioned in the left section of the frame with the man in the center. [0:01:47 - 0:01:51]: The man, still holding the steak, appears to be placing it carefully on the plate. He then adjusts the placement and adds some seasonings from nearby containers. The oil and other condiments are still visible on the counter, while the background shows the lower half of blue and wooden cabinets. The person is focused on preparing the meat and ensuring it is well-coated with the seasoning. There are also additional kitchen utensils nearby. [0:01:52 - 0:01:56]: The camera now shifts to a slightly frontal view of the kitchen setup. The person in the gray shirt stands behind the counter, preparing the food. Behind him, the white-tiled wall has various kitchen tools and shelves holding bowls, glasses, and other kitchen essentials. Distinctively, a sign that reads \"COOK\" in bright red letters hangs on the top shelf. The setup emphasizes a well-organized kitchen environment. [0:01:57 - 0:01:58]: As the action progresses, the man is shown still actively involved in the cooking process. He appears to be offering instructions, with his hands moving expressively. Surrounding him are the various tools and ingredients he is using. His attention is divided between the food he is cooking and the additional commentary or instructions he seems to be providing. The pans on the stovetop are now clearly visible, and the organized kitchen environment continues to play a significant background role. [0:01:59]: The video concludes with a close-up view of the man as he seems to wrap up his cooking process. He continues to gesture with his hands, possibly detailing the final steps or providing additional tips related to the cooking. The background retains the blue cabinetry and variety of kitchen items previously described, maintaining a consistent and organized theme throughout the video.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What object is the person holding and gesturing towards right now?",
        "time_stamp": "0:01:25",
        "answer": "D",
        "options": [
          "A. A frying pan.",
          "B. A bowl of radishes.",
          "C. A jar of chili flakes.",
          "D. A plate with raw meat."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_12_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:03]: The person, wearing a dark-colored t-shirt, is positioned in a kitchen setting with a blue and yellow color scheme. There are kitchen utensils and ingredients spread across the counter. They are holding red tongs and appear to be handling food on a plate. [0:03:01 - 0:03:04]: The camera captures the array of ingredients on the counter. Several bowls contain items such as cherry tomatoes, spices, eggs, and limes. The person moves their hands, using the red tongs to manipulate something on the plate in front of them. [0:03:02 - 0:03:05]: The person continues to work with the tongs, moving food around on a plate positioned on a cutting board. The kitchen counter displays an assortment of utensils, including a mortar and pestle, a cheese grater, and bottles of oil and sauces. [0:03:03 - 0:03:06]: Focusing on the plate, the individual uses the tongs to rearrange the food on it. The counter remains cluttered with various kitchen implements and ingredients, presenting a busy cooking environment. [0:03:04 - 0:03:07]: From an overhead perspective, the person’s hands adjusted the food with the red tongs, positioning it on a plate next to bowls containing eggs and other ingredients. [0:03:05- 0:03:08]: Using the tongs, the person prepares a dish on a white plate. The camera captures this action from above, showing a wooden cutting board, bowls with ingredients, and a stovetop nearby. [0:03:06 - 0:03:09]: The individual continues handling the food with tongs on a plate, which involves drizzling or sprinkling some ingredients. The ingredients and utensils on the counter remain prominently in view. [0:03:07 - 0:03:10]: The person uses the tongs to make final adjustments to the dish on the plate. Surrounding items include various bowls, bottles, and kitchen tools spread across the counter. [0:03:08 - 0:03:11]: After rearranging the meal on the plate with tongs, the individual places the plate on the counter. They reach towards another ingredient, preparing to add more to the dish. [0:03:09 - 0:03:12]: Utilizing the tongs, the person carefully holds a food item over the plate before placing it down. The countertop features a range of kitchen items, including bottles of oil and spices. [0:03:10 - 0:03:13]: Presented from close range, the individual’s hands are seen using red tongs to position food on the plate. The countertop displays a well-arranged selection of utensils and ingredients. [0:03:11 - 0:03:14]: With the plate positioned in front of them, the individual adds finishing touches, lifting food using the tongs. A variety of cooking tools and ingredients remain visible on the counter. [0:03:12 - 0:03:15]: In a more zoomed-out view, the kitchen’s full length is visible. Shelves lined with cookbooks, jars, and kitchen tools create a background as the person speaks and gestures with their hands in front of the stove. [0:03:13 - 0:03:16]: The person talks animatedly in front of the stove, making hand movements to emphasize their points. Kitchen shelves in the background hold various cooking items and decor. [0:03:14 - 0:03:17]: Engagement with the audience continues as the person speaks energetically, likely explaining a recipe or cooking process. Shelves filled with kitchen items and a large “Cook” sign form a backdrop. [0:03:15 - 0:03:18]: Still in front of the stove, the individual gestures expressively while speaking, seemingly in the midst of a cooking demonstration. The kitchen backdrop displays a mix of decor and functional items. [0:03:16 - 0:03:17]: With the person nearing the end of their speech, they use succinct hand gestures to conclude their point. The organized kitchen shelves and clean design provide a structured setting. [0:03:17 - 0:03:18]: The individual turns their attention back to the countertop, ready to proceed with the next step of their cooking process. Kitchen appliances and items continue to form an orderly and functional workspace. [0:03:18 - 0:03:19]: Returning focus to the dish, the person uses tongs to lift a piece of food, preparing to place it onto a plate with an array of ingredients positioned nearby. The setting continues to emphasize a dynamic and active cooking scene.\n[0:03:20 - 0:03:40] [0:03:20 - 0:03:23]: A person is holding a piece of marinated meat with red tongs over a plate that has a sauce on it. The person’s right hand is tilted slightly downwards towards a large skillet on a stovetop. The background features a kitchen with blue cabinets and a counter full of various kitchen tools and ingredients;  [0:03:24 - 0:03:27]: The person places the meat into the heated skillet. The countertop has several other skillets and cooking utensils visible, and the blue cabinets continue along the back wall. Mounted on the wall behind the stove is a set of knives and multiple shelves with kitchen items aligned on them; [0:03:28 - 0:03:30]: The person begins wiping their hands with a towel while glancing towards the stove. The shelves in the background have stacks of plates, glass jars, and other kitchen items neatly arranged; [0:03:31 - 0:03:33]: The camera focuses on the pan, with the meat sizzling inside. The person holds a bottle of oil above the skillet, dripping oil onto the meat. Steam rises from the skillet as the oil hits the hot surface. A kitchen timer graphic appears in the bottom right corner of the frame, showing “08:00” at first and then counting down to “07:56”; [0:03:34 - 0:03:37]: The person steps back from the stove briefly, still holding the towel. The kitchen remains brightly lit with a large window to the left letting in natural light. There are potted plants near the window, adding a touch of greenery to the space. The shelves and counters continue to display a variety of kitchen utensils and ingredients; [0:03:38 - 0:03:39]: The camera angle shifts to an overhead view, showing three skillets on the stove. One of the skillets holds the piece of meat which is now sizzling slightly. The person’s hands are seen adjusting the position of one of the empty skillets on the stove.\n[0:03:40 - 0:04:00] [0:03:40 - 0:03:53]: A hand is holding a pan with a piece of meat sizzling inside it. The pan is on a stovetop next to two other empty pans. The setting is a kitchen, with a tiled floor visible underneath the stovetop. The hand is adjusting the pan's position on the stove. The stove appears metallic and has three cooking areas as well as five black knobs on the right side.  A person is standing in a modern kitchen, facing the camera while speaking. The kitchen features a white brick wall with shelves holding various items, including jars, dishes, and utensils. The word \"COOK\" is prominently displayed in red letters on one of the shelves. Two pendant lights hang from the ceiling, illuminating the scene. The person gestures with their hands as they speak.  The person continues speaking and gestures towards the stovetop in front of them, which has three pans placed on it. The kitchen counter in front of the person is filled with several items, including some bowls, utensils, a bottle of oil, and a few small containers with various ingredients.  The person turns towards the stovetop and appears to engage with one of the pans, possibly adjusting something in it. Their focus seems to be on cooking, and their movements are deliberate. The kitchen's organization and cleanliness are well-maintained, with everything neatly arranged. The person returns their attention to the camera and continues speaking, gesturing slightly with their hands. The stovetop remains in the frame, with steam or smoke rising from one of the pans. The kitchen's decor, including the white brick wall, open shelves, and pendant lights, provides a modern and welcoming atmosphere. The person leans slightly to the right, maintaining their focus on the cooking process. They appear to be checking or adding something to one of the pans on the stovetop. The surrounding kitchen items, including a variety of utensils and ingredients, suggest a professional cooking environment. The person's hands are visible as they handle utensils and work on the stovetop. Their actions are precise and controlled, indicating experience and familiarity with the cooking process. The focus remains on the interaction with the pans and the ongoing culinary activity. [0:03:54 - 0:03:59]: The top-down view of the stovetop shows the person holding a bottle of oil and pouring it into one of the empty pans. The layout features a piece of meat in the left pan, while the right pan receives a swirl of oil. The countertop surrounding the stovetop is clean and spacious, with some kitchen utensils visible at the edges. The person's hand and arm are steady as they handle the bottle and distribute the oil evenly in the pan. The overall scene emphasizes a careful and methodical approach to cooking.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person holding in their hand?",
        "time_stamp": "0:03:05",
        "answer": "D",
        "options": [
          "A. A spatula.",
          "B. A wooden spoon.",
          "C. A fork.",
          "D. Red tongs."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_12_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:04]: A man wearing a dark t-shirt is in a kitchen setting with white brick walls, a countertop, and various kitchen utensils and ingredients around him. He is preparing food on a wooden cutting board positioned on the countertop. There are several pans on the stove, with one containing a piece of meat, and various bowls and ingredients including a jar of salt, half a lemon, and small red vegetables are scattered around. The man is holding a spoon in his right hand and a small white bowl in his left hand. [0:09:05]: The man continues mixing something in the small white bowl while the meat sizzles in the pan on the stove. A voice in the background says, \"2 minutes!\" [0:09:06 - 0:09:09]: The man looks up and starts moving to the right while still holding the bowl and spoon. He adjusts his stance and facial expression as if responding to the announcement. The kitchen in the background features two large green hanging lamps, a shelf with dishes, bottles, and jars, and a large red sign that reads \"COOK\" hanging on the wall. The stove has multiple pans and cooking items on it. [0:09:10 - 0:09:12]: The man picks up a pan with a towel and examines its contents. He tilts the pan towards himself while holding it with his right hand and a cloth in his left hand. His face shows focused attention on the cooking process, and the blue cabinets, various utensils, and plants near the windows are visible in the background. [0:09:13 - 0:09:15]: The man moves back to the center of the counter, still holding the towel and pan. He reaches out with his right hand to grab another pan or utensil, indicating that he is multitasking in the kitchen. The white brick wall and shelves brimming with kitchen equipment serve as a backdrop. [0:09:16 - 0:09:19]: The man continues cooking and adjusting the food in the pans on the stovetop. He opens and closes various drawers and cabinets, takes out utensils, and occasionally glances up, possibly responding to someone or something outside the camera's view. He places a white plate under the counter, handles the food with precision, and starts plating the dish. The background remains consistent with the arrangement of knives, dishes, and decorative items on the shelves and countertops.\n[0:09:20 - 0:09:40] [0:09:20 - 0:09:22]: The video begins with someone holding a pan over a stove with a flame beneath it. The person is tilting the pan to pour its contents into a larger serving pan positioned on the stovetop. The stovetop is surrounded by various kitchen items, including a plate, bowls, and utensils. [0:09:23 - 0:09:26]: The perspective shifts to an overhead view. The person continues to pour the contents from the pan into a serving dish. Various kitchen ingredients and tools are laid out, such as a bowl of chopped vegetables, utensils, and spices. [0:09:27 - 0:09:29]: The camera angle changes to a person in front of a tiled wall with shelves holding kitchen items. The person is wearing a dark shirt, and steam rises from the pan they are holding while they add the food to a serving dish. [0:09:30 - 0:09:32]: The person continues to pour food from the frying pan into the serving dish. They adjust the pan and use a spoon to help transfer the contents. The background shows a well-organized kitchen with a sign above the shelves reading “COOK.” [0:09:33 - 0:09:35]: The focus is again on transferring the food from the pan to a plate. The food appears to be some kind of stir-fry with vegetables. The surrounding kitchen items remain in place on the counter. [0:09:36]: The person stands behind the counter, wiping the area with a towel. The kitchen shelves, with various items placed neatly, are visible in the background. [0:09:37 - 0:09:39]: The camera shifts to an overhead view once more, showing a stovetop with multiple frying pans. One pan contains a piece of meat, another has two fried eggs, and a third has remnants of the recently cooked food. The person's hands are in motion, preparing to handle the frying pans on the stove.\n[0:09:40 - 0:10:00] [0:09:40 - 0:09:41]: The video begins with a view of a kitchen setup. The scene captures a person in a black t-shirt standing behind a kitchen counter. The kitchen has a white brick backsplash with wooden shelves holding various utensils and plates. The individual is holding a white plate with a dish and seems to be focused on the preparation process. [0:09:42]: The person tilts to the left, placing the plate down on the counter. Kitchen appliances like a sink and stove are visible in the background, indicating a busy cooking scene. [0:09:43]: The camera shifts its focus slightly lower, displaying a close-up of two frying pans on the stove. The person is handling a piece of cooked meat with tongs and a knife, about to transfer it. [0:09:44 - 0:09:49]: Now with a top-down perspective, the individual moves the cooked meat onto a wooden cutting board adjacent to a dish of rice and vegetables. The person starts to slice the meat with a knife, carefully making several precise cuts. [0:09:50 - 0:09:52]: The sliced meat is then picked up and placed on top of the rice dish. The person arranges it neatly, ensuring an even distribution over the bed of rice and vegetables. [0:09:53]: The view zooms out to reveal more of the kitchen setup. Various ingredients, bowls, and utensils are spread across the counter. The person is leaning forward, adding final touches to the dish with a spoon. [0:09:54 - 0:09:56]: Continuing from the previous actions, the person picks up a frying pan and tilts it to pour the remaining sauce or drippings onto the dish, adding a glossy finish to the plated meat. [0:09:57 - 0:09:58]: The camera captures a close-up of the sauce being drizzled over the meat. The dish is almost ready for serving, with the person handling the final steps with precision. [0:09:59]: The final frame zooms out to an overhead shot showing the entire work area. The countertop is cluttered with various ingredients and cooking tools, indicating a busy and vibrant cooking environment. The person appears to be frying eggs in another pan, showcasing the diversity of activities in the kitchen.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the man holding in his right hand while preparing food on the cutting board?",
        "time_stamp": "0:09:04",
        "answer": "D",
        "options": [
          "A. A jar of salt.",
          "B. Half a lemon.",
          "C. A red vegetable.",
          "D. A small white bowl."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_12_real.mp4"
  },
  {
    "time": "[0:11:00 - 0:11:06]",
    "captions": "[0:11:00 - 0:11:06] [0:11:00]: A person wearing a green t-shirt stands in a kitchen with a backdrop of white tiled walls. Kitchen utensils, dishes, glasses, and a shelf lined with cookbooks are visible behind him. The individual appears to be in mid-speech, with hands clasped together and a serious facial expression.  [0:11:01]: The scene transitions with a large graphic arrow, partially obscuring the previous view but hinting at continued kitchen themes behind it. [0:11:02 - 0:11:04]: A cookbook titled \"Gordon Ramsay in 10: Delicious Recipes Made in a Flash\" is featured prominently on a wooden surface. The cover includes images of various dishes and a picture of someone cooking, emphasizing the theme of quick and delicious recipes.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the title of the cookbook prominently featured?",
        "time_stamp": "00:11:04",
        "answer": "A",
        "options": [
          "A. \"Gordon Ramsay in 10: Delicious Recipes Made in a Flash\".",
          "B. \"Jamie Oliver's 30-Minute Meals\".",
          "C. \"Gordon Ramsay's Ultimate Cookery Course\".",
          "D. \"Nigella Express: Good Food Fast\"."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_12_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:11]: The video starts with a view of an amusement park themed like a colorful, fantasy game world. Various characters, vibrant objects, and structures are featured, including large red and yellow mushrooms and green pipes. The scene is busy with people walking, heading toward a large green tunnel labeled \"Mario Kart: Bowser's Challenge.\" The left side features an entrance manned by staff in red attire, while the right side showcases different colorful sections, including checkered patterns and a grey stone statue with a menacing face near the top of the structure. [0:00:11 - 0:00:20]: The perspective moves closer to the Mario Kart ride entrance. The details of the tunnel's bright green structure and the surrounding vibrant landscape become more apparent. People continue to walk into the tunnel, passing under a red and white umbrella near the entrance. Inside, the tunnel gradually reveals a dimly lit interior with green walls and additional decorations and lighting that enhance the fantasy atmosphere. The video ends with the first-person view partly inside the tunnel, moving forward.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is labeled on the large green tunnel people are heading toward?",
        "time_stamp": "0:00:11",
        "answer": "A",
        "options": [
          "A. \"Mario Kart: Bowser's Challenge\".",
          "B. \"Super Mario World\".",
          "C. \"Luigi's Mansion\".",
          "D. \"Princess Peach's Castle\"."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_320_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:26]: The video begins with a group of people, including a person in a red uniform, a middle-aged person wearing a white shirt, and a child, walking through a dimly lit corridor. The corridor is adorned with multiple screens displaying animated characters and vibrant colors. The group is moving towards an open doorway leading into a brightly lit room;  [0:02:27 - 0:02:29]: As they progress, the brightly lit room becomes more visible, revealing a lively environment filled with themed decorations, neon lights, and colorful signs. The floor has a large emblem with the words \"Universal Studios\" and \"Mario Kart\" written on it; [0:02:30 - 0:02:34]: The group continues forward, passing themed displays with character costumes and toys enclosed in glass cases along the walls. These display shelves contain oversized character suits and colorful, cartoonish objects. The room is vibrant with reds, greens, and yellows, adding to the energetic atmosphere; [0:02:35 - 0:02:36]: The focus shifts towards the further end of the room where a collection of character costumes is neatly arranged in shelves. The shelves contain suits resembling oversized animated characters, a frog-like character, and a dinosaur-green character holding a mushroom; [0:02:37 - 0:02:39]: The video transitions to a new scene where a group of people, who appear to be seated in what looks like a theme park ride with cars stylized after characters from a popular franchise. The environment is dark but illuminated by small light sources, and the walls are designed to look like stone bricks with a large logo that reads \"Universal Studios Mario Kart\" hanging above an arched doorway. The overall ambiance suggests a fun and adventurous theme park experience.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What was the group doing just now?",
        "time_stamp": "0:02:26",
        "answer": "D",
        "options": [
          "A. Standing in a queue.",
          "B. Sitting on a theme park ride.",
          "C. Entering a theater.",
          "D. Walking through a dimly lit corridor."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_320_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:42]: An underwater scene with vibrant, colorful marine life and a submarine-like vehicle moving through the water. The background has rocky formations and floating sea plants. A large, red fish with a menacing expression appears to be chasing the submarine. [0:04:42]: A white, cartoonish squid with big round eyes faces the viewer, seemingly startled. The large red fish remains in the background. [0:04:43 - 0:04:45]: The screen is mostly dark with a few tiny, glowing lights, giving the impression of deep sea creatures or bioluminescence. [0:04:46 - 0:04:49]: The scene brightens back up, showing characters resembling an armored spiky creature surrounded by various dangerous-looking sea creatures. Blue spotlights illuminate parts of the scene, emphasizing some of the creatures as they swim. [0:04:50 - 0:04:52]: The armored spiky character moves forward as more jellyfish-like creatures hover above. Bright spotlights and glowing effects create a dynamic, intense atmosphere. [0:04:53]: The character encounters a large skeletal fish with a glowing eye, giving a sense of impending danger. [0:04:54 - 0:04:56]: The skeletal fish appears to attack, while the armored character tries to evade. The background shows a mix of dark rocky formations and the glow from the skeletal fish's eye. [0:04:57 - 0:04:59]: The armored character continues to navigate the underwater environment amidst bubbles and glowing elements. The skeletal fish is present in the background, adding to the sense of urgency and action.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the large fish chasing the small fishes?",
        "time_stamp": "00:04:42",
        "answer": "C",
        "options": [
          "A. Blue fish.",
          "B. Green fish.",
          "C. Red fish.",
          "D. Yellow fish."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_320_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:03]: In the video, vibrant, multicolored streaks of light stretch outwards on a dark background, creating a sense of rapid movement, resembling a warp speed effect. Two animated characters, Bowser and Bowser Jr., are visible, appearing to move quickly alongside the light streaks, with their distinct colors and shapes becoming more prominent. [0:07:03 - 0:07:06]: The view shifts to a rainbow-colored track that Bowser and Bowser Jr. are steering onto. Strange multicolored lines and patterns surround them. They seem to be gaming characters engaged in an intense action sequence. The track consists of vividly colored tiles, and Bowser Jr. is seen unfurling a glider. [0:07:06 - 0:07:13]: As they zoom ahead, the video shows them nearing the finish line, which is marked by a broad banner. Bowser Jr. glides towards it amidst the colorful background. Tanooki Mario appears on the finish platform. Excited celebratory signs and sparkles highlight the cross-line moments, emphasizing their victory. [0:07:13 - 0:07:15]: The scene transitions to the prize podium. Bowser Jr. and teammates celebrate under bright, colorful spotlights and signs. The “Team Bowser Wins!” banner is prominently displayed amidst the surrounding confetti and sparkles. [0:07:15 - 0:07:17]: The camera angle shifts to showcase the team's celebration from a different perspective, displaying vibrant celebratory graphics and more of the gaming environment's dark background. [0:07:17 - 0:07:20]: The view changes, focusing on a castle-like structure adorned with checkered flags and digital screens on its walls. The lighting in the scene is significantly dimmed, giving a sense of conclusion to the event. The area is predominantly dark with minimal light accents highlighting parts of the structure.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action does Bowser Jr. perform as they steer onto the rainbow-colored track?",
        "time_stamp": "0:07:06",
        "answer": "D",
        "options": [
          "A. Jumps.",
          "B. Throws a shell.",
          "C. Activates a boost.",
          "D. Unfurls a glider."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Event Understanding",
        "question": "What is the main event happening as Bowser Jr. reaches the finish line?",
        "time_stamp": "0:07:13",
        "answer": "D",
        "options": [
          "A. Bowser Jr. avoids obstacles.",
          "B. Bowser Jr. prepares to glide.",
          "C. Tanooki Mario joins the race.",
          "D. Excited celebratory signs and sparkles appear."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_320_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which store's name is visible on the left side of the street?",
        "time_stamp": "00:00:04",
        "answer": "A",
        "options": [
          "A. Bed Bath & Beyond.",
          "B. Barnes & Noble.",
          "C. Target.",
          "D. Best Buy."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_385_real.mp4"
  },
  {
    "time": "[0:02:02 - 0:02:07]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which brand logo is visible on the store awnings?",
        "time_stamp": "00:02:03",
        "answer": "B",
        "options": [
          "A. Walmart.",
          "B. Trader Joe's.",
          "C. Whole Foods.",
          "D. Starbucks."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_385_real.mp4"
  },
  {
    "time": "[0:04:04 - 0:04:09]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the Citibank sign located right now?",
        "time_stamp": "00:04:07",
        "answer": "B",
        "options": [
          "A. On the right side of the road.",
          "B. On the left side of the road.",
          "C. In the front of the car.",
          "D. Behind the car."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_385_real.mp4"
  },
  {
    "time": "[0:06:06 - 0:06:11]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the notable color of the taxi right now?",
        "time_stamp": "00:06:10",
        "answer": "C",
        "options": [
          "A. Red.",
          "B. Blue.",
          "C. Yellow.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_385_real.mp4"
  },
  {
    "time": "[0:08:08 - 0:08:13]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the brand of the visible pharmacy right now?",
        "time_stamp": "00:08:10",
        "answer": "B",
        "options": [
          "A. Walgreens.",
          "B. Duane Reade.",
          "C. Rite Aid.",
          "D. CVS."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_385_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the content of the video just shown?",
        "time_stamp": "00:00:10",
        "answer": "D",
        "options": [
          "A. An employee prepares a breakfast sandwich, adds condiments, and serves it to the customer.",
          "B. An employee prepares a sandwich, toasts the buns, adds sauce, lettuce, and serves it in a box.",
          "C. An employee prepares a salad, chops vegetables, and arranges them in a bowl.",
          "D. An employee prepares a burger, adds mayo, lettuce, and packs it in a box."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_349_real.mp4"
  },
  {
    "time": "[0:01:39 - 0:01:49]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the worker's actions just now?",
        "time_stamp": "00:01:49",
        "answer": "A",
        "options": [
          "A. An employee prepares a burger, adds cheese, lettuce, and sauce, and packs it in a box.",
          "B. An employee fries a chicken patty, adds mayo and lettuce, and packs it in a bag.",
          "C. An employee prepares a sandwich, adds cucumbers, tomatoes, and sauce, and packs it in a wrap.",
          "D. An employee prepares a vegan burger, adds vegan cheese, lettuce, and sauce, and packs it in a box."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_349_real.mp4"
  },
  {
    "time": "[0:03:18 - 0:03:28]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the worker's actions just now?",
        "time_stamp": "00:03:28",
        "answer": "A",
        "options": [
          "A. The worker toasts a burger bun, adds tartar sauce and lettuce, and places the assembled burger back on the counter.",
          "B. The worker grills a patty, adds ketchup and onions to the bun, and places the assembled burger in a bag.",
          "C. The worker fetches ingredients from the refrigerator, cuts tomatoes, and adds them to a bowl.",
          "D. The worker fries a fish fillet, adds mayo and pickles to a bun, and places the assembled sandwich in a wrap."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_349_real.mp4"
  },
  {
    "time": "[0:04:57 - 0:05:07]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the employee's actions just now?",
        "time_stamp": "00:05:21",
        "answer": "B",
        "options": [
          "A. The employee prepared a chicken sandwich by toasting buns, adding mayonnaise and lettuce, and packing it in a box.",
          "B. The employee prepared a fish burger by toasting buns, adding cheese, and placing it in a box.",
          "C. The employee prepared a fish sandwich by toasting buns, adding tartar sauce and lettuce, and packing it in a box.",
          "D. The employee prepared a veggie burger by toasting buns, adding vegetables, and packing it in a box."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_349_real.mp4"
  },
  {
    "time": "[0:06:36 - 0:06:46]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the content of the video just shown?",
        "time_stamp": "00:06:46",
        "answer": "C",
        "options": [
          "A. The employee cooked and served fries, adding salt and packing them in a container.",
          "B. The employee grilled a chicken patty, added lettuce and mayo, and packed the sandwich in a box.",
          "C. The employee packed chicken nuggets into boxes.",
          "D. The employee prepared a fish sandwich, adding tartar sauce and lettuce, and packed it in a paper wrap."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_349_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:16",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_88_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:08",
        "answer": "A",
        "options": [
          "A. 4.",
          "B. 5.",
          "C. 6.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_88_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:11",
        "answer": "B",
        "options": [
          "A. 2.",
          "B. 4.",
          "C. 3.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_88_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:01",
        "answer": "B",
        "options": [
          "A. 2.",
          "B. 5.",
          "C. 4.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_88_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:06:19",
        "answer": "D",
        "options": [
          "A. 4.",
          "B. 8.",
          "C. 6.",
          "D. 7."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_88_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the power lines located right now?",
        "time_stamp": "00:00:08",
        "answer": "D",
        "options": [
          "A. Crossing diagonally over the road.",
          "B. Parallel to the right side of the road.",
          "C. Intersecting the road perpendicularly.",
          "D. Running parallel to the left side of the road."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_171_real.mp4"
  },
  {
    "time": "[0:02:05 - 0:02:25]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the red graffiti?",
        "time_stamp": "00:02:16",
        "answer": "D",
        "options": [
          "A. On the right side of the road.",
          "B. On the left side of the road.",
          "C. In front of the road.",
          "D. Above the entrance of the tunnel."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_171_real.mp4"
  },
  {
    "time": "[0:04:10 - 0:04:30]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is on the left side of the road right now?",
        "time_stamp": "00:04:10",
        "answer": "D",
        "options": [
          "A. A river.",
          "B. A small building.",
          "C. An open field.",
          "D. Dense vegetation."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_171_real.mp4"
  },
  {
    "time": "[0:06:15 - 0:06:35]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located on the left side of the path right now?",
        "time_stamp": "00:06:27",
        "answer": "D",
        "options": [
          "A. A cornfield.",
          "B. A tall building.",
          "C. A dense forest.",
          "D. A patch of grass."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_171_real.mp4"
  },
  {
    "time": "[0:08:20 - 0:08:40]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located on the right side of the road right now?",
        "time_stamp": "00:08:34",
        "answer": "D",
        "options": [
          "A. A field of corn.",
          "B. A forest.",
          "C. A river.",
          "D. A field of wheat."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_171_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video begins with a wide-angle view of a football stadium crowded with spectators. The field is lush green, and there are multiple advertisements on the outskirts of the pitch. Players and a referee are seen, with one player in a yellow jersey and another in blue. The player in the yellow jersey is near the center. [0:00:04 - 0:00:05]: The focus shifts to a close-up of a player in a yellow jersey. The player appears focused and somewhat intense, with the background slightly blurred, emphasizing his face. [0:00:06 - 0:00:07]: Another scene shows the player wiping his face with his hand, possibly indicating concentration or stress. [0:00:07 - 0:00:09]: The next frame captures the goalkeeper from the back, wearing a bright green jersey with the name \"PICKFORD\" and the number one written on it. The background includes spectators and other players. [0:00:10]: Another close-up shows the player in the yellow jersey again, who has his eyes closed or looking downward, further emphasizing focus or perhaps a moment of contemplation. [0:00:11 - 0:00:14]: Two players in yellow jerseys, with \"L. MURIEL 14\" and \"number 15\" on their backs, are seen embracing each other in front of the goal post. A crowd is visible in the background. [0:00:15 - 0:00:18]: The focus moves to the field where a close-up of a player’s feet wearing white cleats with ADIDAS branding can be seen. The player adjusts the position of a football, which has a distinctive red and black pattern. The white socks worn by the player are dirty, indicating the intensity of the match. [0:00:19 - 0:00:20]: The final frame features a player’s lower body in white shorts and socks, standing on the field. The action seems to be gearing up for a critical moment like a kick or a move.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What name is written on the goalkeeper's bright green jersey?",
        "time_stamp": "00:00:14",
        "answer": "B",
        "options": [
          "A. NEUER.",
          "B. PICKFORD.",
          "C. DE GEA.",
          "D. COURTOIS."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Object Recognition",
        "question": "What brand is visible on the player's white cleats?",
        "time_stamp": "00:00:18",
        "answer": "C",
        "options": [
          "A. Nike.",
          "B. Puma.",
          "C. Adidas.",
          "D. Under Armour."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_5_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:24]: The video shows a penalty shootout in a soccer match. A player in a yellow jersey is preparing to take a penalty kick. He stands a few meters away from the ball, which is placed on the penalty spot. The goalkeeper, in a bright green jersey, is positioned in front of the goal, getting ready to defend the shot. The crowd in the stands behind the goal is waving flags and banners, and there is a visible advertisement board with \"Coca-Cola\" repeated on it. [0:02:25 - 0:02:29]: After taking the shot, the player in the yellow jersey runs forward a few steps and begins to celebrate. His facial expression shows determination and joy. He pumps his fist as he runs, clearly elated by the outcome of his kick. [0:02:30 - 0:02:33]: The video cuts back to a close-up of the player in the yellow jersey just as he strikes the ball with his right foot. The goalkeeper dives to his left, trying to reach the ball, but misses. The ball goes into the net, indicating a successful penalty. [0:02:34 - 0:02:35]: The goalkeeper is seen getting up from the ground after his failed attempt to save the penalty. His body language shows frustration. [0:02:36 - 0:02:39]: The view zooms out to show the entire field and the cheering crowd in the stands. The scoreboard at the bottom of the screen shows the current score: Colombia 2-1 England. The excitement in the stadium is palpable as the camera captures the celebratory atmosphere.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the outcome of the penalty kick just now?",
        "time_stamp": "00:02:27",
        "answer": "C",
        "options": [
          "A. The goalkeeper saves the ball.",
          "B. The ball hits the post.",
          "C. The ball goes over the crossbar.",
          "D. The ball goes into the net."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_5_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:44]: A penalty kick scene is depicted with a goalkeeper diving to his right, attempting to save a shot. The ball is headed towards the goal, and spectators in the background are intensely watching. Some stewards in neon vests and photographers are visible behind the goal, with a sponsorship banner behind them. [0:04:45 - 0:04:49]: The next sequence shifts focus away from the goal to show players in the playfield. A player in a yellow jersey appears to be walking away from the penalty area, as a group of players in red jerseys are standing together in anticipation. The LED display shows the match information with the score at 3-2, indicating a tense moment in the game. [0:04:50 - 0:04:53]: The scene shifts to the coaching or managerial area. A man in a suit, likely a coach, visibly expresses distress with his hand over his face while being consoled by others. This area also shows the live match updates with the ongoing score displayed at the bottom. [0:04:54 - 0:04:57]: The camera then focuses on a close-up of a player in a yellow jersey, possibly reflecting the emotional aftermath of the penalty kick. Another view captures the interaction between the referee and the player, where the referee's back is towards the camera. [0:04:58 - 0:04:59]: Finally, the focus shifts to a player in a bright green jersey, likely the goalkeeper, who appears to be processing the outcome, showing mixed expressions of frustration and composure. The player lowers his head briefly, indicating a moment of contemplation or disappointment.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the goalkeeper do during the penalty kick?",
        "time_stamp": "00:04:33",
        "answer": "B",
        "options": [
          "A. Stood still.",
          "B. Dived to his right.",
          "C. Dived to his left.",
          "D. Ran towards the ball."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_5_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:05]: A goalkeeper dressed in a bright green kit with the number \"1\" on the back is poised to the left side of the goal line, preparing to dive as a player in a yellow kit takes a penalty kick. The goalkeeper dives to his right, fully stretched in an attempt to save the shot. The ball is seen mid-air to the right of the frame as the goalkeeper reaches out towards it. [0:07:06 - 0:07:09]: A close-up shot shows a man with a serious expression standing next to another individual. The background is blurred, revealing a filled stadium with spectators. Both individuals are focused on the ongoing game. [0:07:10 - 0:07:13]: The focus shifts to a player in a red kit with a look of concentration, possibly preparing for a turn to take a penalty kick. The background shows blurred spectators watching intently. [0:07:14 - 0:07:15]: The same player in the red kit is shown bending over, adjusting his position. An official in black stands close by, overseeing the situation. [0:07:16 - 0:07:17]: A goalkeeper in a teal kit with a determined look is seen standing in front of the goal, anticipating the next move. The net behind him and the crowd in the background are visible. [0:07:18 - 0:07:19]: The focus returns to the player in the red kit, capturing his focused expression as he prepares for his task at hand. The blurred background remains filled with spectators.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What number is on the back of the goalkeeper's kit who is preparing to dive for the penalty kick?",
        "time_stamp": "0:07:23",
        "answer": "D",
        "options": [
          "A. 10.",
          "B. 7.",
          "C. 5.",
          "D. 1."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_5_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the white road marker positioned relative to the cyclist right now?",
        "time_stamp": "00:00:06",
        "answer": "A",
        "options": [
          "A. On the both sides of the cyclist.",
          "B. In front of the cyclist.",
          "C. To the left of the cyclist.",
          "D. Behind the cyclist."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_178_real.mp4"
  },
  {
    "time": "[0:01:51 - 0:02:11]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "How are the road signs oriented right now?",
        "time_stamp": "00:01:55",
        "answer": "A",
        "options": [
          "A. Indicating a left turn.",
          "B. Pointing straight ahead.",
          "C. Indicating a right turn.",
          "D. Indicating a U-turn."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_178_real.mp4"
  },
  {
    "time": "[0:03:42 - 0:04:02]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located at the far end of the left side of the road right now?",
        "time_stamp": "00:03:57",
        "answer": "A",
        "options": [
          "A. A tall tree.",
          "B. A hospital.",
          "C. A park.",
          "D. A bridge."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_178_real.mp4"
  },
  {
    "time": "[0:05:33 - 0:05:53]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the blue car located right now?",
        "time_stamp": "00:05:38",
        "answer": "A",
        "options": [
          "A. On the right side of the cyclist.",
          "B. On the left side of the cyclist.",
          "C. Directly in front of the cyclist.",
          "D. Behind the cyclist."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_178_real.mp4"
  },
  {
    "time": "[0:07:24 - 0:07:44]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the red car located right now?",
        "time_stamp": "00:07:38",
        "answer": "A",
        "options": [
          "A. On the right side of the cyclist.",
          "B. On the left side of the cyclist.",
          "C. Directly in front of the cyclist.",
          "D. Behind the cyclist."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_178_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a black screen. After a second, it transitions to a view of a character in Minecraft wearing purple armor. The backdrop includes a glowing portal on the left side and some chests and blocks on the right. [0:00:02 - 0:00:03]: The camera angle changes, showing the character from a slightly lower angle, with large text \"um\" appearing on the screen. The camera then zooms in on the character's face, showing its pixelated green eyes. [0:00:04 - 0:00:05]: The camera zooms out to show the character fully again in front of the portal. The background remains consistent with chests and blocks visible. [0:00:06]: The camera zooms back into the character's eyes. [0:00:07]: The camera offers an aerial view of a Minecraft landscape including mountains, water bodies, and blocks. [0:00:08 - 0:00:10]: The character in purple armor is shown flying with wings, as the camera follows its movements from a first-person perspective. The blocky Minecraft landscape passes below. [0:00:11 - 0:00:13]: The perspective transitions to a view from above, showing colorful block structures and vast expanses of water and trees. [0:00:14]: The character continues flying over the landscape, descending slightly. The view reveals more details of the terrain and water below. [0:00:15 - 0:00:16]: The camera shifts to an underwater scene, showing fish swimming around and aquatic plants. [0:00:17 - 0:00:18]: The perspective returns to the character flying over the landscape, with green and brown blocks along with a body of water visible below. [0:00:19 - 0:00:20]: The camera points upward towards the sky, showing a blue expanse with white clouds. The video ends with this upward perspective.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action is the character in purple armor performing when the camera follows its movements from a first-person perspective?",
        "time_stamp": "00:00:10",
        "answer": "D",
        "options": [
          "A. Mining blocks.",
          "B. Building a structure.",
          "C. Walking through a portal.",
          "D. Flying with wings."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_198_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:42]: The video starts with a first-person view of a character looking out from a white tunnel into a larger room filled with lava, floating yellow and black creatures, and particles falling from the ceiling. The lava appears thick and orange, and the creatures are blocky with flames around them. [0:04:43]: As the character moves backward, they turn away from the lava room into a darker hallway made of red and black brick textures, contrasting sharply with the white tunnel. The character's hand holds an item resembling a white block. [0:04:44 - 0:04:48]: The character places the white block in the wall and proceeds to create an opening. The brick texture changes to a mix of dark and lighter blocks, indicating a transition between areas. The dark bricks give way to a more open room with flowing lava columns and openings in the walls. [0:04:49 - 0:04:50]: The character navigates through the newly opened doorway into a broader area filled with red and black bricks and pillars, augmented by flowing lava columns on each side. [0:04:51 - 0:04:52]: The character wields a sword, approaching a yellow and black floating creature near the lava column. The area is dark, and the creature emits smoke particles, indicative of the hostile environment. [0:04:53]: The character moves closer to the yellow and black creature, observed with the sword still in hand while staunchly looking up at it against the dark brick background, lava casting an orange glow. [0:04:54]: The creature advances closer to the character, floating slightly upwards near the lava column with its roughly rectangular and fiery appearance. Its proximity indicates a brewing confrontation. [0:04:55]: The character swipes with the sword at the creature, resulting in a flying blue and white lightning particle effect marking the impact, showing that a strike on the creature took place. [0:04:56 - 0:04:57]: The creature emits a large burst of flames and smoke particles, indicating it has been hit and is being damaged. The dark and fiery background enhances the visibility of these effects. [0:04:58 - 0:05:00]: After a few seconds, the creature finally succumbs to the attack, disintegrating into smaller particles that fade away. The area returns to the previous state, with the character ready and looking towards another yellow and black creature across the room, near another lava column.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What item is the character holding in their hand right now?",
        "time_stamp": "00:04:41",
        "answer": "D",
        "options": [
          "A. A sword.",
          "B. A shield.",
          "C. A torch.",
          "D. A white block."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Action Recognition",
        "question": "What action does the character perform after turning away from the lava room?",
        "time_stamp": "00:04:44",
        "answer": "D",
        "options": [
          "A. Fights a creature.",
          "B. Collects a resource.",
          "C. Lights a torch.",
          "D. Places a white block in the wall."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_198_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:20 - 0:09:40] [0:09:20 - 0:09:22]: The video starts in a mountainous terrain with green fields and patches of trees in the background. The terrain is composed of blocks and appears to be from the video game Minecraft. The player is flying over the landscape.  [0:09:23 - 0:09:27]: The player moves closer to the ground and a cave entrance. The cave is dark with visible waterfalls and lava flows inside. The environment becomes darker as they descend further into the cave. [0:09:28 - 0:09:33]: As the player continues flying deeper into the cave, text reading \"usefulness\" appears prominently on the screen. The cave is complex, with multiple layers, waterfalls, and lava streams lighting up various parts. [0:09:34]: The screen transitions to an image of a Minecraft character labeled \"farm #6 as minecraft steve.\" [0:09:36 - 0:09:39]: The scene changes to a twilight outdoor setting with a Minecraft Warden, a large and dark creature with distinctive white and teal markings, standing on a grassy field. The background shows a setting sun, casting an orange hue on the sky. The Warden appears to be standing still.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is prominently displayed on the screen as the player continues flying deeper into the cave?",
        "time_stamp": "00:09:33",
        "answer": "A",
        "options": [
          "A. Usefulness.",
          "B. Danger.",
          "C. Exploration.",
          "D. Achievement."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_198_real.mp4"
  },
  {
    "time": "[0:14:00 - 0:15:00]",
    "captions": "[0:14:00 - 0:14:20] [0:14:00 - 0:14:01]: The scene starts with a first-person perspective view of a Minecraft player flying over a landscape using elytra wings. They appear to be at a significant height above the ground, moving forward with a clear view of the surrounding terrain. There's an eye-level scene of a white structure with wooden accents below and various green trees in the background. [0:14:01 - 0:14:05]: The camera continues to move forward, showing a hilly landscape with patches of trees and a distinct large mountain in the distance. The sky is predominantly clear with a few scattered clouds. The player's hotbar is visible at the bottom of the screen, indicating they have a variety of items available. [0:14:05 - 0:14:06]: As the player moves closer to a tall, snowy mountain, their altitude appears to slightly decrease. The Minecraft landscape continues to change, displaying different biomes including snowy areas and green hills. [0:14:06 - 0:14:09]: Cartoon graphics with colorful text overlay appear on the screen, partially obscuring the view. The background still shows the player flying over the landscape. The terrain below is largely green with patches of brown. [0:14:09 - 0:14:11]: The camera tilts downward to show a bird's-eye view of the ground below. The landscape consists of green blocks with scattered patches of brown, likely representing grass and dirt, and a few isolated trees. [0:14:11 - 0:14:13]: The view switches to a third-person perspective, showing the character equipped with purple-colored elytra wings flying above the landscape. The character maintains a steady altitude while heading towards a mountain. [0:14:13 - 0:14:16]: The player continues to fly forward, descending slightly as they approach the ground. The terrain below is diverse, featuring grassy areas and some elevated hills. [0:14:16 - 0:14:18]: The player increases altitude again, flying through a partly cloudy sky. The blocky clouds are white against a blue sky. The snowy mountain is visible to the left, and the player is heading away from it. [0:14:18 - 0:14:20]: The player flies at a high altitude over a large expanse of green terrain. The viewpoint shifts slightly, revealing a mix of biomes with snowy peaks in the distance and green hills and valleys below.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What happens as the player approaches the tall, snowy mountain?",
        "time_stamp": "00:14:16",
        "answer": "D",
        "options": [
          "A. The player increases altitude significantly.",
          "B. The player changes to a third-person perspective.",
          "C. The player starts building a structure.",
          "D. The player start to fly upwards."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_198_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the vest the man is wearing right now?",
        "time_stamp": "00:00:03",
        "answer": "A",
        "options": [
          "A. Green.",
          "B. Orange.",
          "C. Red.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_454_real.mp4"
  },
  {
    "time": "[0:02:10 - 0:02:15]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is mounted on the dashboard right now?",
        "time_stamp": "00:02:10",
        "answer": "A",
        "options": [
          "A. A phone with GPS application.",
          "B. A camera.",
          "C. A radio.",
          "D. A small fan."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_454_real.mp4"
  },
  {
    "time": "[0:04:20 - 0:04:25]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the dashboard right now?",
        "time_stamp": "00:04:23",
        "answer": "B",
        "options": [
          "A. Blue.",
          "B. Black.",
          "C. Gray.",
          "D. Brown."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_454_real.mp4"
  },
  {
    "time": "[0:06:30 - 0:06:35]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What type of vehicle is the vehicle ahead right now?",
        "time_stamp": "00:06:33",
        "answer": "C",
        "options": [
          "A. Taxi.",
          "B. Delivery van.",
          "C. School bus.",
          "D. Ambulance."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_454_real.mp4"
  },
  {
    "time": "[0:08:40 - 0:08:45]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is attached to the dashboard right now?",
        "time_stamp": "00:08:41",
        "answer": "C",
        "options": [
          "A. A notebook.",
          "B. A GPS device.",
          "C. A mobile phone.",
          "D. A calculator."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_454_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:08]: A person in a striped shirt and white shorts is running along a sidewalk in a city area, which features tall buildings, palm trees, and residential structures. The sidewalk is adjacent to a street with sparse traffic. To the left, there are green grassy areas, and on the right side, there are plants and shrubs in landscaped sections. The sky is clear and sunny. On the top left corner of the frames, there is a small screen showing a streaming overlay with chat messages. [0:00:08 - 0:00:13]: The runner continues along the sidewalk, moving past a sign on their right side and several trash cans. The background maintains the urban setting with visible buildings and parked cars along the street. The area is well-maintained with neatly trimmed grass and sidewalk. [0:00:13 - 0:00:18]: The person approaches a section of the sidewalk where it curves slightly, passing by more palm trees, a stone wall with plants, and a residential house on the right. A few cars are visible on the road, and a pedestrian appears in the distance walking towards the runner. The backdrop consistently features tall buildings and a clear sky. [0:00:18 - 0:00:19]: The runner nears the pedestrian, who appears to be walking in the opposite direction. The surroundings remain similar, with additional elements like more palm trees, plants, and buildings emphasizing the urban landscape. [0:00:19 - 0:00:20]: The perspective briefly changes, and a green filter overlays the screen, revealing what appears to be a map interface briefly before switching back to the running scene. The runner is now closer to the pedestrian, who is more clearly visible on the sidewalk.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the person wearing a striped shirt do just now?",
        "time_stamp": "00:00:20",
        "answer": "D",
        "options": [
          "A. Grabbed a trash can.",
          "B. Stopped to tie their shoelaces.",
          "C. Engaged in a conversation with the pedestrian.",
          "D. Ran closer to the pedestrian."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_272_real.mp4"
  },
  {
    "time": "0:05:20 - 0:05:40",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:27]: The video appears to be a first-person perspective of someone using an inventory system in a video game. The screen shows an interface divided into several sections labeled \"Player,\" \"Ground,\" and \"Backpack,\" each containing grids for item storage. There's also a section at the bottom left labeled \"Personal Information.\" On the left side of the screen, a webcam feed displays an individual with headphones and a microphone, looking at the screen. They show various expressions during this time. [0:05:27 - 0:05:28]: The scene transitions from the inventory interface to an outdoor urban environment in a video game. It's a sunny day, with palm trees, buildings, and cars visible in the background. Two characters are standing near a palm tree. The character facing away from the viewer is bald and wearing a black shirt and blue jeans.  [0:05:28 - 0:05:38]: The camera shifts to provide a clearer view of the surroundings. The two characters continue their conversation while standing on the grass near a sidewalk. A fire hydrant is visible behind them, and other characters and vehicles occasionally pass by in the background. The taller character wears a striped shirt and shorts, facing another character holding a phone. Buildings and a traffic intersection become more evident in the background. [0:05:38 - 0:05:40]: The camera focuses more on the interaction between the two characters. The character with the phone appears to be showing something to the other character, gesturing with his hand. The background shows more details of the urban environment with street lamps, palms, and a modern building with red foliage trees. The character in the striped shirt listens attentively.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the relationship between the fire hydrant and the two characters standing near the palm tree?",
        "time_stamp": "00:05:42",
        "answer": "C",
        "options": [
          "A. The fire hydrant is in front of them.",
          "B. The fire hydrant is to the right of them.",
          "C. The fire hydrant is behind them.",
          "D. The fire hydrant is to the left of them."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_272_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:07]: A settings menu for a video game is displayed on the screen. It features various key bindings for vehicle controls, listed in a table format. To the left of the screen, a sub-menu highlights the \"Key Bindings\" option. In the top-left corner, a picture-in-picture (PiP) displays a person wearing a headset, likely the player, with a focused expression and hands possibly on a keyboard or controller. The right side features chat messages and viewer interactions, indicating a livestream or recorded gameplay session. [0:08:08 - 0:08:12]: The settings screen changes, presenting a broader view of categories such as \"Game,\" \"Info,\" \"Stats,\" and \"Settings.\" \"Key Bindings\" is still highlighted. The PiP shows the same individual as before, maintaining a consistent facial expression and posture. Chat messages continue to scroll on the right side. [0:08:13 - 0:08:14]: The game settings menu disappears, revealing gameplay with a small car at an intersection. The game’s perspective is in third-person view. The PiP of the individual remains, showing them focused on the gameplay. [0:08:15 - 0:08:20]: The car is stationary at a red light with a clear view of a multi-lane road and buildings in the background. The traffic light turns green, and the car starts to move. The PiP remains consistent, and chat interactions continue on the side. The player’s focus is on the road ahead, navigating through traffic lights and urban scenery with buildings and trees along the street.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the chat message section located on the screen right now?",
        "time_stamp": "00:08:20",
        "answer": "B",
        "options": [
          "A. In the top-left corner.",
          "B. On the right side.",
          "C. At the bottom.",
          "D. On the left side."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Action Recognition",
        "question": "What is the player doing right now?",
        "time_stamp": "00:08:15",
        "answer": "C",
        "options": [
          "A. Adjusting game settings.",
          "B. Watching a cutscene.",
          "C. Driving a car in the game.",
          "D. Selecting key bindings."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_272_real.mp4"
  },
  {
    "time": "0:09:40 - 0:09:50",
    "captions": "[0:09:40 - 0:09:50] [0:09:40 - 0:09:50]: The video shows a first-person perspective of a computer screen displaying a game settings menu. In the background, it appears to be a vehicle dashboard visible through a semi-transparent overlay. The menu lists various key bindings under the \"ACTION\" column, including indicators for turning on and off lights and toggling high beams. Each action is associated with a specific key listed under the \"PRIMARY\" column. There is a small webcam overlay in the top left corner showing a person, possibly the player, with some animated icons and text overlays on the right side of the screen. The person appears to be concentrating, possibly reviewing the game settings. Additional graphical elements include a small image of a dog's face near the bottom left of the menu. The background outside the game settings appears to be in shades of green and dark, indicating a night scene inside the game environment. The person in the webcam occasionally looks around, possibly interacting with the game or stream on another monitor.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the small image of a cat's face located right now?",
        "time_stamp": "00:09:50",
        "answer": "A",
        "options": [
          "A. Top right of the menu.",
          "B. Bottom left of the menu.",
          "C. Center of the menu.",
          "D. Top left of the menu."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_272_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the actions taken just now?",
        "time_stamp": "0:00:10",
        "answer": "D",
        "options": [
          "A. The individual cleaned the kitchen floor using a mop and then organized the countertop.",
          "B. The individual prepared ingredients, chopped vegetables, and placed them into a frying pan.",
          "C. The individual unpacked a delivery box, organized the contents, and disposed of the packaging.",
          "D. The individual washed their hands, applied soap, and rinsed thoroughly under running water."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_375_real.mp4"
  },
  {
    "time": "[0:00:49 - 0:00:59]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the actions taken just now?",
        "time_stamp": "0:00:59",
        "answer": "D",
        "options": [
          "A. The individual washed potatoes, chopped them, and started cooking.",
          "B. The individual peeled potatoes, rinsed them, and placed them in a container.",
          "C. The individual sorted potatoes, discarded the bad ones, and organized the rest in a basket.",
          "D. The individual emptied a sack of potatoes into a sink, discarded the sack, and prepared the workstation."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_375_real.mp4"
  },
  {
    "time": "[0:01:38 - 0:01:48]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the actions taken just now?",
        "time_stamp": "0:01:48",
        "answer": "D",
        "options": [
          "A. The individual emptied a bag of potatoes into the sink, washed them under running water, and dried them with a towel.",
          "B. The individual peeled the potatoes, rinsed them under running water, and placed them in a storage container.",
          "C. The individual sorted the potatoes, removed any that were spoiled, and organized the rest into neat stacks.",
          "D. The individual placed potatoes into the sink, washed them under running water, and sliced them using a potato slicer."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_375_real.mp4"
  },
  {
    "time": "[0:02:27 - 0:02:37]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the actions taken just now?",
        "time_stamp": "0:02:37",
        "answer": "D",
        "options": [
          "A. The individual boiled potatoes, mashed them, and added seasoning.",
          "B. The individual peeled potatoes, chopped them, and placed them into a baking dish.",
          "C. The individual washed potato slices, rinsed them, and placed them into a pot of water.",
          "D. The individual sliced potatoes, washed the slices in water, and prepared them for cooking."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_375_real.mp4"
  },
  {
    "time": "[0:03:16 - 0:03:26]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the actions taken just now?",
        "time_stamp": "0:03:26",
        "answer": "D",
        "options": [
          "A. The individual sliced potatoes thinly, washed them gently under running water, and arranged them on a tray for drying.",
          "B. The individual poured sliced potatoes into a water bath, agitated them using a strainer, and transferred them to a drying area.",
          "C. The individual submerged the sliced potatoes in warm water, rinsed them with a colander, and then patted them dry with a cloth.",
          "D. The individual placed potato slices into a container of water, mixed them vigorously for cleaning, and separated them using a divided compartment."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_375_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video begins with a first-person perspective view driving a light blue car on a wide, multi-lane urban street. The sky is slightly pink, indicating either dawn or dusk. There is a large billboard on the right side with the text \"Meteorite.\" Several palm trees line the streets, and buildings, including stores and restaurants, are visible in the background. The scene is generally well-lit with street lamps and vehicle headlights. [0:00:04 - 0:00:06]: The vehicle continues driving forward, approaching an intersection. The billboard for \"Meteorite\" becomes more prominent, while additional buildings and palm trees can be seen lining the sides of the road. Traffic lights are visible ahead. [0:00:07 - 0:00:10]: As the car makes a slight left turn at the intersection, a pedestrian crossing and a traffic signal are observed. There is also an inset webcam overlay on the bottom left corner showing a driver.  [0:00:11 - 0:00:17]: The car proceeds straight along a road flanked by more palm trees and streetlights. A series of smaller store signs and vehicles are visible in the background. The inset video of the driver remains in the bottom left corner of the screen. [0:00:18 - 0:00:20]: The vehicle continues to drive down the street, nearing another building complex with several signs marking different shops. The car is about to turn right into a parking lot. The buildings have various business signs with advertisements. [0:00:21 - 0:00:25]: The blue car enters a parking lot of a shopping center, with multiple storefronts lining the back. There are visible parking spaces and a sidewalk alongside the stores. [0:00:26 - 0:00:28]: The car moves further into the parking lot, approaching the storefronts which are more visible now. Various signs indicating different store names, such as \"click lovers\" and \"Food & Nature,\" are readable. [0:00:29 - 0:00:32]: The car pulls up and parks near the entrance of an electronics store named \"click lovers.\" The driver begins to disembark from the vehicle as the view focuses on the store entrance. [0:00:33 - 0:00:38]: A young man wearing a striped shirt exits the car and walks toward the entrance of the store. The camera follows his movement from behind. [0:00:39 - 0:00:43]: Approaching the entrance of the electronics store, the individual seems to be preparing to enter. Inside the store, some electronic items and shelves are visible. [0:00:44 - 0:00:46]: The person reaches the door, opens it, and steps inside the store. Once inside, the arrangement of products on shelves, a counter, and other customers becomes apparent. [0:00:47 - 0:00:51]: Inside the store, the camera captures a closer look at various electronic items displayed on shelves. The individual walks towards the checkout counter where a few other people are present. [0:00:52 - 0:00:55]: The individual walks up to the counter where another customer is being assisted by a store clerk. The shelves are filled with a variety of electronics, and the store is well-lit with a modern design. [0:00:56 - 0:00:59]: The person stands at the counter where another customer, dressed in white, and the store clerk are engaged in a transaction. More details of the store, including shelves stocked with electronics, are visible in the background. [0:01:00 - 0:01:03]: Another store clerk attends to the customer, and the person engages in browsing items on the counter. The interior of the store showcases neatly arranged products on the shelves and well-organized sections. [0:01:04 - 0:01:08]: The section culminates with the individual, along with other customers, interacting at the checkout counter with the store clerk. The professional layout and inventory are visible throughout the store.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the car do just now?",
        "time_stamp": "00:00:13",
        "answer": "C",
        "options": [
          "A. It stopped at a traffic light.",
          "B. It turned left at an intersection.",
          "C. It parked near a shopping center.",
          "D. It reversed into a parking space."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_277_real.mp4"
  },
  {
    "time": "0:02:40 - 0:03:00",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:45]: The video features a first-person view from inside a car driving on a wide, multi-lane bridge during early evening or dusk. Buildings and streetlights can be seen along the sides of the bridge, and the sky is partly cloudy. There is a second view embedded in the top left corner of the video, showing a person in front of a computer monitor with a gaming setup, potentially live streaming the drive. [0:02:46 - 0:02:51]: The car continues to drive along the road, approaching a traffic light. Other cars are visible ahead, a few red, positioned in the lanes, driving in the same direction. [0:02:52 - 0:02:55]: The car moves further down the road, nearing an intersection and passing additional vehicles. Surrounding buildings begin to look more commercial, and the car keeps to the left lane approaching cars ahead. [0:02:56]: The car reaches the intersection, now next to a red car. Buildings and stores can be seen at the corners of the intersection. [0:02:57 - 0:02:58]: The light remains red at the intersection, and the car prepares to make a turn to the left, following the road. Streetlights already illuminate the area. [0:02:59 - 0:03:00]: The car completes the left turn at the intersection onto a new road. Visible in the distance is a bridge structure ahead, and the surroundings include more industrial buildings and utility poles.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the car doing right now?",
        "time_stamp": "00:02:44",
        "answer": "C",
        "options": [
          "A. Preparing to make a right turn.",
          "B. Driving on a multi-lane bridge.",
          "C. Completing a right turn at the intersection.",
          "D. Stopping at a red light."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_277_real.mp4"
  },
  {
    "time": "0:05:20 - 0:05:40",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:29]: The video starts with a first-person perspective view of a computer screen displaying a game interface titled \"Trucking Progression\" in an area labeled BONVILLE. Various mission options are listed with details such as destinations, rewards, and expiration times. Each mission display features a similar image of a truck and a snowy landscape in the background. The interface is bordered by a blue frame and there is a chat window on the right side filled with messages and emotes. [0:05:30 - 0:05:34]: The scene transitions to an outdoor environment. The camera focuses on a small group of people. One individual, wearing a striped shirt, stands facing two others. The second person, in a blue hoodie, is holding and looking at a tablet or phone. The setting appears to be a poorly lit industrial area with some buildings and parked vehicles in the background. [0:05:35 - 0:05:37]: The view moves to display a map interface on the previously seen tablet or phone. The map shows a detailed layout of streets and buildings with the current location marked. The background of the map interface is turquoise, highlighting the area it represents. [0:05:38 - 0:05:40]: The perspective shifts back to the outdoor scene. The striped-shirt individual continues to stand with the others. They are gathered around a person wearing a uniform/suit who seems to be writing or reading something. The lighting of the scene remains dim, typical of an early morning or evening setting. The ground is littered with a few scattered objects. The overall atmosphere suggests the individuals might be discussing or preparing for an activity related to the game on the screen shown at the start.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the individual in the blue hoodie doing right now?",
        "time_stamp": "00:05:32",
        "answer": "C",
        "options": [
          "A. Checking their reflection in a window.",
          "B. Writing on a piece of paper.",
          "C. Holding and looking at a tablet.",
          "D. Adjusting their clothing."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_277_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:05]: The video begins with a first-person perspective from what seems to be a video game interface. The screen shows a player driving a large vehicle down a dark street at night. The vehicle has a distinctive red light around its rear. To the left of the screen, a smaller window displays a live feed of a person, possibly the player, while to the right, various chat messages from other players or viewers appear in a vertical list. [0:08:06 - 0:08:09]: The vehicle continues to move forward along the street, approaching a few parked cars on the right side of the road. The surrounding area appears urban with sidewalks and illuminated buildings. The player window remains in view with the person focused on the screen, while the chat continues its activity. [0:08:10 - 0:08:14]: The vehicle decreases speed and seems to be getting ready to park along the sidewalk. The scene becomes clearer, showing more details of the buildings and street lamps. The sky is dark, indicating nighttime. The smaller live feed window and the chat are still visible. [0:08:15 - 0:08:18]: The vehicle comes to a stop next to a building entrance. The name \"grime\" is visible on the side of the vehicle, which is now identifiable as a van. The sidewalk and building entrance are well-lit, emphasizing the nighttime setting. [0:08:19 - 0:08:20]: The van remains stationary as the scene focuses on its placement in front of the building. The person in the small window appears attentive, perhaps planning their next action. The chat continues to scroll with messages. [0:08:21 - 0:08:23]: The player, seemingly the same person from the live feed, exits the driver’s seat and steps out of the van. The camera angle shifts slightly to follow this movement. It's evident that the person in the live feed is controlling the on-screen actions. [0:08:24 - 0:08:26]: The perspective shows the person standing at the side of the van, looking around, indicating they might be assessing the surroundings. The urban environment remains consistent, with street lamps and buildings providing illumination. [0:08:27 - 0:08:30]: The player walks towards the rear of the van. Another character, controlled by a different player possibly, appears on screen and approaches the van from the sidewalk. The live feed and chat continue to display on the screen. [0:08:31 - 0:08:33]: Both characters stand at the rear of the van, where the user interface displays an option to open the back. The characters seem to be interacting with the van's cargo, preparing to unload or retrieve something. [0:08:34 - 0:08:36]: A detailed inventory screen appears, showing various items and storage options labeled Player, Cargo, Backpack, and Ground. The live feed remains visible, showing the person focused on managing the inventory. The chat continues with its active commentary. [0:08:37 - 0:08:45]: The inventory management continues with the player’s interface indicating interactions with different items. The person in the live feed appears concentrated, and various comments and suggestions from the chat are visible. The cargo section highlights the interaction with items possibly stored in the van. [0:08:46 - 0:08:50]: The inventory screen retains focus as the player manages the items, indicating an involved and meticulous process. The live feed and chat maintain their activity, providing a sense of interaction and engagement from the community watching or participating in the game. [0:08:51 - 0:08:55]: The player seems to finalize their inventory management and prepares to exit the inventory screen. The chat activity remains significant, and the person in the live feed appears intent on their task. [0:08:56 - 0:08:59]: The inventory screen starts to close, reverting focus back to the characters standing at the rear of the van. The two characters look like they are ready to proceed, possibly with their next in-game action. [0:09:00]: The characters stand next to the now-closed van. The live feed shows the person expressing a reaction, likely in response to something in-game or from the chat. The chat continues its lively scroll on the right side.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the player do just now?",
        "time_stamp": "00:08:20",
        "answer": "C",
        "options": [
          "A. Exiting the driver's seat.",
          "B. Driving the vehicle.",
          "C. Finalizing inventory management.",
          "D. Interacting with another character."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_277_real.mp4"
  },
  {
    "time": "0:10:00 - 0:10:08",
    "captions": "[0:10:00 - 0:10:08] [0:10:00 - 0:10:02]: Two people are walking on a sidewalk next to a parked dark vehicle at night in a city. One person is wearing a striped shirt and shorts while the other is wearing a blue hoodie and dark pants, leading the way. The city lights illuminate the background, revealing tall buildings. [0:10:02 - 0:10:04]: The person in the striped shirt is turning towards the dark vehicle, while the person in the blue hoodie remains slightly ahead. There's a chat window overlay on the right side of the frame, showing various messages from viewers. [0:10:04 - 0:10:08]: Both individuals stop at the rear of the dark vehicle. The person in the striped shirt looks toward the person in the blue hoodie who is opening the rear door of the vehicle. The interior of the vehicle is not clearly visible, but a light appears to be on inside. The chat window overlay on the right side continues showing messages.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person in the blue hoodie doing right now?",
        "time_stamp": "00:10:07",
        "answer": "D",
        "options": [
          "A. Walking on the sidewalk.",
          "B. Stopping at the rear of the vehicle.",
          "C. Turning towards the dark vehicle.",
          "D. Puting the goods into the trunk of the car."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_277_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:06]: A close-up view of a hand holding four round keychains, each with a leopard print design. The keychains are glittery, with two being golden, and one pink. The text \"Café,\" \"Warm,\" and \"Joy\" is written on each keychain respectively. The keychains have small tassels attached to them, hanging from silver/metallic rings. The background is plain white, and there is a logo for \"DIY Craft Tutorials\" in the bottom right corner. [0:00:07 - 0:00:10]: The screen transitions to a dark background displaying the text \"Leopard Glitter Keychains\" in the center. [0:00:11]: The screen turns completely black. [0:00:12 - 0:00:16]: Instructions appear with the heading \"STEP I,\" followed by the step description: \"Upload Cut File to your cutting machine. I'm using a Cricut Joy.\" [0:00:17 - 0:00:18]: The same instruction screen slightly zoomed out, revealing the Cricut software interface with a leopard print keychain design on a grid layout.  [0:00:19 - 0:00:20]: The Cricut Joy software interface now features a leopard print keychain design along with various design options, including a grid pattern and additional design elements like circles and other shapes, suggesting the available tools and choices for customization.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is being shown on the screen right now?",
        "time_stamp": "00:00:05",
        "answer": "D",
        "options": [
          "A. A dark background displaying \"Leopard Glitter Keychains.\".",
          "B. Instructions for using a cutting machine.",
          "C. The Cricut Joy software interface with design options.",
          "D. A hand holding glittery leopard print keychains."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_65_real.mp4"
  },
  {
    "time": "0:01:40 - 0:02:00",
    "captions": "[0:01:40 - 0:02:00] [0:01:40 - 0:01:41]: From a first-person perspective, a pair of hands, wearing a maroon long-sleeve top, are seen loading a sheet of white material into a Cricut Joy machine. The machine has a turquoise top and white body, positioned on a dark gray, grid-patterned craft mat. [0:01:42 - 0:01:47]: The material is fed into the machine, and the hands are removed from the frame. The machine starts pulling the sheet in. As the sheet progresses, it remains centered within the machine's guides. [0:01:48 - 0:01:49]: The scene darkens slightly as the machine continues to pull in the material. The sheet turns black as it is fed further, indicating a possible transition or new type of material being used. [0:01:50 - 0:01:58]: The machine continues its operation undisturbed. There's a label on the craft mat that reads \"DIY Craft Tutorials\". The black material advances further, occupying almost the entire visible area fed through the machine. [0:01:59]: The scene transitions to a pair of hands holding scissors and cutting the now black sheet material horizontally. The area is well-lit with the grid pattern craft mat forming the background.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action is being performed right now?",
        "time_stamp": "0:02:00",
        "answer": "D",
        "options": [
          "A. Loading a sheet into the machine.",
          "B. Pulling out a sheet from the machine.",
          "C. Labeling the material.",
          "D. Cutting the material vertically."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_65_real.mp4"
  },
  {
    "time": "0:03:20 - 0:03:40",
    "captions": "[0:03:20 - 0:03:40] [0:00:32 - 0:00:37]: The video begins with a close-up shot of a hand holding a round, thin, silver-colored stencil with small, irregularly shaped holes. The hand has a light skin tone and is positioned centrally in the frame. The hand is wearing a dark pink, ribbed long-sleeve sweater. The background consists of a grid-patterned mat, mostly grey with white lines creating small squares. [0:00:38 - 0:00:39]: The hand begins to place the round stencil down on the grid-patterned mat next to another identical stencil, which is already resting on the mat slightly to the left. To the top right of the hand, two square white pieces of paper with the stencil design printed on them are visible, placed adjacent to each other. [0:00:40 - 0:00:41]: As the hand continues to place the stencil down on the mat, it aligns the stencil to the left of the two white papers. The right hand moves towards another object situated on the mat, located further right beyond the frame. [0:00:42]: The right hand picks up another round stencil from the mat and brings it towards the center. [0:00:43 - 0:00:45]: The right hand aims to align this new stencil next to the first one, ensuring they are symmetrically placed. The left hand, still holding the stencil, makes slight adjustments. [0:00:46 - 0:00:47]: The right hand picks up a small white rectangular object, which appears to be a label with text printed on it. [0:00:48 - 0:00:49]: The right hand continues to hold the label while moving it closer to the two stencils on the mat. The left hand remains stationary, holding the stencil down. [0:00:50 - 0:00:51]: The label with text is now directly above the stencil held by the left hand. The right hand begins to apply the label onto the stencil, pressing down gently to ensure it sticks. [0:00:52 - 0:00:54]: Both hands work together to firmly apply the label onto the stencil. The right hand rubs the label to ensure it is securely adhered. [0:00:55 - 0:00:57]: The left hand steadies the stencil while the right hand continues to press down on the label. The stencils and white papers remain in their respective positions on the mat. [0:00:58 - 0:00:59]: The right hand continues to secure the label onto the stencil by applying consistent pressure. The left hand stabilizes both the stencil and the label. [0:01:00 - 0:01:02]: The label is now securely adhered to the stencil. The right hand begins to move away, leaving the label in place. [0:01:03]: The right hand moves out of the frame. The left hand, still holding the stencil, begins to adjust its position. [0:01:04 - 0:01:07]: The left hand rotates the stencil slightly to examine the label placement. The right hand returns to the frame momentarily, assisting with the adjustment. [0:01:08 - 0:01:09]: The left hand places the completed stencil back onto the mat beside the first stencil while the right hand picks up a small tool from the side. [0:01:10 - 0:01:11]: The right hand uses the small tool to lift the edge of the applied label, ensuring the adhesive bond is secure. [0:01:12 - 0:01:13]: The right hand carefully peels back a portion of the label from the stencil to check adhesion quality. [0:01:14 - 0:01:15]: After confirming the adhesion, both hands work together to smooth the label back onto the stencil, securing any lifted edges. [0:01:16 - 0:01:17]: The hands shift slightly, giving one final press to the label on the stencil to ensure it is fully attached and smooth. [0:01:18 - 0:01:19]: The right hand then moves away, and the left hand places the stencil onto the grid mat beside the other finished stencil. [0:01:20 - 0:01:21]: The frame changes to show the two stencils side by side on the mat, with the label neatly attached to each, and the white papers still at the top of the frame.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are the hands doing right now?",
        "time_stamp": "0:03:13",
        "answer": "D",
        "options": [
          "A. Drawing on a piece of paper.",
          "B. Washing the stencil.",
          "C. Lifting the stencil off the mat.",
          "D. Applying a label to a stencil."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_65_real.mp4"
  },
  {
    "time": "0:05:00 - 0:05:20",
    "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:03]: A pair of gloved hands is seen holding a white object with a dotted pattern and black edges. The object is placed on a white textured cloth laid over a blue surface. There are two small cups, one filled with golden glitter and the other possibly with red glitter or similar colored substance. The left hand holds the white object, while the right hand uses a thin stick to apply a layer of golden glitter.  [0:05:04 - 0:05:17]: The right hand continues to spread the golden glitter evenly over the white object using the thin stick. The glitter is slowly covering the entirety of the surface within the black edges, ensuring it remains inside the boundary without spilling over. The process is meticulous, and the object begins to have a shiny, fully covered golden top.  [0:05:18 - 0:05:19]: The text overlay at the bottom reads, \"Resin domes - take it to the edge and it shouldn't go over,\" indicating guidance on the application technique being used.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the right hand doing right now?",
        "time_stamp": "00:05:17",
        "answer": "D",
        "options": [
          "A. Applying a layer of red glitter.",
          "B. Holding a pair of cups.",
          "C. Removing glitter from the object.",
          "D. Spreading golden glitter evenly over the white round object."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_65_real.mp4"
  },
  {
    "time": "0:06:40 - 0:07:00",
    "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:41]: Two hands, wearing a purple long-sleeved shirt, are centrally involved in handling a white piece of paper with a transparent adhesive on top. The background is a gray, grid-patterned mat with three round coasters placed horizontally at the upper edge of the frame. The coasters exhibit intricate patterns in green, brown, and multicolored designs respectively, from left to right. [0:06:42 - 0:06:43]: The left hand is pressing on the top part of the adhesive with the pointer finger while the right hand starts pulling the backing off of the adhesive from the bottom. [0:06:44 - 0:06:45]: The right hand continues to pull the adhesive backing upwards while the left pointer finger remains pressing the adhesive down, ensuring it sticks correctly. [0:06:46]: Both hands maintain pressure on the paper and the adhesive as more of the adhesive backing is peeled upwards. [0:06:47]: As the right hand progresses in peeling off the adhesive backing, the paper remains stationary, guided steadily by the left hand’s pressure. [0:06:48 - 0:06:49]: The right hand now starts pulling the adhesive backing horizontally, nearing the end of the peeling process. [0:06:50 - 0:06:51]: The left hand lifts up the piece of paper, showing the freshly applied adhesive surface while the right hand completely removes the adhesive backing. [0:06:52]: The left hand inspects the adhesive application on the paper, holding onto the upper edge, while the right hand holds the removed backing. [0:06:53]: The right hand turns the adhesive backing sideways ensuring it's completely removed, while the left hand holds the now adhesive-applied paper piece and examines it. [0:06:54 - 0:06:57]: The hands work together to remove any excess backing still left, revealing a clean adhesive surface beneath. This clear, freshly adhesive-applied paper is then maneuvered, ready to be utilized for the crafting project. [0:06:58]: The left hand flips the adhesive-applied paper, examining the sticky side, and then both hands prepare to place it down. [0:06:59]: Holding the edges delicately, the left hand places the adhesive-applied paper against one edge of the transparent backing on the left coaster. [0:07:00]: The hands position the paper precisely over the adhesive on the left coaster, ensuring it's aligned well. [0:07:01]: Both hands begin pressing the adhesive-applied paper down on the coaster, ensuring it adheres smoothly. [0:07:02 - 0:07:03]: The left hand continues pressing down while the right hand adjusts the paper's position to ensure adhesion. [0:07:04]: The right hand then lifts the paper to inspect the adhesion on the coaster, revealing a clean application. [0:07:05]: Holding up the coaster now, the hands showcase a successful adhesive application on the round coaster. [0:07:06]: The hands maintain their hold on the coaster, showing the applied adhesive surface is smooth and well-fitted. [0:07:07]: The hands place the next coaster centrally, between the first and last coasters, preparing to apply the next adhesive-applied paper. [0:07:08 - 0:07:09]: The hands swiftly move to press the adhesive-applied paper on the central coaster, pressing down to ensure adhesion is firm and smooth. [0:07:10]: The right hand presses down firmly on the central coaster while the left hand holds the edges, securing the adhesive. [0:07:11]: Both hands then lift the coaster and examine, once more revealing a smooth, well-fitted adhesive application on the central coaster as the video comes to an end.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the hands doing right now?",
        "time_stamp": "0:06:56",
        "answer": "D",
        "options": [
          "A. Pulling the adhesive backing upwards.",
          "B. Adjusting the paper's position on the coaster.",
          "C. Inspecting the adhesive surface on the paper.",
          "D. Placing the adhesive-applied paper against the golden round object."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_65_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video depicts a wide stone pathway along a waterfront area. Lamp posts line the pathway, which is flanked by a metal railing on the left side. A woman in short denim shorts and a dark top walks ahead, while another person in a green dress walks slightly in front closer to the water. Several outdoor café tables with white umbrellas are arranged next to a building on the right, which features old, multi-story structures with colorful facades.  [0:00:02 - 0:00:03]: The same two individuals continue walking forward. The person in the green dress carries a small black bag, and the woman in denim shorts walks beside another individual wearing a dark shirt and jeans. The waterfront area reveals more details with additional boats and a small floating dock on the left. [0:00:04 - 0:00:07]: The camera moves forward, showing more of the busy promenade. A few small vehicles, possibly taxis or rideshare services, are parked along the road. Passersby can be seen near the buildings and cafés. The sky is partly cloudy with vibrant blue patches. [0:00:08 - 0:00:09]: The video continues to detail the same area, showing a bustling riverside walkway. A woman in a white dress and a man with tattoos are seen walking towards the camera. More people appear to be sitting and walking around the café tables under large umbrellas. [0:00:10 - 0:00:12]: As the camera advances, the scene continues with views of residential buildings with balconies, lively with people walking and bunches of parked cars on the pathway. The video captures the continuous stream of people engaging in various activities along the waterfront. [0:00:13 - 0:00:14]: One of the cars now appears more prominently, a blue vehicle parked by the path. More people are seen closer to the camera, revealing the mixed bustle of the walkway with multiple vendors and café attendees. [0:00:15 - 0:00:17]: Ongoing activity around the café areas continues to be shown, with umbrellas casting shade on the tables. The camera approaches more closely to the blue vehicle. The people from earlier situate themselves nearer to various shop entrances on both sides of the pathway. [0:00:18 - 0:00:20]: The silver vehicle seen earlier has parked along the street now more visibly in front of the café seating area. Additional people can be seen entering and exiting shops, while the central characters from earlier remain walking down the path. The camera continuously moves forward, capturing more of the street’s length and activities.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the weather like in the video?",
        "time_stamp": "0:00:07",
        "answer": "D",
        "options": [
          "A. Completely cloudy.",
          "B. Rainy.",
          "C. Overcast.",
          "D. Partly cloudy."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_329_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:49]: The video takes place in a sunny outdoor plaza lined with tall, colorful buildings on the right side. These buildings have red and yellow facades with balconies on the upper floors. On the right, there are various cafes and restaurants with white umbrellas and outdoor seating where people are dining and enjoying the day. The ground is made of stone tiles, and the place is bustling with activity. Down the center of the plaza, various people are walking towards the camera, including individuals in shorts and t-shirts. On the left, there's a street where cars are parked, and some pedestrians are near vehicles. Additionally, there are street lamps and poles along the plaza's edge. [0:02:50 - 0:02:59]: A red phone booth is visible near the right side of the buildings. Multiple groups of people are walking toward and away from the camera. A person in a red shirt and white shorts is holding hands with someone in white. The left side of the frame shows the riverside with boats docked in the water and some people near a metal fence, capturing the lively and picturesque environment of the waterfront. The sky is mostly clear, showcasing a beautiful day.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is visible near the right side of the buildings right now?",
        "time_stamp": "0:02:57",
        "answer": "D",
        "options": [
          "A. A blue mailbox.",
          "B. A green bench.",
          "C. A yellow taxi.",
          "D. A red phone booth."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_329_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:21]: The video opens with a view of a cobblestone street lined with buildings on both sides. On the left, a tall building with an orange exterior is visible, while the right side features a beige building with several windows. In the foreground, a young man and woman walk past the camera, and in the background, a person wearing a blue outfit and a hat is walking up the street, carrying a shopping bag in one hand. The sky is bright blue with a few wispy clouds. [0:05:21 - 0:05:24]: As the camera moves forward, the young man and woman exit the frame, and more of the background becomes visible. The street ahead has some bollards and a traffic sign, and a red building with white accents can be seen in the distance. A few more pedestrians are moving along the street. [0:05:24 - 0:05:27]: The camera continues to move up the street. The man in the blue outfit and hat continues walking up the street, moving slightly to the right. Additional pedestrians, including a person in a red outfit, are present in the background near the red building.  [0:05:27 - 0:05:32]: The camera reaches the top of the street, approaching the intersection. The red building in the background becomes clearer, showcasing its detailed architecture. Pedestrians are seen crossing the street at the intersection, engaging in various activities. A prominent statue surrounded by a small green area also comes into view, situated in front of the red building.  [0:05:32 - 0:05:35]: More pedestrians walk in front of the statue and green area. The building to the left features classical architectural elements, including columns and a clock tower. On the right side of the frame, the blue-clad individual is seen walking along the sidewalk, now further ahead.  [0:05:35 - 0:05:36]: The camera slightly pans to capture the broader intersection view, displaying more pedestrian activity and traffic movements.  [0:05:36 - 0:05:38]: As the camera moves forward, the main focus remains on the intersection, the statue, and the red building. More detailed elements of the street and architecture are visible, including another structure with a crane on the right, indicating some construction activity.  [0:05:38 - 0:05:40]: The video closes with a clear view of the intersection, the statue, and the surrounding area. The red building remains prominent, with green trees on either side and further pedestrians visible in various parts of the frame. The cobblestone street and the historical architecture create a picturesque scene under the bright blue sky.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person in the blue outfit carrying?",
        "time_stamp": "00:05:21",
        "answer": "D",
        "options": [
          "A. A briefcase.",
          "B. A backpack.",
          "C. A suitcase.",
          "D. A shopping bag."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_329_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:02]: The scene begins with a street view showing a cobblestone road and a stone building on the left. There are three people standing near a traffic light. One person wears a white shirt and shorts, another in a black shirt and shorts, and the third person is walking away from the camera wearing a floral top and holding a bag. The background shows a green tree, street signs, and buildings in the distance. [0:08:03 - 0:08:05]: The person in the floral top keeps moving forward. The other two individuals remain near the traffic light, which is red. The camera pans slightly to the right, showing another tree and part of the green area across the street. [0:08:06 - 0:08:08]: The camera continues to track the person in the floral top, who reaches the edge of the frame as they walk along the street. A person in blue shorts and a white shirt walks towards the camera from the opposite direction on the left side of the frame. The stone building remains prominent on the left side of the frame. [0:08:09 - 0:08:11]: The camera focuses on the left side of the street, capturing the person in blue shorts walking past. The background shows street signs and parked cars along the right side, with a cobblestone sidewalk running parallel to the building. [0:08:12 - 0:08:14]: The person in blue shorts moves out of the frame. The sidewalk on the left side of the street becomes more evident as the camera captures more of the cobblestone. Parked cars remain on the opposite side of the street. [0:08:15 - 0:08:17]: The camera shifts higher up the street, showing more of the sidewalk and cobblestone road. Trees and buildings, including a red structure, appear on the right. A parked vehicle can be seen ahead on the left side of the street. [0:08:18 - 0:08:20]: As the video progresses, the camera continues to show the upper view of the street with the stone building on the left. The viewer can see two parked cars, and a clearer view of the road ahead, with more trees and buildings in the distance.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where was the historical stone building located in the frame right now?",
        "time_stamp": "00:08:09",
        "answer": "A",
        "options": [
          "A. On the left side.",
          "B. On the right side.",
          "C. In the center.",
          "D. In the background."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_329_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:49]",
    "captions": "[0:09:40 - 0:09:49] [0:09:40 - 0:09:48]: The video features a first-person perspective as the viewer walks along a cobblestone sidewalk. To the right, there is a striking bright red building with a large outdoor area enclosed by a matching red fence. The building has signs indicating \"HARD CLUB\" in red letters on a black background. On the left side of the frame, parallel to the sidewalk, a narrow street runs uphill, lined with parked cars and historic buildings with balconies. The buildings on the left are painted in light colors with green railings and have old-fashioned street lamps attached. In the distance, a taller bell tower and other structures are visible. The sidewalk includes bollards, waste bins, a metal post, and a small staircase. The sky appears mostly clear with patches of light clouds.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the name of the left side establishment indicated by the signs on the building?",
        "time_stamp": "00:09:49",
        "answer": "A",
        "options": [
          "A. HARD CLUB.",
          "B. HARD ROCK.",
          "C. CLUB HARD.",
          "D. ROCK CLUB."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_329_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins with a black background displaying the word \"And\" in white font. The text transitions smoothly to \"video\" against the same black background.  [0:00:02 - 0:00:05]: As the video continues, the next frames have a blurred background with blue and green shades, indicating an outdoor environment. The text on the screen reads, \"I've decided to build something that I've never seen built before.\" [0:00:05 - 0:00:06]: The background remains blurry, showing hints of greenery and a bright sky. The text changes to \"Well I say built but it's mostly...\" [0:00:07]: The scene shifts to a clear view of a character in a blocky, pixelated world, wearing blue armor and holding a purple and yellow item. The character stands on a wooden platform with wooden barrels on both sides. Behind the character, there is a hilly area with trees and a wooden structure. [0:00:08 - 0:00:09]: The view shifts slightly, with the character now standing closer to the edge of the wooden platform. The background includes a vast body of water with the sun near the horizon, indicating either sunset or sunrise. The hills and trees are still visible in the background. [0:00:10 - 0:00:11]: The perspective remains similar with minor adjustments in the character's position. The sun's position and the coloration of the sky suggest it is either early morning or late evening. [0:00:12]: The camera now focuses on the wooden structure's entrance, showing detailed wooden planks, a hanging lantern beside the door, and a barrel to the left. The character's hand is visible, holding what appears to be a yellow and white item. [0:00:13]: The camera pans to the right, capturing more of the scenic view. The sun is still near the horizon, casting a warm glow over the water and land. The lush greenery beside the water contrasts with the brown earth and blue water and sky. [0:00:14 - 0:00:15]: The scene transitions with \"Objectives:\" text appearing against the blurred background, followed by \"Get a bunch of TNT\" in red text.  [0:00:15 - 0:00:16]: The view returns to a clear scene of the water and sky. The sun's light remains prominent, and the surroundings maintain their vibrant colors with a mix of green, blue, and brown. [0:00:17 - 0:00:18]: The scene shifts indoors, showing a room with wooden walls and stone flooring. Various items like a brewing stand, a crafting table, and a brick structure are visible. [0:00:19]: The camera angle adjusts to reveal more of the interior. Additional elements such as chests, an anvil, and other assorted blocks can be seen in the background, indicating a well-equipped room. [0:00:20]: The video concludes with a close-up of an inventory screen in a chest. Various items, including weapons, armor, and resources, are neatly arranged. One item is highlighted as \"Golden Horse Armor.\"",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is one of the objectives mentioned in the video?",
        "time_stamp": "0:00:20",
        "answer": "B",
        "options": [
          "A. Build a house.",
          "B. Get a bunch of TNT.",
          "C. Collect water.",
          "D. Find a treasure."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_197_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:01 - 0:03:02]: The video opens with a view of a grassy terrain featuring a large tree prominently in the center. The landscape includes patches of grass and upturned soil. A wooden crafting table lies to the right of the tree, while in the background, a body of water stretches out towards the horizon with distant hills covered with trees visible;  [0:03:02]: The camera is now directed upwards, capturing a large wooden disc-like structure suspended in the sky, with small flames observable in the center of the underside. A single tall pole appears to support the structure, extending down into the ground out of view. The word \"well\" is visible in a black and white outlined font at the bottom of the frame;  [0:03:03 - 0:03:10]: The initial angle changes, providing another view of a large tree with a wooden crafting table at its base. The background contains a body of water, and the camera then pans upwards, continuing to focus on the disc-shaped structure from various angles, emphasizing its broad circumference and the small amount of smoke emanating from its center. A stick-like pole remains as a support, and patches of green bamboo appear along the right edge of the frame; [0:03:10 - 0:03:11]: The camera zooms in towards the central part of the underside, showing an array of flames that give off a light, fiery glow. The fire's base extends towards a platform made out of different materials below the disc;   [0:03:12 - 0:03:15]: With a clear sky as a backdrop, the camera shifts to an upward-facing angle, capturing the top of the large tree and the long pole disappearing into its foliage. The bright, clear sky provides a stark contrast to the tree’s dark green leaves. There are clouds scattered across the sky; [0:03:15 - 0:03:17]: The angle switches back to ground level, where the grassy terrain and various structures such as bamboos, a bright yellow bed, and the large disc-shaped structure suspended on the pole are in view. A small crafting table is once again present on a raised patch of land. The camera then zooms into a black and white inventory screen, showing various items neatly categorized; [0:03:17 - 0:03:20]: The camera shifts back to the ground level, displaying the green grassy fields spreading towards the horizon, a bed, and a few torches placed at intervals. The words \"but that should not be a problem\" appear at the bottom of the screen, creating a concluding statement.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What object is positioned to the left of the large tree?",
        "time_stamp": "0:03:00",
        "answer": "A",
        "options": [
          "A. A wooden crafting table.",
          "B. A stone statue.",
          "C. A blue tent.",
          "D. A metal bench."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the bed seen in the video?",
        "time_stamp": "0:03:32",
        "answer": "A",
        "options": [
          "A. Yellow.",
          "B. Red.",
          "C. Blue.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_197_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:00 - 0:09:20] [0:09:00]: The video begins with a view of a bright blue sky scattered with small, white clouds. The camera points towards a distant, elevated terrain, displaying a rugged, brown hill covered with sparse patches of green foliage. The foreground includes a HUD (Heads-Up Display) at the bottom, featuring health, tools, and inventory items such as a sword and pickaxe. [0:09:01]: The perspective shifts to the right, revealing more details of the landscape. The player stands adjacent to a tall, thick tree on the left, partially obscuring the view. The grass is yellowish-green and the terrain includes a cliff dropping to a lower level. [0:09:02 - 0:09:03]: As the player moves forward, a large cliff face becomes more prominent in the frame. The cliff face has a unique, square-hole formation resembling a pixelated face carved into the stone. The surrounding area consists of extensive brown and gray rocky surfaces. [0:09:04]: The camera angle adjusts slightly to provide a clearer view of the entire cliff. The rugged terrain below is complemented by a blue lake visible on the left side, bordered by rough, uneven ground. [0:09:05 - 0:09:06]: The scene remains focused on the mountainside, with the carved-out square-hole formation now entirely visible. The landscape around consists of mixed green and brown patches of rock and vegetation. [0:09:07 - 0:09:08]: The perspective changes subtly, keeping the focus on the detailed cliff and distant terrain. The top of the cliff shows bare stone with occasional green patches and a line of trees is visible at the horizon. [0:09:09 - 0:09:12]: The player’s viewpoint slowly tilts downward, scanning the steep slope leading towards a large, calm lake. The water is deep blue and clear. The adjacent terrain features small patches of greenery among extensive rocky ground. [0:09:13]: The pace of the footage slows as the camera remains fixed on the detailed, jagged rocks leading down to the lake. The inventory bar at the bottom of the screen stays visible. [0:09:14]: The scene captures the wide expanse of the lake and surrouding landscape. Details of the rocky ground near the water’s edge stand out, emphasizing the ruggedness of the environment. [0:09:15 - 0:09:16]: The perspective remains unchanged, allowing more time to observe the mixture of rocky and green surfaces throughout the scene. The player does not move, maintaining focus on the landscape. [0:09:17]: The camera tilts slightly to capture more of the rocky terrain in the foreground. The lake remains a central feature, its clear blue water contrasting with the rocky banks and cliffs around it. [0:09:18]: Without changing the overall scene, the focus slightly shifts more towards the rocky foreground, emphasizing the detailed texture of the rocks and small vegetation patches. [0:09:19]: The last frame of the video focuses on the same area, with the rugged cliffs and calm, expansive lake remaining the focal points. The HUD is still visible, indicating no change in the player’s status or inventory items.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What inventory items are visible in the player's HUD?",
        "time_stamp": "0:09:19",
        "answer": "A",
        "options": [
          "A. Sword and pickaxe.",
          "B. Sword and shield.",
          "C. Axe and bow.",
          "D. Pickaxe and bow."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_197_real.mp4"
  },
  {
    "time": "[0:11:00 - 0:11:48]",
    "captions": "[0:11:40 - 0:11:48] [0:11:39 - 0:11:40]: The video opens with a view of a gaming environment featuring a spacious room with wooden flooring and beams. A prominent crystal structure stands in the center emanating a blue light, anchored in a circular pool of water surrounded by a stone border. To the left, an ornate purple portal-like structure made of amethyst sits above stone stairs, contrasting with the sturdy wooden chests aligned against the wall on the right. [0:11:41]: The perspective shifts slightly to display more of the left side, including a character wearing bright diamond armor and wielding a sword. Behind the character, more details of the stone brick walls are visible, adding texture to the otherwise primarily wooden room. [0:11:42]: The viewpoint nudges to the right, showcasing the character holding a sword, now featuring an additional view of the trimmed wooden staircase leading downward and more of the aligned chests. [0:11:43 - 0:11:44]: The scene doesn't change much, but the position indicates slight movement, reinforcing the wooden and stone aesthetic, with minute adjustments highlighting the room's intricacies and arrangement of storage chests. [0:11:45 - 0:11:46]: Attention is drawn towards the center character, still in view, standing sturdily atop the stairs with a clearer view of the background, which includes additional walls and wooden textures. [0:11:47]: A \"Subscribe\" button animation overlays on the screen, symbolizing a typical call-to-action found in many online video recordings, below the character who remains the focus. [0:11:48]: The \"Subscribe\" button transitions to a clicked state with animation, indicating interaction, while the character maintains its pose, the room's details surrounding them with a blend of wood and stone. [0:11:49]: The “Subscribe” button changes to \"Subscribed,\" completing the animation, as the character stands stalwartly framed within the stone-and-wood detailed background, illustrating a completed interaction. [0:11:50]: The final frame highlights the character for one last second with no significant changes to the background or surrounding details, reinforcing the consistent structure and aesthetic of the scene as the video ends.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action is depicted with the \"Subscribe\" button?",
        "time_stamp": "00:11:49",
        "answer": "C",
        "options": [
          "A. It remains static.",
          "B. It disappears.",
          "C. It changes to \"Subscribed\".",
          "D. It moves to the top of the screen."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_197_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What type of car is featured in the video right now?",
        "time_stamp": "00:00:10",
        "answer": "D",
        "options": [
          "A. Ferrari F8 Tributo.",
          "B. Lamborghini Huracan.",
          "C. BMW M4.",
          "D. Porsche 911 GT3 RS."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_262_real.mp4"
  },
  {
    "time": "[0:01:25 - 0:01:30]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the current gear level of the car?",
        "time_stamp": "00:01:28",
        "answer": "D",
        "options": [
          "A. 1.",
          "B. 2.",
          "C. 3.",
          "D. 4."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_262_real.mp4"
  },
  {
    "time": "[0:02:50 - 0:02:55]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "Right now, what is indicated by the green column?",
        "time_stamp": "00:02:52",
        "answer": "A",
        "options": [
          "A. Braking.",
          "B. Tire pressure.",
          "C. Speed.",
          "D. Throttle."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_262_real.mp4"
  },
  {
    "time": "[0:04:15 - 0:04:20]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Right now, what gear is the car using?",
        "time_stamp": "00:04:20",
        "answer": "D",
        "options": [
          "A. 3.",
          "B. 4.",
          "C. 6.",
          "D. 5."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_262_real.mp4"
  },
  {
    "time": "[0:05:40 - 0:05:45]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Right now, what is the current speed of the car?",
        "time_stamp": "00:05:40",
        "answer": "D",
        "options": [
          "A. approx 226 km/h.",
          "B. approx 189 km/h.",
          "C. approx 190 km/h.",
          "D. approx 249 km/h."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_262_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video starts with a black screen;  [0:00:02 - 0:00:03]: A residential street is visible on a clear, sunny day. A black car is parked on the right side of the road, facing away from the camera. To the left is a grassy area leading up to a house with a white exterior roof. Several houses and trees line the street;  [0:00:04 - 0:00:06]: The camera perspective advances along the sidewalk, showing additional details of the houses, trees, and greenery on the left side. The black car remains on the right side;  [0:00:07 - 0:00:08]: The camera continues to move along the street. A tall evergreen tree stands prominently in the middle. The sidewalk curves slightly as the camera moves forward;  [0:00:09 - 0:00:10]: The view of the residential area remains consistent. The street is slightly inclined, with more homes visible along the left side. A white car is parked further ahead on the opposite side of the road;  [0:00:11 - 0:00:13]: As the camera proceeds, more of the neighborhood comes into view, displaying a receding grassy area and a light blue house to the left. The white car approaches closer in the frame;  [0:00:14 - 0:00:16]: The camera angle progresses, showing a street sign on a green lamppost and more greenery. The white car is now quite near;  [0:00:17 - 0:00:19]: The view shows the white car in detail, along with more hedges and another house on the left. The grassy area extends towards more houses in the distant background. The sky is a clear blue;  [0:00:20]: The camera is now close to the house with vibrant green bushes on the left, while the white car remains parked in front of it, facing the camera direction.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What notable feature is seen in the middle of the street as the camera moves forward?",
        "time_stamp": "0:00:08",
        "answer": "A",
        "options": [
          "A. A tall evergreen tree.",
          "B. A parked truck.",
          "C. A street vendor.",
          "D. A bicycle lane."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_302_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:20 - 0:01:40] [0:01:20 - 0:01:21]: The camera captures a tall, dense, conical evergreen tree located in a lush garden. In the background, there is a two-story house with large windows and light-colored siding. The sky is clear and blue. [0:01:21 - 0:01:24]: The view changes as the camera moves forward, revealing more of the garden and house. Another evergreen tree comes into view, and the house's roof is more visible. [0:01:24 - 0:01:27]: The camera continues to move, passing by manicured shrubs and bushes with varying shades of green. The path appears to lead down and to the left side of the frame, indicating the cameraman is walking around the property. [0:01:27 - 0:01:29]: The camera approaches an area with a stone pathway flanked by garden beds containing various plants. To the left, houses alongside a road become visible. The sidewalk runs parallel to the street. [0:01:29 - 0:01:31]: The camera moves down a residential sidewalk shaded by large trees, with more greenery on either side and occasional garden beds. Several parked cars are visible along the street, and the road curves gently to the left. [0:01:31 - 0:01:36]: As the camera advances along the path, it captures foliage on the right and the sidewalk on the left. Trees lining the sidewalk provide a canopy of green above, and the sunlight filters through the leaves. [0:01:36 - 0:01:37]: A larger property with a blue house and a well-maintained garden comes into view. The blue house has multiple windows, a grey-fenced yard, and a large shaded area under a tree. [0:01:37 - 0:01:39]: The camera gets closer to the right-side property. The blue house and its detailed architecture dominate the frame, including its prominent windows and a side entrance. Green bushes obscure part of the scene, adding to the lush environment.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What does the camera capture after showing the manicured shrubs and bushes?",
        "time_stamp": "0:01:33",
        "answer": "A",
        "options": [
          "A. A stone pathway flanked by garden beds.",
          "B. A wooden bridge over a stream.",
          "C. A large fountain in a central plaza.",
          "D. An open field with wildflowers."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_302_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:42]: The scene is a sidewalk beside a grassy lawn, bordered by a hedge on the right and a row of parked cars on the left. Tall conifer trees are adjacent to the sidewalk, their dense foliage dominating the right side of the view. The sky is clear and blue. [0:02:43 - 0:02:45]: The sidewalk stretches forward, lined by the hedge and trees on the right, while the row of parked cars continues on the left. The dense hedge partially reveals a house and more greenery further along the path. [0:02:46 - 0:02:48]: As the camera moves forward, the hedge on the right begins to thin, revealing more of the house and additional plants behind it. The parked cars are still visible on the left side of the sidewalk. [0:02:49 - 0:02:51]: The sidewalk continues straight ahead with the thinning hedge on the right. More parts of the house and its landscaped garden become visible. Trees still border the path on the left, and the parked cars remain in view. [0:02:52 - 0:02:53]: Now past the dense part of the hedge, a more open view of the garden and houses on the right side is clear. The leafy branches of a tree hang above, partially shading the sidewalk. [0:02:54 - 0:02:56]: The camera captures a clear view of the house and part of its surrounding garden. The sidewalk is bordered by trimmed grass, progressing towards the back of the houses.  [0:02:57 - 0:02:59]: The view becomes more residential, with clear sights of multiple houses and their well-maintained gardens. The sky remains bright and cloudless, enhancing the vibrant greenery surrounding the houses.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What becomes visible past the dense part of the hedge now?",
        "time_stamp": "0:02:53",
        "answer": "C",
        "options": [
          "A. A park.",
          "B. A row of shops.",
          "C. The garden and houses.",
          "D. A busy street."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_302_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:02]: The video begins on a bright, sunny day with a clear blue sky. The scene is a quiet suburban area with trees and bushes offering some greenery. To the left, there is a paved road bordered by a gravel shoulder. The background includes mountains visible under a cloudless sky.  [0:04:02 - 0:04:05]: A white car is parked on the gravel shoulder of the road. The road curves slightly to the left. There are noticeable cracks and patches on the pavement. The residential area includes houses partially obscured by trees and bushes lining the road.  [0:04:05 - 0:04:08]: As the camera moves forward, the white car becomes more prominent, parked along the right side of the road. On the left, a house surrounded by tall trees and bushes is visible. The camera appears to be moving closer to the parked car, with shadows cast by the greenery and the trees alongside the road. [0:04:08 - 0:04:11]: The car parked has blue stripes on the hood. The driver’s side of the car, along with the front windshield and license plate, are visible. Further down the road, another vehicle, possibly a van, is seen in the distance. The path along the road is lined with overgrown grass and shrubbery. [0:04:11 - 0:04:14]: Approaching the car, the camera view gets closer to the car’s front. The road continues straight ahead, while the house on the left is partly visible, surrounded by more trees and shrubs. The parked car's side mirrors and part of the house's driveway appear. [0:04:14 - 0:04:17]: The camera now passes by the white car's front and proceeds forward along the sidewalk. Another vehicle can be seen more clearly up the road. Bushes and long grass are on the left side, and the overgrown pavement edge is visible. [0:04:17 - 0:04:20]: The camera continues along the sidewalk, moving past the white car. Tall trees, bushes, and grass line the way, and the vehicle in the distance becomes more prominent. The van parked further up the road looks more detailed as the perspective advances. The shadow of the trees falls across the pavement, with beams of sunlight shining through.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What distinguishes the car closest to the camera now?",
        "time_stamp": "00:04:07",
        "answer": "D",
        "options": [
          "A. It has red stripes on the hood.",
          "B. It has green stripes on the hood.",
          "C. It has yellow stripes on the hood.",
          "D. It has blue stripes on the hood."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "What is the position of the house relative to the white car?",
        "time_stamp": "00:04:13",
        "answer": "C",
        "options": [
          "A. To the right, behind the car.",
          "B. To the left, in front of the car.",
          "C. To the left, beside the car.",
          "D. To the right, beside the car."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_302_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins with a dark and chaotic background primarily composed of textures that appear to be multi-colored, with dominant hues of purple, black, and white. It implies a galaxy or space-like theme with irregular patterns.  [0:00:02 - 0:00:05]: As the video transitions, a bright and colorful logo appears prominently in the center of the frame. The logo has a yellow background with a red and blue arrow design pointing to the right. The text \"GRAVITY THROTTLE RACING\" is displayed in white uppercase letters. [0:00:06]: The scene shifts to a miniature model landscape featuring a desert terrain with a highway running diagonally from the bottom left corner to the top right. The backdrop consists of blue sky and mountainous terrain. A yellow and black model car is visible on the highway.  [0:00:07 - 0:00:08]: A model train appears on the left side of the frame, traveling on tracks parallel to the highway. The train is black with a lit front light and multiple cars following behind. [0:00:09 - 0:00:11]: The train continues to move past the viewer, revealing additional cars including red and white cargo containers. The scene maintains the desert and mountain backdrop. [0:00:12]: The train is now out of the frame, with the focus shifting back to the empty highway and railway tracks. The painted mountains and sky dominate the background.  [0:00:13]: The scene captures a bird's-eye view of a miniature airplane approaching a runway. The plane is white with a red stripe near the end of each wing. Below, various yellow and white model vehicles are featured near the start of the runway. [0:00:14 - 0:00:18]: The airplane is seen descending towards the runway, aligning itself with the landing path marked by painted lines. The runway extends forward into the desert terrain, framed by the railway tracks on the right side and a few scattered model buildings. [0:00:19]: The airplane touches down on the runway, continuing to move along the centerline. The final frame captures the airplane's tire marks on the runway with the surrounding desert terrain still in view.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What happens to the airplane as it aligns itself with the landing path?",
        "time_stamp": "0:00:20",
        "answer": "C",
        "options": [
          "A. It takes off.",
          "B. It crashes.",
          "C. It lands on the runway.",
          "D. It continues to fly in circles."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_496_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:07]: The video is set on a miniature snow-covered race track, featuring five toy cars ready to race. The cars are positioned at the starting line, which is directly in front of a gate with three lights. The cars in their respective lanes are as follows: a purple car in the first lane from the left, a green car in the second lane, a silver car with black stripes in the third lane, a red car in the fourth lane, and an orange car in the fifth lane. The track consists of five winding lanes with noticeable elevation changes. The environment resembles a mountainous snowy landscape, painted on the backdrop and the terrain. [0:04:08 - 0:04:09]: The race begins as the cars leave the starting line, speeding down the incline. The red car takes an early lead, followed by the silver and green cars. The purple car follows closely behind, with the orange car trailing. They navigate through a curved section of the track, maintaining their respective positions. [0:04:10 - 0:04:12]: The camera viewpoint shifts, showing the cars from behind as they continue on the snowy track. They navigate another set of curves and elevations as the scenery changes to a more mountainous backdrop, with snow-covered trees lining the track. The red car still leads, with a slight gap between it and the silver car. The other cars follow at close distances. [0:04:13 - 0:04:16]: The cars speed along the track with the red car maintaining the lead. The perspective shifts to various angles showing the winding path of the track carved into the snowy hillside. The scenery shows cliffs and snow-covered pines, adding to the sense of speed and motion. The silver car gains some ground on the red car, while the green car stays in third place, and the purple and orange cars race closely behind them. [0:04:17 - 0:04:19]: The track meanders through a more rugged part of the miniature landscape with brown rocky terrain and sparse vegetation. The red car still leads but is now followed very closely by the silver car, which almost catches up. The green car maintains its third position but starts to fall slightly behind the leading two. The purple and orange cars continue to follow through the winding path. [0:04:19]: The race progresses as the cars approach a tunnel near the end of the track. The red car enters first, with the silver car close behind. The green car is third, with a slight distance from the tunnel entrance. The purple and orange cars approach the tunnel last. The surroundings include varied terrain, from rocky hillsides to a train track course winding alongside the race track.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Which car won first place in this competition?",
        "time_stamp": "0:04:35",
        "answer": "C",
        "options": [
          "A. The blue car.",
          "B. The dark purple car.",
          "C. The green car.",
          "D. The red car."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_496_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: The video starts with a snowy landscape featuring small trees scattered around. There are winding roads with a gray car, red car, and purple car driving on them. The cars are moving from the right side of the frame towards the left. [0:08:01 - 0:08:02]: The camera angle shows a sharp turn in the road, with the three cars still visible. The snowy terrain has become more elevated. [0:08:02 - 0:08:03]: The camera focuses on a cliff edge with a red car leading on a curved road. The road is bordered by red and white checkered barriers. The background is snowy with green trees. [0:08:03 - 0:08:04]: The red car travels along a road that curves steeply downhill. The landscape shifts to include more rocky terrain and sparse vegetation. [0:08:04 - 0:08:05]: The red car continues down the winding road, now passing over a bridge. There's a drop below to another level of road and some green foliage. [0:08:05 - 0:08:06]: From this elevated perspective, the red car is proceeding along the road which curves tightly. Other cars can be seen ahead, navigating the same winding path. [0:08:06 - 0:08:07]: The red car is seen from a higher angle, descending along the curve with a purple car following closely behind. The terrain includes more greenery. [0:08:07 - 0:08:08]: The red car continues to descend, with train boxcars visible below next to the road. The road curves sharply alongside the hill. [0:08:08 - 0:08:09]: The red car approaches a tunnel that cuts through a rock formation. A train, including boxcars, is traveling next to the road. [0:08:09 - 0:08:10]: The perspective shifts to a desert landscape. The red car exits the tunnel and continues to descend the winding path. The scene includes scattered small trees. [0:08:10 - 0:08:11]: The red car travels on a road parallel to the moving train. There are various types of cargo cars on the train. The background remains a desert landscape. [0:08:11 - 0:08:12]: The red car drives on a two-lane marked road, alongside the train. The background shows a painted mountainous landscape. [0:08:12 - 0:08:13]: The red car continues to drive on the road which is now further along the railway. The desert terrain features green bushes and rocky outcrops. [0:08:13 - 0:08:14]: The road leads the red car past a sign indicating a turn, where several cars are seen parked near a hangar. An airplane is on the ground beside the hangar. [0:08:14 - 0:08:15]: The car passes by the hangar area and airplane, continuing on the road. The landscape features high rock formations in the background. [0:08:15 - 0:08:16]: The view shows the red car driving past another train. The road curves around a large rocky hill in the desert. [0:08:16 - 0:08:17]: The red car continues descending, passing by the rocky hill. More vehicles are seen travelling around the hill on winding roads. [0:08:17 - 0:08:18]: The camera focuses on a section of the rail track as the perspective zooms out. The train cars continue to be visible. [0:08:18 - 0:08:19]: The red car drives under a bridge structure, closely followed by a purple car. The road is still part of the desert landscape. [0:08:19 - 0:08:20]: The road continues straight, with the red car leading ahead of the purple car. Sparse bushes and small trees are seen on both sides of the road.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Which car won first place in this competition?",
        "time_stamp": "0:08:23",
        "answer": "D",
        "options": [
          "A. The blue car.",
          "B. The dark purple car.",
          "C. The green car.",
          "D. The red car."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_496_real.mp4"
  },
  {
    "time": "[0:12:00 - 0:13:00]",
    "captions": "[0:12:00 - 0:12:20] [0:12:00 - 0:12:05]: The video begins with a view of a small, snowy racetrack from a first-person perspective. Four toy cars, a green SUV, a silver sports car, a red sports car, and a purple SUV, are positioned at the starting line. The racetrack has various lanes, each designated for one car. The starting lights are red; the green and white checkered starting banner is visible above the lanes. The scene is set against a blue background, with snow banks on each side of the racetrack and a mountainous background. [0:12:05 - 0:12:07]: The starting lights turn from red to green, signaling the start of the race. The toy cars begin to move off the starting line, with the purple SUV in the lead, followed closely by the red and silver sports cars, while the green SUV is slightly behind. The racetrack is winding and continues downward. [0:12:08 - 0:12:11]: The toy cars move along the racetrack, maintaining a relatively close distance. The paths are curving, with the purple SUV slightly ahead on a straight section. The snowy track is surrounded by snow banks, and the scene indicates more curves ahead. [0:12:12 - 0:12:13]: As the cars continue, they encounter more winding sections of the track. The purple SUV leads, followed by the red sports car, while the silver sports car and green SUV follow closely behind. Snow-covered hills and miniature evergreen trees are part of the background, creating a wintery landscape. [0:12:14 - 0:12:15]: The cars navigate a sharp curve; the purple SUV is ahead, negotiating the bend, followed by the red car closely behind. The silver and green cars follow. The background includes more snow-covered terrain with scattered trees. [0:12:16]: The race continues as the toy cars move into a section with a steep drop and a left bend in the track. The snowy terrain remains, but now there is more visible green vegetation. [0:12:17 - 0:12:19]: The cars navigate the downward and winding path, with the purple SUV still leading. The racetrack now winds around hills with sharp curves; green patches, suggesting a less snowy area, become more prominent. The red sports car tries to close the gap with the leader. The scene includes more diverse scenery with both snow and greenery.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Which racing car won first place in this competition?",
        "time_stamp": "0:12:35",
        "answer": "B",
        "options": [
          "A. The blue car.",
          "B. The silver and red car.",
          "C. The green car.",
          "D. The red car."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      },
      {
        "task_type": "Object Recognition",
        "question": "What is surrounding the racetrack right now?",
        "time_stamp": "0:12:03",
        "answer": "C",
        "options": [
          "A. Sand dunes.",
          "B. Tall grass.",
          "C. Snow banks.",
          "D. Rocky cliffs."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_496_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video begins with a black screen.   [0:00:01 - 0:00:03]: In an elegant room with classic wooden furniture and ornate decorations, a man in a white shirt and white pants stands near a closed white door. There are two lit candelabras on small tables, one on either side of the door, with a small dresser between them. The man seems to be positioned slightly to the right of the room, with another man in a dark suit standing to his left.   [0:00:04 - 0:00:06]: The first man, who has a distinctive handlebar mustache, looks forward before turning his head slightly downward. His hair is slightly tousled, and he seems to be in thought. The background includes a large framed painting of a person. The second man is partially visible, standing close to the first man.   [0:00:07]: The man in the white shirt starts to extend his arms forward as if preparing to put on a garment. The second man, dressed in dark clothing, offers an open black jacket.   [0:00:08 - 0:00:09]: The man in the white shirt reaches for the jacket, held by the second man. Their interaction suggests an exchange or assistance with dressing.   [0:00:10 - 0:00:13]: The man in the white shirt moves his hands through the sleeves of the jacket, beginning to put it on. The second man continues to assist, ensuring the jacket is correctly positioned. Both are focused on the task.   [0:00:14 - 0:00:16]: Close-up shots focus on the man in the white shirt adjusting his shirt and jacket. Details of the fabric and sleeve buttons are highlighted, as he ensures a proper fit.   [0:00:17 - 0:00:19]: The man completes the adjustments, straightening his attire and securing the final touches of his jacket. The camera captures the practiced movements and fabric texture as he smooths down the clothing.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the distinctive feature of the man's appearance?",
        "time_stamp": "0:00:05",
        "answer": "B",
        "options": [
          "A. A scar on his cheek.",
          "B. A handlebar mustache.",
          "C. A monocle.",
          "D. A bow tie."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_153_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:04]: There is a man with light brown hair wearing a dark suit, visible from a side profile. He stands in a room, slightly bent over, with a serious expression on his face. He is in front of a painting with a golden frame, and there are lit candles on a candelabrum on a piece of furniture below the painting. The room has light-colored walls, with a white door visible to the right. [0:01:05 - 0:01:06]: The man's hands and lower body are seen as he bends over, adjusting his dark trousers over highly polished black shoes. The floor appears to be covered with a patterned rug in tones of red and orange. [0:01:07 - 0:01:10]: The man continues to adjust his trousers and shoes, now sitting on a chair. His hands are positioned on his knees, and a part of his white shirt with a hint of a cuff is visible. [0:01:11 - 0:01:15]: The man stands and a second person, only partially visible, helps adjust the first man's shirt cuff. Both individuals appear to be dressed in formal attire. The second person's hands, also in dark sleeves, help to fasten the cuff link on the first man's sleeve. [0:01:16 - 0:01:19]: The first man is seen from a slightly wider view, now standing fully dressed in formal attire. A second man, younger with darker hair and a mustache, is also fully visible, dressed in a white shirt with suspenders. He stands in front of the candles and a piece of furniture with a red curtain to the left side of the frame. The first man turns and starts walking away to the left, while the second man stands still. The room's detailed decor, including the ornate furniture and the rug, are clearly noticeable.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What item is located below the painting with the golden frame?",
        "time_stamp": "00:01:04",
        "answer": "C",
        "options": [
          "A. A bookshelf.",
          "B. A table with a vase.",
          "C. A candelabrum.",
          "D. A mirror."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Action Recognition",
        "question": "What is the man doing while sitting on a chair?",
        "time_stamp": "00:01:10",
        "answer": "C",
        "options": [
          "A. Reading a book.",
          "B. Polishing his shoes.",
          "C. Adjusting his trousers and shoes.",
          "D. Writing a letter."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_153_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:02]: In a well-furnished room with classical decor, a man is helping another man with his attire. The man getting assistance stands with his back to a door, while the helper, dressed in a dark coat, adjusts the collar of the man’s white shirt and beige vest. [0:02:03 - 0:02:06]: The camera zooms in, showing a close-up view of the helper’s hands adjusting the man's collar and tie. The man being assisted closes his eyes briefly, suggesting he is comfortable with the situation. [0:02:07 - 0:02:10]: After the closer view, the camera returns to a wider angle showing the helper continuing to make final adjustments to the man's collar. The room's details, like a red curtain, a sizable painting, and partly opened doors are visible again. [0:02:11 - 0:02:12]: The helper finishes adjusting the collar and bow tie. The man being helped has his eyes closed and appears composed and ready. [0:02:13 - 0:02:14]: The man looks down slightly as the attire adjustment is completed. He seems to be examining or feeling the fit of the tie and collar on his neck. [0:02:15 - 0:02:17]: The helper steps back slightly, taking a towel, possibly to dust off or final detailing. They remain in front of the elaborately decorated room, which includes items like candelabras and antique furniture. [0:02:18 - 0:02:20]: The man being helped lifts his arm to perhaps inspect his attire more closely or to make room for any last adjustments. The helper continues to assist with minor detailing as the man begins to move slightly, indicating completion.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the vest of the man being helped?",
        "time_stamp": "00:02:20",
        "answer": "B",
        "options": [
          "A. White.",
          "B. Beige.",
          "C. Black.",
          "D. Gray."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Action Recognition",
        "question": "What does the man being helped do after the helper finishes adjusting his collar and bow tie?",
        "time_stamp": "00:02:15",
        "answer": "D",
        "options": [
          "A. Looks in the mirror.",
          "B. Closes his eyes.",
          "C. Lifts his arm.",
          "D. Looks down slightly."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_153_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:16",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      },
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:56",
        "answer": "A",
        "options": [
          "A. 1.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_99_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:01:26",
        "answer": "A",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 1."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_99_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:26",
        "answer": "B",
        "options": [
          "A. 3.",
          "B. 4.",
          "C. 6.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_99_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:41",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 8.",
          "C. 3.",
          "D. 7."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_99_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:24",
        "answer": "B",
        "options": [
          "A. 2.",
          "B. 8.",
          "C. 3.",
          "D. 7."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_99_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:12",
        "answer": "A",
        "options": [
          "A. 1.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      },
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:28",
        "answer": "B",
        "options": [
          "A. 1.",
          "B. 2.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " B."
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_72_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:22",
        "answer": "B",
        "options": [
          "A. 1.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_72_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:18",
        "answer": "C",
        "options": [
          "A. 1.",
          "B. 3.",
          "C. 4.",
          "D. 5."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_72_real.mp4"
  },
  {
    "time": "[0:12:00 - 0:13:00】",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:13:30",
        "answer": "D",
        "options": [
          "A. 10.",
          "B. 11.",
          "C. 12.",
          "D. 13."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_72_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is mounted on the left side of the helicopter cockpit right now?",
        "time_stamp": "00:00:04",
        "answer": "C",
        "options": [
          "A. A phone.",
          "B. A GPS device.",
          "C. A tablet.",
          "D. A camera."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_426_real.mp4"
  },
  {
    "time": "[0:02:09 - 0:02:14]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What type of store is the person in right now?",
        "time_stamp": "00:03:16",
        "answer": "D",
        "options": [
          "A. A clothing store.",
          "B. A pet store.",
          "C. A grocery store.",
          "D. A costco."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_426_real.mp4"
  },
  {
    "time": "[0:04:18 - 0:04:23]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What brand of chips is in the shopping cart right now?",
        "time_stamp": "00:04:22",
        "answer": "C",
        "options": [
          "A. Lay's.",
          "B. Pringles.",
          "C. Miss Vickie's.",
          "D. Doritos."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_426_real.mp4"
  },
  {
    "time": "[0:06:27 - 0:06:32]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What brand of paper towels is visible on the ground right now?",
        "time_stamp": "00:06:31",
        "answer": "B",
        "options": [
          "A. Bounty.",
          "B. Scott.",
          "C. Viva.",
          "D. Sparkle."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_426_real.mp4"
  },
  {
    "time": "[0:08:36 - 0:08:41]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which brand of bottled water is visible on the passenger seat right now?",
        "time_stamp": "00:08:39",
        "answer": "C",
        "options": [
          "A. Dasani.",
          "B. Evian.",
          "C. Ice Mountain.",
          "D. Aquafina."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_426_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: An individual with black gloves maneuvering a flatbed cart piled with boxes. The setting is within a store, organized with metal shelves packed with neatly stacked product cartons. Some boxes are scattered around the floor. The person appears to be unloading boxes onto the top shelf. [0:00:02 - 0:00:05]: The individual picks up a carton with a green and white label from the flatbed cart and places it on the shelf. The shelves are labeled with price tags, and there are other dairy cartons on the lower and top shelves. [0:00:06 - 0:00:09]: The individual places another carton on the shelf. The first person perspective shows the individual is wearing a sleeve with a black jacket and a gray glove. The activity continues as the individual stocks shelves meticulously. [0:00:10 - 0:00:12]: The person pauses for a moment, possibly to arrange the boxes on the flatbed cart stacked with cartons. They place one more carton on a side shelf, ensuring proper stocking. [0:00:13 - 0:00:15]: More boxes are scattered in the store's aisle. The individual reaches for an empty shelf location and continues to place the cartons in their appropriate sections. [0:00:16 - 0:00:18]: The person starts to gather the empty cardboard boxes from the floor scattered around the cart. They consolidate these boxes, clearing up the surrounding space. [0:00:19 - 0:00:20]: Adjust the remaining boxes on the flatbed cart, organizing them to continue stocking shelves. The background context remains consistent, with orderly shelves packed with products.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the state of the shelves as the individual continues to stock them?",
        "time_stamp": "0:00:20",
        "answer": "D",
        "options": [
          "A. Shelves are mostly empty.",
          "B. The shelves are already full.",
          "C. Shelves are cluttered and disorganized.",
          "D. Shelves are orderly and packed with products."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_448_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:23]: The video showcases a first-person perspective of someone handling cardboard boxes in a storage or warehouse area. The boxes are medium-sized and uniformly brown with various labels. The individual is seen moving boxes from one location to another, stacking them, and arranging them neatly.  [0:02:24 - 0:02:28]: The person continues to organize the boxes, placing them onto a platform or trolley. Shelves filled with other items and more stacked boxes are visible in the background. The individual is wearing black gloves and dark clothing.  [0:02:29 - 0:02:33]: The person moves to a different section, still handling boxes. They focus on adjusting and straightening the boxes on the platform. The nearby shelves are stocked with various packaged products, possibly in a retail or stockroom environment. [0:02:34 - 0:02:37]: The individual shifts their attention to the shelves, placing some boxes from the platform onto a bottom shelf. There are price tags displayed on the shelves, suggesting a retail setting. The surrounding area is organized, but several boxes are still scattered on the floor. [0:02:38 - 0:02:39]: The person reaches for an item on the middle shelf and inspects it. The shelves are well-stocked, and labels are clearly visible on the products. The scene is brightly lit, enhancing the visibility of the items and surroundings.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type of perspective is shown in the video?",
        "time_stamp": "0:02:23",
        "answer": "C",
        "options": [
          "A. Third-person perspective.",
          "B. Aerial perspective.",
          "C. First-person perspective.",
          "D. Side perspective."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_448_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:42]: A person is in a store, crouching near a shelf stocked with various cartons of milk. They are placing small white bottles of what appears to be supplements into a cardboard box. The cartons are white with different colored sections and labels. There are price tags on the shelf indicating prices around 20-24 krona. [0:04:43 - 0:04:45]: The person reaches for a carton of milk labeled \"Vanilj\" priced at 31.50 krona on the upper shelf. They are wearing black gloves. [0:04:46 - 0:04:50]: The person places the carton on the lower shelf and continues picking up white bottles to place them in the box. They repeat the process, reaching for items on the shelf and placing them into their box. [0:04:51 - 0:04:52]: The person continues this pattern, occasionally reaching for different brands of cartons and rearranging items on the shelves. [0:04:53 - 0:04:54]: The camera perspective moves back, showing more of the aisle. There are several cardboard boxes scattered on the floor and stacks of items against the wall and shelves. [0:04:55 - 0:04:56]: The person moves around, picking up one of the empty cardboard boxes from the floor near a red shelf and an orange pallet jack. [0:04:57 - 0:04:59]: The person is sorting through the scattered boxes on the floor, moving them from one pile to another, organizing items. The perspective consistently shows a view from above the person's shoulders.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color are the cartons of milk on the shelf?",
        "time_stamp": "00:04:51",
        "answer": "B",
        "options": [
          "A. Yellow with different colored sections.",
          "B. White with different colored sections.",
          "C. Green with different colored sections.",
          "D. Red with different colored sections."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_448_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:01]: The video begins inside a storage or warehouse area. The first-person view shows a hand reaching towards a metallic cart. Nearby, there are stacked red crates and a large pile of flattened cardboard boxes. To the left, various products are organized on shelves. [0:07:02 - 0:07:03]: The perspective shifts slightly, facing down more towards the floor and the pile of cardboard. There is a stack of small boxed items placed on a platform along with an orange trolley. [0:07:04 - 0:07:05]: The camera focuses more on the platform with small boxes. The person is seen moving towards the boxes, preparing to move them. [0:07:06 - 0:07:07]: The person starts picking up the boxes from the platform and placing them in a metallic cart. The surroundings include shelves on the left and the pile of flattened cardboard on the right. [0:07:08 - 0:07:09]: The person continues reaching for more boxes, placing them in the cart. The number of boxes on the ground decreases as they are transferred to the cart. [0:07:10 - 0:07:11]: The focus shifts again. The view zooms out slightly, showing the person organizing and placing more boxes on the cart. The remaining boxes are getting fewer on the platform. [0:07:12 - 0:07:13]: The person starts moving the cart towards the green and white stacked boxes on the right side. The camera view moves past the shelves with various products. [0:07:14 - 0:07:15]: The action shifts to the person grabbing a large green box labeled \"Bregott\" and placing it onto the cart. The surrounding area shows more stacked items and shelves filled with different products. [0:07:16 - 0:07:17]: Larger green boxes are being arranged and stacked on the cart. The person positions these boxes carefully, and the camera view shows more of the packed shelves in the background. [0:07:18 - 0:07:19]: The person continues to maneuver the cart with the large green boxes, moving deeper into the storage area. The view shifts to a broader perspective of the room, with more items and equipment scattered around. [0:07:20]: The video ends as the person approaches another section of the warehouse with the cart fully loaded with green boxes and small boxed items. The surroundings include more storage shelves, tools, and stacked equipment.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color are the boxes labeled \"Bregott\"?",
        "time_stamp": "00:07:15",
        "answer": "C",
        "options": [
          "A. Red.",
          "B. Blue.",
          "C. Green.",
          "D. Orange."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Action Recognition",
        "question": "What action did the person perform repeatedly just now?",
        "time_stamp": "00:07:11",
        "answer": "C",
        "options": [
          "A. Organizing products on shelves.",
          "B. Flattening cardboard boxes.",
          "C. Picking up and placing boxes in a cart.",
          "D. Moving the cart towards the door."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_448_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "Right now, what is the weather like on the street?",
        "time_stamp": "00:00:22",
        "answer": "D",
        "options": [
          "A. Sunny.",
          "B. Snowy.",
          "C. Clear.",
          "D. Rainy."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_403_real.mp4"
  },
  {
    "time": "[0:02:35 - 0:02:40]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type of vehicle is directly in front of the camera right now?",
        "time_stamp": "00:02:37",
        "answer": "A",
        "options": [
          "A. Taxi.",
          "B. Bus.",
          "C. Bicycle.",
          "D. Motorcycle."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_403_real.mp4"
  },
  {
    "time": "[0:05:10 - 0:05:15]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which brand's store sign is seen on the right-hand side of the street right now?",
        "time_stamp": "00:05:12",
        "answer": "D",
        "options": [
          "A. Nike.",
          "B. Tesla.",
          "C. Adidas.",
          "D. UGG."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_403_real.mp4"
  },
  {
    "time": "[0:07:45 - 0:07:50]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the current condition of the pavement right now?",
        "time_stamp": "00:07:45",
        "answer": "D",
        "options": [
          "A. Dry.",
          "B. Sandy.",
          "C. Snowy.",
          "D. Wet."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_403_real.mp4"
  },
  {
    "time": "[0:10:20 - 0:10:25]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What makes of the yellow car is waiting at the red light right now?",
        "time_stamp": "00:10:08",
        "answer": "C",
        "options": [
          "A. Honda.",
          "B. Ford.",
          "C. Toyota.",
          "D. Nissan."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_403_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: I’m tired of all of my items breaking on me.  [0:00:03 - 0:00:05]: So that’s why I’m gonna build the coolest villager breeder  [0:00:05 - 0:00:06]: you have ever seen.  [0:00:07 - 0:00:09]: Subscribe [0:00:12 - 0:00:13]: Hope you enjoy.  [0:00:13 - 0:00:16]: So for us to be able to get this build on the road  [0:00:16 - 0:00:17]: we’re gonna need  [0:00:17 - 0:00:18]: a looting 3  [0:00:18 - 0:00:20]: enchanted sword.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Why does the speaker want to build the coolest villager breeder?",
        "time_stamp": "0:00:20",
        "answer": "A",
        "options": [
          "A. He is tired of their items breaking.",
          "B. He wants to impress their friends.",
          "C. He need more resources.",
          "D. He enjoys building new things."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_190_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:00:00 - 0:00:01]: A minecart with a chest is displayed on the screen, showing its contents in an organized grid format. Various items such as ingots, enchanted books, and a golden apple are visible in the chest. Below the chest interface is the player's inventory, filled with numerous items including weapons, blocks, and tools. [0:00:02 - 0:00:03]: The inventory view remains on the screen, with some items highlighted in the chest on the upper right, including several torches. The player’s experience level is displayed as 19, and various tools and items are scattered in the inventory slots. [0:00:04 - 0:00:05]: The scene shifts perspective to a dark, enclosed underground area with cobblestone and wooden structures. There are rails on the ground, indicating a mineshaft setting. In front of the player, a post stands in the middle of the pathway. [0:00:06 - 0:00:07]: The environment remains dimly lit, with wooden beams supporting the structure. The player’s hand holds a purple-colored weapon, possibly enchanted, with other icons such as hearts and food indicators at the bottom. [0:00:08 - 0:00:09]: Focus shifts to a nearby corner where a spider, hanging from the ceiling on its web, is visible. The player holds a piece of bread in their hand. [0:00:10 - 0:00:11]: The perspective changes as the player shifts their weapon to a diamond axe, still fixated on the upper portion of the wooden beam where the spider is positioned. A torch illuminates the scene nearby. [0:00:12 - 0:00:13]: The spider now lands on the ground close to the torch. The player's diamond axe remains in hand, ready for action. [0:00:14 - 0:00:15]: The player strikes the spider with the axe. The spider shows signs of damage with red heart animations appearing. [0:00:16 - 0:00:17]: After defeating the spider, focus shifts to a mob spawner surrounded by cobblestone and wooden structures. The spawner is topped with a lit torch, preventing further mobs from spawning. [0:00:18 - 0:00:19]: The player navigates through a narrow pathway between two wooden pillars. The pathway ahead is dimly lit and appears to be part of an interconnected tunnel system. [0:00:20 - 0:00:21]: A new scene displays a dark section of the tunnel system with water flowing down from the walls. The player's viewpoint shows the flowing water, which creates small pools on the ground. [0:00:22 - 0:00:23]: The player draws a diamond axe while facing the direction of the cascading water. The light in the tunnel remains limited. [0:00:24 - 0:00:25]: The player switches from the diamond axe to a diamond pickaxe, preparing to possibly mine resources within the dimly lit, water-filled tunnel section. [0:00:26 - 0:00:27]: The view moves towards another section of the mineshaft where a minecart with a chest sits on rails. Webs are visible above, indicating the lingering remnants of spiders. [0:00:28 - 0:00:29]: The player opens the minecart's chest, revealing its contents. The chest contains items such as rails, redstone, and a name tag. [0:00:30]: The player comments with an audible “oh!” [0:00:31 - 0:00:32]: Continues viewing the chest's items, appreciating the findings with an exclamation, “Yes! Perfect!” [0:00:33 - 0:00:34]: The chest's contents are examined more closely; rails, redstone, torches, and a name tag are visible. [0:00:35 - 0:00:36]: The player continues to look through the chest, focusing on the various items available for retrieval. [0:00:37 - 0:00:39]: The player expresses gratitude, saying “thank you very much,” as He proceed to collect the desired items from the chest.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What does the player do immediately after the spider lands on the ground?",
        "time_stamp": "0:04:10",
        "answer": "C",
        "options": [
          "A. Runs away.",
          "B. Places a torch.",
          "C. Strikes the spider with an axe.",
          "D. Switches to a sword."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_190_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:02]: The video begins with a top-down view of a village in a green landscape. The village consists of several small buildings with brown rooftops, surrounded by green grass and paths made of dirt blocks. Adjacent to the village is a body of water with a few docks extending into it. The player's inventory bar is visible at the bottom of the screen, showing tools and items such as a diamond sword, pickaxe, and other supplies. [0:08:03 - 0:08:04]: The perspective shifts, and the village begins to recede from view as the camera turns to the right. The scene reveals a larger landscape with forests and plains expanding into the distance. The same black structure is now shown extending outwards, indicating the player is positioned on a higher platform of some structure, possibly flying or standing on an elevated area. [0:08:05 - 0:08:09]: Turning further, a red balloon-like structure appears in the distance, floating above the landscape. The balloon has white trimmings and a basket hanging beneath it, secured to the ground by a long rope. The landscape beneath it features forests, a river, and some mountains further away. The inventory bar shows the player holding a block of gray concrete, indicating a potential building activity. [0:08:10 - 0:08:14]: As the player's view shifts left, the red balloon appears more central in the frame. The surrounding terrain shows mixed biomes with both forested and open grassy areas. The player seems to be moving towards the edge of the structure, possibly aligning for constructing something. [0:08:15 - 0:08:16]: The perspective zooms in on the balloon, highlighting its detailed design with a basket below and ropes anchoring it. The balloon's texture appears made from red wool blocks, and finer details are crafted meticulously. [0:08:17 - 0:08:19]: The camera retreats again, offering a broader overhead view of the landscape. The balloon remains in sight, but the focus shifts back to the structural platform the player is standing on. The black and gray hues of the platform contrast sharply with the greenery below. The player’s inventory shows another gray concrete block, indicating continued construction as more blocks are added while navigating the platform.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color are the rooftops of the village buildings?",
        "time_stamp": "00:08:02",
        "answer": "D",
        "options": [
          "A. Red.",
          "B. Green.",
          "C. Gray.",
          "D. Brown."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_190_real.mp4"
  },
  {
    "time": "[0:12:00 - 0:13:00]",
    "captions": "[0:12:00 - 0:12:20] [0:12:00 - 0:12:01]: The video begins with a view of a large, pyramid-shaped structure. The pyramid is constructed from black and white blocks and is set against a clear blue sky. To the left, a red hot air balloon with intricate designs is floating in the sky. There are small patches of green land and water visible on the lower left side of the screen. [0:12:01 - 0:12:02]: The perspective shifts slightly, moving closer towards the pyramid. The hot air balloon remains to the left, but now a bit more obscured due to the change in angle. [0:12:02 - 0:12:03]: Transitioning further, the pyramid now takes up more of the frame. A small figure in blue appears on the side of the pyramid, ascending the structure. [0:12:03 - 0:12:04]: More of the pyramid is visible now, with the blue-clad figure moving further up. Several orange figures appear near the base of the pyramid to the left, indicating some movement or possible action. [0:12:04 - 0:12:05]: The figures near the base of the pyramid seem to gather, and smoke or a gray cloud is faintly visible on the left side, suggesting some form of activity or disturbance. [0:12:05 - 0:12:06]: The camera continues to move, primarily showing the pyramid and a few figures along its levels. The blue figure has climbed higher, nearing the peak. [0:12:06 - 0:12:07]: The pyramid's intricate stepped design is clear, with the blue figure continuing its ascent. The surroundings, including the left patches of green land and water, are partially visible. [0:12:07 - 0:12:08]: The focus remains on the pyramid as the blue figure navigates along one of its white block paths. The structure's symmetry and design details are prominently displayed. [0:12:08 - 0:12:09]: The pyramid's peak is more centered in the frame, with the blue figure almost reaching the top. Vegetation and landscape details become slightly clearer on the perimeters of the screen. [0:12:09 - 0:12:10]: The camera angle elevates, showing the blue figure near the summit of the pyramid on a white block path. [0:12:10 - 0:12:11]: The blue figure reaches the top of the pyramid. The detailed stepped structure and the vast platform around the pyramid are clearly visible. [0:12:11 - 0:12:12]: The camera starts to move around the pyramid, displaying its meticulously designed steps and symmetry. The blue figure stands near the peak, while the hot air balloon and surrounding landscapes are distant. [0:12:12 - 0:12:13]: The camera angle shifts further, continuing to circle around the pyramid. The blue figure remains at the top, and the sun casts shadows emphasizing the pyramid's geometric shapes. [0:12:13 - 0:12:14]: The pyramid's height and its detailed structure are highlighted as the camera moves. The blue figure is seen slightly descending, while the surrounding regions are partially visible. [0:12:14 - 0:12:15]: The pyramid's extensive stepped design is emphasized, with the camera angle showcasing its height from various perspectives.  [0:12:15 - 0:12:16]: The view shifts to show more of the pyramid’s surroundings, with additional greenery visible on the perimeter. The blue figure appears smaller as it remains near the top. [0:12:16 - 0:12:17]: The structure's intricate pattern is prominently displayed as the camera continues to move around it. More landscape details such as water and grass patches become visible. [0:12:17 - 0:12:18]: The blue figure moves slightly lower on the pyramid steps. The camera angle broadens, giving a comprehensive view of the entire structure and some of the surrounding environment. [0:12:18 - 0:12:19]: The pyramid remains the focal point, with the blue figure still in view but lower on the steps. The surrounding landscape, including patches of land and water, is more distinctly visible.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is floating in the sky to the left of the pyramid right now?",
        "time_stamp": "00:12:01",
        "answer": "A",
        "options": [
          "A. A red hot air balloon.",
          "B. A blue kite.",
          "C. A white bird.",
          "D. A black drone."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_190_real.mp4"
  },
  {
    "time": "[0:15:00 - 0:15:52]",
    "captions": "[0:15:40 - 0:15:52] [0:15:40 - 0:15:43]: The video begins with a first-person perspective, looking at a large, circular structure suspended in the sky. The structure is supported by a narrow, vertical column that extends down to the landscape below. The landscape consists of grassy plains and a few scattered trees. The sky is mostly clear with a few clouds. [0:15:44 - 0:15:47]: The perspective shifts to a third-person view showing a character standing on a wooden platform. The character is wearing blue armor and is positioned towards the center of the frame. In the background, the large, circular structure remains prominent in the sky. [0:15:48 - 0:15:51]: The character continues to stand on the platform, slightly adjusted in position. A red “Subscribe” button with a white YouTube logo appears at the bottom of the frame. The landscape in the background includes rivers and forests. [0:15:52 - 0:15:55]: The “Subscribe” button changes to include a clicking animation, indicating the action of subscribing. The character on the platform remains fixed in position with the circular structure still visible in the sky. [0:15:56 - 0:15:57]: The animation of the “Subscribe” button concludes, and it changes to a “Subscribed” button with a checkmark. The character on the platform appears unchanged, still wearing blue armor. [0:15:58 - 0:15:59]: The video briefly shows the third-person perspective of the character on the platform, with the \"Subscribed\" button still displayed. The background features the wide landscape of rivers, forests, and the large, floating structure.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What does the red button at the bottom of the frame display initially?",
        "time_stamp": "0:15:51",
        "answer": "A",
        "options": [
          "A. A “Subscribe” button with a white YouTube logo.",
          "B. A “Like” button with a white thumbs-up icon.",
          "C. A “Share” button with a white arrow icon.",
          "D. A “Follow” button with a white star icon."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_190_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video opens with a first-person perspective looking up at a tall residential building. There is a red sign in the foreground, slightly to the right, adorned with several advertisements. The building façade displays numerous windows and air conditioning units. A yellow logo and the words \"HONG KONG CITYWALK\" are prominently featured over the red sign.  [0:00:04 - 0:00:07]: The camera tilts slightly downward, still gazing upwards at the building. Overlaid text appears, displaying \"2023 / 7 / 7\", \"Hong Kong\", \"Tsim Sha Tsui\", and \"7:00pm\". The angle shifts to include a busy street scene with vehicles and pedestrians. [0:00:05 - 0:00:07]: A red double-decker bus moves through the frame from left to right. The background features multiple commercial buildings, including a large screen displaying advertisements, and the entrance to a transportation hub is visible on the right. [0:00:08 - 0:00:09]: The camera continues descending, focusing closer on the street level. Now, the entrance to the Tsim Sha Tsui MTR station is prominently seen on the right, with people walking past it. [0:00:10 - 0:00:12]: There's a clear view of the MTR station entrance, with the station name lit up in red and white. The camera perspective shows various pedestrians walking in and out of the station, alongside a few walking on the cobblestone pedestrian path. [0:00:13 - 0:00:16]: The scene captures more street-level detail. The pathway is occupied by various pedestrians walking in both directions. Shops and commercial establishments line the pathway, displaying products in their windows. [0:00:17 - 0:00:19]: The camera continues to focus on the bustling pedestrian walkway. More people walk by, with some heading toward the MTR station entrance and others moving further along the path. The video concludes with an extended view of the vibrant street scene, capturing the ongoing hustle and bustle of the city area.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What details are displayed in the overlaid text just now?",
        "time_stamp": "0:00:13",
        "answer": "D",
        "options": [
          "A. \"2023 / 6 / 7\", \"Hong Kong\", \"Tsim Sha Tsui\", and \"8:00pm\".",
          "B. \"2022 / 7 / 7\", \"Hong Kong\", \"Central\", and \"7:00pm\".",
          "C. \"2023 / 7 / 7\", \"Hong Kong\", \"Tsim Sha Tsui\", and \"6:00pm\".",
          "D. \"2023 / 7 / 7\", \"Hong Kong\", \"Tsim Sha Tsui\", and \"7:00pm\"."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_332_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:45]: The video shows a display in a shopping mall, centered on a large structure designed to look like a bridge made of brick with an archway. On the bridge are large cat figures in a variety of colors (black, orange, white, and others). The surrounding area features various brands and storefronts with colorful and bright lighting.  [0:02:46 - 0:02:48]: The camera pans to the right, revealing a large, round, yellow structure and a white figure that resembles a cloud. There are people walking around, some looking at their phones, and others appearing to admire the display.  [0:02:49 - 0:02:55]: As the camera continues to move, more of the yellow structure becomes visible, which is now seen to be a large fruit or vegetable with a green stem-like element. There is a woman wearing a mask and taking a photo of the display. The scene shows various other decorative elements. [0:02:56 - 0:02:59]: Moving further to the right, the camera captures more detailed views of the decorations, including a large white cat figure sitting among pink flowers. The yellow structure is part of an exhibit, with branding and additional decor in the background. The scene shows more people interacting with the display.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What colors are the largest cat figures on the bridge right now?",
        "time_stamp": "0:03:00",
        "answer": "D",
        "options": [
          "A. Orange.",
          "B. Black.",
          "C. grey.",
          "D. white."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Event Understanding",
        "question": "What is the woman wearing a mask doing right now?",
        "time_stamp": "0:02:55",
        "answer": "D",
        "options": [
          "A. Talking on the phone.",
          "B. Walking around.",
          "C. Looking at her phone.",
          "D. Taking a photo."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_332_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:29]: The video shows a busy urban street with tall buildings lined on either side. The buildings are mainly rectangular with lots of windows and colorful signage, illuminated against a dusky sky. Numerous pedestrians are walking across a yellow zebra crossing divided by intermittent dashed yellow lines. Some pedestrians are carrying shopping bags, while others appear to be having conversations. On the left side of the street, multiple storefronts display bright signs—some predominantly yellow, some pink, and others with varying colors. There is significant vehicular traffic, including a bus approaching from the background, and several stationary vehicles visible. [0:05:30 - 0:05:39]: The street continues to get busier as more people cross the road, and vehicles move closer. A red bus and several cars are visible, some of them moving. Pedestrians on the right side of the street near the sidewalk are seen walking, standing, and interacting. One man in an orange shirt stands near the foreground, apparently speaking on his phone. Billboards with advertisements in bright colors, including yellow and red, capture attention. The building lights create a lively urban night setting with various shops and signs illuminating the area.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the zebra crossing that pedestrians are walking across right now?",
        "time_stamp": "0:05:29",
        "answer": "D",
        "options": [
          "A. White.",
          "B. Blue.",
          "C. Red.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_332_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:05]: The video starts with a view of a bustling city sidewalk. On the left, there is a building with bright white lights above its entrance area. Several people are walking in the same direction as the camera, while others are moving in the opposite direction. A large yellow and red bus is parked on the right side, with potted plants filled with colorful flowers placed between the bus and the sidewalk. The sky is overcast, hinting at the late evening or early twilight. [0:08:06 - 0:08:09]: The camera continues moving forward along the sidewalk. A man in a light gray shirt and light blue jeans walks toward the camera, while several other pedestrians, including a group of three people dressed in casual attire, are observed walking in the opposite direction. A stall selling various items is visible on the left side. [0:08:10 - 0:08:13]: The camera moves past the stall selling various items, which is located on the left side of the sidewalk. Ahead, more people are observed walking in clusters, some heading towards the camera, while others move away. The large building on the left has a facade made of white and gray materials with garage-like doors at regular intervals. [0:08:14 - 0:08:17]: The sidewalk remains crowded with pedestrians, and the camera maintains a steady pace forward. The street to the right appears busy with vehicles, and the potted plants filled with colorful flowers continue to line the edge of the sidewalk. The bus from the earlier scene is still visible, stationary, creating a constant backdrop in this urban landscape. [0:08:18 - 0:08:20]: The video concludes with the camera continuing down the same sidewalk, nearing the edge of the bus. More pedestrians are walking around, including some children. The background features tall buildings with lit signage and visible green-leaved palm trees, maintaining the bustling city atmosphere.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the overall atmosphere of the scene in the video?",
        "time_stamp": "0:08:20",
        "answer": "D",
        "options": [
          "A. Calm and serene.",
          "B. Quiet and empty.",
          "C. Tense and chaotic.",
          "D. Bustling and busy."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_332_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: A person's hand holds a paintbrush and begins to paint a sketched portrait on a white canvas. The brush applies dark brown paint to the upper left area of the head, suggesting the start of painting hair. [0:00:02 - 0:00:06]: The painting progresses with more dark brown paint added to the left side of the head, covering more hair area as the hand continues to brush downward. [0:00:07 - 0:00:09]: The hand moves the brush to the right side of the head and begins to apply dark brown paint near the right ear of the sketched portrait.  [0:00:10 - 0:00:12]: The brush continues to add dark brown paint around the right ear and then moves downward to fill in more of the hair. [0:00:13 - 0:00:15]: More paint is added to the upper and right side of the head, further defining the hair and giving it more volume and shape. [0:00:16 - 0:00:17]: The hand holding the paintbrush moves back to the left side of the head and starts adding brown paint to fill in more of the hair, with the color becoming richer and more defined. [0:00:18 - 0:00:19]: The brush continues to blend and perfect the hair, ensuring even coverage and more detailed texture. The hair on the portrait starts to take a more complete shape, emphasizing the overall look of the sketched portrait.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Which side of the head does the person paint first?",
        "time_stamp": "0:00:09",
        "answer": "B",
        "options": [
          "A. The top.",
          "B. The left side.",
          "C. The right side.",
          "D. The back."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_129_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:25]: The video begins with a close-up view of a painting in progress, depicting a person with short brown hair, a slightly distressed expression, and a white shirt with a bit of black being added. The painter, using a paintbrush, focuses on adding darker shades to the collar area, creating a shadow effect to enhance the depth of the shirt. [0:02:26 - 0:02:29]: The painter continues to work on the figure’s collar, reinforcing the area with bold black brushstrokes. The background remains a plain white canvas, directing focus on the subject’s face and upper body. [0:02:30 - 0:02:32]: The focus shifts to the clothing of the subject. The painter starts to add red to the figure's upper garment, beginning to define what looks like a jacket. The colors are vibrant, with red dominating the garment. [0:02:33 - 0:02:36]: The detail in the face stays consistent as the artist transitions to adding more highlights and depth to the hair. The brush moves swiftly, accentuating lighting and shadows. [0:02:37 - 0:02:39]: The artist finalizes the painting details. The jacket’s shades and folds are defined, enhancing the three-dimensionality of the portrait. The background remains untouched, ensuring the subject stands out prominently.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the painting process that was just shown in the video?",
        "time_stamp": "0:02:39",
        "answer": "B",
        "options": [
          "A. Adding final touches to a landscape.",
          "B. Detailing a person with short brown hair and a red coat.",
          "C. Painting an abstract art piece.",
          "D. Working on a cityscape with vibrant colors."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_129_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:42]: The video shows a detailed painting of a person wearing a red jacket. The painting has a realistic quality, focusing on the upper body and face of the subject. The background is white, and around the canvas are objects partially visible, like a plant with green leaves on the left side and colorful abstract shapes on the wall. [0:04:43 - 0:04:52]: As the video progresses, a hand holding a thin paintbrush appears in the frame, refining details on the face, particularly around the eyes and mouth area. The hand moves the brush precisely, enhancing the realistic texture and depth of the artwork. [0:04:53 - 0:04:55]: The paintbrush shifts to other parts of the face, adjusting the shading on the left eyebrow and the lower chin. The artist’s hand shows controlled, deliberate strokes as they work. [0:04:56 - 0:04:59]: The final touches are applied to the facial features, emphasizing expressions. The overall video focuses on the process of refining the painting, showcasing the artist’s precision and attention to detail.",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why is the artist applying touches to the facial features frequently?",
        "time_stamp": "00:04:59",
        "answer": "B",
        "options": [
          "A. To correct mistakes.",
          "B. To emphasize expressions.",
          "C. To add abstract elements.",
          "D. To change the background color."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_129_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:20]: The video starts with a focus on a detailed painting of a person, capturing a realistic expression. The painting has vibrant colors, with the person wearing a red jacket. The background is mainly white with some subtle color gradients making the portrait more striking. The scene is set in a studio, with some plants visible on the left side and art supplies, such as a computer or tablet, on a table beneath the painting. The artist's hand starts to appear around [0:07:03], holding a paintbrush and making precise strokes on the canvas. The hand reaches towards the top part of the painting, around the forehead, [0:07:10], adding details to the hair. The artist continues to work on fine details in the hair and other areas like the ear and background, adding subtle strokes and adjustments till the end of this segment. The video captures the careful and deliberate motions of the artist as they enhance the already detailed painting.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the plants located in relation to the painting in the studio?",
        "time_stamp": "00:07:20",
        "answer": "C",
        "options": [
          "A. To the right side of the painting.",
          "B. On a table beneath the painting.",
          "C. On the left side of the painting.",
          "D. Behind the artist."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the content that was just shown in the video?",
        "time_stamp": "00:07:20",
        "answer": "B",
        "options": [
          "A. The artist is setting up the studio before starting to paint.",
          "B. The artist is making precise adjustments to an already detailed portrait.",
          "C. The artist is selecting colors and preparing brushes for a new painting.",
          "D. The artist is cleaning up the studio after completing a painting."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_129_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the older woman appear annoyed or angry?",
        "time_stamp": "00:00:26",
        "answer": "B",
        "options": [
          "A. Because the character shouted loudly.",
          "B. Because the character disturbed her.",
          "C. Because the character knocked on the door.",
          "D. Because the character broke something."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_249_real.mp4"
  },
  {
    "time": "[0:02:15 - 0:02:45]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why is Mr. Bean under the bed now?",
        "time_stamp": "0:02:25",
        "answer": "A",
        "options": [
          "A. Because he wanted to snatch back the teddy bear doll that was taken by the cat.",
          "B. Because he dropped something valuable and is trying to retrieve it.",
          "C. Because he is hiding from someone who just entered the room.",
          "D. Because he thought he saw something strange under the bed."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_249_real.mp4"
  },
  {
    "time": "[0:04:30 - 0:05:00]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why did the character exit the building and approach a green car?",
        "time_stamp": "0:06:02",
        "answer": "C",
        "options": [
          "A. Because he forgot something important in the car.",
          "B. Because he needs to pick up a friend from the nearby station.",
          "C. Because he wants to drive to the pet hospital to treat his teddy bear doll.",
          "D. Because he plans to run an errand across town."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_249_real.mp4"
  },
  {
    "time": "[0:06:45 - 0:07:15]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why Mr. Bean would be very panicked now.",
        "time_stamp": "0:08:38",
        "answer": "C",
        "options": [
          "A. Because he realized he lost his wallet.",
          "B. Because he accidentally locked himself out of his house.",
          "C. Because he found that his teddy bear doll was not in the box.",
          "D. Because he noticed that his car keys were missing."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_249_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:30]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why did the character react with noticeable concern after stopping the car?",
        "time_stamp": "00:10:17",
        "answer": "C",
        "options": [
          "A. Because they remember they left something important behind.",
          "B. Because they have car trouble and need to check the engine.",
          "C. Because he thought the one that was flattened was the old lady's cat.",
          "D. Because they see a strange figure in the distance."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_249_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located on the right side of the road right now?",
        "time_stamp": "00:00:15",
        "answer": "A",
        "options": [
          "A. A river.",
          "B. A forest.",
          "C. A field.",
          "D. A building."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_185_real.mp4"
  },
  {
    "time": "[0:02:02 - 0:02:22]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located on the left side of the path right now?",
        "time_stamp": "00:02:22",
        "answer": "A",
        "options": [
          "A. A line of trees.",
          "B. A lake.",
          "C. A canal.",
          "D. A field."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_185_real.mp4"
  },
  {
    "time": "[0:04:04 - 0:04:24]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is on the right side of the path right now?",
        "time_stamp": "00:04:10",
        "answer": "A",
        "options": [
          "A. A river.",
          "B. A grassy field.",
          "C. A dense forest.",
          "D. A parking lot."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_185_real.mp4"
  },
  {
    "time": "[0:06:06 - 0:06:26]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the canal located right now?",
        "time_stamp": "00:06:26",
        "answer": "A",
        "options": [
          "A. On the right side of the path.",
          "B. On the left side of the path.",
          "C. Directly behind the cyclist.",
          "D. Directly ahead of the cyclist."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_185_real.mp4"
  },
  {
    "time": "[0:08:08 - 0:08:28]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is on the right side of the path right now?",
        "time_stamp": "00:08:18",
        "answer": "A",
        "options": [
          "A. A river.",
          "B. A forest.",
          "C. A mountain.",
          "D. A road."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_185_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: A race track curves from the left foreground to the right background. The track is black, with a smooth, shiny surface. On the left side, there is a chain-link fence with colorful signage behind it. There are miniature cars and objects near the fence. The backdrop is a scenic mural with a blue sky, white clouds, green hills, and trees. The perspective indicates a downhill slope. [0:00:02 - 0:00:05]: The initial scene with the race track transitions to a black screen displaying a distorted logo with the text \"NEXT GEN DIECAST\" and \"The Next Generation of Diecast Racing.\" The text and logo appear glitchy and colorful. [0:00:06]: The logo and text stabilize, remaining clear and centered on a black background with the same message, \"NEXT GEN DIECAST\" and \"The Next Generation of Diecast Racing.\" [0:00:07 - 0:00:08]: Back to a race track scene, showing a close-up view of three diecast cars lined up from left to right: a red car, a silver car, and a green car with “N2O COLA” written on it. The track's surface has lane dividers and is surrounded by artificial grass. There are Thunder Bunny Racing signs and other signage around. [0:00:09 - 0:00:13]: Text “NEXT-GEN PISTON CUP” and “Race 1 - Final” overlays the scene with the three cars. The cars remain stationary, lined up at the starting line. The background shows parts of the chain-link fence and the area behind it. [0:00:14 - 0:00:20]: The scene continues focusing on the starting line of the race. The text \"Race 1 - Final\" remains superimposed over it. The green car in the center lane slightly moves forward, suggesting the beginning of the race. The surrounding signs and track layout stay the same.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is written on the green car in the race track scene?",
        "time_stamp": "0:00:08",
        "answer": "A",
        "options": [
          "A. N2O COLA.",
          "B. Speedy Cola.",
          "C. Racing Fuel.",
          "D. Nitro Power."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Clips Summarize",
        "question": "What main event is depicted in the captions?",
        "time_stamp": "0:00:20",
        "answer": "B",
        "options": [
          "A. A diecast car race victory lap.",
          "B. The beginning of a car model race.",
          "C. The construction of a race track.",
          "D. The final lap of a race."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_484_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:23]: Four toy race cars, red, blue, gray, and another color, are on a black race track at various positions. The track has a fenced pit stop area on the right, where four small toy trucks are parked in front of a garage. The setting includes a background with green hills and trees under a bright sky; [0:05:24 - 0:05:29]: The cars drive past the pit stop area and are now on a different section of the race track, which curves around a hillside. There are signs in the background indicating sponsors or racing teams. The landscape is lush with greenery and a painted sky with clouds; [0:05:30 - 0:05:32]: The cars continue on the curving track. The camera view shifts to focus on a part of the race track that has a green grassy area in the middle of a curve. There are more toy trucks and cars visible off the track, along with racing signs; [0:05:33 - 0:05:34]: The cars are still racing, moving steadily along the track while the background scenery maintains a consistent look of green hills and painted sky; [0:05:35 - 0:05:39]: The first blue car moves further ahead from the other cars on the track past the pit stop area, while the other cars follow behind. The pit stop area scenery remains unchanged.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What happens to the green-purple car car when the racing car sprints?",
        "time_stamp": "00:05:42",
        "answer": "D",
        "options": [
          "A. It falls behind other cars.",
          "B. It crashes into the pit stop area.",
          "C. It leaves the race track.",
          "D. It moves further ahead from the other cars."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_484_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00]: The video starts with a racing track in view. A silver racing car with purple accents speeds by on a black track surface. The track is elevated and has a fence on the right side. In the background, there are green hills and a white building resembling a pit stop or garage area with several toy-like vehicles. [0:08:01 - 0:08:10]: The track times for the race are displayed on screen. The background remains consistent, showing the racing track and the green hills in the backdrop. The track times board lists the top five racers: #39 Michael Rotor with a time of 8.103 seconds, #94 Jay with a time of 8.267 seconds, #35 Ronald with a time of 8.348 seconds, #28 Tim Treadless with a time of 8.417 seconds, and #123 Jonas Carvers with a time of 8.485 seconds. The display stays on screen while the camera perspective remains the same. [0:08:11 - 0:08:19]: The standings for Race 1 - Final are shown. The racers are listed with their corresponding points: #68 H.J. Hollis is in the first place with 13 points, #39 Michael Rotor is in the second place with 12 points, and #123 Jonas Carvers is also in the second place with 12 points (noted by *1 indicating an asterisk for some condition), and #57 Junyi is in the third place with 6 points. The background remains consistent with the previous shots, showing the end of the racing track and the garage area.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Who is in first place in Race 1 - Final standings?",
        "time_stamp": "0:08:19",
        "answer": "C",
        "options": [
          "A. Michael Rotor.",
          "B. Jay.",
          "C. H.J. Hollis.",
          "D. Jonas Carvers."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_484_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:45]",
    "captions": "[0:09:40 - 0:09:45] [0:09:40 - 0:09:42]: A racetrack with a black surface is visible, bordered by a fence on the right side and green grass on the left. Multiple toy racecars of different colors, including green, blue, and purple, are moving along the track. In the background, there's a white garage with an open front displaying various miniature vehicles such as forklifts, and a grassy hill. [0:09:42 - 0:09:44]: The blue and red racecars continue to move forward while the purple racecar exits the frame. A gray racecar appears from the background, heading towards the foreground. The landscape remains the same, with a glimpse of trees and more greenery visible in the distance. [0:09:44 - 0:09:45]: The red racecar continues to move closer to the foreground, while the gray racecar follows behind. The background elements, including the white garage and the miniature vehicles, remain unchanged. The fence on the right side of the track is consistently visible.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the sequence of the racecars when the racecars sprints?",
        "time_stamp": "0:09:45",
        "answer": "A",
        "options": [
          "A. Red racecar followed by blue racecar.",
          "B. Blue racecar followed by green racecar.",
          "C. Purple racecar followed by blue racecar.",
          "D. Gray racecar followed by red racecar."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_484_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What type of sign is displayed on the pole right now?",
        "time_stamp": "00:00:04",
        "answer": "A",
        "options": [
          "A. Bus zone sign.",
          "B. Speed limit sign.",
          "C. No parking sign.",
          "D. Pedestrian crossing sign."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_450_real.mp4"
  },
  {
    "time": "[0:01:52 - 0:01:57]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the car visible in the distance on the right side of the road?",
        "time_stamp": "00:01:54",
        "answer": "A",
        "options": [
          "A. White.",
          "B. Red.",
          "C. Blue.",
          "D. Black."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_450_real.mp4"
  },
  {
    "time": "[0:03:44 - 0:03:49]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the sign visible on the left side of the road right now?",
        "time_stamp": "00:03:47",
        "answer": "A",
        "options": [
          "A. Blue.",
          "B. Red.",
          "C. Yellow.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_450_real.mp4"
  },
  {
    "time": "[0:05:36 - 0:05:41]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color are the parking signs visible on the right side of the road right now?",
        "time_stamp": "00:03:47",
        "answer": "A",
        "options": [
          "A. Blue.",
          "B. Red.",
          "C. Yellow.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_450_real.mp4"
  },
  {
    "time": "[0:07:28 - 0:07:33]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type car is the orange car visible right now?",
        "time_stamp": "00:07:29",
        "answer": "A",
        "options": [
          "A. SUV.",
          "B. Van.",
          "C. Bus.",
          "D. Truck."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_450_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why is Mr. Bean scolded by the older woman?",
        "time_stamp": "00:00:27",
        "answer": "A",
        "options": [
          "A. Because he was making too much noise.",
          "B. Because he was late.",
          "C. Because he was shining a flashlight at the teddy bear.",
          "D. Because he was not paying attention."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_234_real.mp4"
  },
  {
    "time": "[0:02:02 - 0:02:32]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the character place the cheese on a plate?",
        "time_stamp": "0:02:35",
        "answer": "B",
        "options": [
          "A. Because he plans to make a sandwich with the cheese.",
          "B. Because his refrigerator is empty except for a piece of vegetable.",
          "C. Because he thinks the cheese will taste better at room temperature.",
          "D. Because the character organizes the fridge."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_234_real.mp4"
  },
  {
    "time": "[0:04:04 - 0:04:34]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the man successfully retrieve the bottle from the top shelf?",
        "time_stamp": "00:04:24",
        "answer": "A",
        "options": [
          "A. Because he uses his shopping cart for support.",
          "B. Because he asks for assistance from a store employee.",
          "C. Because he finds a step stool to reach the bottle.",
          "D. Because he uses a long tool to knock the bottle down."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_234_real.mp4"
  },
  {
    "time": "[0:06:06 - 0:06:36]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why Mr. Bean is very shocked.",
        "time_stamp": "00:06:40",
        "answer": "D",
        "options": [
          "A. Because his own pie just fell to the floor.",
          "B. Because the opponent did a magic trick with the pie.",
          "C. Because the pie suddenly exploded when the opponent touched it.",
          "D. Because the opponent finished the pie in one go."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_234_real.mp4"
  },
  {
    "time": "[0:08:08 - 0:08:38]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the Mr. Bean look tired or exhausted?",
        "time_stamp": "0:08:36",
        "answer": "B",
        "options": [
          "A. Because he had to run around the supermarket to find items.",
          "B. Because he lost the competition in the supermarket.",
          "C. Because he stayed up all night preparing for the competition.",
          "D. Because he had to carry heavy bags out of the supermarket."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_234_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:09]: A man with blonde hair and a serious expression stands in front of a stainless steel refrigerator, wearing a navy blue T-shirt. The background consists of white subway tiles on the wall. He begins speaking to the camera with his hands clasped in front of him and occasionally gestures while talking. A window is visible to his right side, showing an outdoor scene with greenery. [0:00:10 - 0:00:15]: The man continues to speak, gesturing with his hands to emphasize his points. His expression occasionally changes to a smile. [0:00:16]: The scene changes to a graphic with the number 10 and an image of a spoon, all displayed on a blue background. [0:00:17 - 0:00:19]: A new scene shows the man from the previous frames dressed in a white chef's coat, arms crossed, standing in front of the text \"RAMSAY in 10\" with a similar blue background.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the man standing in front of in the initial video?",
        "time_stamp": "00:00:20",
        "answer": "D",
        "options": [
          "A. A wooden cabinet.",
          "B. A glass door.",
          "C. A blackboard.",
          "D. A stainless steel refrigerator."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_41_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:00]: An arm is moving across a white bowl, partially blocking the view. The bowl contains a white substance, possibly a batter or sauce, situated on a kitchen counter. There are other kitchen items situated near the bowl, but their details are unclear due to the arm's obstruction. [0:01:01 - 0:01:02]: A woman with long brown hair, wearing a black sleeveless top, is holding a jar in her right hand while her left hand is near a white bowl on the counter. There is a loaf of bread, a carton of eggs, a carton of milk, and various kitchen utensils spread out on the marble countertop. Behind the woman is a stainless steel refrigerator, a tiled white wall with a window showing greenery outside, and a cabinet with potted plants. [0:01:02 - 0:01:03]: The woman opens the jar she is holding and looks down at the countertop. She is positioned slightly to the left of the refrigerator doors. A black frying pan is seen next to the bowl on top of a stove. [0:01:03 - 0:01:04]: A close-up view of the white bowl displays two cracked eggs with yolks visible, resting in the white substance. The background reveals parts of the stove and a dark object, possibly a piece of cloth, next to the bowl. [0:01:04 - 0:01:04]: The woman's hand, with neatly manicured nails, is holding a jar of seasoning above the bowl. The seasoning appears to be about to be sprinkled onto the eggs and white substance in the bowl. [0:01:05 - 0:01:06]: The woman begins to scoop some seasoning from the jar with a metal spoon. The ridged surface of the countertop and part of the stove can be seen in the background.  [0:01:06 - 0:01:07]: The woman’s hands are shown holding the jar and the spoon, preparing to sprinkle the seasoning onto the eggs. The white bowl remains stationary on the counter. [0:01:07 - 0:01:08]: Close-up of the woman’s hands sprinkling the seasoning directly into the bowl over the eggs. The black stove and its components are visible in the background. [0:01:08 - 0:01:09]: A close-up view of the bowl reveals the sprinkled seasoning on top of the egg yolks and whites. The substance mixture surrounds the eggs in the bowl. [0:01:09 - 0:01:10]: The eggs and seasoning in the bowl are clearly shown. The detailed texture of the stove surface next to the countertop is visible. [0:01:11 - 0:01:12]: The woman turns to her left, reaching towards an unseen object. The countertop still displays a loaf of bread, egg carton, and other items, while the refrigerator and tiled wall remain in the background. [0:01:12 - 0:01:13]: The woman is seen holding a lemon in her right hand as she looks down towards the countertop. Her left hand reaches towards the items on the marble counter. [0:01:13 - 0:01:14]: The woman begins to zest the lemon over the bowl with a zester held in her left hand. Her focus is directed towards the bowl. [0:01:14 - 0:01:15]: The woman continues zesting the lemon over the bowl, and the yellow zest starts to fall onto the egg mixture. The surrounding kitchen items maintain their positions on the counter. [0:01:15 - 0:01:16]: Close-up view of the lemon being zested, with yellow lemon zest visible over the egg mixture in the bowl. The setup of the kitchen counter and the stove surface remains the same. [0:01:16 - 0:01:17]: More lemon zest falls into the bowl as the woman continues to zest the lemon. The lemon zest starts to accumulate on top of the egg mixture. [0:01:17 - 0:01:18]: The woman continues to zest the lemon while looking intently at the bowl. Her left hand holds the zester firmly, and she is focused on grating more zest. [0:01:18 - 0:01:19]: The woman briefly pauses, still holding the lemon and zester. She turns her head to the left as if to glance at something off-camera. The kitchen counter, tools, and stove are in their original positions.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the woman holding in her left hand?",
        "time_stamp": "0:01:04",
        "answer": "D",
        "options": [
          "A. A loaf of bread.",
          "B. A carton of milk.",
          "C. A frying pan.",
          "D. A jar."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Action Recognition",
        "question": "What does the woman do after opening the jar?",
        "time_stamp": "0:01:03",
        "answer": "A",
        "options": [
          "A. Pour the seasoning inside it into the white bowl with eggs.",
          "B. Looks out the window.",
          "C. Moves to the refrigerator.",
          "D. Picks up a loaf of bread."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_41_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00]: A woman with long dark hair, wearing a sleeveless black top, stands in a kitchen holding a white shaker bottle with a black top. The background features stainless steel appliances and white tiles on the walls. [0:02:01 - 0:02:05]: The woman is in front of a marble kitchen counter full of ingredients: a large white bowl, a smaller bowl, a loaf of bread, an egg carton, some butter, a brown plate, and various containers. She is reaching for a box of cereal. [0:02:05 - 0:02:07]: She starts opening the cereal box, holding it with both hands, and situating it in front of her. Various ingredients remained spread across the counter. [0:02:07 - 0:02:08]: She opens the cereal box and prepares to pour its contents. Next to the ingredients on the counter are a large white bowl, butter, eggs, spices, and other containers. [0:02:09 - 0:02:14]: She pours the cereal from the box into a clear shaker bottle held by her left hand. The camera captures her movements in front of the stainless steel fridge and kitchen cabinets.  [0:02:15 - 0:02:19]: The woman continues pouring cereal into the shaker bottle until it is filled. Various kitchen items like a knife, a green lid, spices, and butter are on the counter next to her.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the woman doing right now?",
        "time_stamp": "0:02:00",
        "answer": "A",
        "options": [
          "A. Pour the cereal from the box into a transparent glass of water.",
          "B. Pour the sugar from the jar into a cup of hot tea.",
          "C. Pour the pasta from the bag into a pot of boiling water.",
          "D. Pour the flour from the container into a bowl of milk."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_41_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:01]: A close-up of a hand holding a slice of bread, dipping it into a bowl filled with a yellow custard-like mixture. The bread is slightly submerged, and the hand is positioned on the right side of the bowl, guiding the bread in with careful precision. [0:04:01 - 0:04:02]: The hand continues to dip the slice of bread into the mixture, covering it thoroughly. The fingers are slightly spread to maintain a firm grip on the bread, and the bowl is white with a ribbed texture on the outside. [0:04:02 - 0:04:03]: The camera zooms out to reveal a woman standing in a modern, well-lit kitchen. She is placing the bowl with the dipped bread on the counter while focusing on the task at hand. There are two slices of bread, a carton of eggs, and a mixing bowl on the countertop. [0:04:03 - 0:04:04]: The woman continues to focus on preparing the bread, now using a spatula to ensure the bread is evenly coated with the mixture. The stainless steel refrigerator and the white subway tile backsplash are visible in the background.  [0:04:04 - 0:04:05]: The woman shifts her attention to the bowl, lifting the soaked bread with the spatula. The bread is visibly soaked in the yellow mixture, and she is careful to let any excess drip back into the bowl before moving it. [0:04:05 - 0:04:06]: She holds the bread over the bowl, allowing any extra liquid to drip off before moving it to the hot surface. Her movements are precise and deliberate, ensuring minimal mess. [0:04:06 - 0:04:07]: As she continues holding the bread, she maintains a steady grip with her left hand, slightly tilting the bowl to prevent spills. The kitchen is bright, with natural light illuminating the space through a large window. [0:04:07 - 0:04:08]: She places the soaked bread back into the bowl briefly, possibly to adjust or soak the other side evenly. The other kitchen items remain in their original positions on the counter. [0:04:08 - 0:04:09]: She gently shakes the bread to ensure it is thoroughly coated, preparing to move it to the cooking surface. The woman’s focus remains on her task, showing a methodical approach to her cooking. [0:04:09 - 0:04:10]: The bread is then carefully transferred to another container or surface, outside the current frame, likely for cooking. Her hands are steady, and the countertop remains clean and organized. [0:04:10 - 0:04:11]: Another close-up shows the bread being dipped into a different mixture, potentially for another coating. The texture of the mixture appears consistent and smooth. [0:04:11 - 0:04:12]: The bread receives another thorough coating, with the hands ensuring all sides are covered uniformly. The white bowl used earlier is still present and filled with the mixture. [0:04:12 - 0:04:13]: The camera zooms out to reveal the woman back at the counter, focusing on preparing the next piece of bread. The kitchen’s modern appliances and decor, including the potted plant by the window, are visible in the background. [0:04:13 - 0:04:14]: She sets down the coated bread, possibly on a cooking surface, and prepares to reach for another slice. Her attention to detail is evident as she maintains the workspace. [0:04:14 - 0:04:15]: The woman momentarily pauses, perhaps to adjust an item or check the progress of the cooking bread. The kitchen remains brightly lit, showcasing its clean and modern design. [0:04:15 - 0:04:16]: She reaches towards the left side of the counter, picking up another kitchen tool or ingredient. The focus remains on her consistent and deliberate actions. [0:04:16 - 0:04:17]: She prepares another slice of bread for dipping, ensuring it's evenly sliced and ready for the mixture. The countertop reflects the bright lighting of the kitchen. [0:04:17 - 0:04:18]: The bread is being sliced carefully, with a knife held in her right hand. She continues to maintain a methodical approach to the preparation. [0:04:18 - 0:04:19]: A close-up of the bread being dipped into the mixture again shows careful attention to even coating. The yellow mixture appears thick and smooth.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What does the woman use to ensure the bread is evenly coated with the mixture?",
        "time_stamp": "0:04:04",
        "answer": "D",
        "options": [
          "A. A spoon.",
          "B. Her fingers.",
          "C. A fork.",
          "D. A spatula."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_41_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What mathematical concept will likely be demonstrated next?",
        "time_stamp": "00:00:09",
        "answer": "D",
        "options": [
          "A. Subtraction.",
          "B. Addition.",
          "C. Multiplication.",
          "D. Division."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_229_real.mp4"
  },
  {
    "time": "[0:02:42 - 0:03:12]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What is the presenter likely to do next?",
        "time_stamp": "00:03:12",
        "answer": "D",
        "options": [
          "A. Add 48 to 52.",
          "B. Multiply 48 by 52.",
          "C. Divide 48 by 52.",
          "D. Subtract 48 from 52."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_229_real.mp4"
  },
  {
    "time": "[0:05:24 - 0:05:54]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:05:38",
        "answer": "A",
        "options": [
          "A. The steps to perform the division.",
          "B. The final quotient of the division.",
          "C. The relevance of the remainder.",
          "D. How to multiply the dividend."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_229_real.mp4"
  },
  {
    "time": "[0:08:06 - 0:08:36]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker do next?",
        "time_stamp": "00:08:44",
        "answer": "A",
        "options": [
          "A. Subtract 440 from 528.",
          "B. Divide 528 by 55.",
          "C. Subtract 88 from 528.",
          "D. Add 5 to 88."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_229_real.mp4"
  },
  {
    "time": "[0:10:48 - 0:11:18]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:11:30",
        "answer": "A",
        "options": [
          "A. How to address the situation where the remainder is less than the divisor.",
          "B. How to check the division result.",
          "C. How to proceed with the next division step.",
          "D. How to interpret the decimal expansion."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_229_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the barista's actions just now?",
        "time_stamp": "00:00:14",
        "answer": "D",
        "options": [
          "A. The barista cleaned the counter, sanitized cups, and organized coffee beans on the shelves.",
          "B. The barista wiped down the espresso machine, added a new milk container, and prepared a cappuccino.",
          "C. The barista organized the espresso machine, ground fresh coffee beans, and created a detailed latte art design.",
          "D. The barista cleaned the milk frothing equipment, and brought the two cups of prepared espresso to the customers.."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_358_real.mp4"
  },
  {
    "time": "[0:01:42 - 0:01:52]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the bartender's actions just now?",
        "time_stamp": "00:01:55",
        "answer": "D",
        "options": [
          "A. The bartender filled a pitcher with beer and served it to customers at the bar.",
          "B. The bartender cleaned the taps, filled a pitcher with hot water, and steeped tea bags in it.",
          "C. The bartender prepared a fruit smoothie by blending fresh fruits and ice, then pouring it into a glass to serve.",
          "D. The bartender steamed milk using a frothing wand, cleaned the wand, and combined the steamed milk with an espresso shot."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_358_real.mp4"
  },
  {
    "time": "[0:03:24 - 0:03:34]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken by the individual just now?",
        "time_stamp": "00:03:33",
        "answer": "D",
        "options": [
          "A. The individual prepared coffee beans by weighing them, grinding them, and adjusting the grind size.",
          "B. The individual brewed a coffee by immersing the beans in boiling water and stirring frequently.",
          "C. The individual cleaned the coffee machine, steamed milk, and poured it into a cup to serve a cappuccino.",
          "D. The individual prepared a portafilter with ground coffee, weighed it, and used a spoon to adjust the coffee amount."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_358_real.mp4"
  },
  {
    "time": "[0:05:06 - 0:05:16]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken by the barista just now?",
        "time_stamp": "00:05:16",
        "answer": "D",
        "options": [
          "A. The barista steamed milk, weighed it, and poured it into a coffee cup.",
          "B. The barista cleaned the coffee machine, weighed coffee grounds, and placed them into the portafilter.",
          "C. The barista poured milk into a glass, placed it on a scale, and then threw the milk container away.",
          "D. The barista poured milk into a measuring cup, weighed it, and then returned the container to the fridge."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_358_real.mp4"
  },
  {
    "time": "[0:06:48 - 0:06:58]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the barista's actions just now?",
        "time_stamp": "00:06:55",
        "answer": "D",
        "options": [
          "A. The barista poured milk into a pitcher, steamed it, and then mixed it into a hot cup of coffee.",
          "B. The barista filled a pitcher with milk, adjusted the coffee machine, rinsed the pitcher, and cleaned the counter area.",
          "C. The barista filled a pitcher with milk, cleaned the pitcher, and prepared an iced coffee.",
          "D. The barista poured milk into a glass, and steamed the milk in a pitcher."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_358_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the driver currently doing with his hands?",
        "time_stamp": "00:00:06",
        "answer": "C",
        "options": [
          "A. Honking the horn.",
          "B. Shifting gears.",
          "C. Holding the steering wheel.",
          "D. Adjusting the dashboard."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_267_real.mp4"
  },
  {
    "time": "[0:02:04 - 0:02:09]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What logo is visible right now?",
        "time_stamp": "00:02:05",
        "answer": "C",
        "options": [
          "A. JKS-Racing-Club.",
          "B. JKS-Sports-Car.",
          "C. JKS-Classic-Cars.",
          "D. JKS-Rally-Sport."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_267_real.mp4"
  },
  {
    "time": "[0:04:08 - 0:04:13]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is placed in front of the co-driver?",
        "time_stamp": "00:04:10",
        "answer": "C",
        "options": [
          "A. A map.",
          "B. A joystick.",
          "C. A notebook.",
          "D. A bottle of water."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_267_real.mp4"
  },
  {
    "time": "[0:06:12 - 0:06:17]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color are the driver's gloves right now?",
        "time_stamp": "00:06:14",
        "answer": "B",
        "options": [
          "A. Blue.",
          "B. Red.",
          "C. Yellow.",
          "D. Black."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_267_real.mp4"
  },
  {
    "time": "[0:08:16 - 0:08:21]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What brand is visible on the steering wheel right now?",
        "time_stamp": "00:08:18",
        "answer": "B",
        "options": [
          "A. JKS.",
          "B. OMP.",
          "C. Freem.",
          "D. Classic-Cars."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_267_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:04]: On top of a black surface, there are two transparent containers laid out horizontally, filled with various tiny colorful flower petals, organized by type in separate compartments. Below these containers, three hexagonal keychains are placed neatly in a row. These keychains have names 'Emma', 'Lory', and 'Susan' written on them in elegant cursive with embedded flowers of different shapes and colors. Scattered around on the table are smaller flower petals in various vibrant colors including pink, blue, red, and yellow, along with green leaf-shaped pieces;  [0:00:05]: The screen goes black;  [0:00:06 - 0:00:09]: The text \"Flower Keychains\" appears displayed steadily against a solid black background;  [0:00:10]: In a well-lit workspace with a gridded cutting mat, four plain white hexagonal shapes are arranged in a line. A pair of hands, wearing black gloves, holds two containers, labeled \"PUDUO Epoxy Resin Part A\" and \"PUDUO Epoxy Resin Hardener Part B,\" respectively. In the center, a small, transparent plastic cup is placed;  [0:00:11 - 0:00:16]: The person shows the two bottles of Epoxy Resin to the camera, clearly labeled as part A and part B. Details such as \"Crystal Clear,\" \"Self Leveling,\" and \"Low Odor\" can be seen on the labels of both containers. The hands hold the bottles steadily over the workspace, giving a clear view of the instructions and details;  [0:00:17 - 0:00:19]: The hands begin to pour the contents of the \"Epoxy Resin Part A\" into the transparent plastic cup, positioning it next to the hexagonal shapes prepared for the crafting process in the workspace. The area remains well-organized with the tools neatly positioned for the subsequent steps.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:00:20",
        "answer": "A",
        "options": [
          "A. Pouring Epoxy Resin Part A into a plastic cup.",
          "B. Mixing flower petals in a container.",
          "C. Writing names on the keychains.",
          "D. Organizing the hexagonal shapes."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_63_real.mp4"
  },
  {
    "time": "0:01:40 - 0:02:00",
    "captions": "[0:01:40 - 0:02:00] [0:01:41 - 0:01:49]: A pair of hands wearing black gloves is seen from a first-person perspective. The hands are working on a blue cutting mat. They hold a transparent plastic box containing various small, colorful items, likely decorative pieces. There is a jar with a mixing stick to the left. Some small floral decorations are positioned on the right, while three white hexagon-shaped tags are laid out horizontally across the top. [0:01:50 - 0:01:58]: The jar with the mixing stick, the transparent box with decorative items, and the floral decorations remain in the same positions. The hands continue to manipulate the small items using tweezers, carefully selecting and moving pieces. [0:01:59]: Attention shifts to a small clear container being held above the hexagon-shaped tags. This container appears to be related to the crafting process.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is currently placed under the hexagonal label?",
        "time_stamp": "00:01:59",
        "answer": "A",
        "options": [
          "A. A small clear container.",
          "B. A transparent plastic box with colorful items.",
          "C. A mixing stick.",
          "D. Tweezers."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_63_real.mp4"
  },
  {
    "time": "0:03:20 - 0:03:40",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:21]: In the initial frame, there is a detailed close-up of a workspace, with a blue cutting mat covering the surface. On the mat, there is a small clear plastic cup containing liquid being held in a black gloved hand positioned on the left side of the frame. Directly in front of the cup, a white piece of material with an irregular, slightly hexagonal shape is placed. To the right, another gloved hand uses a wooden stick to manipulate some material on the hexagon-shaped piece. Nearby, small decorative items such as miniature flowers and leaves are scattered around. [0:03:22 - 0:03:23]: The gloved hand on the right continues to adjust the small decorative elements on the hexagon-shaped piece using the wooden stick. The hand holding the clear cup remains steady, ensuring that the liquid is ready to be poured if necessary. [0:03:24 - 0:03:26]: The positions of the hand and the decorative elements stay largely consistent. The liquid in the clear cup remains unused, as the focus stays on arranging the small decorations on the hexagon-shaped piece. The background consists of several compartments containing various colorful items and tools. [0:03:27]: The clear cup with the liquid is set down on the blue cutting mat to the left of the workspace, while the other hand continues to use the wooden stick to adjust the decorations on the hexagon-shaped piece. [0:03:28]: The right gloved hand now holds a tool with a pink handle near the hexagon-shaped piece, while the left still holds the clear cup. The decorative elements on the hexagon are being carefully placed. [0:03:29]: The left hand places another hexagon-shaped piece on the workspace, preparing it for the next steps. The right hand does not manipulate any decorations at this moment. [0:03:30 - 0:03:32]: The right hand uses a pen-like tool to make precise adjustments to the newly placed hexagon-shaped piece, ensuring everything is in the correct position. [0:03:33]: The right hand reaches down to the workspace. The left hand is seen picking up a tool with a pink handle while the clear cup with the liquid remains on the blue cutting mat.  [0:03:34 - 0:03:35]: The left-hand lifts the clear cup with the liquid again, and the right hand uses the wooden stick to stir or align contents within the cup. [0:03:36]: The scene remains focused on the left hand holding the clear cup slightly tilted, and the right hand continuing to manipulate the contents, preparing to add them to the hexagon-shaped piece. [0:03:37 - 0:03:38]: Switching tools, the right hand holds a different tool, possibly for precision work. Meanwhile, the left hand continues to hold the clear cup, ready to assist with the next task. [0:03:39]: The scene shifts slightly as the tool held by the right hand is now closer to the hexagon-shaped piece, indicating new decorative elements or details being added. The workspace in the background remains cluttered with various tools and materials.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the left gloved hand doing right now?",
        "time_stamp": "00:03:25",
        "answer": "A",
        "options": [
          "A. Holding a clear cup with liquid.",
          "B. Using a wooden stick to manipulate decorations.",
          "C. Placing another hexagon-shaped piece on the workspace.",
          "D. Making precise adjustments with a pen-like tool."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_63_real.mp4"
  },
  {
    "time": "0:05:00 - 0:05:20",
    "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:06]: A hand wearing a black glove is holding a transparent cup containing a clear liquid on the left side of the frame. The hand is positioned above a blue grid cutting mat, which covers the entire background of the scene. On the mat, there are four hexagonal pieces with floral decorations aligned horizontally near the top. The first and second pieces from the left contain pink and red flowers, while the third contains green foliage, and the fourth is empty. To the right, a few loose pieces of pink flowers and green leaves are scattered. A paintbrush with a pink handle is placed close to the right edge of the mat. The hand holding the cup is seen pouring the liquid into the first and second hexagonal pieces containing pink flowers.  [0:05:07]: The gloved hand points to an empty hexagonal piece on the mat with an index finger while the other hand holds the transparent cup to the left. A wooden stick is held in the right hand, positioned above the mat. [0:05:08]: The left hand still holds the cup, while the right hand, now closer to the mat, holds a white tool with a pink handle. The tool is used to scrape something from the surface of the hexagonal piece. [0:05:09 - 0:05:17]: Both hands are engaged in the activity. The left hand continues to hold the transparent cup, while the right hand uses a wooden stick to work with the empty hexagonal piece on the mat. The stick and cup move in coordinated motions, suggesting that the liquid is being poured into the hexagonal piece. Occasionally, the right hand adjusts the placement of small floral and foliage decorations inside the hexagonal piece to arrange them properly. [0:05:18 - 0:05:19]: The hand continues to precisely place the clear liquid into the hexagonal piece using a wooden stick. The other hand is steady, supporting the movements by holding the cup firmly. Small floral and foliage decorations are carefully positioned within the hexagon.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the gloved hand doing right now?",
        "time_stamp": "00:05:10",
        "answer": "A",
        "options": [
          "A. Pouring liquid into a hexagonal piece.",
          "B. Scraping the surface of the hexagonal piece.",
          "C. Adjusting floral and foliage decorations.",
          "D. Holding the paintbrush with a pink handle."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_63_real.mp4"
  },
  {
    "time": "0:06:40 - 0:07:00",
    "captions": "[0:06:40 - 0:07:00] [0:06:51 - 0:06:54]: A pair of hands, with a ring on one of the fingers, is operating on a hexagonal-shaped translucent piece with pink and green markings. This piece is placed in the center of the frame, which is set against a grey grid-patterned work surface. Surrounding this central piece on the top are three more hexagon pieces and three rectangular white pieces, all evenly spaced. [0:06:54 - 0:06:58]: The hands start to apply pressure on the green marking of the hexagonal-shaped piece, making sure the piece stays secure and in position. You can also notice that the person's sleeves are grey, which complements the grey grid-patterned background. [0:06:58 - 0:07:01]: The hands slightly adjust the hexagonal piece, ensuring it is precisely placed. The surrounding pieces remain in their positions, untouched. The background maintains its grey grid pattern throughout this segment.  [0:07:01 - 0:07:03]: A new tool is introduced into the frame. A small purple device is held in the right hand while still pressing the hexagonal piece with the left. The purple device appears to be aimed at the hexagonal piece, demonstrating careful handling and positioning. [0:07:03 - 0:07:05]: The purple tool is used to press something onto the hexagonal piece. The other elements—the three hexagonal pieces and the three rectangular pieces—stay in their original positions, undisturbed. [0:07:05 - 0:07:08]: The pressure is applied consistently with the purple tool on the hexagonal piece in the center, ensuring precise contact. The background and position of surrounding pieces remain unchanged. [0:07:08 - 0:07:11]: The hands move the purple tool away, setting it aside from the working area. The untreated pieces (three hexagonal and three rectangular white pieces) stay in place on the grey grid-patterned surface. [0:07:11 - 0:07:13]: The left hand now maneuvers the hexagonal piece again, ensuring it's properly adjusted and secure after the use of the purple tool. The grid-patterned surface continues to serve as the background. [0:07:13 - 0:07:16]: The hands then pick up another tool, possibly tweezers or a pick, and hold it near the hexagonal piece, starting to engage in another step of the process. The rest of the pieces stay immobile. [0:07:16 - 0:07:19]: Attention shifts slightly as the tool in the right hand works on the hexagonal piece, focusing on a specific part of the piece. The detail work implies delicacy and precision. [0:07:19 - 0:07:22]: The hands pause for a fraction of a second, seemingly verifying the progress on the hexagonal piece. The other craft pieces and the grey background remain constant",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:07:22",
        "answer": "A",
        "options": [
          "A. Attaching the name to the transparent hexagonal object.",
          "B. Adjusting the hexagonal piece with their left hand.",
          "C. Applying pressure on the green marking with their left hand.",
          "D. Using a purple tool to press onto the hexagonal piece."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_63_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the sequence of actions taken just now?",
        "time_stamp": "0:00:10",
        "answer": "A",
        "options": [
          "A. The person unlocks the door, opens it to let in light, and mentions the opening.",
          "B. The person prepares a dish of pasta, garnishes it, and places it on a serving tray.",
          "C. The person sets tables, arranges seats, and places cutlery in a coffee shop.",
          "D. The person cleans a window, arranges flowers, and adjusts a sign."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_410_real.mp4"
  },
  {
    "time": "[0:03:49 - 0:03:59]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions performed just now?",
        "time_stamp": "0:03:59",
        "answer": "A",
        "options": [
          "A. The individual measured and poured milk into a container, stored the milk, and then started steaming the milk.",
          "B. The individual prepared a sandwich, added vegetables, and wrapped it for serving.",
          "C. The individual prepared a salad by chopping and mixing various vegetables.",
          "D. The individual polished dishes, arranged them on a tray, and placed them on a shelf."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_410_real.mp4"
  },
  {
    "time": "[0:07:38 - 0:07:48]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the sequence of actions taken just now?",
        "time_stamp": "0:07:54",
        "answer": "A",
        "options": [
          "A. The person pours steamed milk into a cup of espresso, creating a latte, and places it on the counter.",
          "B. The person fills a pitcher with water, hands it to a customer, and wipes the counter.",
          "C. The person makes a cup of tea, adds sugar, and serves it to a customer.",
          "D. The person brews a pot of coffee, fills a mug, and hands it to a coworker."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_410_real.mp4"
  },
  {
    "time": "[0:11:27 - 0:11:37]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the sequence of actions taken just now?",
        "time_stamp": "0:11:37",
        "answer": "A",
        "options": [
          "A. The person steamed milk, poured it into a cup with espresso.",
          "B. The person took a cup, added sugar, and stirred it with a spoon.",
          "C. The person brewed a fresh cup of coffee, added milk, and handed it to the customer.",
          "D. The person prepared tea, added honey, and placed it on the counter."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_410_real.mp4"
  },
  {
    "time": "[0:15:16 - 0:15:26]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "0:15:26",
        "answer": "A",
        "options": [
          "A. The person weighed coffee beans.",
          "B. The person prepared vegetables, cooked them, and served them on a plate.",
          "C. The person measured milk, steamed it, and poured it into a cup.",
          "D. The person brewed tea, added honey, and handed the cup to the customer."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_410_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:16",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_90_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:44",
        "answer": "B",
        "options": [
          "A. 3.",
          "B. 5.",
          "C. 6.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_90_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:14",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 4.",
          "C. 3.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_90_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:27",
        "answer": "B",
        "options": [
          "A. 2.",
          "B. 8.",
          "C. 4.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_90_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:06:24",
        "answer": "C",
        "options": [
          "A. 4.",
          "B. 8.",
          "C. 10.",
          "D. 7."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_90_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions just taken?",
        "time_stamp": "00:00:10",
        "answer": "C",
        "options": [
          "A. The person brewed a pot of tea, added lemon, and served it.",
          "B. The person prepared a glass of milk by measuring its weight and pouring it into a jug.",
          "C. The person measured ground coffee beans, and gathered different types of milk alternatives.",
          "D. The person set up the kitchen for baking by arranging flour, sugar, and eggs."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_355_real.mp4"
  },
  {
    "time": "[0:02:03 - 0:02:13]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions just taken?",
        "time_stamp": "00:02:13",
        "answer": "D",
        "options": [
          "A. The person filled a cup with hot water, added a tea bag, and let it steep.",
          "B. The person prepared a cup for a latte, steamed oat milk, and gathered different types of syrups.",
          "C. The person filled a pitcher with water, added ice, and prepared to brew coffee.",
          "D. The person steamed oat milk, and prepared to make a coffee."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_355_real.mp4"
  },
  {
    "time": "[0:04:06 - 0:04:16]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions just taken?",
        "time_stamp": "00:04:16",
        "answer": "B",
        "options": [
          "A. The individual brewed a pot of tea and added a slice of lemon.",
          "B. The individual cleaned a pitcher, filled it with milk, and steamed it.",
          "C. The individual washed a jug, filled it with water, and placed it in a fridge.",
          "D. The individual rinsed a coffee cup, added hot water, and wiped the counter."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_355_real.mp4"
  },
  {
    "time": "[0:06:09 - 0:06:19]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions just taken?",
        "time_stamp": "00:06:19",
        "answer": "B",
        "options": [
          "A. The person prepared a glass of water and added a slice of lemon.",
          "B. The person cleaned a kitchen surface, wiped it down, and dried it with a cloth.",
          "C. The person used a spray bottle to sanitize the counter, then dried it with a towel.",
          "D. The person wet a cloth, cleaned the surface with it, then dried it with paper towels."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_355_real.mp4"
  },
  {
    "time": "[0:08:12 - 0:08:22]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions just taken?",
        "time_stamp": "00:08:20",
        "answer": "B",
        "options": [
          "A. The individual brewed a cup of coffee and set it on the counter.",
          "B. The individual cleaned coffee grounds by a little broom and disposed of the waste in a bin.",
          "C. The individual refilled the coffee grinder with fresh coffee beans.",
          "D. The individual washed and dried a coffee cup before using it for a new drink."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_355_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:00:8",
        "answer": "C",
        "options": [
          "A. The individual poured milk into a cup, stirred it, and then took it to the customer.",
          "B. The individual brewed a pot of tea, poured it into a thermal flask, and brought it to a table.",
          "C. The individual finished preparing a latte, placed it alongside a black coffee.",
          "D. The individual selected pastries from a display case, packed them in a box, and handed them to a customer."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_364_real.mp4"
  },
  {
    "time": "[0:02:54 - 0:03:04]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:03:04",
        "answer": "B",
        "options": [
          "A. The individual checked order status on a tablet, cleaned a countertop area, and served a sandwich.",
          "B. The individual prepare matcha, carried the matcha to a sink, and then checked order details on a tablet.",
          "C. The individual brewed coffee, poured it into a cup, and then prepared a latte with milk.",
          "D. The individual selected utensils, eco-certified cleaning solutions, and organized a storage area."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_364_real.mp4"
  },
  {
    "time": "[0:05:48 - 0:05:58]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:05:58",
        "answer": "B",
        "options": [
          "A. The individual selected tea leaves, boiled water, and brewed a fresh pot of tea.",
          "B. The individual poured milk into a metal jug, steamed the milk, and prepared it for use.",
          "C. The individual prepared an espresso shot, added steamed milk, and created latte art.",
          "D. The individual grabbed utensils, washed them, and organized them on a drying rack."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_364_real.mp4"
  },
  {
    "time": "[0:08:42 - 0:08:52]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:08:51",
        "answer": "B",
        "options": [
          "A. The individual filled a cup with hot water, steeped a tea bag, and served the tea to a customer.",
          "B. The individual poured milk into a container, steamed it, and cleaned the workstation with a cloth.",
          "C. The individual brewed a pot of coffee, poured it into multiple cups, and served it to customers.",
          "D. The individual mixed various ingredients into a jug, prepared a smoothie, and cleaned the blender."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_364_real.mp4"
  },
  {
    "time": "[0:11:36 - 0:11:46]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:11:46",
        "answer": "B",
        "options": [
          "A. The individual poured hot water into a cup, steeped the tea, and served it.",
          "B. The individual weighed matcha powder.",
          "C. The individual mixed chocolate powder into a glass and added hot milk.",
          "D. The individual cleaned the workspace with a cloth and disposed of the trash."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_364_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03, 0:00:05 - 0:00:07, 0:00:09, 0:00:12, 0:00:15]: In a brightly lit room with tall windows revealing a green, leafy exterior, a nurse in a white and blue uniform with a red cross on her apron is seen making a bed. She is diligently straightening the bedspread and adjusting the pillows. The surroundings include vintage iron-framed beds, ceramic bowls, and mugs on a table covered with a patterned cloth. The nurse moves with focused intent, showing careful attention to ensuring the bed is neatly made. A Union Jack flag hangs on the wall beside her. [0:00:04]: The scene abruptly shifts to a close-up of muddy boots on wooden planks, indicating a rough, wet environment. [0:00:08]: In a dark, stormy night, branches and leaves fly around, carried by a strong wind amidst ominous surroundings. Debris and dirt are being violently stirred up. [0:00:10 - 0:00:11]: A soldier in a helmet and dark uniform is seen in a chaotic trench environment. There is intense smoke and explosions around him, suggesting a battlefield. He appears to be in distress, moving hurriedly through the trench, with one hand holding onto his helmet, indicating the dire situation. [0:00:13 - 0:00:14]: The perspective changes to looking through a tattered net, revealing only glimpses of what lies beyond it. The view is obstructed but it suggests a hidden or protective stance, trying to see while staying concealed. [0:00:16 - 0:00:19]: The final frames show a desolate, war-torn landscape. A soldier lies motionless on the ground amidst the dirt and rubble. The scene is gloomy, shrouded in dust and despair, with the figure remaining prone until the screen fades to black.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is hanging on the wall beside the nurse?",
        "time_stamp": "0:00:14",
        "answer": "C",
        "options": [
          "A. A clock.",
          "B. A portrait.",
          "C. A Union Jack flag.",
          "D. A medical certificate."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Action Recognition",
        "question": "What is the nurse doing in the room?",
        "time_stamp": "0:00:20",
        "answer": "C",
        "options": [
          "A. Preparing a meal.",
          "B. Writing notes.",
          "C. Making a bed.",
          "D. Cleaning the floor."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_162_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:00 - 0:01:20] [0:01:00]: A small dark blue cloth bag is lying on a textured white surface, likely a bedspread. The bag has a white label with writing on it. [0:01:01 - 0:01:02]: Three individuals are in a room with light-colored walls and large windows showing a green landscape outside. Two women dressed as nurses are attending to a person sitting on a chair. The setting appears to be a historical or medical environment. Both nurses, identifiable by their uniforms, are kneeling in front of the seated person, who is dressed in white. [0:01:03 - 0:01:09]: Close-up view of the two nurses assisting the seated person by putting on a pair of green socks. The action is focused on the hands and feet of the individuals, with detailed wooden flooring visible in the background. [0:01:10 - 0:01:19]: The nurses complete putting on the socks and continue to interact with the seated person. The room features metal-framed beds with white linens, a wooden floor, and a British flag hanging on the wall to the right. Various medical utensils can be seen next to one of the beds. One nurse is adjusting the seated individual's socks, while the other begins to stand up. The seated person looks on with a neutral expression while the nurses continue their duties. The room remains brightly lit by the natural light streaming in through the large windows.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color are the socks being put on the seated person?",
        "time_stamp": "00:01:12",
        "answer": "D",
        "options": [
          "A. Red.",
          "B. Blue.",
          "C. Yellow.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_162_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:02]: In a well-lit room, a man wearing a white shirt and red tie is sitting on a chair. Two nurses in white uniforms with red crosses on their chests are assisting him, adjusting his attire. The room has large windows in the background, offering a view of greenery outside. Two neatly made beds are placed parallel to the windows, with a British flag draped on the wall to the right. Various medical supplies are placed on a bedside table. [0:02:03 - 0:02:07]: One of the nurses begins to prepare an item, likely a medical gown, which is laid on one of the beds. She picks up the gown and holds it out, possibly to assist the man in wearing it. The other nurse continues to be attentive to the man, ensuring he is comfortable. [0:02:08 - 0:02:11]: Both nurses now focus on helping the man put on the gown. They carefully lift and adjust it, ensuring it is properly positioned. The man remains seated, slightly adjusting his arms to facilitate the process. The three maintain a cooperative interaction, with a clear division of tasks. [0:02:12 - 0:02:14]: The nurses continue to carefully assist the man in putting on the gown. They attentively handle the gown, making sure it is correctly placed on him. The man slightly leans forward to help them complete the task. [0:02:15 - 0:02:16]: All three are now bending slightly near the floor, likely adjusting the lower part of the gown or dealing with an accessory. The cooperation among them remains evident as they smoothly proceed with their tasks. [0:02:17 - 0:02:19]: As they finalize the adjustments, both nurses and the man continue their coordinated efforts, with the man showing minor movements to assist. The room remains orderly, with medical supplies on hand and an air of calm efficiency in the interactions.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the man's tie?",
        "time_stamp": "0:02:04",
        "answer": "D",
        "options": [
          "A. Blue.",
          "B. Green.",
          "C. Yellow.",
          "D. Red."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_162_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:05]: The video features a room with large windows and wooden floors. There are two metal-framed beds on the right side, each covered with white linens. Against the far right wall, a British flag is draped. In the foreground, a man in white shirt, navy blue pants, and red suspenders is being assisted by two women dressed in white and gray uniforms with white head coverings. The man appears to be preparing to dress or finishing dressing. One of the women holds a navy blue jacket. [0:03:06 - 0:03:11]: The man continues to put on the navy blue jacket, with both women assisting him. One of the women helps him put his arms through the sleeves while the other adjusts the jacket at the shoulders.  [0:03:12 - 0:03:15]: Once the jacket is on, the women make final adjustments to ensure it fits correctly. They stand close to the man, ensuring that the jacket is properly positioned. [0:03:16 - 0:03:19]: The camera angle shifts to show a close-up of the man's hands and feet. He is now seated, tying the laces of his black boots. The women assist him in securing the laces, ensuring they are tight and properly fastened.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What are the women doing as the man puts on the navy blue jacket?",
        "time_stamp": "0:03:11",
        "answer": "B",
        "options": [
          "A. They are leaving the room.",
          "B. They are helping him put on the jacket and adjusting it.",
          "C. They are folding the linens.",
          "D. They are standing by the window."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_162_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:01 - 0:00:02]: The video starts by showing a multicolored toy train made of interlocking building blocks. The train is predominantly orange, green, yellow, blue, and red. It consists of a rectangular body with windows formed by orange and yellow blocks, and blue and green blocks at the top. It has red wheels and is placed on a multicolored track made of blocks in blue, pink, orange, green, and yellow. The word \"Train\" appears on the top left in a stylish yellow font;  [0:00:03 - 0:00:05]: The focus then shifts to the right side of the track. The train starts to move, guided by someone’s hand, which appears from the top of the frame. The hand is pushing the train forward along the track;  [0:00:06 - 0:00:09]: The hand continues to move the train along the track. The train is being moved slowly, and the track remains consistent with its multicolored pattern. The hand maintains a steady grip on the train's green block on top as it progresses forward;  [0:00:10 - 0:00:12]: The hand still guides the train, maintaining the same pace. The blocks of the train and the track are clearly defined, with the red wheels rolling over the colorful track. The background remains a plain light-colored wall;  [0:00:13 - 0:00:17]: The hand is no longer visible. The train is now stationary, positioned slightly further along the track than in previous frames. The visible parts of the track include sections in orange, green, yellow, and blue blocks in an orderly pattern;  [0:00:18 - 0:00:19]: The view changes slightly to the back of the train, showing a slightly angled perspective. The train still sits firmly on the multicolored track, with the solid, light-colored wall in the background.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action is the hand performing on the train?",
        "time_stamp": "0:00:12",
        "answer": "D",
        "options": [
          "A. Lifting the train.",
          "B. Assembling the train.",
          "C. Painting the train.",
          "D. Push the train to the right."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_201_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:03]: A hand is seen placing a yellow Lego brick on a flat, light-colored surface. In front of the hand is a row of yellow LEGO bricks aligned horizontally. The background wall is a light tan color. [0:01:04 - 0:01:09]: Another hand appears holding a green Lego brick and places it on top of the yellow row, towards the left end. The process is repeated as more green bricks are added on top of the first few yellow bricks. [0:01:10 - 0:01:16]: The individual continues stacking the green Lego bricks vertically, creating a small green column attached to the horizontal yellow base.  [0:01:17 - 0:01:20]: Another color is introduced as the hand places a blue Lego brick adjacent to the green column, continuing to build upwards. The blue Lego is positioned on the yellow base, next to the green Lego pieces.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What color Lego brick is placed first on the surface?",
        "time_stamp": "0:01:03",
        "answer": "D",
        "options": [
          "A. Blue.",
          "B. Green.",
          "C. Red.",
          "D. Yellow."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_201_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:04]: A hand is seen constructing a structure with colorful interlocking toy bricks on a flat, white surface. Initially, the person places orange and green bricks on top of a yellow base. The structure already appears partially built, featuring a combination of orange, green, and blue bricks. The backdrop is a plain, light-colored wall. [0:02:05 - 0:02:10]: The hand continues to add more green bricks to the structure, placing them very carefully on top of the existing formation. The movements are deliberate, and the green bricks are aligned to create a rooftop-like appearance. [0:02:11 - 0:02:14]: Following the roof completion, the hand starts aligning more yellow bricks next to the existing yellow base, extending the structure horizontally. The construction seems to be following a linear pattern. [0:02:15 - 0:02:19]: The hand continues to add additional yellow bricks, systematically expanding the structure further to the right. The placement of bricks is precise, and the hand moves out of the frame after each addition, making the sequence smooth and continuous.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What was the sequence of actions performed by the hand just now?",
        "time_stamp": "00:02:29",
        "answer": "B",
        "options": [
          "A. Placing orange and green bricks, adding blue bricks, extending the yellow base.",
          "B. Adding green bricks for the roof, extending the yellow base, expanding the structure to the right.",
          "C. Starting with blue bricks, adding a rooftop, then dismantling it.",
          "D. Completing the structure and then painting it."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      },
      {
        "task_type": "Counting",
        "question": "How many colors of bricks have been used so far?",
        "time_stamp": "00:02:19",
        "answer": "C",
        "options": [
          "A. Two.",
          "B. Three.",
          "C. Four.",
          "D. Five."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_201_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:09]: A colorful structure made of interlocking plastic bricks is observed on a flat surface, against a light-colored wall. It comprises primarily of orange, yellow, green, blue, and less frequently, red bricks. The bricks are assembled in an organized manner, creating a structure with alternating color patterns and a consistent horizontal layout. At the initial timestamp, a hand, emerging from the right side of the frame, is seen moving towards the structure and begins adjusting bricks on the right end. The hand moves deliberately, placing and removing the blue and green bricks while maintaining the structure's integrity. [0:03:10 - 0:03:14]: The hand moves to the left side of the structure and rotates it, revealing the underside. This side shows that the bottom layer consists of yellow bricks arranged to create a stable base. The underside’s layout exposes the interconnected design of the bricks, with hollow sections visible. The hand then places the structure back on its original side, maintaining its overall form. [0:03:15 - 0:03:17]: The hand takes additional green bricks and places them onto the structure’s right end, incrementally adding height and creating layered stacks. The movement is precise, positioning the bricks to ensure they align with the existing design. [0:03:18 - 0:03:19]: The hand picks up two red circular pieces, resembling wheels, and attaches them to the top surface of the right end of the structure, providing a hint of mobility or embellishing the model. The hand's actions conclude with the pieces being securely placed on the structure.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type of pieces did the hand attach to the top surface of the structure just now?",
        "time_stamp": "00:03:25",
        "answer": "D",
        "options": [
          "A. Square pieces.",
          "B. Triangular pieces.",
          "C. Rectangular pieces.",
          "D. Circular pieces resembling wheels."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_201_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:18",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_75_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:23",
        "answer": "B",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_75_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:31",
        "answer": "B",
        "options": [
          "A. 4.",
          "B. 5.",
          "C. 3.",
          "D. 2."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_75_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:18",
        "answer": "D",
        "options": [
          "A. 1.",
          "B. 3.",
          "C. 2.",
          "D. 5."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_75_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:06:37",
        "answer": "D",
        "options": [
          "A. 1.",
          "B. 3.",
          "C. 5.",
          "D. 7."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_75_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:16",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_93_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:01:14",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 1."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_93_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:26",
        "answer": "B",
        "options": [
          "A. 3.",
          "B. 4.",
          "C. 6.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_93_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:07",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 4.",
          "C. 3.",
          "D. 5."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_93_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:31",
        "answer": "B",
        "options": [
          "A. 2.",
          "B. 7.",
          "C. 8.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_93_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual just now?",
        "time_stamp": "00:00:12",
        "answer": "D",
        "options": [
          "A. The individual is cleaning trays, organizing workspaces, and baking bread in the oven.",
          "B. The individual is taking an order, chopping vegetables, and making a quick salad.",
          "C. The individual is preparing a dessert by mixing ingredients and placing them in the refrigerator.",
          "D. The individual is putting on gloves, picking up a hot dog bun, and assembling a hot dog."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_350_real.mp4"
  },
  {
    "time": "[0:02:20 - 0:02:30]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual just now?",
        "time_stamp": "00:02:31",
        "answer": "D",
        "options": [
          "A. The individual is taking an order, grilling a hamburger patty, and placing it in a bun.",
          "B. The individual is preparing a sandwich by slicing bread, adding fillings, and wrapping it up.",
          "C. The individual is filling a tray with fries, adding salt, and serving it to a customer.",
          "D. The individual is putting on gloves, picking up a hot dog bun, adding the wiener, and preparing toppings."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_350_real.mp4"
  },
  {
    "time": "[0:04:40 - 0:04:50]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual just now?",
        "time_stamp": "00:04:50",
        "answer": "D",
        "options": [
          "A. The individual is assembling pastries, adding icing, and placing them on a cooling rack.",
          "B. The individual is grilling a hot dog, applying condiments, and wrapping it in foil.",
          "C. The individual is chopping vegetables, tossing them into a bowl, and serving a salad.",
          "D. The individual is assembling an order by packaging items into a paper bag and preparing it for delivery."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_350_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:07:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual just now?",
        "time_stamp": "00:07:39",
        "answer": "D",
        "options": [
          "A. The individual is arranging condiments on a counter and placing food items in the refrigerator.",
          "B. The individual is taking orders from customers and showing them to their seats.",
          "C. The individual is pouring beverages into cups and setting them on a tray for delivery.",
          "D. The individual is putting on gloves, picking up a hot dog bun, adding the wiener, and preparing toppings."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_350_real.mp4"
  },
  {
    "time": "[0:09:20 - 0:09:30]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual just now?",
        "time_stamp": "00:09:31",
        "answer": "D",
        "options": [
          "A. The individual is grilling a hamburger patty, adding cheese and condiments, and wrapping it up in paper.",
          "B. The individual is filling a drink cup with a beverage, adding ice, and sealing it with a lid.",
          "C. The individual is assembling a vegetarian wrap with fresh vegetables, dressing, and a tortilla.",
          "D. The individual is preparing a hot dog, topping it with grilled onions and meat sauce, and wrapping it in wrapping paper."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_350_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: In a promenade area, numerous people are gathered around a statue of a woman holding a lit globe in her raised left hand. The statue is positioned on a pedestal that is illuminated by yellow lights. Many of the people appear to be tourists, conversing, taking photos, and mingling. The background features a waterfront view with a skyline of tall buildings, including a recognizable skyscraper, with some areas appearing to be under cloudy skies. [0:00:04 - 0:00:06]: The crowd continues around the statue, with a man wearing a mask walking past. Several people are seen holding cameras or mobile phones, taking photos, and capturing the scenic view. [0:00:07 - 0:00:11]: As the video progresses, it focuses more on the statue, with many people still gathered around it, taking pictures and observing. The waterfront and the cityscape with a variety of building lights in the background remain visible and become more prominent. [0:00:12 - 0:00:17]: The focus zooms in closer to the statue, highlighting the illuminated pedestal. The crowd's activity around the statue continues, with people looking out towards the waterfront, capturing photos, and admiring the view. [0:00:18 - 0:00:20]: The scene showcases the vibrant colors of the illuminated buildings and boats at the waterfront. The crowd is still densely packed around the statue, with some people pointing toward the distant skyline, engaging in conversations, and taking in the scenic view around them.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the woman in the statue holding in her left hand?",
        "time_stamp": "0:00:19",
        "answer": "D",
        "options": [
          "A. A book.",
          "B. A sword.",
          "C. A shield.",
          "D. A lit globe."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Action Recognition",
        "question": "What are many people in the crowd doing around the statue?",
        "time_stamp": "0:00:12",
        "answer": "D",
        "options": [
          "A. Dancing.",
          "B. Eating.",
          "C. Singing.",
          "D. Taking photos."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_334_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:59]: The video captures a broad plaza with a distinctive dome-shaped structure situated prominently to the left center of the frame. The dome illuminates with a soft yellowish light near its base, enhancing its visibility against the backdrop of an impending evening sky. Multiple tall buildings, adorned with various lighting arrangements, frame the background. The skyscrapers display different architectural designs and light up with both cooler blue tones and warmer white lights. Two major buildings have visible logos, one on the far left and another on the far right. In the foreground, the ground consists of a mosaic of light-colored tiles arranged in a pattern. A few individuals populate the scene, engaging in casual activities. Some are walking across the plaza, while others are standing near the dome, seemingly absorbed in viewing or conversing. In addition to the dome, a low, rectangular beige structure is positioned slightly off-center to the right. The plaza itself is open and spacious, emphasizing the scale of both the dome and the surrounding buildings. To the far left, part of a modern canopy structure with vertical beams is visible, adding a contrast to the more rounded forms of the dome. There is also a small lamp post with light reflections on the ground near one of the walking individuals. The ambient light gradually dims, indicating the transition from dusk to evening, with the artificial lighting becoming more pronounced as time progresses.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What kind of light enhances the visibility of the dome?",
        "time_stamp": "00:03:00",
        "answer": "D",
        "options": [
          "A. A bright white light.",
          "B. A cooler blue light.",
          "C. A flashing red light.",
          "D. A soft yellowish light."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_334_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:25]: The video shows a first-person perspective of a covered walkway with bright overhead lights. The path is paved with small, square, stone-like tiles and has a tree planted near the left side. On the left side, there is a large sign for HKMOA with some art displayed. The path extends forward, with pedestrians visible at a distance. [0:05:26]: The perspective moves slightly forward, the large HKMOA poster remains visible to the left. There are more signs and posters visible further down the path, and more pedestrians become apparent. [0:05:27]: Moving further along the path, more of the right side becomes visible, showing a clear pool of water and some steps leading down to it. [0:05:28 - 0:05:30]: Continuing forward, the same elements persist with more clarity on the right side. The tiled walkway continues and more pedestrians can be seen under the covered area. [0:05:31 - 0:05:35]: The movement continues with the covered path extending straight ahead. The left side shows more signs and seating areas. The right side consistently shows the pool of water, and the walkway still has its small square tiles. [0:05:36 - 0:05:38]: The forward motion persists; the structure and elements stay the same. On the left side, there is now a display booth or information stand. Pedestrians walk down the path in both directions. [0:05:39]: The perspective continues forward with the covered walkway ahead. The structure of the area remains consistent with lights overhead and pedestrians walking along the tiled path. The area maintains a balanced, symmetric appearance with objects and people aligning along the path.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What largest text is visible on the left side of the walkway right now?",
        "time_stamp": "00:05:27",
        "answer": "A",
        "options": [
          "A. HKMOA.",
          "B. City Park.",
          "C. Art Museum.",
          "D. Train Station."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_334_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:59]",
    "captions": "[0:09:40 - 0:09:59] [0:09:40 - 0:09:59]: The video showcases a vibrant cityscape at night, taken from a first-person perspective along a waterfront. The skyline features an array of brightly-lit skyscrapers with various colors such as blue, red, and green. Reflections of these lights shimmer on the calm water, creating a dazzling effect. The horizon also includes a prominent tower illuminated in yellow, which stands taller than the surrounding buildings. To the left of the frame, there is a sturdy concrete barrier, curving slightly, adding depth to the composition. In the background, further left, a mountain range is partially visible under the night sky. An overhanging tree branch appears in the upper part of the frame, providing a natural contrast to the urban environment. Several boats with colorful lights, including a distinctive purple one, are moving across the water, enhancing the lively atmosphere of the scene.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the tallest tower's illumination?",
        "time_stamp": "00:09:59",
        "answer": "D",
        "options": [
          "A. Blue.",
          "B. Green.",
          "C. Red.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_334_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video begins with a view of a football field, showing green grass with a bit of wear, and a white line running horizontally across the frame. Part of a person’s leg in black shorts and cleats is visible on the right side. [0:00:01 - 0:00:02]: The perspective changes to show several individuals on the football field. There are about three players in blue jerseys, black shorts, and two in darker clothing. They appear to be running and engaging in a game. The background includes a fence, trees, and some structures. [0:00:03 - 0:00:04]: A red and white football is prominently featured. Initially, it is closer to the camera, then it is seen lying on the grass further away as the player seems to move towards it. [0:00:05 - 0:00:08]: The player interacts with the ball, possibly dribbling or controlling it. The player's hand can be seen pointing or gesturing. Other players in blue jerseys are visible in the distance. They seem to be part of the same play, running and positioning themselves. [0:00:09 - 0:00:12]: More interaction with the ball is shown. The player appears to be advancing it downfield. Other players, including one in blue and another in red, are seen running ahead, possibly towards the goal area. The player's leg is captured kicking the ball. [0:00:13 - 0:00:18]: The scene shifts to an individual wearing a white face mask, black shirt, and backpack, standing near a pile of wooden poles. A description at the bottom of the video mentions he is at Bangkok FC's training center and trying to film their training in a first-person view. [0:00:19]: The video concludes with a blurred image of a logo or emblem, mostly red and white.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the color of the football prominently featured in the video?",
        "time_stamp": "0:00:04",
        "answer": "D",
        "options": [
          "A. Blue and white.",
          "B. Black and white.",
          "C. Green and white.",
          "D. Red and white."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_265_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:01]: The video starts with a first-person perspective view following a white line on a grassy soccer field. On the left, there are buildings and a goalpost. [0:03:01 - 0:03:11]: The view pans across the field, displaying more players in blue shirts scattered around the field. Some players are seen closer to the goalposts, possibly indicating a practice session or game being played. [0:03:11 - 0:03:12]: Suddenly, a soccer ball rolls into view from the bottom of the screen, approaching the camera. The field remains in the background. [0:03:12 - 0:03:14]: The focus moves closer to the ground, with a view of the soccer ball, part of a player's leg in blue cleats, and the lower part of the player's body. The player seems to be preparing to kick the ball. [0:03:14 - 0:03:16]: The perspective shifts again to follow the ball as it is kicked. A goalpost and several players in blue can be seen in the background. [0:03:16 - 0:03:20]: The first-person view now returns to a wider angle, showing more of the field and players. The player who kicked or interacted with the ball is now focusing on teammates further down the field. Toward the end, the text \"Good pass Aim!\" and \"Good job!\" are displayed, indicating verbal encouragement likely from the player operating the camera.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What text appeared just now?",
        "time_stamp": "00:03:20",
        "answer": "D",
        "options": [
          "A. \"Nice shot!\".",
          "B. \"Great goal!\".",
          "C. \"Well played!\".",
          "D. \"Good pass Aim!\" and \"Good job!\"."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_265_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:02]: The video begins on a green soccer field with the goalpost in the background. The viewer's point of view includes an individual running towards a ball on the grass. A blue structure and red banners with advertisements for Toyota and others are visible on the left. The weather appears overcast. [0:06:03 - 0:06:05]: The camera moves closer to the ball, which is now more centrally positioned in the frame. One can see the individual's arm and leg, indicating they are approaching the ball to kick it. [0:06:06 - 0:06:08]: As the individual reaches the ball, they appear to be preparing to kick it. The goalposts and other players in blue jerseys in the background become more apparent. The field markings are also more visible. [0:06:09 - 0:06:12]: After the ball is kicked, it rolls away, and another player in a blue jersey is visible running towards it. Additional players in the background are seen playing or positioning themselves on the field.  [0:06:13 - 0:06:16]: The camera follows the ball, which is now in the possession of another player in a blue shirt. This individual is heading towards the opposing team's side. The background shows a fence, a goal in the distance, and some trees. [0:06:17 - 0:06:20]: The viewer captures an interaction where a player in a blue shirt passes the ball, which rolls on the ground, and another player in a dark shirt attempts to intercept it. The video's focus remains on the ball and the footwork of the players involved.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What advertisement is visible right now?",
        "time_stamp": "00:06:02",
        "answer": "D",
        "options": [
          "A. Honda and Chang.",
          "B. Nissan.",
          "C. Ford.",
          "D. Toyota and Chang."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_265_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:01]: The video begins with a downward perspective focused on a white and red soccer ball on a green grassy field. A player’s blue shoe is slightly visible at the bottom of the frame, suggesting they are in motion. [0:09:02 - 0:09:03]: The view shifts to the soccer field where several players are seen in dynamic positions, one preparing to kick the ball towards the goal, and another standing near the goalposts in the background. The sky is overcast and the background shows green trees and goalposts. [0:09:04 - 0:09:05]: The video focuses on the forward movement of players on the field, with one player in a red shirt closest to the goal and another in blue nearby. The field lines and playing area become more evident. [0:09:06 - 0:09:07]: A player in a red shirt stands ready in front of the goal as the ball approaches, positioned centrally in the goal area with another player in blue closing in. [0:09:08]: The goalkeeper, dressed in red, dives to the ground attempting to save the ball, while two players in blue watch closely from nearby. [0:09:09]: Multiple players are seen congregating in front of the goal post, with some dressed in red and blue, indicating a close engagement during the play. [0:09:10]: The scene shifts to the player’s perspective holding a water bottle, indicating a moment of rest or pause in gameplay. The sky remains overcast in the background. [0:09:11]: The player lowers the water bottle, revealing the soccer field extending ahead with distant players and spectators visible around the perimeter. [0:09:12]: A broad view of the field shows it partially with other players dispersed around, indicating some distance from the active play zone. [0:09:13]: The player's hand holding the water bottle returns into view, with the scene showing similar overcast skies and vast green field. [0:09:14]: The focus shifts again to a broader field view with buildings and field boundaries visible, and players continuing in the background. [0:09:15 - 0:09:16]: Attention pivots towards a broader area with the scene capturing a player in blue with a command “Tiger!” being shouted, possibly indicating a nickname or instruction. [0:09:17 - 0:09:19]: The video concludes with two players in blue closer to a distant goal and a defensive player, maintaining the engagement level and field details consistent throughout. The call out “Tiger!” is reiterated.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the player wearing the camera holding right now?",
        "time_stamp": "00:09:10",
        "answer": "D",
        "options": [
          "A. A soccer ball.",
          "B. A whistle.",
          "C. A flag.",
          "D. A water bottle."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Action Recognition",
        "question": "What action does the goalkeeper in red perform as the ball approaches the goal?",
        "time_stamp": "00:09:08",
        "answer": "A",
        "options": [
          "A. Dives to the ground.",
          "B. Kicks the ball away.",
          "C. Jumps to block the ball.",
          "D. Stands still."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_265_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:18",
        "answer": "C",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 0.",
          "D. 4."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_79_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:05",
        "answer": "C",
        "options": [
          "A. 3.",
          "B. 4.",
          "C. 2.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_79_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:02",
        "answer": "C",
        "options": [
          "A. 5.",
          "B. 6.",
          "C. 3.",
          "D. 4."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_79_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:01",
        "answer": "C",
        "options": [
          "A. 2.",
          "B. 5.",
          "C. 3.",
          "D. 4."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      },
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:33",
        "answer": "C",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 5."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_79_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What is the best summary of the actions taken just now?",
        "time_stamp": "0:00:12",
        "answer": "C",
        "options": [
          "A. A worker picked up a bag, added condiments, and handed it to another worker.",
          "B. A worker grabbed a bag, placed a cheeseburger inside, and passed it to a colleague.",
          "C. A worker retrieved a bag, added fries and a cheeseburger, checked the order slip, and moved to the fryer area.",
          "D. A worker took a bag, filled it with a drink, and proceeded to the front counter."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_345_real.mp4"
  },
  {
    "time": "[0:01:38 - 0:01:48]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What is the best summary of the actions taken just now?",
        "time_stamp": "0:01:48",
        "answer": "C",
        "options": [
          "A. A worker placed fries into a fryer, prepared drinks, and gave them to a colleague.",
          "B. A worker assembled a hamburger, wrapped it, and handed it to a customer.",
          "C. A worker collected fries in a bag, placed an order slip on it, and moved it to the counter.",
          "D. A worker took a customer's order, processed the payment, and handed over a receipt."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_345_real.mp4"
  },
  {
    "time": "[0:03:16 - 0:03:26]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What is the best summary of the actions taken just now?",
        "time_stamp": "0:03:26",
        "answer": "C",
        "options": [
          "A. A worker filled a bag with fries, attached an order slip, delivered the bag to the customer, and returned to their station.",
          "B. A worker packed a bag with a cheeseburger, sealed it, printed an order slip, and handed it to another worker.",
          "C. A worker grabbed a bag, placed the order receipt with it, and headed to the pick-up counter to deliver the order.",
          "D. A worker assembled a sandwich, confirmed the customer's name, and moved to the serving counter."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_345_real.mp4"
  },
  {
    "time": "[0:04:54 - 0:05:04]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What is the best summary of the actions taken just now?",
        "time_stamp": "0:05:04",
        "answer": "C",
        "options": [
          "A. A worker reviewed a customer's order, cleaned the workstation, and prepared it for the next task.",
          "B. A worker assembled a new order from scratch, wrapped each item, and handed it to the kitchen staff.",
          "C. A worker retrieved a filled order bag, attached the order slip to the bag, and delivered it to the front counter.",
          "D. A worker took a customer's order, processed the payment, and handed the receipt to a colleague."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_345_real.mp4"
  },
  {
    "time": "[0:06:32 - 0:06:42]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What is the best summary of the actions taken just now?",
        "time_stamp": "0:06:42",
        "answer": "C",
        "options": [
          "A. A worker checked an order ticket, filled a bag with food items, and placed the bag on the counter.",
          "B. A worker retrieved a receipt, confirmed the order with the customer, and then handed the order to another worker.",
          "C. A worker attached a ticket to a bag, moved to the front counter, and handed the bag to a customer.",
          "D. A worker processed a payment, handed the change to a customer, and returned to the kitchen."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_345_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:04]: The video begins with a first-person perspective view of an illustration featuring a white clock face with silver knife and fork hands, set at 10 o'clock. The number \"10\" is prominently displayed to the left of the clock on a gray background.  [0:00:01 - 0:00:04]: The scene transitions to a man standing against a background that reads \"RAMSAY in 10.\" He is wearing a navy blue shirt. [0:00:05]: A transition screen features a dark purple and light blue geometric shape overlaying the previous frame. [0:00:05 - 0:00:06]: The scene shifts to a bright kitchen with white brick walls and colorful shelves. Various cooking utensils and ingredients are arranged neatly on the shelves. The word \"COOK\" in bright red letters is displayed on the top shelf. The man, now wearing a green shirt, stands in the center of the frame, explaining something. [0:00:07 - 0:00:08]: The man continues his explanation, making expressive gestures with his hands. He appears to be very engaged in his demonstration. [0:00:09 - 0:00:10]: The man turns to his right, speaking towards an off-screen audience or object. The background reveals more kitchen equipment including an oven and some jars. [0:00:11 - 0:00:13]: He faces forward again, continuing his explanation while occasionally gesturing. The kitchen setting remains the same with the word \"HOT\" appearing on the bottom left of the frame. [0:00:14]: The man looks directly into the camera, emphasizing his point. [0:00:15 - 0:00:16]: He claps his hands together, continuing to talk, showing enthusiasm in his expressions. [0:00:17 - 0:00:18]: He gestures sideways, looking animated and engaged. The angle provides a closer view of the jars and appliances in the kitchen. [0:00:19]: The video ends with the man standing calmly, finishing his explanation, with the word \"COOK\" still visibly prominent in the background.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What word appears on the bottom left of the frame while the man continues his explanation?",
        "time_stamp": "00:00:13",
        "answer": "C",
        "options": [
          "A. COOK.",
          "B. RAMSAY.",
          "C. HOT.",
          "D. READY."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_31_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:01]: A man wearing a green t-shirt stands in a kitchen with white brick walls and open wooden shelves. On the countertop in front of him, there are various items, including a glass bowl of beaten eggs, a bowl of raw chicken pieces, and various ingredients and utensils. He is holding a piece of chicken over the bowl while sprinkling what seems to be seasoning or flour onto it. [0:02:01 - 0:02:02]: The man continues to prepare the chicken by placing it into the glass bowl. Behind him, there are several shelves with neatly arranged dishes, and on the left, there is a red \"COOK\" sign on the top shelf. [0:02:02 - 0:02:03]: The man is focused on the task, which involves handling the chicken pieces. In the background, various utensils and jars are visible on the shelves, while a gas stove and several frying pans are noticeable on the countertop. [0:02:03 - 0:02:04]: The man continues working with the chicken, ensuring it is coated evenly. At this point, he picks up another piece of chicken. Various kitchen tools, like a whisk and some bottles, are scattered around the working area. [0:02:04 - 0:02:05]: Closer view of the man's hands as he handles the raw chicken fillet, about to dip it into the beaten egg mixture. Both bowls are transparent, making the contents visible. There is some cilantro placed on the cutting board. [0:02:05 - 0:02:06]: The man carefully places the chicken piece into the beaten egg mixture. The workspace is organized, with cutting boards, bowls, and other ingredients aligned in a practical manner. [0:02:06 - 0:02:07]: The man dips the chicken piece into the beaten egg mixture, ensuring it is well-coated. The kitchen appears orderly, with knives hanging on a magnetic strip on the wall behind him. [0:02:07 - 0:02:12]: He continues this process, turning towards the counter to his right, where a bowl with more chicken pieces is placed. The background includes a sink with a faucet and a neatly organized shelf with bowls and plates. Green and brown bottles and a lush bunch of herbs are also present on the counter. [0:02:12 - 0:02:13]: The man focuses on coating the chicken pieces, putting them into the egg mixture one at a time. He appears to be explaining something, possibly providing cooking tips as he works. [0:02:13 - 0:02:14]: An overhead shot shows the man's hand coating the chicken in flour. Surrounding the workspace are various bowls filled with ingredients like chopped nuts, herbs, and seasoning, suggesting they are part of the recipe. [0:02:14 - 0:02:15]: The man proceeds to coat the chicken pieces, which are now in the egg mixture, with flour from another bowl. The countertop remains so neatly arranged with cooking ingredients and equipment, ensuring efficiency. [0:02:15 - 0:02:16]: The man shakes off the excess flour from a piece of chicken that has been dipped in the egg mixture and then the flour, preparing it for cooking. He is very meticulous in his method, emphasizing thorough coating. [0:02:16 - 0:02:17]: Returning to the bowl with the coated chicken pieces, the man continues to handle the ingredients attentively. The organized kitchen setup indicates a well-prepared cooking session. [0:02:17 - 0:02:18]: The man looks focused as he coats another piece of chicken. Various ingredients are laid out systematically to ensure each step of the cooking process is streamlined. The backdrop includes a shelved arrangement with knives, plates, and other kitchen essentials. [0:02:18 - 0:02:19]: The man continues to work diligently, coating the chicken pieces. His actions suggest a level of expertise and comfort in the kitchen. The setting remains consistent with various cooking ingredients and tools visible.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the man wearing in the kitchen?",
        "time_stamp": "00:02:40",
        "answer": "C",
        "options": [
          "A. A blue apron.",
          "B. A red t-shirt.",
          "C. A green t-shirt.",
          "D. A white shirt."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Action Recognition",
        "question": "What is the man doing with the chicken right now?",
        "time_stamp": "00:02:00",
        "answer": "D",
        "options": [
          "A. Frying it.",
          "B. Cutting it.",
          "C. Washing it.",
          "D. Coat the chicken with flour."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_31_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:02]: A person is seen standing in a kitchen, dipping their hand into a bowl of beaten eggs on a wooden cutting board. The bowl is transparent, and another bowl containing breadcrumbs is positioned to the left of the egg bowl. In the background, a stove with a pot is present on the left side, and there are blue and wooden cabinets on either side of the countertop. A bunch of green herbs is placed near the edge of the cutting board. [0:04:03 - 0:04:03]: The person starts transferring the egg-coated item into the bowl of breadcrumbs. The individual is wearing a green short-sleeve shirt. [0:04:04 - 0:04:06]: In a wider shot, the same person continues handling the food while standing in a white-tiled kitchen with wooden shelves laden with plates, bowls, and various utensils. The countertop also holds additional items, including bottles of oil and a dish with green vegetables. [0:04:06 - 0:04:09]: The person stands in the same position, adding seasoning or another ingredient to the bowl of breadcrumbs and continuing to coat the item. [0:04:09 - 0:04:10]: The perspective shifts to an overhead view showing the two bowls on the cutting board. The person uses both hands to cover the food item in the breadcrumbs mixture. Around the board are bowls containing spices and condiments. [0:04:11 - 0:04:13]: The person continues to firmly press the breadcrumbs onto the food item, ensuring it is well-coated. [0:04:14 - 0:04:16]: The scene returns to the front view of the person standing in the kitchen, adding more breadcrumbs to the bowl. Utensils and ingredients are arranged neatly on the countertop around them. [0:04:17 - 0:04:19]: The person starts cleaning up, using a cloth to wipe their hands, and the bowls with ingredients are seen on the wooden cutting board. The kitchen's layout includes modern appliances, with more utensils and ingredients arranged nearby.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person dipping their hand into right now?",
        "time_stamp": "00:04:02",
        "answer": "C",
        "options": [
          "A. A bowl of flour.",
          "B. A bowl of breadcrumbs.",
          "C. A bowl of beaten eggs.",
          "D. A bowl of green herbs."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_31_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:01]: A person in a green t-shirt stands in a kitchen with white brick walls and shelves holding various kitchen items like utensils, jars, and bowls. He holds a white cloth in one hand while looking down towards the countertop, which has a chopping board and several ingredients on it. [0:06:01 - 0:06:02]: The person in the green t-shirt extends his arm to the left towards a bottle of olive oil while looking at the countertop. The shelves in the background display additional kitchen items such as dishes and cooking pots. [0:06:02 - 0:06:03]: The person moves a green vegetable, possibly a cucumber, on the chopping board with his right hand while maintaining his gaze downwards. The background remains consistent, showing the organized shelves. [0:06:03 - 0:06:05]: Positioned behind the kitchen counter, the person continues preparing the ingredients on the chopping board. He maintains focus, occasionally glancing up towards the background where various kitchen tools and dishes are neatly arranged on the shelves. [0:06:05 - 0:06:06]: While still standing near the kitchen counter, the person turns slightly to his right and reaches towards the upper shelf, appearing to grab a jar off a shelf. The countertop holds an assortment of green vegetables and bowls. [0:06:06 - 0:06:07]: Returning his attention to the chopping board, the person appears to hold an object, possibly a piece of vegetable in his right hand. He shifts his position slightly to the left while glancing down. [0:06:07 - 0:06:08]: The perspective shifts to a top-down view, focusing on the kitchen counter where different items, including sliced vegetables, a cucumber on a chopping board, and a frying pan, are visible. The person’s arm reaches for the cucumber. [0:06:08 - 0:06:09]: A close-up view captures the person slicing a cucumber on the chopping board. He holds the cucumber steadily while his other hand manages the knife. Several kitchen items, including olive oil and a bunch of parsley, are within reach on the countertop. [0:06:09]: The person is seen slicing cucumbers with a knife on the chopping board, with several kitchen items like olive oil and parsley positioned nearby on the countertop. [0:06:10 - 0:06:11]: The person shifts his focus while slicing, appearing to concentrate on the task. Surrounding kitchen items provide a backdrop to the chopping activity as he works precisely with the knife. [0:06:11 - 0:06:12]: With continued focus on the chopping board, the person cuts the cucumber into smaller pieces. His careful movements ensure the task is performed efficiently amidst the neatly organized kitchen setup. [0:06:12 - 0:06:13]: The person completes slicing the cucumber pieces, placing them aside on the chopping board. The organized kitchen backdrop remains consistent, displaying various utensils and ingredients neatly arranged on the shelves and countertop. [0:06:13 - 0:06:14]: Standing upright again, the person holds the cucumber, turning slightly towards the left. He engages momentarily with the task at hand against the background setting of the organized kitchen shelves filled with dishes and jars. [0:06:14 - 0:06:15]: Shifting towards the right side of the countertop, the person repositions some items while continuing to hold a piece of cucumber. His gaze moves briefly towards the countertop setup and the neatly arrayed kitchen items behind him. [0:06:15 - 0:06:16]: The person appears to inspect and remove the cucumber's peel. His focus remains on the vegetable, and the kitchen's backdrop continues to feature organized kitchenware on the shelves. [0:06:16 - 0:06:17]: Paying close attention, the person uses a peeler to remove the cucumber's skin. The kitchen background continues to showcase various cooking items and utensils well-organized on the shelves. [0:06:17 - 0:06:18]: Concentrated on peeling the cucumber, the person efficiently manages the task while positioned behind the counter. The well-maintained and neatly arranged kitchen backdrop remains visible. [0:06:18 - 0:06:19]: Consistent attention is given to the cucumber as the person completes the peeling task. The kitchen's background remains organized, exhibiting various utensils and ingredients neatly placed on the shelves and countertop.",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many pieces of fried chicken are there in the frying pan right now?",
        "time_stamp": "00:06:12",
        "answer": "B",
        "options": [
          "A. 3.",
          "B. 4.",
          "C. 7.",
          "D. 9."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_31_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with the text \"UNDER CONSTRUCTION TOURNAMENT\" displayed in the center of a black screen with white text.  [0:00:02]: The screen turns completely black. [0:00:03 - 0:00:05]: The frame transitions to a new title screen that reads \"GRAVITY THROTTLE RACING.\" The background features a yellow triangle with a red arrow pointing to the right and a blue border. The background of this title screen is a mix of dark colors with subtle lighting effects. [0:00:06 - 0:00:10]: The next scenes provide a view of what appears to be a miniature race track in four different quadrants labeled \"Drone1,\" \"Runway3,\" \"Drift Turn,\" and \"Drone2.\" Each quadrant displays sections of the race track from various angles, showcasing different parts of the track layout. These sections primarily use white, gray, and some black colors, giving an impression of a detailed and complex design.  [0:00:11 - 0:00:15]: The view changes to a new set of four quadrants labeled \"Train1,\" \"Scrambler,\" \"Train2,\" and \"Big-U.\" These sections show different parts of another miniature landscape with painted backgrounds depicting mountains and sky, along with sections of the track winding through the terrain. The tracks are primarily grey, and the backgrounds are colorful with shades of brown, green, and blue. [0:00:16 - 0:00:19]: The camera shifts to a first-person point of view, showing a track layout close up with detailed scenery. A small, inset video in the upper left corner shows a person speaking, accompanied by red and blue text that introduces \"Bayreuth, Germany\" and \" 'Munch' Cromartie.\" The track consists of winding paths, railings, and miniature vehicles. As the camera navigates through the track, it details more specific sections and elements, including other miniature trains and cars moving around the set landscape.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is depicted in the quadrants labeled \"Train1,\" \"Scrambler,\" \"Train2,\" and \"Big-U\"?",
        "time_stamp": "0:00:15",
        "answer": "B",
        "options": [
          "A. Various miniature race tracks.",
          "B. Four different sections of the race track.",
          "C. A detailed map of a large city.",
          "D. A diagram of an amusement park."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_497_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:02]: The scene shows a model racetrack with multiple toy cars racing along a winding, elevated section of the track. Two cars, one blue and one white with red accents, are seen closely competing on a sharp turn. The track itself is detailed with railings and has multiple levels including a higher track section with train tracks. [0:04:03 - 0:04:05]: The cars continue to race along the track, with a background featuring a painted scenery. Additional toy trains are visible on a parallel track, one of which is a larger black locomotive. The racing cars encounter straight and curved segments of the track, competing for position. [0:04:06 - 0:04:08]: The scene shifts slightly, showing the cars from a higher vantage point. The leading blue car is closely followed by the white car with red accents. The track has a mix of straight and curve sections, with the toy trains still visible on the adjacent tracks. [0:04:09 - 0:04:11]: Another angle shows the cars racing in a straight line. They are closely grouped together, with the white car now making a move to overtake the leader. Trains continue to move along the track in the background. [0:04:12 - 0:04:14]: The cars are seen taking a sharp turn. The white car with red accents has taken the lead, while the blue car follows closely. Another white car is nearby, and the vibrant track details and background appear consistent with previous frames. [0:04:15 - 0:04:20]: The scene transitions to a scoreboard titled \"FINAL RACE\" from Gravity Throttle Racing showing scores for different rounds. The scores for each round are visible, with cars of different colors and designs represented, indicating their performance in various rounds. The background has a peaceful scenery of mountains and fields, while the scoreboard provides a detailed summary of the race results.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What does the scoreboard titled \"FINAL RACE\" show?",
        "time_stamp": "0:04:21",
        "answer": "B",
        "options": [
          "A. The number of laps completed.",
          "B. The cars' performance in round 1 and round 2.",
          "C. The fastest lap time.",
          "D. The starting positions of the cars."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_497_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:05]: The video shows a first-person angle of three toy cars racing on a miniature sandy racetrack. One black car, one white car with black and red patterns, and one red car with a white roof are traveling down a slope with grooves creating three separate lanes in the sandy terrain. The background is primarily blue, with the track extending to the right and uphill. [0:08:06 - 0:08:09]: The cars continue up a cardboard ramp, transitioning from the sandy track to a smooth, winding track with curves and elevated sections. The track is lined with low walls, and the background features a painted scenery of mountains and greenery. The cars navigate the turns, showing controlled, smooth movement, emphasizing the track’s design. [0:08:10 - 0:08:16]: As they race further, the white car with the red roof leads into a wider, more complex section of the track, which includes a train set with miniature rail cars and engines. The train tracks run parallel to the race track, and the detailed background features a hilly landscape with green and red hues. The white car with red patterns continues to lead, maintaining its pace and trajectory, followed closely by the red car and the black car. [0:08:17 - 0:08:20]: Approaching the final section of the track, the cars reach a designated area resembling an airport runway. White markings and small model airplanes are visible on the track. The white car with red patterns is still in the lead, navigating a runway turn followed by the black car, while the red car trails behind. The surrounding landscape continues to be detailed with mountains and sky painted in the background.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Which car is leading right now?",
        "time_stamp": "0:08:29",
        "answer": "C",
        "options": [
          "A. Black car.",
          "B. Red car.",
          "C. White car with red roof.",
          "D. Green car."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_497_real.mp4"
  },
  {
    "time": "[0:11:00 - 0:12:00]",
    "captions": "[0:12:00 - 0:12:20] [0:12:00 - 0:12:08]: In a set-up resembling a model racetrack, a small white toy car is seen navigating a curved, elevated track. The track is surrounded by white sculpted terrain and a blue wall. The car moves onward, making its way down the curve of the track. The area appears to be finely detailed with careful attention to the curves and edges of the track. [0:12:09 - 0:12:10]: The toy car continues along the track. The perspective of the shot remains largely the same, showcasing the intricate details of the track's structure. [0:12:11 - 0:12:15]: The scene shifts to a wide shot of a road in front of a mural backdrop displaying a mountainous landscape with vibrant colors, including reds, greens, and browns. The road is marked with white lines, and a toy car, now blue and different from the previous one, travels the straight path, captured from a higher viewpoint. [0:12:16]: The blue toy car continues on the marked road, and more of the mural landscape becomes visible, featuring green hills blending into rugged mountains. [0:12:17 - 0:12:18]: The track changes orientation, and another section of the mural with towering cliffs is visible. The blue toy car moves forward, and the terrain in the foreground includes large, nuanced sculptures, giving the appearance of a rugged, arid environment. [0:12:19]: The scene reveals a larger portion of the model track setup with various cars on different track levels. The landscape mural in the background remains, adding depth and context to the scene. Multiple cars are seen navigating the complex track system.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Which car won first place in this competition?",
        "time_stamp": "0:11:40",
        "answer": "C",
        "options": [
          "A. The blue car.",
          "B. The dark purple car.",
          "C. The blue and black car.",
          "D. The red car."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_497_real.mp4"
  },
  {
    "time": "[0:12:00 - 0:13:00]",
    "captions": "[0:12:00 - 0:12:20] [0:12:00 - 0:12:08]: In a set-up resembling a model racetrack, a small white toy car is seen navigating a curved, elevated track. The track is surrounded by white sculpted terrain and a blue wall. The car moves onward, making its way down the curve of the track. The area appears to be finely detailed with careful attention to the curves and edges of the track. [0:12:09 - 0:12:10]: The toy car continues along the track. The perspective of the shot remains largely the same, showcasing the intricate details of the track's structure. [0:12:11 - 0:12:15]: The scene shifts to a wide shot of a road in front of a mural backdrop displaying a mountainous landscape with vibrant colors, including reds, greens, and browns. The road is marked with white lines, and a toy car, now blue and different from the previous one, travels the straight path, captured from a higher viewpoint. [0:12:16]: The blue toy car continues on the marked road, and more of the mural landscape becomes visible, featuring green hills blending into rugged mountains. [0:12:17 - 0:12:18]: The track changes orientation, and another section of the mural with towering cliffs is visible. The blue toy car moves forward, and the terrain in the foreground includes large, nuanced sculptures, giving the appearance of a rugged, arid environment. [0:12:19]: The scene reveals a larger portion of the model track setup with various cars on different track levels. The landscape mural in the background remains, adding depth and context to the scene. Multiple cars are seen navigating the complex track system.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What does the scoreboard titled \"FINAL RACE\" show?",
        "time_stamp": "0:12:38",
        "answer": "A",
        "options": [
          "A. The cars' performance from round 1 to round 7.",
          "B. The cars' performance in round 1 and round 2.",
          "C. The fastest lap time.",
          "D. The starting positions of the cars."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_497_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video starts with an animation showing a large number \"10\" prominently in the center. The number is white and has a silver knife and fork aligned as clock hands, forming an analogy to a clock. The background is a gradient of blue colors;  [0:00:01 - 0:00:05]: The scene transitions to a man in a dark blue shirt, smiling broadly. Behind him, the text \"RAMSAY in 10\" is displayed. The text is in blue and white colors, matching the color scheme of the previous frame. He remains smiling consistently, and the background remains a gradient of blue; [0:00:05 - 0:00:10]: The scene changes to a kitchen setting. The man now wears a green shirt and stands in front of a white brick wall with shelves filled with kitchen items. The shelves have colored glass bottles, bowls, and various kitchen utensils arranged neatly. The word \"COOK\" appears in large red letters on the top left shelf. Several knives are mounted below the shelves. He appears to be talking, gesturing with both his hands. The background includes a few green bottles and some cookbooks. The countertop is wooden, and utensils are arranged meticulously behind him; [0:00:11]: A close-up side view of the man shows him animatedly speaking with hand gestures. The kitchen is partially visible in the background, showcasing a modern kitchen setup with blue cabinets and built-in ovens; [0:00:12 - 0:00:19]: The focus returns to a frontal view of the man standing in the kitchen. He continues to talk, gesturing occasionally. His facial expressions become more serious and intent, indicating he is explaining something important. Throughout these frames, there are slight variations in his hand movements and facial expressions, but the background details of the kitchen remain constant. The video frames depict a continuous scene from a perspective in a kitchen setting, featuring a man engaging with the audience in a detailed explanation or conversation related to cooking or kitchen tasks.\n[0:00:20 - 0:00:40] [0:00:20 - 0:00:21]: The video begins with a scene in a modern kitchen. In the center stands a person wearing a teal polo shirt and gesturing with their hands. The background features white brick walls with shelves that hold various kitchen items, including cups, bowls, and decorative items. On the main counter, to the left, large red letters spell out the word \"COOK,\" while below on the counter, smaller white letters spell \"HOT.\" Various kitchen tools like knives and bottles are also visible. [0:00:21 - 0:00:22]: The person begins to turn to their right, seeming to prepare for the next step. The countertop continues to show an array of bowls, jars, and utensils next to a stove on the left side. [0:00:22 - 0:00:23]: The person, now facing the camera again, stands in front of a wooden cutting board that holds several pieces of raw chicken. With hands slightly above the board, they start pointing at the chicken, ready to demonstrate something. [0:00:23 - 0:00:24]: A close-up of the cutting board shows the person's hands emphasizing particular parts of the chicken. In the foreground are various ingredients, including a red tin, some metal bowls, a white ceramic bowl, and a bottle with a black top. [0:00:24 - 0:00:25]: The camera shifts slightly to the right, showing a better view of the assortment of ingredients on the countertop. Items include spices, a glass bowl for mixing, jars, and a wooden chopping block. The person is less visible now, but their hand movements continue above the cutting board. [0:00:25 - 0:00:26]: A similar scene shows the person's hands hovering over the chicken pieces on the cutting board. The focus remains on the ingredients, highlighting the prepared workspace with various bowls, jars, and cooking tools strategically placed around. [0:00:26 - 0:00:27]: The person continues to use dynamic hand movements, possibly explaining the next steps in the cooking process. The view emphasizes the organized kitchen setup, including the clearly labeled wooden board, Roos S. [0:00:27 - 0:00:28]: The camera captures the individual performing another hand gesture, possibly related to seasoning the chicken. The surrounding ingredients remain consistent, indicating preparedness for the cooking activity. [0:00:28 - 0:00:31]: The camera zooms closer to the cutting board as the person sprinkles a substance onto the chicken. The scene displays a range of fresh ingredients, including lemons, avocados, and herbs, indicating a planned, comprehensive recipe. [0:00:31 - 0:00:32]: The angle shifts slightly again, focusing on the person who is now looking down at the cutting board with an intent gaze, possibly considering the next step. The well-organized kitchen remains a prominent backdrop. [0:00:32 - 0:00:33]: The person is once again facing the camera and gesturing with both hands over the cutting board. Beside them, a large glass mixing bowl and a collection of spices and oils underline the preparatory phase of the cooking session. [0:00:33 - 0:00:34]: The scene shows the person leaning slightly forward, reaching out to touch or rearrange something on the cutting board. The broad workspace continues to show a range of ingredients, suggesting an elaborate dish in progress. [0:00:34 - 0:00:35]: The individual gestures towards a series of spice containers lined up on the counter. The side view emphasizes the length of the workspace and the meticulous arrangement of kitchen tools and ingredients. [0:00:35 - 0:00:36]: The person is seen picking up a container, possibly to add a seasoning to the dish. The assortment of items on the counter, including small bowls, a bottle of oil, fresh fruits, and spices, strongly complements the culinary theme. [0:00:36 - 0:00:37]: Camera focuses on the person's hand as they pick up a pinch of spice from a small container. The organized kitchen counter, displaying lemons, a mix of sauces, and additional cooking utensils, serves as a backdrop to the activity. [0:00:37 - 0:00:39]: The camera angle changes to an overhead view, showing the person’s hand sprinkling the spice onto the chicken from above. The workspace showcases a detailed arrangement of ingredients, including a bowl of cheese, lemons, and other cooking essentials. [0:00:39]: The person smiles at the camera, holding an ingredient in the middle of the preparation process. The display of fresh ingredients and kitchen tools around him underscores a highly organized and methodical\n[0:00:40 - 0:01:00] [0:00:40 - 0:00:43]: The video opens with a first-person perspective of a kitchen. There is a man, wearing a dark green polo shirt, standing behind a countertop. The countertop is filled with various cooking ingredients and equipment. On the left side, there is a cutting board with pieces of what looks like prepared chicken. The background features white brick walls with shelves containing various kitchen items including plates, jars, and a brown clay pot. The man is seen reaching out to pick something up from the countertop. [0:00:44]: The focus shifts to a close-up of the ingredients on the countertop. There is a bowl of white sauce in the center. Surrounding it, there are two avocados, a lemon, a lime, an assortment of herbs, and several small bowls containing different spices and seasonings. The prepared chicken can still be seen on the cutting board in the background. [0:00:45 - 0:00:49]: Another close-up shot from above captures more of the countertop. In addition to the previously mentioned items, there is a bottle of what appears to be olive oil next to the avocados, a jar of honey, and a small glass container with a red lid containing more spices. The camera then moves back slightly, showing the man’s arm reaching down to interact with the ingredients. [0:00:50 - 0:00:51]: The focus returns to the man as he speaks, holding a small item between his thumb and index finger. The background remains the same, with the kitchen's blue cabinets and white brick walls. [0:00:52 - 0:00:55]: The man steps back slightly, continuing to speak. His gestures suggest he is explaining something. On the countertop, the cooking items are still clearly visible. Behind him, kitchen appliances and shelves filled with various utensils and containers are seen. [0:00:56 - 0:00:57]: The man looks down at the countertop, raising one finger as though emphasizing a point. The background maintains the same setting as before, with the shelves and kitchen items still in view. [0:00:58 - 0:00:59]: The focus shifts once again to a close-up of the man as he reaches for a small bowl with sauce and places it in a larger mixing bowl on the counter. His facial expression is concentrated, and he appears to be explaining the next step in his process. The chicken pieces on the cutting board remain in frame, along with other nearby ingredients.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is displayed prominently in the center at the very beginning of the video?",
        "time_stamp": "0:00:20",
        "answer": "D",
        "options": [
          "A. A clock with hands.",
          "B. A knife and fork.",
          "C. A kitchen setting.",
          "D. A large number \"10\"."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_14_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:05]: A man stands in a kitchen with a white-tiled backsplash and shelves filled with dishes and ingredients. He wears a dark green polo shirt and begins preparing a recipe. He reaches for a glass mixing bowl on the wooden countertop, near a raw, spatchcocked chicken displayed on a cutting board. [0:01:05 - 0:01:07]: The man adds seasoning to the bowl, starting with ground spices from a small jar. He sprinkles the contents evenly into the bowl while wooden cabinets and additional kitchen utensils are visible in the background. [0:01:07 - 0:01:15]: The man continues seasoning by adding various ingredients, including spices from a thin, red, rectangular can. He meticulously sprinkles these spices, adding layers of flavor to the mixture in the bowl, which is positioned next to the raw chicken on the cutting board. Meanwhile, the kitchen environment, including a stovetop and an assortment of nearby bowls and utensils, remains visible. [0:01:15 - 0:01:18]: Positioned at an angle that captures more of the kitchen’s layout, the man moves swiftly as he seasons the mixture with salt from a grinder. The spacious kitchen is warm and well-lit, with a mixture of modern and rustic elements, such as potted plants near large windows and industrial lights overhead. [0:01:18 - 0:01:20]: The video ends with a close-up of the seasoned mixture in the glass bowl next to the raw chicken, highlighting the combination of spices added so far.\n[0:01:20 - 0:01:40] [0:01:20 - 0:01:24]: A person wearing a dark green polo shirt is seen in what appears to be a kitchen. The kitchen has white brick walls with two dark blue cabinets, one of which is open. There are various items on wooden shelves, including glassware, bottles, cookware, and a red \"Cook\" sign. In front of the person on the countertop, several items, including a large cutting board, a glass mixing bowl, an assortment of fresh vegetables, and other cooking ingredients, can be seen. The person is in the process of slicing food on a cutting board.  [0:01:25 - 0:01:29]: The person can be seen mixing food in the glass bowl. The mixing bowl is placed on the cutting board, and the person uses both hands to knead or mix the food thoroughly. At this point, the person appears to be focusing on ensuring the ingredients in the bowl are well combined. Given the hands' movement, the content in the bowl gets mixed vigorously. [0:01:30 - 0:01:34]: The perspective shifts to a different location in the kitchen, showing a closer view of the person repetitively kneading or mixing the food in the bowl. The surrounding kitchen layout continues to show the same elements, such as the white backsplash, wooden shelves filled with kitchen utensils and ingredients, and various small kitchen appliances. The person's movements are deliberate and consistent as they continue to mix the food. Various bowls, bottles, and utensils on the countertop add to the kitchen's busy and well-used appearance.  [0:01:55 - 0:01:58]: A top-down view shows the person's hands actively mixing food in the bowl placed on the cutting board. The cutting board is stained with food, suggesting ongoing prep work. The surrounding countertop features various small bowls and jars containing ingredients like spices and liquids. The person's hands are engaged in repetitive motions, ensuring the mixture inside the bowl is being worked on thoroughly. This perspective provides a clear and detailed view of the cooking process, highlighting the textures and movements of the ingredients being mixed. [0:01:39]: Returning to ground level, the person is still focused on the glass bowl, their hands moving rhythmically as they continue to mix the contents inside the bowl. The countertop around them, filled with various kitchen supplies, remains visible, depicting an active cooking session. The combination of motion and preparation tools suggests that the person is meticulously working on a culinary creation, continuously blending the ingredients.\n[0:01:40 - 0:02:00] [0:01:40 - 0:01:42]: In a kitchen, a man is standing at a counter, focused on a bowl containing a mixture. Wearing a dark short-sleeved polo shirt, he positions his hands inside the bowl, working with the contents. The bowl is placed on a wooden chopping board on the counter, which also holds various ingredients and utensils, including bowls, a knife, and bottles. Behind him, a brick wall lined with shelves holds cooking equipment and other items. [0:01:43 - 0:01:47]: As the man continues mixing the contents in the bowl, he adjusts the mixture and uses one hand to manipulate what appears to be a piece of meat or a similar ingredient. His expressions change slightly, showing concentration and engagement with the task at hand. The kitchen background, with its modern appliances and neatly arranged items on the shelves, remains consistent. [0:01:48 - 0:01:50]: He looks up momentarily from his task, turning his head to the side as if responding to something. Afterward, he directs his attention back to the bowl and continues his preparatory activities.  [0:01:51 - 0:01:54]: The man shifts focus, wiping his hands with a cloth while stepping sideways, moving from the counter with the chopping board to a stove area. He places the bowl with the mixture aside on another counter. [0:01:55 - 0:01:57]: Next to the stove, he carefully pours oil onto a grill pan, preparing the cooking surface. The oil is in a small bottle with a spout, allowing controlled pouring. [0:01:58 - 0:01:59]: Continuing the task, he pours more oil, ensuring it spreads evenly on the grill. The camera angle provides a clear view of the grill pan and the surrounding stove area with other utensils and ingredients within reach.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What item is the man holding while adding seasoning to the bowl?",
        "time_stamp": "0:01:14",
        "answer": "D",
        "options": [
          "A. A small jar.",
          "B. A wooden spoon.",
          "C. A bottle of oil.",
          "D. A thin, red, rectangular can."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_14_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:00:00 - 0:00:03]: In a kitchen with white brick walls and shelves filled with dishes and utensils, a person wearing a dark green polo shirt stands behind a counter. At center-right, the person mixes ingredients in a clear glass bowl, positioned atop a wooden cutting board. Nearby, there are stacked bowls, a small jar, a bottle of oil, a lemon, and other cooking essentials;  [0:00:04 - 0:00:06]: The camera focuses on the person's hands working in the glass bowl, blending and lifting chicken pieces with a reddish seasoning. With a teal-colored cabinet in the background, the person's actions are meticulously captured as they handle and prepare the chicken;  [0:02:07 - 0:02:08]: The person transfers the seasoned chicken to a hot grill pan on the stove, carefully placing it on the heated surface. The kitchen setup includes a saucepan on an adjacent burner, emphasizing the cooking process;  [0:02:09 - 0:02:14]: An overhead shot shows the chicken sizzling on the grill. The person’s hands move the glass bowl out of the way while sprinkling additional seasoning onto the meat. The action captures the essence of methodically cooking and seasoning the chicken for optimal flavor;  [0:02:15 - 0:02:18]: Back to a side angle, the individual continues basting the chicken while holding the glass bowl. Turning briefly and gesturing with one hand, they engage with an unseen audience, indicating possible instructions or explanations regarding the cooking technique;  [0:02:19]: Another angle portrays the person passionately explaining something, hands gesturing above the countertop. The kitchen backdrop with its organized elements and the ongoing cooking process remain visible, enhancing the relatability and realism of the scene.\n[0:02:20 - 0:02:40] [0:02:20 - 0:02:21]: A person is standing in front of a kitchen counter, facing the stove. The back of their head shows short, light brown hair. They are wearing a dark green polo shirt. The kitchen has a modern design with teal cabinets, white brick walls, and wooden shelves holding various kitchen items. On the counter, there are bowls and a griddle with pieces of seasoned chicken. [0:02:21 - 0:02:24]: The person continues facing the counter, apparently preparing something on the work surface. The griddle on the stove with seasoned chicken is visible. An open saucepan and other kitchen utensils can be seen in the background. [0:02:24 - 0:02:31]: The camera shifts focus to the griddle where the pieces of seasoned chicken are cooking. The chicken pieces appear to be in a frying stage, with a slight hint of smoke or steam rising from the griddle. [0:02:32 - 0:02:34]: The person now turns towards the camera, moving slightly to the left. The kitchen background remains consistent with shelves, dishes, and utensils. The framed word \"HOT\" is visible on the counter's edge beside other cooking items. [0:02:34 - 0:02:38]: Facing the camera, the person seems to be explaining something, using hand gestures to emphasize points. The stovetop and griddle with the chicken remain constant in the background, with the modern kitchen décor as the backdrop. [0:02:38 - 0:02:39]: The person continues to talk, tilting their head slightly and making more hand gestures. They pick up a pan from the stovetop, holding it up beside them as if showing something related to the cooking process. Vegetation visible through a window or a glass door adds a touch of greenery to the scene.\n[0:02:40 - 0:03:00] [0:02:40 - 0:02:41]: In a modern kitchen with white subway tile backsplash and stainless-steel appliances, a man stands at a countertop, speaking animatedly while holding his hands together in front of him. To his right is a stove with a large pan on it, and various utensils are scattered on the counter to his left. [0:02:41 - 0:02:42]: The man looks down and keeps his hands together in front of him. Behind him, shelves holding various kitchen items like dishes and containers are visible, and a double oven is built into the wall to his right. [0:02:42 - 0:02:43]: The man lifts a lid off the pan on the stove, revealing several pieces of food cooking. He uses his left hand to hold the lid and his right hand to adjust the food. [0:02:43 - 0:02:44]: The man points his right index finger up, possibly indicating a point, while still holding the lid in his left hand. He looks to his right, focusing intently. [0:02:44 - 0:02:45]: He reaches over with his right hand to adjust something off-camera while continuing to cook the food on the stove. His body is slightly turned to the right. [0:02:45 - 0:02:46]: The man now holds a pair of red tongs in his right hand, examining them closely while looking at the food on the stove. He appears focused on what he is doing. [0:02:46 - 0:02:47]: Using the tongs, the man starts to turn over the pieces of food on the stove. His left hand is placed on the counter for support, and the food sizzles on the grill pan. [0:02:47 - 0:02:48]: He continues to turn the food with the tongs, ensuring each piece is evenly cooked. The camera angle shows a close-up of his actions. [0:02:48 - 0:02:49]: The man meticulously adjusts the pieces of food on the grill pan, making sure they are placed correctly. The food starts to develop a charred, crispy surface. [0:02:49 - 0:02:50]: From an overhead perspective, the man uses the tongs to turn a piece of food on the grill pan, ensuring it cooks evenly. The adjacent skillet remains on the stove, unused at the moment. [0:02:50 - 0:02:51]: He adjusts another piece of food with the tongs, focusing intently on his task. The overhead view provides a clear view of his precise movements. [0:02:51 - 0:02:52]: The man checks the positioning of the food once more, using the tongs to move them around on the grill pan. The overhead perspective highlights his efficient method of cooking. [0:02:52 - 0:02:53]: He pauses momentarily, holding the tongs in his right hand over the food, preparing for the next step. The food continues to sizzle on the grill pan. [0:02:53 - 0:02:54]: The man turns back to face the counter with the stove and skillets, focusing once again on the cooking process. His left hand rests near the edge of the counter. [0:02:54 - 0:02:55]: Using the red tongs, he carefully picks up one piece of food from the grill pan, inspecting it closely. His focus remains on the food to ensure it's cooked properly. [0:02:55 - 0:02:56]: He continues holding the food piece with the tongs, looking at its underside. His left hand is still positioned close to the stove, ready to adjust anything if needed. [0:02:56 - 0:02:57]: The man raises the piece of food from the grill pan, possibly to check its doneness. His expression shows concentration as he evaluates the food. [0:02:57 - 0:02:58]: Returning to a close-up, the food piece is turned over by the red tongs, revealing a well-cooked, charred outer layer. The man ensures it is cooked to perfection. [0:02:58 - 0:02:59]: He continues to turn and adjust another piece of food on the grill pan using the tongs, while smoke from the hot grill pan rises into the air. [0:02:59]: The detailed close-up reveals the man’s steady hand turning the food on the grill pan to ensure even charring and cooking on all sides.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What does the person do immediately after transferring the seasoned chicken to the grill pan?",
        "time_stamp": "0:02:12",
        "answer": "B",
        "options": [
          "A. Adds oil to the pan.",
          "B. Sprinkles additional seasoning onto the meat.",
          "C. Places the glass bowl in the sink.",
          "D. Turns up the heat on the stove."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_14_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:01]: A person wearing a dark green shirt tears a bunch of small green herbs and places them into a plastic blender cup, which is already loaded with a white liquid and other green herbs. The blender cup is sitting on a wooden cutting board on a kitchen counter with a stove visible in the background. [0:04:01 - 0:04:02]: The person continues to tear more herbs and drop them into the blender cup. [0:04:02 - 0:04:05]: The camera angle shifts to show more of the kitchen, which features a white brick backsplash and shelves with various kitchen items. The person continues to tear herbs and place them in the blender cup.  [0:04:05 - 0:04:06]: The person plucks several small leaves from a stem and adds them to the blender cup, continuing the same motion. [0:04:06 - 0:04:07]: The perspective changes to an overhead shot directly above the cutting board. The person’s hands tear herbs and drop them into the blender cup from above, emphasizing the focus on the hands. [0:04:07 - 0:04:13]: The overhead view continues as the person tears and adds more green herbs into the blender cup, working systematically. A lid for the blender cup and other kitchen items like small bowls containing spices and salt are visible around the cutting board. [0:04:13 - 0:04:14]: The person places the last few leaves into the blender cup and begins to prepare the area for the next steps. [0:04:14 - 0:04:15]: The person reaches for a container near the cutting board, adding what seems to be more seasoning ingredients into the blender cup. [0:04:15 - 0:04:16]: The perspective returns to a wider shot of the person in the green shirt, who closes the blender cup and positions it on the counter. [0:04:16 - 0:04:17]: The person adjusts the positioning of the blender cup and reaches for a small bottle from a nearby group of condiments on the counter. [0:04:17 - 0:04:18]: The person picks up an avocado and starts to cut it cautiously using a knife, preparing it for addition to the blender cup. [0:04:18 - 0:04:19]: The person continues to handle the avocado with a knife, carving out the flesh as part of the preparation process.\n[0:04:20 - 0:04:40] [0:04:20 - 0:04:22]: A man in a green polo shirt is holding an avocado in his left hand and using a small knife to cut into it with his right hand. The kitchen backdrop includes a white tile backsplash, wooden shelves with various dishes, and a counter with kitchen tools. [0:04:23 - 0:04:24]: The man has finished cutting the avocado and begins to twist the two halves apart, still holding the knife in his right hand. The kitchen counter in front of him has a cutting board and a blending container. [0:04:25 - 0:04:28]: The man has successfully separated the two halves of the avocado. He holds up one half to show the inside. The background continues to show the wooden cabinets and kitchen counter with various tools and ingredients. [0:04:29 - 0:04:30]: The man uses a spoon to scoop out the avocado flesh, holding it over the blending container as seen in the overhead shot from above the cutting board. Ingredients like herbs are already in the blending container. [0:04:31 - 0:04:32]: The man shows the scooped-out avocado half to the camera, pointing to it with the spoon. The background reveals more of the kitchen setup, including a blue cabinet and white brick wall. [0:04:33 - 0:04:34]: The man starts placing the scooped avocado pieces into the blending container, which contains other ingredients. There is a stove in the background with a pot on it, and the counter has several small bowls. [0:04:35 - 0:04:36]: The man reaches for a bottle and begins to open it. The counter in the background has various kitchen equipment and ingredients. [0:04:37 - 0:04:38]: The man holds the open bottle and pours the liquid into the blending container, which now has avocado and other ingredients in it. The kitchen background continues to show various items on the shelves and counter. [0:04:39]: The man continues pouring the liquid, focusing on the blending container. The kitchen setup in the background includes a stove, countertops, and various utensils and ingredients neatly arranged.\n[0:04:40 - 0:05:00] [0:04:40 - 0:04:40]: A person is seen standing at a kitchen counter wearing a green shirt. On the wooden cutting board in front of them, there is a clear plastic blender cup containing ingredients such as green herbs, a slice of lemon, and a white liquid. They are holding a bottle with a golden cap, pouring a dark liquid into the blender cup. [0:04:41 - 0:04:41]: The camera now focuses on the person who places the blender cup on the counter. The background showcases a white tiled wall with wooden shelves holding plates and green bottles, and various kitchen utensils hanging on the wall. [0:04:42 - 0:04:42]: The person screws on the lid of the blender cup. Their right hand firmly holds the cup while their left hand twists the lid. The action takes place in front of the same kitchen setup with neatly arranged shelves and hanging utensils. [0:04:43 - 0:04:43]: The person continues tightening the lid by turning it several times. Their focus remains on ensuring the lid is secure. The white tiled background and organized kitchen tools and items remain visible. [0:04:44 - 0:04:44]: After securing the lid, the person lifts the blender cup slightly, appearing ready to attach it to a blender base. The camera angle gives more visibility to the bottles and jars placed on the kitchen counter. [0:04:45 - 0:04:46]: The person moves the blender cup towards a blender base. The camera captures a better view of their arm movement and the counter setting, which includes containers, utensils, and the wooden cutting board. [0:04:47 - 0:04:49]: The person places the blender cup onto the base, pressing down firmly with both hands to ensure it is properly attached. Visible on the counter are a bottle with a golden cap and other kitchen tools, indicating preparation for blending. [0:04:50 - 0:04:52]: The blender is turned on, and the person holds it down to keep it steady. The ingredients inside the cup start to blend, gradually changing consistency. The background remains consistent with the previous frames, showcasing the organized kitchen setup. [0:04:53 - 0:04:54]: The person stops the blender and detaches the blender cup from the base. There is a noticeable change in the texture of the contents, looking smoother. The person is still standing at the kitchen counter with the array of ingredients and tools around them. [0:04:55 - 0:04:55]: The person gives the blended mixture a good shake to ensure everything is well mixed. They seem content with the texture as they closely examine the contents of the cup. [0:04:56 - 0:04:57]: The person pours the contents from the blender cup into a separate container, with the camera focusing on the action. The kitchen counter remains cluttered with various tools and ingredients. [0:04:58 - 0:04:59]: The person places the now empty blender cup back onto the counter. The scene concludes with them organizing the blender base and the cup, ensuring everything is tidy on the kitchen counter. The background continues to show the well-organized kitchen setup.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What does the person do with the avocado after cutting it?",
        "time_stamp": "0:04:34",
        "answer": "C",
        "options": [
          "A. Adds it whole to the blender.",
          "B. Mashes it with a spoon.",
          "C. Scoops out the flesh and places it into the blending container.",
          "D. Peels off the skin and discards it."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_14_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:10:47]",
    "captions": "[0:10:00 - 0:10:20] [0:00:00 - 0:00:03]: A kitchen setting is visible, with white brick walls and shelves stocked with dishes, glasses, and cooking utensils. A man stands near a gas stove, holding a skillet with red tongs on the right side and a kitchen towel in his left hand. He tilts the skillet towards a plate on the right side of the counter, where a cooked dish is placed. [0:00:03 - 0:00:04]: He places the food from the skillet onto the plate on the right and aligns the skillet back onto the stove. [0:00:04 - 0:00:06]: Still holding the skillet with the tongs and the towel, he turns to face forward and glances at the camera, next to the stove. [0:00:06 - 0:00:07]: The man puts the skillet down on the stove and starts adjusting it. [0:00:07 - 0:00:08]: He continues to look towards the camera while holding a kitchen towel in his left hand. [0:00:08 - 0:00:10]: He begins talking, making a few hand gestures, and appears to give instructions. His facial expression is animated. [0:00:10 - 0:00:12]: He leans slightly forward, continuing to talk, with the blue cabinets and modern kitchen appliances visible in the background. [0:00:12 - 0:00:13]: He stands straight again, holding a kitchen towel with both hands, and appears to be listening intently. [0:00:13 - 0:00:14]: He keeps talking, glancing slightly to the right. The words \"COOK\" are displayed in large red letters on the shelf above the counter. [0:00:15 - 0:00:16]: A close-up of the cooked dish on a wooden cutting board is visible, showcasing grilled meat, roasted vegetables, and a fresh salad with avocado and dressing. [0:00:16 - 0:00:17]: The camera remains focused on the dish, providing a detailed view of the charred, juicy meat and the vibrant colors of the salad. [0:00:17 - 0:00:18]: The camera then zooms in further on the grilled meat, emphasizing its texture and the marks from grilling. [0:00:18 - 0:00:19]: The camera shifts to a top-down view of the cutting board, displaying the entire dish, including the meat, salad, grilled vegetables, and a small bowl of creamy dressing.\n[0:10:20 - 0:10:40] [0:10:20 - 0:10:21]: On a wooden cutting board, there is a beautifully presented dish. The left side of the board features grilled, seasoned chicken with hints of char marks. It is accompanied by a vibrant salad composed of lettuce, red cabbage, slices of avocado, chunks of feta cheese, scattered croutons, and grilled rounds of zucchini.  [0:10:22]: The dish appears from a different angle, with more visibility of the small white bowl filled with a green, creamy dressing located at the top right corner of the wooden board. [0:10:23]: The view transitions to a kitchen setting where a man is speaking, gesturing with his hands. He is wearing a green shirt and standing in front of a background that includes shelves stocked with jars, plates, and kitchen utensils. [0:10:24 - 0:10:26]: The man continues to speak, moving his hands expressively. Behind him is a blue cabinet and shelves filled with various kitchen items, including jars with grains and spices. The background also contains a wooden cutting board and a white-brick wall, adding a rustic charm to the setting. [0:10:27 - 0:10:28]: The man appears animated as he talks, his facial expressions suggest engagement. The background remains the same, with shelves holding neatly arranged crockery and kitchen essentials. [0:10:29 - 0:10:30]: The man remains the focal point, speaking and gesturing. The word \"COOK\" in red letters is prominently displayed on a shelf in the background, alongside green bottles and white dishes. [0:10:31 - 0:10:33]: The scene shifts slightly, showing the man in mid-conversation, appearing enthusiastic. The blue kitchen cabinet and various jars on shelves in the background are still visible, maintaining continuity in the kitchen setting. [0:10:34 - 0:10:36]: Returning to a centered shot, the man continues to talk, now with less exaggerated gestures. The kitchen background remains consistent, showcasing an organized array of kitchenware. [0:10:37 - 0:10:39]: The man concludes his speech, standing calmly with his hands clasped in front of him. The neat and rustic kitchen setting with the white brick wall and tidy shelves filled with jars and plates remains visible in the backdrop.\n[0:10:40 - 0:10:47] [0:10:40 - 0:10:41]: The scene begins with clear visibility of a kitchen. A man, wearing a green t-shirt, stands centrally in front of a white-tiled wall with shelves holding various items like dishes, glasses, and green bottles. On the upper right corner of the screen shows the timestamp \"10:40\". The word \"COOK\" in red letters is displayed prominently on a shelf behind him. He has light brown hair and is looking directly at the camera, with both hands clasped in front of him, slightly lower than chest level. The countertop behind him has a cutting board, and several knives are mounted on the wall to his left.  [0:10:42 - 0:10:45]: The perspective shifts to focus on a wooden surface with a book titled \"GORDON RAMSAY RAMSAY IN 10\" placed atop it. The wooden surface appears to be the same countertop from the previous frames. The subtitle of the book reads, \"Delicious Recipes Made in a Flash\". The book cover features multiple images, including pictures of prepared dishes and some glimpses of the man seen earlier, likely in different settings or poses. The background seems to be consistent with a textured, concrete-like floor. The timestamp in each consecutive frame is visible in the top left corner, showing \"10:42\", \"10:43\", \"10:44\", and \"10:45\".",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What are some of the components of the salad on the cutting board?",
        "time_stamp": "0:10:21",
        "answer": "D",
        "options": [
          "A. Spinach, red cabbage, tomato, feta cheese, and olives.",
          "B. Lettuce, red cabbage, tomato, feta cheese, and croutons.",
          "C. Spinach, red cabbage, avocado, feta cheese, and croutons.",
          "D. Lettuce, red cabbage, avocado, feta cheese, and croutons, chicken."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_14_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:18",
        "answer": "C",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 0.",
          "D. 4."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_77_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:28",
        "answer": "C",
        "options": [
          "A. 2.",
          "B. 4.",
          "C. 6.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_77_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:31",
        "answer": "C",
        "options": [
          "A. 8.",
          "B. 7.",
          "C. 9.",
          "D. 10."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_77_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:18",
        "answer": "C",
        "options": [
          "A. 10.",
          "B. 7.",
          "C. 9.",
          "D. 8."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_77_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:07:07",
        "answer": "C",
        "options": [
          "A. 13.",
          "B. 10.",
          "C. 12.",
          "D. 11."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_77_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:01]: A man in a dark shirt stands in front of a kitchen counter with a marble top. He holds up a smartphone with his right hand. On the counter behind him are various kitchen appliances, including a mixer, toaster, and instant pot. The cabinets have glass doors showing plates and glassware inside; [0:03:02]: The man has started to walk away from the counter and heads towards the open door of a refrigerator, which stands adjacent to the marble countertop. A faucet is visible on the island in the foreground, and steam is emanating from two pots on the stove; [0:03:02 - 0:03:03]: The scene shifts to a new room with light wooden flooring. A person is seated on the floor in front of a fireplace and beside a small child who is playing. Toys and miniature furniture are scattered around; [0:03:04]: The same individual, who appears to be a woman, smiles and looks down at the child. They are both situated near a white cabinet housing various small toys and objects; [0:03:05]: A close-up of a small child who is playing with the person sitting next to them. The child is reaching for something near their feet; [0:03:06]: The child is now sitting on the floor close to the person in the floral dress. There is a wooden miniature kitchen playset behind them; [0:03:07]: A small white dog comes into view and is lying on the floor beside the person in the floral dress; [0:03:08]: A close-up of the small white dog staring at the camera. It is sitting on the wooden floor, partially leaning against the person in the floral dress; [0:03:09]: The camera begins to shift quickly, causing a blurred, disoriented image. A white wall and a part of the previous room are faintly visible as the camera moves; [0:03:10]: The scene changes back to the wooden floored hallway as the small white dog scurries down the corridor towards another room. The walls are white, and there is a wooden trim at the bottom; [0:03:11]: The camera angle shifts upwards, showing a dining area with windows in the background. There are large pendant lights hanging from the ceiling, providing illumination to the space; [0:03:12]: The direction shifts again, a rapid movement through a hallway towards another part of the kitchen. Some elements of the previously seen kitchen come back into view, including dark cabinets and a light-colored countertop; [0:03:13]: The scene is back in the kitchen. The man in a dark shirt is once again in the view, standing next to the stovetop. Pots are boiling, with steam rising from them; [0:03:14]: The man continues to prepare food as he stands before the stovetop. The countertop shows different ingredients and utensils for the cooking process; [0:03:15]: The camera angle shifts slightly to the right. The man begins handling meat and potatoes that are placed on the counter. A chopping board and a kitchen knife are close by; [0:03:16]: A close-up of the man, now facing the camera, possibly taking instructions from his phone, as he looks at it intently. Various cooking ingredients lie neatly arranged on the counter beside him; [0:03:17]: The man appears to focus on his phone, typing or reading from it while standing in the kitchen. Some utensils and ingredients are clearly visible in front of him; [0:03:18]: The scene catches the man in mid-motion, as he possibly transitions from interacting with his phone back to focusing on the cooking tasks at hand; [0:03:19]: The man energetically raises his arms, displaying enthusiasm, while continuing to work in the kitchen. His posture suggests he is deeply engaged in his cooking activity.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the man holding in his right hand?",
        "time_stamp": "0:03:16",
        "answer": "C",
        "options": [
          "A. A pen.",
          "B. A spatula.",
          "C. An olive oil bottle.",
          "D. A remote."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_35_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:01]: A person wearing a dark shirt and a towel draped over their shoulder is standing in a kitchen. They are holding a knife and a piece of garlic over a wooden cutting board. To the right, a bowl filled with peas can be seen, and a plate with various vegetables is in the lower right corner. The countertop is made of marble.  [0:06:01 - 0:06:02]: The person adjusts their grip on the knife, preparing to cut the garlic. The garlic cloves are positioned on the wooden cutting board, and the man is standing with their right hand ready to slice. [0:06:02 - 0:06:03]: The person starts to chop the garlic while keeping their left hand steady on the board. A portion of the garlic has been sliced, and the left hand hovers just above the pieces. [0:06:03 - 0:06:04]: The person continues to chop the garlic into smaller pieces with precision. The left hand has moved away slightly to give space for the knife to work. [0:06:04 - 0:06:05]: After chopping the garlic, the person motions over to the right of the cutting board, preparing to move the chopped garlic. The bowl of peas remains in the background. [0:06:05 - 0:06:06]: The person now moves the chopped garlic to the side with the flat side of the knife. More detailed parts of the kitchen can be seen in the background, including drawers and utensils. [0:06:06 - 0:06:07]: A close-up view of the person holding the freshly chopped garlic. They spread the chopped pieces with the flat part of the knife to further mince them on the wooden cutting board. [0:06:07 - 0:06:08]: The remaining garlic clove lies on the cutting board, with minced pieces around. The person then lifts the mince garlic pieces using the knife. [0:06:08 - 0:06:09]: Moving over to the stove, the person holds the minced garlic on the knife near a hot skillet with steaks sizzling on it. The person appears ready to add the garlic to the pan. [0:06:09 - 0:06:10]: The perspective shifts to a close-up view of the skillet containing multiple steaks, slightly seared and cooking on a gas burner. The red utensils and sizzling steaks make this the focal point. [0:06:10 - 0:06:11]: The camera retreats slightly and shows the cutting board now empty, except for some residual garlic left behind. The pan with the steaks is still evident in the capture, with a broader look at the kitchen setup. [0:06:11 - 0:06:12]: The person shifts focus back to the pan with steaks, which can be seen cooking actively on a gas stove. The person prepares to flip or stir the steaks. [0:06:12 - 0:06:13]: The camera zooms out to show the person attending to the cooking steaks. The kitchen background reveals more details, including spice containers and cooking appliances on countertops. [0:06:13 - 0:06:14]: The person, holding a towel, stands next to the counter with cooking ingredients arranged orderly. The person continues to focus on the hot skillet with steaks. [0:06:14 - 0:06:15]: After briefly attending to the steaks, the person moves back to the cutting board, readying to continue food preparation with the various ingredients. [0:06:15 - 0:06:16]: With the towel now off to the side, the person stands near the wooden cutting board. Various kitchen utensils, a blender, and food containers are visible behind the person. [0:06:16 - 0:06:17]: The person begins arranging the trimmed garlic cloves on the cutting board. The kitchen equipment, such as an immersion blender and containers, are also in sight. [0:06:17 - 0:06:18]: The camera angle changes, revealing more of the kitchen, including two toasters, spices, and other preparations. The person adjusts their focus back to slicing vegetables on the cutting board. [0:06:18 - 0:06:19]: The person's attention remains on the cutting board, with more intense chopping activity in progress. The arranged onions and potatoes are prepared for further chopping.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person holding in his hand right now?",
        "time_stamp": "0:06:00",
        "answer": "B",
        "options": [
          "A. A spoon and a piece of ginger.",
          "B. A knife and a towel.",
          "C. A fork and a piece of onion.",
          "D. A spatula and a piece of pepper."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Action Recognition",
        "question": "What action is the person performing right now?",
        "time_stamp": "0:06:09",
        "answer": "C",
        "options": [
          "A. Stirring a pot.",
          "B. Slicing an onion.",
          "C. Chopping garlic.",
          "D. Peeling potatoes."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_35_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:01]: The scene shows a close-up shot of a frying pan on a stovetop. The pan contains several pieces of meat, likely chops, searing with a few small vegetables around them. The handle of the pan is red, and to the side is another pan partially visible with other food cooking in it. The person's hand holding a cloth is situated above the frying pan. [0:09:01 - 0:09:13]: The camera pulls back to reveal a broader kitchen setting. The person cooking is wearing a dark shirt and holding a pair of tongs in their right hand and a cloth in their left. The stovetop has multiple burners with different pans cooking various ingredients. The environment is well-organized, with kitchen utensils and gadgets arranged on the countertop behind. A pot with a glass lid is also visible, with green vegetables being cooked inside. [0:09:13 - 0:09:15]: The camera captures a woman in a floral dress holding a child while standing beside the stove. The woman is observing the cooking process, and the child is also watching intently. The background shows a clean and neatly arranged kitchen, with white tiles on the backsplash and dark-colored cabinets. [0:09:15 - 0:09:17]: The scene continues with the person cooking making an adjustment to the pan with the tongs. They are standing to the right of the stove, wearing a cloth over their shoulder. The woman is still holding the child and watching the person cook.  [0:09:17 - 0:09:18]: The cook reaches into the second pan to adjust the vegetables cooking there. The woman and child remain watching. The kitchen is busy and filled with different cooking activities. [0:09:18]: Finally, the person cooking adds a piece of butter to the center of the pan with the chops and vegetables, ensuring proper seasoning as the cooking continues.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What color is the handle of the frying pan that contains meat inside?",
        "time_stamp": "00:09:01",
        "answer": "C",
        "options": [
          "A. Black.",
          "B. Silver.",
          "C. Red.",
          "D. Blue."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_35_real.mp4"
  },
  {
    "time": "[0:12:00 - 0:13:00]",
    "captions": "[0:12:00 - 0:12:20] [0:12:00 - 0:12:01]: The video begins with a first-person perspective focusing on a black pot placed on a gas stove, with blue flames visible beneath it. A person is holding a copper colander in one hand and a kitchen towel in the other.  [0:12:01 - 0:12:02]: In the next moment, the person begins to pour green vegetables from the colander into the black pot on the stove, with steam rising from the hot contents.  [0:12:02 - 0:12:03]: The colander is almost empty now, and the green vegetables are mostly inside the pot, with just a few remaining in the colander.  [0:12:03 - 0:12:04]: The colander is fully emptied into the pot, and the person's hand moves away.  [0:12:04 - 0:12:05]: The person places the empty copper colander into the sink and steps away, leaving the pot on the flaming stove with green vegetables inside. [0:12:05 - 0:12:06]: The scene shifts to show a kitchen counter with various cooking items and ingredients, including a cutting board, knife, and a pan with cooked food.  [0:12:06 - 0:12:07]: The person moves toward the stove area, reaching for a small item on the counter, as the cooking pot and ingredients are in the foreground.  [0:12:07 - 0:12:08]: The person shifts slightly to the right side of the kitchen, reaching into a dish with some grated yellow substance.  [0:12:08 - 0:12:09]: The person adds the grated yellow substance into the pot with the green vegetables, holding a container and using a utensil.  [0:12:09 - 0:12:10]: The person begins mixing the contents of the pot with a spoon, while another person in the background holds a phone, possibly recording or viewing the process. [0:12:10 - 0:12:11]: The camera angle briefly focuses on the stove burner, with the visible blue flames indicating that the heat is still on. [0:12:11 - 0:12:12]: The person shifts back to the stove, pot in hand, places it on the counter, and reaches for a lid. [0:12:12 - 0:12:13]: The person uses a spoon to stir the contents of the pot, with the rest of the kitchen counter visible, including colorful ingredients and kitchen tools.  [0:12:13 - 0:12:14]: The focus shifts to a plate on the counter as the person moves items and prepares other ingredients, including placing dishes and containers. [0:12:14 - 0:12:15]: The person opens a tin of seasoning or sauce, holding it over the pot containing the green vegetables. [0:12:15 - 0:12:16]: The person spoon feeds some of the seasoning from the tin into the pot. [0:12:16 - 0:12:17]: The person continues to add more seasoning into the pot, carefully measuring each portion. [0:12:17 - 0:12:18]: The person begins to stir the ingredients in the pot, ensuring the seasoning is well mixed with the green vegetables. [0:12:18 - 0:12:19]: The person reaches for an item on the counter, possibly to add more ingredients into the pot, with other kitchen items visibly organized around the workspace.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What does the person do after adding yogurt into the pot?",
        "time_stamp": "0:12:09",
        "answer": "B",
        "options": [
          "A. Turns off the stove.",
          "B. Begins mixing the contents of the pot.",
          "C. Places a lid on the pot.",
          "D. Adds more green vegetables."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_35_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:00 - 0:01:20] \n[0:01:20 - 0:01:40] [0:01:20 - 0:01:24]: The scene begins with a woman standing in a modern kitchen. She's facing the camera wearing a white sleeveless top with black polka dots. In front of her on a light wooden cutting board, there's an array of ingredients including a zucchini, some chopped garlic, and a large white bowl. She holds a small glass jar of a red spice and a spoon in her hands, poised above the bowl. The kitchen background features stainless steel appliances and white subway tiles. [0:01:25 - 0:01:28]: The woman continues to spoon the red spice into the bowl, her hands moving steadily. The bowl contains a mixture of green and red ingredients. She holds the spice jar above the bowl, her left hand controlling the spoon. [0:01:29 - 0:01:30]: The camera zooms out slightly, showing the woman talking and making a gesture with her right hand while still holding the spoon and jar in her left hand. She appears to be explaining something. [0:01:31 - 0:01:32]: The woman continues speaking, gesturing with both hands for emphasis. Her expression seems engaged as she communicates with the viewer. [0:01:33 - 0:01:34]: The shot remains steady as the woman continues her explanation. She occasionally looks down at the ingredients on the cutting board. [0:01:35 - 0:01:36]: The scene continues with a similar setup, as the woman makes more gestures and movements while speaking. Her facial expressions remain animated. [0:01:37]: The woman glances to the side briefly while still continuing her explanation. Her right hand is now extended towards the side as she speaks. [0:01:38 - 0:01:39]: The camera cuts to a close-up of the bowl with the ingredients, showing her hands moving towards a green cup. Other utensils and appliances in the background become slightly blurred.\n[0:01:40 - 0:02:00] ",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the woman wearing?",
        "time_stamp": "0:01:00",
        "answer": "D",
        "options": [
          "A. A red dress with white dots.",
          "B. A blue blouse with yellow stripes.",
          "C. A green apron with floral patterns.",
          "D. A white sleeveless top with black polka dots."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Action Recognition",
        "question": "What action is the woman performing with the spice jar?",
        "time_stamp": "0:01:24",
        "answer": "C",
        "options": [
          "A. She is shaking the spice jar over the ingredients.",
          "B. She is pouring the spice directly into the bowl.",
          "C. She is spooning the spice into the bowl.",
          "D. She is closing the lid of the spice jar."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_24_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] \n[0:02:20 - 0:02:40] \n[0:02:40 - 0:03:00] [0:02:40 - 0:02:42]: Hands are pressing a raw hamburger patty flat on a black griddle. The countertop and stove setup indicates a kitchen environment. The patty is maintaining a roughly circular shape, with small droplets of moisture visible on its surface. [0:02:43 - 0:02:47]: In a modern kitchen with clean, white tiled walls and a large stainless steel refrigerator in the background, a woman wearing a white, polka-dot sleeveless top is preparing ingredients on a countertop. Various objects such as a wooden pepper grinder, a bottle of oil, a green mug, and a white bowl are arranged on the counter. [0:02:48 - 0:02:53]: The woman continues to prepare food, speaking actively as she gestures with her hands. There is a sense of movement and engagement in her actions as she organizes the ingredients and utensils around her. [0:02:54 - 0:02:57]: The camera shifts back to the hamburger patty cooking on the black griddle. The patty remains stationary as it continues to cook, with text appearing on the screen reading, \"RECIPE IS IN THE DESCRIPTION BELOW.\" [0:02:58 - 0:02:59]: The scene returns to the woman in the kitchen, pointing and conversing as she reaches for a lemon on the countertop among the various cooking items.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What text appeared on the just now?",
        "time_stamp": "00:02:57",
        "answer": "D",
        "options": [
          "A. \"Cooking Time: 10 minutes\".",
          "B. \"Season to Taste\".",
          "C. \"Ingredients: Beef, Salt, Pepper\".",
          "D. \"Recipe is in the description below\"."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_24_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:02]: The video begins with a close-up of hands squeezing a lemon over a white bowl. The person wears a ring on one finger, and there is a bit of pulp visible in the bowl as the juice drips down. The person wears a white, polka-dotted top, and a spoon rests on the left side of the bowl on the wooden countertop. [0:04:03 - 0:04:04]: The scene shifts to show a wider view of the kitchen. The person, a woman with a ponytail, stands behind a kitchen island with various items, including a large bowl, lemons, and an olive oil bottle. The kitchen has stainless steel appliances, white subway tiles, and green plants around the window. [0:04:05 - 0:04:06]: The woman starts to reach for something on the counter. A closer shot shows her adjusting the bowl on the wooden board before grabbing a bottle of olive oil. [0:04:07 - 0:04:08]: She pours olive oil into the white bowl containing the lemon and other ingredients. The light reflects off the bottle as the golden liquid flows, showing her concentrated expression as she performs the action. [0:04:09 - 0:04:10]: The shot switches to a medium close-up of her vigorously mixing the ingredients in the bowl with a fork or similar utensil. She looks focused as she blends everything. [0:04:11 - 0:04:12]: A close-up image shows the mixture inside the bowl as it is being stirred. The green contents, likely avocado or a similar ingredient, are becoming well-mixed as the fork moves around. [0:04:13 - 0:04:14]: The camera returns to a medium shot of the woman standing behind the counter, continuing to mix the ingredients in the bowl. Various items such as lemons, a pepper mill, and a small green cup are visible on the counter. [0:04:15]: She steps to the right side of the frame to grab a green bottle of olive oil before moving toward the stove area. [0:04:16 - 0:04:17]: Over at the stove, she begins to drizzle olive oil onto a pan that contains a patty or similar item. The kitchen remains consistent in appearance, exuding a clean and modern feel with white cabinetry and a large window in the background. [0:04:18 - 0:04:19]: The final close-up shows the patty on the grill now glistening with olive oil. The surface of the patty appears slightly rough, likely indicating that it is a homemade preparation. The video captures the detail of the patty being cooked thoroughly.\n[0:04:20 - 0:04:40] \n[0:04:40 - 0:05:00] ",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the woman holding right now?",
        "time_stamp": "0:04:06",
        "answer": "D",
        "options": [
          "A. A spoon.",
          "B. A white bowl.",
          "C. A pepper mill.",
          "D. A green bottle of olive oil."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_24_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:00 - 0:05:20] \n[0:05:20 - 0:05:40] [0:05:20 - 0:05:23]: In a bright and modern kitchen, a person wearing a white, polka-dotted sleeveless top and dark pants is tending to food on a large countertop. On the counter, there are bottles of olive oil, pepper grinder, a small green ceramic bowl, and a plate with lettuce. They use a spatula to flip a burger patty on the griddle while a hamburger bun is also warming nearby. [0:05:24]: The focus shifts to a close-up of the griddle where the person lifts the cooked burger patty with a spatula. The hamburger bun is toasted brown on the griddle. [0:05:25]: The scene transitions to a cutting board on the counter, which has slices of tomato, pickles, and lettuce prepared for assembly. The image is slightly out of focus. [0:05:26 - 0:05:28]: The person places a piece of lettuce on the bottom half of the hamburger bun on the cutting board and then places the burger patty on top of the lettuce using a black spatula. Slices of tomato are nearby. [0:05:29 - 0:05:30]: The person focuses on assembling the burger. They are carefully positioning the lettuce and burger patty on the bottom half of the bun. The kitchen setup in the background includes marble countertops, a refrigerator, and kitchen cabinets. [0:05:31 - 0:05:33]: The next few frames show the person continuing to assemble the burger, adjusting the ingredients to ensure they are well placed. Their hair is tied back, allowing a clear view of their focused expression. [0:05:34 - 0:05:35]: The camera briefly zooms in on the assembling process. The top half of the bun is about to be placed on the burger, which now has layers of lettuce and a patty. [0:05:36 - 0:05:37]: The viewpoint momentarily moves back to show an olive oil bottle in the foreground. The forming of the burger continues as the person reaches for ingredients.  [0:05:38 - 0:05:39]: Finally, the person retrieves the fully assembled burger and starts to step away from the counter, holding the burger with some satisfaction. The kitchen counter remains filled with various ingredients and cooking utensils, signifying an active cooking environment.\n[0:05:40 - 0:06:00] ",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "How much time does this person have left now??",
        "time_stamp": "0:05:49",
        "answer": "D",
        "options": [
          "A. 10s.",
          "B. More than ten seconds.",
          "C. 20s.",
          "D. Less than ten seconds."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_24_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the shape of the large structure on the left side of the road right now?",
        "time_stamp": "00:00:04",
        "answer": "C",
        "options": [
          "A. Cube.",
          "B. Pyramid.",
          "C. Sphere.",
          "D. Cone."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_397_real.mp4"
  },
  {
    "time": "[0:02:12 - 0:02:17]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color are the traffic lights right now?",
        "time_stamp": "00:02:16",
        "answer": "C",
        "options": [
          "A. Red.",
          "B. Yellow.",
          "C. Green.",
          "D. Blue."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_397_real.mp4"
  },
  {
    "time": "[0:04:24 - 0:04:29]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the weather like right now?",
        "time_stamp": "00:04:27",
        "answer": "C",
        "options": [
          "A. Sunny.",
          "B. Snowy.",
          "C. Foggy and Rainy.",
          "D. Clear."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_397_real.mp4"
  },
  {
    "time": "[0:06:36 - 0:06:41]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What type of vehicle is the yellow car right now?",
        "time_stamp": "00:06:38",
        "answer": "A",
        "options": [
          "A. Taxi.",
          "B. School bus.",
          "C. Firetruck.",
          "D. Police car."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_397_real.mp4"
  },
  {
    "time": "[0:08:48 - 0:08:53]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the width of the road right now?",
        "time_stamp": "00:08:50",
        "answer": "C",
        "options": [
          "A. Narrow.",
          "B. Tiny.",
          "C. Wide.",
          "D. Very narrow."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_397_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "Which type of vehicle is visible inside the hangar right now?",
        "time_stamp": "00:00:03",
        "answer": "C",
        "options": [
          "A. Truck.",
          "B. Boat.",
          "C. Aircraft.",
          "D. Motorcycle."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_423_real.mp4"
  },
  {
    "time": "[0:02:53 - 0:02:58]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "Which device is currently placed on the right side of the cockpit?",
        "time_stamp": "00:02:56",
        "answer": "A",
        "options": [
          "A. A GPS device.",
          "B. A radio transmitter.",
          "C. An altimeter.",
          "D. A fuel gauge."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_423_real.mp4"
  },
  {
    "time": "[0:05:46 - 0:05:51]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is right now the terrain mainly covered with, as seen from the flying POV?",
        "time_stamp": "00:05:49",
        "answer": "C",
        "options": [
          "A. Desert.",
          "B. Urban buildings.",
          "C. Forest.",
          "D. Water bodies."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_423_real.mp4"
  },
  {
    "time": "[0:08:39 - 0:08:44]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What instrument is the pilot using right now to navigate?",
        "time_stamp": "00:08:42",
        "answer": "C",
        "options": [
          "A. Altimeter.",
          "B. Compass.",
          "C. GPS tablet.",
          "D. Map."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_423_real.mp4"
  },
  {
    "time": "[0:11:32 - 0:11:37]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "Which device is placed next to the pilot's seat right now?",
        "time_stamp": "00:11:36",
        "answer": "B",
        "options": [
          "A. A GPS device.",
          "B. A camera.",
          "C. An oxygen mask.",
          "D. A water bottle."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_423_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:18",
        "answer": "A",
        "options": [
          "A. 0.",
          "B. 2.",
          "C. 3.",
          "D. 4."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_82_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:07",
        "answer": "A",
        "options": [
          "A. 5.",
          "B. 6.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_82_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:09",
        "answer": "A",
        "options": [
          "A. 6.",
          "B. 2.",
          "C. 3.",
          "D. 4."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_82_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:01",
        "answer": "A",
        "options": [
          "A. 7.",
          "B. 2.",
          "C. 6.",
          "D. 8."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      },
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:33",
        "answer": "A",
        "options": [
          "A. 9.",
          "B. 5.",
          "C. 8.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_82_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why Mr. Bean was previously flipping through boxes?",
        "time_stamp": "00:02:06",
        "answer": "B",
        "options": [
          "A. Because he was looking for his lost keys.",
          "B. Because he is looking for his green little hat.",
          "C. Because he was organizing old photographs.",
          "D. Because he wanted to find his shopping list."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_241_real.mp4"
  },
  {
    "time": "[0:02:09 - 0:02:39]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why did the character create a large number of sandwiches?",
        "time_stamp": "00:03:42",
        "answer": "C",
        "options": [
          "A. Because the character is preparing for a big party.",
          "B. Because the character plans to donate them to a local shelter.",
          "C. Because the character intends to go camping.",
          "D. Because the character wants to start a small sandwich business."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_241_real.mp4"
  },
  {
    "time": "[0:04:18 - 0:04:48]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the other character pick up the green cap left on the chair?",
        "time_stamp": "00:04:50",
        "answer": "B",
        "options": [
          "A. Because the green cap looked like it was expensive.",
          "B. Because Mr. Bean accidentally left this green hat on the chair just now.",
          "C. Because the other character needed a cap to shield from the sun.",
          "D. Because the green cap matches the other character's outfit."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_241_real.mp4"
  },
  {
    "time": "[0:06:27 - 0:06:57]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why would Mr. Bean buy the things being towed behind his car?",
        "time_stamp": "00:06:49",
        "answer": "B",
        "options": [
          "A. Because the items were on sale at a huge discount.",
          "B. Because he only had five yuan on him at that time.",
          "C. Because he thought they would be fun to use later.",
          "D. Because he needed them for a project he was working on."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_241_real.mp4"
  },
  {
    "time": "[0:08:36 - 0:09:06]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why did Mr. Bean just feed this dog with a sandwich?",
        "time_stamp": "00:09:53",
        "answer": "D",
        "options": [
          "A. Because Mr. Bean felt sorry for the hungry dog.",
          "B. Because Mr. Bean had extra sandwiches to spare.",
          "C. Because Mr. Bean wanted to make friends with the dog.",
          "D. Because Mr. Bean wants to trick this dog into the house."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_241_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions performed just now?",
        "time_stamp": "00:00:10",
        "answer": "C",
        "options": [
          "A. The worker searched for ingredients by opening cans and containers, selecting items to prepare a salad.",
          "B. The worker prepared a sandwich by taking bread, adding condiments, and wrapping it.",
          "C. The worker wore gloves and prepared to assemble a sandwich, with loaves of bread and various fillings visible.",
          "D. The worker cleaned the workstation and disposed of the packaging of sandwich ingredients."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_378_real.mp4"
  },
  {
    "time": "[0:00:55 - 0:01:05]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions performed just now?",
        "time_stamp": "00:01:03",
        "answer": "C",
        "options": [
          "A. The worker prepared a dessert by gathering ingredients, measuring them, and combining them, ready for baking.",
          "B. The worker washed their hands, prepped vegetables, and started slicing them for a stir-fry dish.",
          "C. The worker sliced various meats, arranged them on a piece of bread, and added slices of cheese to construct a sandwich.",
          "D. The worker cleaned the countertop, sanitized the utensils, and organized the kitchen workspace."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_378_real.mp4"
  },
  {
    "time": "[0:01:50 - 0:02:00]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions performed just now?",
        "time_stamp": "00:01:55",
        "answer": "A",
        "options": [
          "A. The worker took out the toasted sandwich.",
          "B. The worker removed a loaf of bread from storage, sliced it, and placed it on a tray for toasting.",
          "C. The worker washed vegetables and prepped them for a salad, placing them in the designated containers.",
          "D. The worker inspected the quality of the baked goods and discarded those that were not up to standards."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_378_real.mp4"
  },
  {
    "time": "[0:02:45 - 0:02:55]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions performed just now?",
        "time_stamp": "00:03:14",
        "answer": "A",
        "options": [
          "A. The worker prepared a sandwich by adding jalapenos, retrieving ketchup, and then applying ketchup and another condiment before wrapping it up.",
          "B. The worker assembled a salad by chopping vegetables, seasoning the mixture, and serving it to a customer.",
          "C. The worker toasted a sandwich, cut it, added the final toppings, and packaged it for delivery.",
          "D. The worker retrieved various vegetables from storage, cleaned them, and placed them in a food container."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_378_real.mp4"
  },
  {
    "time": "[0:03:40 - 0:03:50]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions performed just now?",
        "time_stamp": "00:03:50",
        "answer": "B",
        "options": [
          "A. The worker topped a sandwich with tomatoes, cucumbers, and onions, then wrapped it for delivery.",
          "B. The worker added fresh ingredients such as tomatoes and cucumbers to a sandwich, aligning them carefully on lettuce and other toppings.",
          "C. The worker prepared a salad by chopping onions, cucumbers, and other vegetables, and then mixed them in a bowl.",
          "D. The worker grilled a sandwich with various ingredients and sliced it before packaging it for a customer."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_378_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:04]: The video begins with a first-person perspective view of driving a vehicle on a city street at night. The scene is dark, illuminated by streetlights and traffic signals. A black van with illuminated red lights in the rear is ahead on the road. The interior of the vehicle from which the video is shot includes a part of a vibrant dashboard with a digital speedometer.  [0:00:05 - 0:00:07]: The vehicle approaches an intersection with traffic signals turning green, indicating it's clear to proceed. The road ahead extends under a bridge, lined with streetlights. [0:00:08 - 0:00:11]: Continuing forward, the vehicle drives under the bridge, which has concrete pillars and darker shaded areas due to the limited lighting.  [0:00:12 - 0:00:13]: Emerging from the other side of the bridge, the view reveals a more open cityscape with scattered buildings and more streetlights. [0:00:14 - 0:00:15]: The vehicle arrives at another intersection, preparing to make a turn. The surroundings include various commercial buildings with illuminated signs and parked cars along the road. [0:00:16 - 0:00:20]: The vehicle turns right, continuing along a road bordered by additional buildings with illuminated signs. The streets remain well-lit by the city lights and occasional headlights and taillights from other vehicles.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the vehicle doing right now?",
        "time_stamp": "00:00:20",
        "answer": "A",
        "options": [
          "A. Driving straight ahead on the street at night.",
          "B. Stopping at a traffic signal.",
          "C. Driving under a bridge.",
          "D. Parking by the side of the road."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_279_real.mp4"
  },
  {
    "time": "0:02:40 - 0:03:00",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:03:00]: The video shows a first-person perspective of someone navigating an in-game menu labeled \"Trucking Progression\" for a place called \"Boxville.\" The screen displays six different job options, each with a small image preview, job title, distance, cargo type, and payout amount. The job titles include \"Los Santos PD,\" \"Los Santos CourtHouse,\" and \"Highway (Gas and Go).\" The payouts range from $1,200 to $2,500. On the right side of the screen is a chat overlay with multiple users' comments. The person selects the \"Details\" button for more information on a specific job, leading to a detailed view showing distance, time, deliveries, and payout statistics. The player then returns to the main job menu and scrolls through the available jobs. The background is a dark-themed interface with blue accents, and the person's gameplay appears to be streamed live, indicated by various real-time viewer interactions in the chat.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:03:00",
        "answer": "A",
        "options": [
          "A. Opened a detailed view of a specific job.",
          "B. Selected a new job from the menu.",
          "C. Closed the chat overlay.",
          "D. Started a new game session."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_279_real.mp4"
  },
  {
    "time": "0:05:20 - 0:05:40",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:30]: The video takes place at night in an outdoor setting, likely a parking lot or industrial area, given the visible cars and buildings. The footage is in a first-person perspective. There are four individuals interacting in the space. One person, wearing a striped shirt, is closest to the camera. Another person in a bright blue jacket is standing nearby. A third person, in dark clothes, is holding what appears to be a clipboard or similar object, standing against a wall. In the background, a shirtless individual in shorts and a cap is visible. The conversation seems relaxed, and occasionally someone looks towards the background, where other individuals and a blue door are visible on a building’s wall. [0:05:31 - 0:05:34]: The group remains in the same general orientation with occasional shifts in body positions. The interaction continues leisurely, with the striped shirt individual and blue jacket person still in the main focus. The shirtless individual remains in the background. [0:05:35 - 0:05:40]: The camera shifts slightly, showing a broader overview of the area. More details of the surroundings appear, such as additional buildings and parked cars. The group continues their conversation without notable changes in dynamics or actions.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person in dark hair and white T-shirt doing right now?",
        "time_stamp": "00:05:40",
        "answer": "A",
        "options": [
          "A. Standing against a wall with a clipboard.",
          "B. Talking to the person in a striped shirt.",
          "C. Walking towards the blue door.",
          "D. Interacting with the shirtless individual."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_279_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:02]: The video begins with a first-person perspective of a dark road at night. The vehicle being driven has a distinctive rectangular red tail light. Numerous lights from surrounding buildings illuminate the background; [0:08:03 - 0:08:04]: As the vehicle proceeds, more city lights and structures become visible in the distance. The road ahead appears to be a straight path with barriers on the sides. There are streetlights along the road providing some illumination; [0:08:05 - 0:08:06]: The vehicle continues to drive forward. Exit signs and road markings are visible on the right side of the road; [0:08:07 - 0:08:08]: The camera captures more of the city's illuminated skyline in the background. The road starts to curve slightly to the right; [0:08:09 - 0:08:10]: The vehicle takes a right turn, and the road begins to curve more noticeably. A few road signs and street lights are present; [0:08:11 - 0:08:14]: Continuing the curve, the vehicle moves along a well-lit road with light posts on both sides. There are other vehicles visible in the background; [0:08:15 - 0:08:16]: The vehicle is now on a straight road again, with more buildings and city lights becoming visible; [0:08:17 - 0:08:18]: The vehicle catches up to another car. Multiple road signs and overpasses are visible in the distance; [0:08:19 - 0:08:20]: The vehicle travels parallel to another car. Road markings direct the traffic flow, and an overpass with illuminated signage becomes more prominent directly ahead.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the vehicle doing right now?",
        "time_stamp": "00:08:17",
        "answer": "A",
        "options": [
          "A. Driving parallel to another car.",
          "B. Making a left turn.",
          "C. Stopping at a traffic light.",
          "D. Reversing into a parking space."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_279_real.mp4"
  },
  {
    "time": "0:09:40 - 0:09:55",
    "captions": "[0:09:40 - 0:09:55] [0:09:40 - 0:09:41]: The video starts with a person interacting with a virtual interface, labeled \"Harry Miller\" with options for actions like \"Open Stock.\" The person seems to be playing a game on a computer screen. [0:09:41 - 0:09:46]: The game interface shows an inventory screen with different sections: \"Player,\" \"Backpack,\" \"Ground,\" and \"Tracking Cargo.\" Items like paper and a book appear in the inventory. The game character’s information is displayed on a side panel. [0:09:46 - 0:09:48]: The player continues interacting with the inventory, moving items around. The overlay chat messages are visible on the right side of the screen. [0:09:48 - 0:09:53]: The game interface remains the same with the player managing their inventory. Items are shifted between different sections. [0:09:53 - 0:09:56]: The scene changes to a night-time outdoor setting. A character with a striped shirt is holding a box while standing near an entrance. [0:09:56 - 0:09:58]: The character moves towards a truck parked nearby. The background features a dimly lit parking lot with some vegetation. [0:09:58 - 0:10:00]: They approach the back of the truck, where an action prompt \"Drop\" appears on the screen. The chat overlay continues on the right. [0:10:00 - 0:10:02]: The character appears to interact with the truck, and the box seems to be getting ready to be dropped into the vehicle’s cargo area. [0:10:02 - 0:10:03]: Back to the game inventory screen, with items being managed. The sections, \"Player,\" \"Cargo,\" \"Backpack,\" and \"Ground\" are visible, with the player organizing different objects. [0:10:03 - 0:10:06]: The inventory management continues with more items being arranged in the virtual character’s slots. [0:10:06 - 0:10:08]: The scene switches back to the night-time outdoor setting. The character is seen near the truck again, having completed the interaction. [0:10:08 - 0:10:10]: The character walks away from the truck towards another person standing near the entrance.  [0:10:10 - 0:10:12]: The two characters stand facing each other, possibly preparing for another action or interaction within the game.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the character do just now?",
        "time_stamp": "00:09:53",
        "answer": "A",
        "options": [
          "A. Dropped a box into the truck's cargo area.",
          "B. Interacted with the inventory screen.",
          "C. Walked away from the truck.",
          "D. Opened a virtual interface labeled \"Harry Miller.\"."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_279_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: A woman with braided hair is seated on a wooden chair. She is putting on a white sock, pulling it up her left leg. She is dressed in a white long-sleeve shirt and a red skirt. The setting seems historical, with stone walls and wooden floors. Behind her, a table covered with a red cloth holds a large pottery jug, a chalice, and some draped dark fabric. [0:00:03 - 0:00:05]: The camera focuses on her legs as she continues to pull up the sock, which reaches just below her knee. The floor features a colorful patterned rug. [0:00:05 - 0:00:10]: The camera angle returns to the initial perspective. The woman finishes adjusting the sock on her left leg and now uses a black strap or garter to secure it in place, wrapping it around her leg. [0:00:10 - 0:00:13]: She continues to secure the garter, making sure it is tight and properly positioned just below her knee.  [0:00:13 - 0:00:19]: The woman continues fixing the garter, adjusting it for a snug fit. The background remains consistent with the initial description, showing the table, the stone wall, and the wooden floor. Her concentration is evident as she ensures both the sock and garter are properly adjusted.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the woman doing to secure the sock in place?",
        "time_stamp": "0:00:13",
        "answer": "B",
        "options": [
          "A. Tying it with a ribbon.",
          "B. Securing it with a black strap or garter.",
          "C. Using a safety pin.",
          "D. Sewing it in place."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_157_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:06]: A woman is seen adjusting a white headscarf on her head. She is wearing a black top and a white apron over her dress. She appears to be in a stone-walled room with a red tablecloth-covered table in the background and a chair in the foreground. The woman is facing towards the camera;  [0:02:07 - 0:02:09]: The woman completes adjusting her headscarf and stands still, looking to her side. She then starts walking towards her left; [0:02:10]: The woman walks out of the frame, leaving an empty room with the focus on the table and chair; [0:02:11 - 0:02:20]: The scene shifts to another part of the same room. Another woman is seen sitting in a chair while two women dressed similarly in black dresses with white aprons and headscarves stand on either side of her. They appear to be preparing or adjusting clothing items for the seated woman. They engage with each other, holding and possibly handing over pieces of cloth. The background includes stone walls, a curtain, and a white-covered table.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What do the two standing women do to the seated woman?",
        "time_stamp": "0:02:30",
        "answer": "B",
        "options": [
          "A. They are talking to her.",
          "B. They are adjusting her clothing items.",
          "C. They are serving her food.",
          "D. They are combing her hair."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_157_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:03]: In this segment, three women are in a stone-walled room with a medieval ambiance. Two women wearing white caps and aprons are on either side of a third woman, who is dressed in a dark gown with a white blouse. The central woman stands with her arms folded while the other two women adjust her clothing. A table with a white tablecloth and various items is visible to the left. [0:04:04 - 0:04:07]: The focus shifts to a close-up of the central woman's hands, which are folded in front of her. The two women on either side are tying or adjusting her clothing, with their hands partially visible in the frame. [0:04:08 - 0:04:09]: The scene zooms out again, showing one of the women completing the adjustments. She holds up a decorative piece of fabric or garment in front of the central woman, possibly preparing to place it on her. [0:04:10 - 0:04:11]: The frame focuses on the central woman's face, giving a close-up view of her expression. One of the women in the background is slightly blurred. [0:04:12 - 0:04:16]: The scene zooms out again, showing the woman on the right kneeling and adjusting the lower part of the central woman's gown. The central woman gazes down at the adjusting woman at one point while the second woman stands by watching the process. [0:04:17 - 0:04:19]: The frame focuses on a close-up of the woman’s gown, showing intricate details of the fabric. One of the adjusting women works on fixing the garment, her hands clearly visible as she ties or sews something.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are the two women wearing white caps and aprons doing?",
        "time_stamp": "00:04:09",
        "answer": "B",
        "options": [
          "A. Standing by the table.",
          "B. Adjusting the central woman's clothing.",
          "C. Preparing food.",
          "D. Cleaning the room."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the table with a white tablecloth positioned in the room?",
        "time_stamp": "00:04:03",
        "answer": "A",
        "options": [
          "A. To the left of the central woman.",
          "B. To the right of the central woman.",
          "C. In front of the central woman.",
          "D. Behind the central woman."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_157_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:01]: The video opens with a young woman standing in a richly decorated room with stone walls. She is dressed in a black outfit with wide sleeves and brown and gold detailing. Two attendants, dressed in white head coverings and aprons, assist her. One of the attendants is adjusting the collar of her outfit. [0:06:02 - 0:06:03]: The attendants continue to adjust the young woman's outfit, ensuring the collar and sleeves are properly positioned. The young woman stands still, looking slightly off to the side. [0:06:04 - 0:06:06]: The attendants finish adjusting the collar and begin working on the sleeves and bottom part of the attire. They appear to ensure all details are correctly in place, while the young woman occasionally looks down to check their progress. [0:06:07 - 0:06:12]: The scene zooms out to show more of the room. The background includes a table with a white cloth, some objects on it, and lit candles. The two attendants are now working on the front of the young woman's outfit, making sure everything is perfectly aligned. [0:06:13 - 0:06:16]: The attendants continue their work, now fastening a decorative belt around the young woman's waist. The golden detailing of the belt matches well with the intricate designs of her outfit. The young woman remains composed, occasionally glancing at the attendants. [0:06:17 - 0:06:19]: In the final moments, the young woman slightly bends forward to check the adjustments made to her attire. She then sits down on an intricately carved wooden chair positioned nearby, with the attendants standing beside her. The room remains richly decorated, with stone walls and ornate curtains providing a historical ambiance.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the young woman sitting right now?",
        "time_stamp": "0:06:19",
        "answer": "B",
        "options": [
          "A. On a cushion on the floor.",
          "B. On an intricately carved wooden chair.",
          "C. On a plain wooden bench.",
          "D. On a cushioned sofa."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_157_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:05]: The video begins with a view of six resin bookmarks made with different colored backgrounds and adorned with golden flakes. The bookmarks are arranged one below another, and each bookmark has a name written in elegant golden script. From top to bottom, the names are \"Elizabeth,\" \"Samantha,\" \"Jennifer,\" \"Brenda,\" \"Melinda,\" and \"Kristi.\" Each bookmark also features a golden tassel attached to one end. The colors of the bookmarks from top to bottom are pink, black, beige, pink, black, and beige, with the upper portion of each bookmark having a cluster of golden flakes. [0:00:06 - 0:00:12]: The camera zooms in on the second bookmark from the top, which has a black background with golden flakes on the left side, and the name \"Samantha\" is written on it. The zooming continues until the frame focuses solely on this bookmark, highlighting its glittery design and the golden script. The golden tassel attached to it is also clearly visible. [0:00:13 - 0:00:16]: The scene transitions to a black screen with white text displaying \"DIY Resin Bookmarks.\" The text remains the same while the background color stays black. [0:00:17 - 0:00:19]: The scene changes to a solid color background with the text \"Links to the materials I used are in the video description\" written in black. The background color changes from a dark grayish color to a light pink while the text remains the same for two seconds.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the camera focusing on right now?",
        "time_stamp": "00:00:12",
        "answer": "C",
        "options": [
          "A. The entire collection of bookmarks.",
          "B. A pink bookmark with the name \"Kristi.\".",
          "C. A balck bookmark with the name \"Jamantha.\".",
          "D. A beige bookmark with no name."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_66_real.mp4"
  },
  {
    "time": "0:02:00 - 0:02:20",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:01]: A close-up view of a gloved hand holding a packet labeled \"RIYA Milk Tea Gold\" mica powder, displayed against a white plate. Behind this packet, two other packets, labeled “RIYA Black” and “RIYA Glitter Pink,” and a cup containing a golden mixture with a wooden stir stick, are visible. Two empty small containers sit beside the display. The background is a plain, light-colored surface. [0:02:01 - 0:02:02]: The hands in black gloves start to open the \"Milk Tea Gold\" mica powder packet over the white plate. The other items such as the packets of mica powder, the cup with the mixture, and the small containers remain in the same position. [0:02:02 - 0:02:07]: One of the gloved hands pours the \"Milk Tea Gold\" mica powder from the packet into one of the empty small containers. The other hand holds a wooden stick, stirring the powder gently as it is poured out, ensuring no spillage onto the white plate. [0:02:07 - 0:02:12]: The hand continues to pour and stir the \"Milk Tea Gold\" mica powder into the small container. More powder is gradually added, with the gloved hand tilting the packet carefully to manage the flow. The metallic golden powder begins to fill the small container, and mingles with some liquid or other powder substance inside. [0:02:12 - 0:02:19]: As the pouring of the mica powder reaches completion, the gloved hand with the wooden stick stirs the mixture within the small container more vigorously. The substance within the container starts to exhibit a more blended consistency and a shinier appearance, indicating that the mica powder is fully integrating.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are the hands in black gloves doing right now?",
        "time_stamp": "0:02:19",
        "answer": "C",
        "options": [
          "A. Displaying the \"RIYA Milk Tea Gold\" mica powder packet.",
          "B. Opening another mica powder packet.",
          "C. Stirring the mixture in a small container.",
          "D. Cleaning the white plate."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_66_real.mp4"
  },
  {
    "time": "0:04:00 - 0:04:20",
    "captions": "[0:04:00 - 0:04:20] [0:00:00 - 0:00:01]: A pair of hands wearing black gloves uses a small cup to pour a pinkish mixture into the middle compartment of a mold. The mold has three elongated compartments and is placed on a pink surface. Gold flakes are already present in both the top and middle compartments. To the right, a transparent cup containing a gold mixture and a wooden stick is visible. [0:00:01 - 0:00:09]: The individual continues pouring the pinkish mixture in the middle compartment, while holding a wooden stick in the other hand, occasionally adjusting the position of the mixture. The first and third compartments contain a clear liquid and a golden mixture respectively, with gold flakes on both ends of the compartments. The transparent cup to the right remains stationary. [0:00:09 - 0:00:10]: The pouring action ceases as the individual raises the small cup and shifts their right hand towards the transparent cup on the right. [0:00:10 - 0:00:11]: The right hand now reaches for the transparent cup containing the gold mixture and holds it, preparing to pour its contents. [0:00:11 - 0:00:15]: The individual tilts the transparent cup and begins pouring the gold mixture into the top compartment. The wooden stick is positioned to control the flow, ensuring the gold flakes are distributed evenly. The black-gloved hands maintain a steady grip on the cup and stick, with precise movements. [0:00:15 - 0:00:16]: The pouring continues as more gold mixture flows into the top compartment. The wooden stick is used to orient and direct the gold flakes in the mixture. [0:00:17 - 0:00:19]: The hands proceed to adjust the placement of the gold flakes within the top compartment of the mold using the wooden stick, ensuring an even spread of the golden elements.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "0:04:19",
        "answer": "C",
        "options": [
          "A. Pouring a pinkish mixture into a mold's middle compartment.",
          "B. Adjusting the mixture in the middle compartment with a wooden stick.",
          "C. Pouring the gold mixture into the top compartment and adjusting the flakes.",
          "D. Holding the transparent cup without pouring."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_66_real.mp4"
  },
  {
    "time": "0:06:00 - 0:06:20",
    "captions": "[0:06:00 - 0:06:20] [0:06:00]: The video begins with a black screen displaying the text \"24 Hours Later\" in the center in white, serif font. [0:06:01]: The scene changes, showing a close-up view of a cutting machine working on a metallic gold sheet. The tool head is blue and white, positioned in the top-left area of the sheet. [0:06:02]: The cutting machine head moves slightly to the right, still working on the same gold sheet. The background includes a green cutting mat with measurements, visible on the bottom part of the frame. [0:06:03 - 0:06:04]: The cutting head continues to move to the right, incrementally revealing lettering being inscribed on the gold sheet. The letters are slowly forming and becoming clearer with each movement. [0:06:05 - 0:06:06]: The scene continues with the cutting tool moving further to the bottom-right side of the metallic sheet. Text appears to be forming clearly on the gold surface as the machine head meticulously engraves the letters. [0:06:07 - 0:06:08]: The focus remains on the cutting machine, which now nearly completes the inscription over the gold sheet. The detailing on the letters becomes more apparent. [0:06:09]: The camera focuses on a pair of hands peeling off a white sheet from a smaller green grid sheet, appearing to be some part of the craft or preparation step. [0:06:10]: The camera zooms in on one of the hands holding the piece of the gold sheet up against a square, cutting mat background. The gold sheet has completed engravings that are now more visible. [0:06:11]: The scene shows a hand using a weeding tool to start peeling away excess material from the gold engraved sheet, focusing on getting precise details cleared. [0:06:12 - 0:06:13]: The hands continue to use the weeding tool to remove the excess material from the gold sheet. The motion is careful and deliberate to ensure the intricacy of the design remains intact. [0:06:14 - 0:06:15]: The focus stays on the hands working with the gold sheet. The weeding process continues meticulously, and the engraved letters appear more defined and clearer. [0:06:16 - 0:06:17]: The hands persist in working on the detailed weeding of the gold sheet, emphasizing particular areas around the inscribed letters to ensure cleanliness and precision. [0:06:18 - 0:06:19]: The final scenes show the hands still engaged in the weeding process, removing all the unnecessary parts from the engraved gold sheet to reveal the final, cleanly inscribed text and design.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is happening with the gold sheet right now?",
        "time_stamp": "00:06:26",
        "answer": "C",
        "options": [
          "A. The gold sheet is being painted.",
          "B. The cutting tool is engraving letters.",
          "C. The engravings are being cleaned.",
          "D. The sheet is being folded."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_66_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:03]: A person wearing white gloves holds a gold-colored calligraphy marker labeled “DecoColor” and inspects it closely. The marker is held horizontally between both hands. The background features a gray grid mat with five rectangular name tags, each adorned with golden embellishments and different colored backgrounds—pink, tan, and black. [0:08:04 - 0:08:06]: The person lowers the marker and picks up one of the black rectangular name tags with golden embellishments. They hold the marker in their right hand, appearing to prepare for use. [0:08:07 - 0:08:07]: The person begins to use the calligraphy marker on the black name tag. The marker's gold tip is positioned near the tag's surface as if ready to draw or write. [0:08:08 - 0:08:19]: The individual slowly and delicately traces over the golden embellishments on the black name tag, making precise movements with the marker. The elaborate craftsmanship is evident as the marker moves along the intricate design, accentuating the golden decorations on the name tag's upper edge. The background remains the same throughout this process, with the other name tags in view below.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:08:19",
        "answer": "B",
        "options": [
          "A. Writing on a gray grid mat.",
          "B. Supplementing the edges of the black nameplate with gold.",
          "C. Picking up another name tag.",
          "D. Inspecting the marker closely."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_66_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video starts with a first-person perspective on a soccer field. The viewer can see two players in black and pink uniforms approaching a soccer ball, which is positioned near a white boundary line on a green artificial turf field. [0:00:02 - 0:00:03]: The scene transitions to a wider view of the field. One player is running toward the viewer, and others are spread across the field. Tall apartment buildings and floodlights are visible in the background. [0:00:04 - 0:00:05]: The perspective changes as the viewer dribbles the ball along the field. Several players in black and pink uniforms are nearby, and the view focuses on the ball being maneuvered. [0:00:06 - 0:00:07]: The viewer continues dribbling the ball, heading toward the goal. The field has a slight tilt in the angle, and one main player in black and pink is closer to the viewer. [0:00:08 - 0:00:09]: The viewer navigates the ball away from a player who is on approach. Text appears on the screen, stating \"3rd challenge to play Neymar in Korea.\" [0:00:10 - 0:00:11]: The viewer moves forward quickly, chasing the ball across the field. The same text remains on the screen, with another player positioned in the distance. [0:00:12 - 0:00:13]: The viewer gets closer to the opposing goal area with the ball. The purpose remains focused on reaching the goal, and the text continues to display the challenge information. [0:00:14 - 0:00:15]: The camera shifts to a new view where another player, in blue shoes, attempts to tackle for the ball. The text changes to \"I tried Neymar challenge in two games.\" [0:00:16 - 0:00:18]: The scene transitions to a nighttime setting; the viewer is sprinting across the field under floodlights. Players are scattered in the background, with a clear goal in sight. The final text about trying Neymar challenge in two games remains displayed.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What does the text on the screen describe right now?",
        "time_stamp": "0:00:18",
        "answer": "B",
        "options": [
          "A. A training session.",
          "B. A Neymar challenge.",
          "C. A match against a rival team.",
          "D. A skills competition."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_256_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:22]: The video starts on a football field at night, featuring players running and playing soccer under bright stadium lights. The perspective is from a first-person viewpoint looking towards the field where multiple players are spread out. The field is enclosed by a fence and has a wall with rocks on the side. [0:03:22 - 0:03:28]: As the video continues, the camera moves forward quickly, indicating the wearer is sprinting across the field. Players in different colored jerseys are seen ahead, with some actively chasing the ball. The scene is illuminated by several floodlights, and the grass appears well-maintained. [0:03:29 - 0:03:32]: The video then shows the camera focusing more on individual players as they approach the penalty area. The players appear to be directing their attention towards the football near the goalpost, preparing for a potential shot or pass. One player in a blue and pink jersey is visible closer to the camera. [0:03:32 - 0:03:37]: The focus shifts to a close-up of the football as the wearer’s leg enters the frame, indicating they are about to kick or maneuver the ball. The perspective continually shifts as the ball is dribbled and passed across the field, with the shadow of the player visible on the ground. [0:03:37 - 0:03:39]: In the final moments, the camera captures a player in blue and pink running ahead of the wearer who follows closely behind, continuing the offensive play towards the opponent's goal. The surrounding environment remains consistent with clear visibility of the players and the field.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action is the player wearing the camera performed just now?",
        "time_stamp": "00:03:39",
        "answer": "C",
        "options": [
          "A. Pass the ball.",
          "B. Dribble the ball.",
          "C. Kick the ball.",
          "D. Tackle an opponent."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_256_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:44]: A soccer field is visible under artificial lighting at night. A player wearing a dark shirt and red shorts stands near the center. In the background, there is a large, tent-like structure, and several other players are scattered on the field. [0:06:44 - 0:06:47]: The camera moves to the right, showing players in action. One player, in particular, wearing a blue shirt and red shorts, is running across the field. [0:06:47 - 0:06:50]: The perspective shifts quickly towards the edge of the field. The player in the blue shirt continues to be in focus, running diagonally across the grassy surface. [0:06:50 - 0:06:52]: The frame shows a close-up of a soccer ball at the feet of the person wearing black and neon-colored soccer shoes. Another player in blue is approaching in the background. [0:06:52 - 0:06:54]: The player with the camera moves the ball skillfully while the other player approaches. A subtitle appears at the bottom of the frames, stating \"Neymar El Clacico in & out success\" in English and what appears to be Korean. [0:06:54 - 0:06:57]: The player with the camera runs towards the right side of the field. The surroundings show a stone fence, with more players in view. The person's movement is rapid and decisive. [0:06:57 - 0:07:00]: The action converges toward the goal, where two players and a goalkeeper are positioned. The players wearing blue and red shirts continue to move dynamically around the goal area, creating an intense and focused atmosphere. The artificial lights illuminate the scene, casting shadows on the lush green field.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What subtitle appeared just now?",
        "time_stamp": "0:06:58",
        "answer": "C",
        "options": [
          "A. \"Messi The Legend\".",
          "B. \"Soccer Championship\".",
          "C. \"Neymar El Clacico in & out success\".",
          "D. \"Football Highlights\"."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_256_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:11:00]",
    "captions": "[0:10:00 - 0:10:20] [0:10:00 - 0:10:03]: The video starts with a scene of a soccer field surrounded by a tall metal fence and some greenery visible in the background. Two players are prominently visible near the goal on the right side of the frame. One player in a yellow vest appears to be moving towards the player in a white jersey who seems to have possession of the ball. The field is well-lit, and the sky is dark, indicating it's nighttime. [0:10:04 - 0:10:11]: The player in the white jersey skillfully dribbles the ball past the player in the yellow vest by executing a nutmeg (passing the ball between the opponent’s legs). The ball then rolls towards the goal, and the player in the white jersey follows up the move by calmly guiding the ball into the net. A few spectators and other players are standing around the field, visibly watching the progression of the play. [0:10:12 - 0:10:14]: The camera shifts to show a closer view of the field. The player who scored, often referred to as Jay in the subtitles, is now off the field and is seen celebrating near the sidelines. Some spectators are seated in the stands, clapping and cheering for Jay. [0:10:15 - 0:10:17]: The perspective shifts to a first-person view where the camera holder is moving towards a soccer ball lying close to the line of the field. The player seems to walk towards the ball, showing their shadow on the grass as they approach. [0:10:18]: The camera view shakes, suggesting the player has been fouled or taken a fall, leading to a blurry and disoriented frame of the field. [0:10:19 - 0:10:20]: The camera then returns to a steady view, capturing the play starting again with the players spread out on the field. One of the notable players in the yellow vest is positioned centrally, seeming to prepare for the next action.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the jersey of the player who performs the nutmeg?",
        "time_stamp": "0:10:11",
        "answer": "A",
        "options": [
          "A. Yellow.",
          "B. Red.",
          "C. Blue.",
          "D. White."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_256_real.mp4"
  },
  {
    "time": "[0:12:00 - 0:12:58]",
    "captions": "[0:12:40 - 0:12:58] [0:12:40 - 0:12:42]: The video begins with an animated sequence featuring a cartoonish drawing of a person wearing a red sports jersey with the number 30 and the word \"PARK\" on the back, surrounded by a circular frame. Below the character, the text \"J Football TV\" appears, becoming clearer as the animation progresses. [0:12:43 - 0:12:44]: The scene transitions to a first-person perspective driving towards the Eiffel Tower on a sunny day. There are several cars ahead on the road, with the text \"The next episode\" superimposed over the scene. [0:12:45 - 0:12:47]: The video cuts to an energetic stadium crowd during a nighttime football match. The stadium is well-lit, and fans can be seen cheering. The text \"In the next episode, a video of Football in France will be uploaded!\" is displayed at the bottom of the screen in both English and another language. [0:12:48 - 0:12:51]: The video continues to show various angles of a football match in a large stadium. The grass is green and well-maintained. Players from two teams, wearing different colored uniforms, are actively engaged in the game. The text about the next episode's football video remains on the screen. [0:12:52 - 0:12:54]: The scene shifts to a busy plaza in front of a famous glass pyramid structure, with many people walking around and taking photos. The sky is clear and blue. The same text about the next episode's football video is visible at the bottom. [0:12:55]: The video briefly shows another wide-angle view of the Eiffel Tower from an elevated position, with beautifully landscaped gardens and pathways below. People are walking and enjoying the scenery. The consistent text about the upcoming video remains on screen. [0:12:56 - 0:12:57]: The final frames feature a green football field with overlay text encouraging viewers to \"Please subscribe and like :)\" and highlights spots for recommended videos and recent uploads with clickable links. The text \"Thanks for watching!\" is displayed at the bottom.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the position of the Eiffel Tower relative to the camera in the first-person driving perspective?",
        "time_stamp": "0:12:44",
        "answer": "B",
        "options": [
          "A. To the left.",
          "B. Directly ahead.",
          "C. To the right.",
          "D. Behind."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_256_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:16",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      },
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:56",
        "answer": "D",
        "options": [
          "A. 1.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_103_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:01:26",
        "answer": "A",
        "options": [
          "A. 0.",
          "B. 3.",
          "C. 4.",
          "D. 1."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      },
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:01:55",
        "answer": "A",
        "options": [
          "A. 0.",
          "B. 3.",
          "C. 4.",
          "D. 1."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_103_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:26",
        "answer": "D",
        "options": [
          "A. 3.",
          "B. 4.",
          "C. 6.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_103_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:00",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 8.",
          "C. 3.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_103_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the motorcycle parked right now?",
        "time_stamp": "00:00:10",
        "answer": "B",
        "options": [
          "A. In a parking lot.",
          "B. In a gas station.",
          "C. On a roadside.",
          "D. In a garage."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_300_real.mp4"
  },
  {
    "time": "[0:02:17 - 0:02:37]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the McDonald's sign located relative to the motorcyclist right now?",
        "time_stamp": "00:02:25",
        "answer": "A",
        "options": [
          "A. Ahead on the left side.",
          "B. Ahead on the right side.",
          "C. Directly behind.",
          "D. Floating above."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_300_real.mp4"
  },
  {
    "time": "[0:04:34 - 0:04:54]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What type of vehicle is directly ahead of the cyclist right now?",
        "time_stamp": "00:04:52",
        "answer": "B",
        "options": [
          "A. A white minivan.",
          "B. A yellow taxi.",
          "C. A blue sedan.",
          "D. A red sports car."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_300_real.mp4"
  },
  {
    "time": "[0:06:51 - 0:07:11]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the person photographing the motorcycles standing right now?",
        "time_stamp": "00:07:02",
        "answer": "B",
        "options": [
          "A. On the sidewalk.",
          "B. In the middle of the street.",
          "C. On the motorcycle.",
          "D. In the next shop."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_300_real.mp4"
  },
  {
    "time": "[0:09:08 - 0:09:28]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What type of vehicle is directly ahead of the cyclist right now?",
        "time_stamp": "00:09:46",
        "answer": "C",
        "options": [
          "A. A white minivan.",
          "B. A yellow taxi.",
          "C. A blue sedan.",
          "D. A red sports car."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_300_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What is the speaker likely to explain next?",
        "time_stamp": "00:00:11",
        "answer": "C",
        "options": [
          "A. How to solve the division problem using a different method.",
          "B. How to add partial quotients.",
          "C. The concept of division with partial quotients.",
          "D. The importance of understanding division."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_228_real.mp4"
  },
  {
    "time": "[0:02:55 - 0:03:25]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:03:24",
        "answer": "C",
        "options": [
          "A. Multiplying 6 by the divisor.",
          "B. Subtracting 6 from the dividend.",
          "C. 2 is too small.",
          "D. Adding 6 to the quotient."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_228_real.mp4"
  },
  {
    "time": "[0:05:50 - 0:06:20]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:06:20",
        "answer": "D",
        "options": [
          "A. How to subtract the next two numbers.",
          "B. How to add the remainder to the divisor.",
          "C. How to multiply 100 by 35 again.",
          "D. How to choose a number to subtract from 2928."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_228_real.mp4"
  },
  {
    "time": "[0:08:45 - 0:09:15]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:09:04",
        "answer": "D",
        "options": [
          "A. How to simplify the calculation further.",
          "B. The application of division in real-life problems.",
          "C. Another example using a more complex number.",
          "D. The advantage of using small steps in division."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_228_real.mp4"
  },
  {
    "time": "[0:11:40 - 0:12:10]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the teacher explain next?",
        "time_stamp": "00:10:42",
        "answer": "C",
        "options": [
          "A. How to perform long division.",
          "B. A different multiplication method for large numbers.",
          "C. Why use samll steps in division is easier.",
          "D. Explaining the significance of the answer 49r2."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_228_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a first-person perspective showing a race track. On the left side, there is a billboard with a black rabbit logo and the text \"TBR\". Four cars, one red, one green, and two white, are visible on the track. They are racing on a black pavement bordered by a grassy landscape with trees in the background. [0:00:01 - 0:00:04]: The perspective changes to show another section of the race track with a different billboard displaying \"JUH\" on it. This part of the track has several toys and a fenced area with more cars. The camera quickly transitions to a black screen with some graphical glitch effects. [0:00:04 - 0:00:06]: The screen shows an animated, glitching logo with the text \"NEXT GEN Diecast\" and \"The Next Generation of Diecast Racing\". The logo stabilizes over these frames.  [0:00:07 - 0:00:13]: The scene switches back to the race track. Three toy cars – one grey with green accents, a red one, and a green one – are positioned at the starting line. There are three signs along the track wall: \"TBR Thunder Bunny Racing\", \"JUH KRAFTS.COM RACING\", and another \"TBR\" logo. The text \"NEXT-GEN PISTON CUP\", \"Race 1 - Round 1\", and \"Group 7\" sequentially appear on the screen across these frames. [0:00:14 - 0:00:18]: The camera captures a closer view of the toy cars on the track. The cars are in motion, emphasizing the race. The green car starts pulling ahead of the others, and the camera follows its movement closely. [0:00:19 - 0:00:20]: The camera focuses on the green toy car, showcasing its details. The car is bright green with white and green decals and the number \"24\" displayed prominently on its side. A fence in the background and other toy cars are barely visible.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is prominently displayed on the side of the green toy car?",
        "time_stamp": "00:00:20",
        "answer": "B",
        "options": [
          "A. The number \"12\".",
          "B. The number \"24\".",
          "C. The number \"36\".",
          "D. The number \"48\"."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_498_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:09]: A scoreboard displayed at the center shows the standings for Group 7 - Heat 1 with the following racers and scores: #94 Jay with 5 points, #35 Ronald with 3 points, #24 Chase Racelott with 2 points, and #28 Tim Treadless with 1 point. The scoreboard is prominently placed against the mountainous background and a grassy slope on the left. Racing tracks run on the left, leading downhill. [0:02:10]: Four race cars are positioned at the starting line. The cars are brightly colored; a green car (#24) is closest to the viewer on the left, followed by a silver car with blue accents (#35), a red car (#28), and another grey car (#94) with a green fin in the back on the right. Small green trees and a backdrop painted to resemble a cloudy sky and distant hills complete the background. [0:02:11 - 0:02:12]: The text “Group 7 - Heat 2” appears at the bottom left corner, indicating that the current segment shows the beginning of Heat 2. The scene remains the same with the race cars at the starting line and the background unchanged. [0:02:13 - 0:02:16]: The cars remain at the starting position, ready for the race to begin. The same painted sky and small trees are seen in the background. [0:02:17]: The race cars start moving forward rapidly, taking off from the starting line. The positioning remains largely the same, with the background trees and painted sky still visible. [0:02:18]: The cars race along the track, the green car (#24) and the grey car (#94) with the blue fin leading the race closely on the left, while the red car (#28) slightly trails behind. [0:02:19]: The cars approach a curve in the track, still tightly packed together. In the background, semi-realistic painted pine trees and a large sign are visible, adding depth to the scene. The blue sky with white clouds stretches across the background, maintaining the racing environment.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "Which car has a green fin in the back?",
        "time_stamp": "0:02:10",
        "answer": "C",
        "options": [
          "A. Car #24.",
          "B. Car #28.",
          "C. Car #35.",
          "D. Car #94."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_498_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:04]: The video begins with a racing scene on a winding track set in a natural, hilly landscape. Multiple toy race cars are navigating a curve on the track. The surroundings include grassy hills, trees, and a blue sky with clouds. The track has barriers and signs around it, and a large \"INSTANT REPLAY\" banner is displayed at the top right.  [0:04:05 - 0:04:09]: As the cars progress, they move into a section of the track adjacent to what appears to be a pit stop area. The fence separates this section from the pit stop, where other cars and equipment are visible. The leading cars are transitioning from the wide curve onto a straighter part of the track. [0:04:10 - 0:04:15]: The video shows cars continuing their race. They speed down a straight path and enter another curve with various advertisements and logos visible. The track remains surrounded by natural scenery, with sections of grass and trees on the surrounding hills. [0:04:16 - 0:04:20]: The cars further navigate through more curves and straight sections, passing by visible details like race-side vehicles and signs. The perspective shifts a bit to follow the cars, giving a dynamic view of the race track's layout and the consistent progress of the racing cars. The fences and pit stop areas remain alongside certain sections of the track.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is displayed at the top right right now?",
        "time_stamp": "00:04:06",
        "answer": "C",
        "options": [
          "A. Race results.",
          "B. Lap times.",
          "C. Instant Replay.",
          "D. Track map."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_498_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:03]: Two cars are racing on a black track with a mesh fence separating the track from a garage area. The cars are green and white, with the green car on the left and the white car on the right. The background includes a grassy area, some trees, and a hill. The garage area contains multiple other cars and vehicles, and above the scene is an \"INSTANT REPLAY\" banner. [0:06:04 - 0:06:19]: The scene changes to display the standings of Group 7 - Final. The standings are shown in a gray box with red \"Standings\" header: 1. #94 Jay (+2) with 15 points  2. #28 Tim Treadless (+1) with 10 points  3. #24 Chase Racelott (+1) with 10 points  4. #35 Ronald with 9 points The black track and grassy area remain visible in the background, alongside the garage fence and other vehicles. The standings remain displayed consistently from 0:06:04 to 0:06:19.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What colors are those two racing cars?",
        "time_stamp": "0:06:00",
        "answer": "B",
        "options": [
          "A. Red and blue.",
          "B. Green and silver.",
          "C. Black and yellow.",
          "D. Blue and orange."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Text-Rich Understanding",
        "question": "Who is in the first place in the Group 7 - Final standings?",
        "time_stamp": "0:06:19",
        "answer": "D",
        "options": [
          "A. Tim Treadless.",
          "B. Chase Racelott.",
          "C. Ronald.",
          "D. Jay."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_498_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the energy drink can in the car right now?",
        "time_stamp": "00:00:17",
        "answer": "C",
        "options": [
          "A. Green.",
          "B. Blue.",
          "C. Red.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_260_real.mp4"
  },
  {
    "time": "[0:02:56 - 0:03:01]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the crane visible right now?",
        "time_stamp": "00:02:56",
        "answer": "B",
        "options": [
          "A. Red and white.",
          "B. Yellow and red.",
          "C. Blue and white.",
          "D. Green and yellow."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_260_real.mp4"
  },
  {
    "time": "[0:05:52 - 0:05:57]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What are the colors of the car's central console right now?",
        "time_stamp": "00:05:52",
        "answer": "D",
        "options": [
          "A. white.",
          "B. Red and blue.",
          "C. Red and white.",
          "D. Yellow and black."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_260_real.mp4"
  },
  {
    "time": "[0:08:48 - 0:08:53]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color are the cars directly in front?",
        "time_stamp": "00:08:41",
        "answer": "C",
        "options": [
          "A. Blue.",
          "B. Orange.",
          "C. Black.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_260_real.mp4"
  },
  {
    "time": "[0:11:44 - 0:11:49]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the car directly in front?",
        "time_stamp": "00:11:57",
        "answer": "C",
        "options": [
          "A. Blue.",
          "B. Orange.",
          "C. Red.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_260_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: A blank white textured canvas is placed horizontally on the left side of a white flat surface, with a rectangular palette containing some paint positioned vertically to the right of the canvas. [0:00:01 - 0:00:02]: A hand holding a blue paint tube appears, and the words \"Easypeasy art\" are displayed over the canvas.  [0:00:02 - 0:00:05]: The paint tube releases a dollop of cerulean blue paint onto the palette. The text “Cerulean blue, White, Flat brush no 6” appears on the top-left corner of the frame. A flat brush then mixes the blue paint before applying it to the canvas.  [0:00:05 - 0:00:15]: The brush starts applying the blue paint to the top edge of the canvas in a smooth horizontal strip. The paint is spread evenly, moving from right to left, then left to right, in successive strokes to cover the upper portion of the canvas. [0:00:15 - 0:00:17]: The brush picks up a bit more blue paint and gradually adds white paint, starting to blend it into the blue while continuing to cover more of the upper canvas area.  [0:00:17 - 0:00:19]: The process of blending white into the cerulean blue continues, creating a gradient effect as the brush moves horizontally across the canvas.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the initial painting process in the clip?",
        "time_stamp": "00:00:19",
        "answer": "A",
        "options": [
          "A. Applying and blending cerulean blue and white paint on the upper part of the canvas.",
          "B. Setting up various colors of paint and brushes on the canvas.",
          "C. Drawing an outline before filling in with different colors.",
          "D. Cleaning the brushes and preparing the paint palette."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_138_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:19]: The video showcases a canvas with a blue sky and green foliage being painted in the foreground. A hand holds a light purple paintbrush, delicately adding detail to the foliage, primarily at the top of the painted tree area. The brush makes small, deliberate strokes, mostly in an upward motion, adding texture to the green paint which forms the silhouette of tree leaves against the light blue sky. The canvas is positioned on a flat white surface, and to the right side of the frame, there is a palette with a mix of blue and white paint, and a small amount of green paint. The painting progresses from the left and is built up towards the right side of the canvas. [0:02:20]: The hand with the paintbrush adjusts its position slightly, preparing to continue the detailed work on the foliage against the blue sky background.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the painting activity just shown in the video?",
        "time_stamp": "0:02:20",
        "answer": "A",
        "options": [
          "A. Adding texture to a tree's foliage against a blue sky.",
          "B. Painting a landscape with mountains and a river.",
          "C. Creating a portrait with detailed facial features.",
          "D. Sketching a building with architectural details."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_138_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:19]: An artist is painting a landscape on a small canvas. The artwork primarily features a bright, blue sky with some white, fluffy clouds. The foreground contains a large, leafy green tree whose branches extend into the sky's expanse. The painting occupies the center of the frame, while to the right, there is a mixing palette holding various shades of green and blue paint. The artist's hand appears intermittently, especially towards the bottom right of the canvas, adding details to the tree. The brush strokes seem delicate, indicating precision in creating the tree's foliage. The background is a clean white surface. The scene shows a continuous progression in adding texture and detail to the greenery.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best describes what the artist is doing?",
        "time_stamp": "00:04:19",
        "answer": "A",
        "options": [
          "A. An artist painting a landscape with a bright sky and a leafy tree.",
          "B. An artist drawing a portrait of a person.",
          "C. An artist sculpting a figure out of clay.",
          "D. An artist photographing a nature scene."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_138_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:08]: A hand is seen painting small pink flowers on a canvas. The canvas shows a scenic view of green foliage and a clear blue sky with some white clouds. The artist's hand is holding a brush and carefully adding pink flowers to the green foliage in the bottom left quadrant of the canvas. The palette with paints is visible to the right of the canvas, containing shades of pink and blue.  [0:06:09 - 0:06:14]: The artist continues to add more pink flowers to the foliage. The brush moves swiftly and precisely, filling in the green leaves with small, intricate flower details. The sky background remains clear and blue, adding contrast to the pink flowers and green leaves. [0:06:15 - 0:06:16]: The artist makes further adjustments, enhancing the density of the pink flowers on the green foliage. Their hand is visible, steady, and focused on creating a floral pattern. [0:06:17]: A close-up view of the paint palette shows blobs of neon pink, neon orange, and white paint. The painting on the canvas remains in the background. [0:06:18 - 0:06:19]: The paintbrush dips into the neon pink and neon orange paint on the palette, ready to continue adding details to the floral scene on the canvas. The canvas with painted flowers is partially visible in the frame.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the artist adding pink flowers on the canvas?",
        "time_stamp": "00:06:08",
        "answer": "B",
        "options": [
          "A. Top right quadrant.",
          "B. Bottom left quadrant.",
          "C. Center of the canvas.",
          "D. Top left quadrant."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the painting process just shown in the video?",
        "time_stamp": "00:06:19",
        "answer": "A",
        "options": [
          "A. An artist adding intricate floral details to a scenic painting.",
          "B. An artist painting a mountainous scene with animals.",
          "C. An artist sculpting a clay model of a tree.",
          "D. An artist drawing a sketch of a cityscape."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_138_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:00:10",
        "answer": "D",
        "options": [
          "A. The person picked up a metal pitcher, cleaned it, and placed it back on the shelf.",
          "B. The person heated a metal pitcher, added ingredients, and served them in a cup.",
          "C. The person cleaned a metal pitcher, filled it with water, and placed it next to other pitchers.",
          "D. The person retrieved one metal pitcher, steamed it, and returned the milk carton to the fridge."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_356_real.mp4"
  },
  {
    "time": "[0:02:16 - 0:02:26]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:02:25",
        "answer": "D",
        "options": [
          "A. The person prepared a coffee mug, poured milk into it, and placed it under a coffee machine.",
          "B. The person cleaned a coffee grinder, filled it with new beans, and started grinding the coffee.",
          "C. The person filled a coffee pot with water, placed it in the machine, and added coffee grounds to the filter.",
          "D. The person ground coffee beans, measured the coffee grounds on a scale, and adjusted the amount until it reached the desired weight."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_356_real.mp4"
  },
  {
    "time": "[0:04:32 - 0:04:42]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:04:41",
        "answer": "D",
        "options": [
          "A. The person prepared a coffee, added sugar, and stirred it with a spoon.",
          "B. The person brewed tea, added honey, and placed it on the counter to be served.",
          "C. The person filled a cup with water, added ice, and handed it to a customer.",
          "D. The person steamed milk, poured it into a cup with some green liquid, and created a latte art."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_356_real.mp4"
  },
  {
    "time": "[0:06:48 - 0:06:58]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:06:57",
        "answer": "D",
        "options": [
          "A. The person rinsed the pitcher with cold water and placed it on a shelf.",
          "B. The person brewed coffee and added milk to it before serving.",
          "C. The person cleaned a coffee machine and replaced the portafilter.",
          "D. The person steamed milk in a metal pitcher while setting a cup under an espresso machine."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_356_real.mp4"
  },
  {
    "time": "[0:09:04 - 0:09:14]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:09:14",
        "answer": "D",
        "options": [
          "A. The person filled a coffee cup, placed it in the sink, and cleaned it with water.",
          "B. The person ground coffee beans, measured the coffee grounds, and prepared a coffee pot.",
          "C. The person retrieved a milk carton from the fridge, measured the milk, and placed it on the counter.",
          "D. The person steamed the milk in one metal pitcher while shaking another metal pitcher simultaneously."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_356_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:04]: The video starts with a blue car driving on a street at dusk. Visible in the distance are buildings and a bridge with lights. Streetlights are on, and the sky is a dusky orange-purple. The camera angle, suggesting a first-person perspective, shows the car moving down the road. [0:00:05 - 0:00:08]: The car approaches an intersection with a \"STOP\" sign. Two pedestrians are standing at the corner to the right. On the left side, there's a billboard with some text and lights of the city in the background. [0:00:09 - 0:00:11]: The car turns right, passing the two pedestrians. The road is lined with low-rise buildings and palm trees, and in the evening light, the headlights of the car are noticeable. [0:00:12 - 0:00:14]: Continuing to drive, the car passes a billboard with the text \"MY DAD\" prominently displayed. The area seems more residential, with several houses and a distant city skyline partially visible. [0:00:15 - 0:00:18]: The car reaches another intersection, with light traffic. Neon and billboard lights are visible further down the street. Another car can be seen on the right side, and the residential feel continues with small buildings and palm trees aligning the road. [0:00:19 - 0:00:22]: The car continues to drive straight through the intersection. In the background, more buildings become visible, illuminated by the city lights. The roadway is clear ahead, and the streetlights provide illumination as dusk settles. [0:00:23 - 0:00:26]: The blue car continues on a fairly empty road. To the left, some small shops or businesses are seen, while to the right, there is a parking lot or open space. The focus seems to remain on the road ahead with occasional glances to the side. [0:00:27 - 0:00:30]: The car reduces speed and comes to a stop at another intersection. Another car is seen passing by. The video focuses on the surroundings, with more buildings and illuminated windows giving a lively urban feel. [0:00:31 - 0:00:33]: The car approaches the intersection, turning into a less busy street. The lighting changes as the car navigates into a relatively dimmer, quieter part of the area. Some distant high-rise buildings are illuminated by city lights. [0:00:34 - 0:00:37]: The blue car drives into what seems to be a parking or loading zone area with sparse lighting. The area has a mix of residential and commercial buildings with minimal activity. [0:00:38 - 0:00:41]: The car comes to a stop, and the driver seems to be waiting or observing the surroundings. Another car, a silver one, appears and parks nearby. Both cars remain stationary in the dimly lit area. [0:00:42 - 0:00:44]: The camera now focuses on the silver car parked to the right. The silver car's driver appears to be stationary with minimal movement visible. [0:00:45 - 0:00:48]: The view shifts slightly, and the environment remains the same with both cars parked next to each other. The surrounding area is quiet, with no visible pedestrian activity. [0:00:49 - 0:00:51]: The cars remain stationary, and the camera slightly adjusts focus. The silver car's lights are more visible now. The surrounding environment remains consistent. [0:00:52 - 0:00:55]: The driver of the blue car seems to be observing the surroundings. There is minimal movement, and the environment is quiet with a few buildings in the background. [0:00:56 - 0:00:59]: The focus remains on the stationary cars. The blue car's driver appears to be checking something, and the video captures the still surroundings. [0:01:00 - 0:01:03]: The cars remain parked. There is a slight shift in camera angles, indicating a first-person perspective. The area remains dimly lit with a mix of shadows and shades from the streetlights and surrounding buildings. [0:01:04 - 0:01:07]: Both cars are still parked. The camera angle remains relatively the same, maintaining a first-person perspective on the blue car's position and the surrounding area. [0:01:08 - 0:01:11]: The video continues with the same view, and there's no significant change in the surroundings. The area remains quiet and dimly lit. [0:01:12 - 0:01:15]: The viewpoint still focuses on",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the blue car doing right now?",
        "time_stamp": "00:00:19",
        "answer": "C",
        "options": [
          "A. Turning at an intersection.",
          "B. Driving down a busy street.",
          "C. Parked and waiting.",
          "D. Speeding through a residential area."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_292_real.mp4"
  },
  {
    "time": "0:02:40 - 0:03:00",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:44]: The video features a first-person perspective of a person at a computer screen displaying a personal account interface. A list of transactions is visible, with titles like \"Purchase from Uncharted,\" \"Purchase from Game Account,\" \"Cash Withdrawl,\" \"Personal Account - 616771,\" and \"Clothing Store,\" along with corresponding amounts. The person looking at the screen appears in a smaller rectangular video feed in the top left corner;  [0:02:45 - 0:02:49]: There are several large buttons on the right side of the interface. The background visible beyond the screen indicates a typical indoor environment with tiled flooring and a glimpse of a welcome mat at the bottom. The counter area shows some items stacked, possibly indicating a retail or service environment. The display continues to show details of the account’s balance and transactions for today;  [0:02:50 - 0:02:54]: The smaller video feed on the top left corner shows the individual, who seems to be talking and sometimes reacting with various facial expressions while they navigate the account screen. The chat feed on the right side of the screen continuously updates with messages from viewers;  [0:02:55 - 0:02:59]: The person in the smaller feed seems to smile or express amusement. Meanwhile, the account interface remains primarily unchanged, showcasing the list of transactions and account details. The chat feed scrolling on the right side continues to display incoming messages from users;  [0:03:00]: The person appears to start selecting an option from the interface. The smaller feed at the top left still displays the individual who continues to react to what is happening on the screen. The account interface shows a new screen that seems to be focusing on a withdrawal process, with fields for selecting accounts, amount, and comment.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:03:00",
        "answer": "A",
        "options": [
          "A. Browsing his personal account from the interface.",
          "B. Typing a comment in the chat feed.",
          "C. Closing the account interface.",
          "D. Logging out of the personal account."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_292_real.mp4"
  },
  {
    "time": "0:05:20 - 0:05:40",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:24]: A person is driving a small blue car through a city at night. The vehicle is in the right lane of a multi-lane road. On the left side of the screen, a secondary view shows the driver playing a game on a computer screen, surrounded by dark streets and buildings.  [0:05:25 - 0:05:29]: The car continues to move down the road, curving slightly to the right. Streetlights line the sides, illuminating the street, which has palm trees and empty sidewalks. The driver appears focused on the road. [0:05:30 - 0:05:34]: The car approaches an intersection with a small traffic island. The driver navigates through, and some illuminated signs and buildings are visible in the background.  [0:05:35 - 0:05:44]: The car turns onto a narrower road lined with palm trees. The driver carefully maneuvers through the night, with stoplights and signposts visible. Brightly lit storefronts and residential buildings line the streets. [0:05:45 - 0:05:54]: The blue car stops at an intersection, near a brightly lit gas station. The streets remain quiet. The driver's view shows signs of concentration as they check for oncoming traffic. [0:05:55 - 0:06:11]: The car moves through a couple of intersections, driving past various buildings and making occasional turns. Streetlights illuminate the road, and the scene contains a mixture of commercial and residential areas. [0:06:12 - 0:06:20]: The car continues to drive through dark city streets, passing colorful lights and closely packed houses. Several turns navigate the driver through a quiet neighborhood, with lit houses and festive decorations visible.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the driver doing right now?",
        "time_stamp": "00:06:20",
        "answer": "C",
        "options": [
          "A. Turning off the car.",
          "B. Parking near a gas station.",
          "C. Driving through dark city streets.",
          "D. Entering a brightly lit store."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_292_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:17]: The video starts with a person driving a small blue car through a city at nighttime. Streetlights illuminate the area, casting light on tall buildings and various urban structures around. The car is initially stationary at a traffic light, with another road visible to the right. The driver then begins to move, navigating through empty city streets. The car turns left at the intersection and proceeds under a bridge. Street lamps and overhead lights cast a glow on the path. There are billboards and signs mounted on buildings, indicating a bustling city atmosphere. Traffic lights change from red to green as the driver procedurally follows the road; [0:08:17 - 0:08:20]: The car continues along the road, passing through an intersection lined with palm trees and commercial establishments, heading down a relatively empty street.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the person driving the car just do?",
        "time_stamp": "00:08:20",
        "answer": "D",
        "options": [
          "A. Turned right at the intersection.",
          "B. Stopped at a traffic light.",
          "C. Turned left at the intersection.",
          "D. Keep going straight on the road."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_292_real.mp4"
  },
  {
    "time": "0:10:00 - 0:10:15",
    "captions": "[0:10:00 - 0:10:15] [0:10:00 - 0:10:05]: The video is filmed from a first-person perspective within a store. The store has various items on display, indicating that it might be a retail environment. Two people are talking in the store. One person is wearing a black shirt and has tattoos on their arms, while the other is wearing a grey t-shirt with a yellow vest over it. Both are wearing headphones. The background shows a range of products, some of which seem to be military or tactical gear, with a shelf and racks adorned with backpacks, jackets, and an American flag mounted on the wall. [0:10:06 - 0:10:10]: One of the individuals looks leftward, possibly checking out other items in the store, while the other continues to face them. The scene remains largely similar, with the store interior still visible. A vending machine for a beverage and a lighted area displaying smaller items can be seen prominently behind the person in the black shirt. Both people continue to engage in what appears to be a conversation. [0:10:11 - 0:10:15]: The view shifts slightly as the person wearing the vest turns his head, revealing more of the store's layout. In the upper left corner of the screen, a smaller inset feed displays someone in front of a different background, likely indicating a livestream or a recording. The walls behind the store counter have numerous firearms mounted on them, along with a sign reading \"LOS SANTOS GUN CLUB.\" Various other equipment and merchandise are spread throughout the store, showcasing a detailed retail setup for tactical and possibly recreational items like weapons and accessories.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the two individuals do just now?",
        "time_stamp": "00:10:10",
        "answer": "A",
        "options": [
          "A. Had a face-to-face conversation while standing.",
          "B. Adjust their headphones.",
          "C. Pick up a backpack from the shelf.",
          "D. Remove their vest."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_292_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video begins with a completely dark screen. [0:00:01]: A splash screen with the text \"Joony Art\" appears over a dark brown background. There are two clouds of color, one pink on the top left and one blue on the top right, surrounding the text. [0:00:02 - 0:00:03]: A white canvas measuring 28 cm by 23 cm (11 inches by 9 inches) is positioned in the center of the frame against a wooden surface. To the right of the canvas is a white palette with the text \"Black\" visible. A hand is holding a tube labeled \"Black\" which aligns with the text on the palette. [0:00:04]: The hand continues to hold the black paint tube, indicating the beginning phase of paint selection. [0:00:05 - 0:00:06]: The hand now holds a different paint tube, this time labeled \"Cerulean Blue,\" positioned in the same spot on the palette. [0:00:07 - 0:00:08]: The paint tube changes again to \"Emerald Green,\" which is positioned accordingly on the palette. [0:00:09]: Next, the hand introduces another tube of paint, labeled \"Vermilion.\" The spot on the palette matches the label. [0:00:10 - 0:00:11]: Following this, the paint tube changes to \"Permanent Yellow,\" with the label aligned on the palette. [0:00:12 - 0:00:13]: The hand takes a tube of white paint labeled \"White.\" [0:00:14]: The white paint tube remains held by the hand, with the label in view. [0:00:15]: The hand is no longer present, revealing the arranged paints on the palette next to the canvas. The labels for each color are visible: Black, Cerulean Blue, Emerald Green, Vermilion, and Permanent Yellow. [0:00:16]: The canvas alone is shown in the center beside the palette containing arranged paints. [0:00:17 - 0:00:18]: A 25mm flat brush is held over the palette, suggesting preparation for painting. [0:00:19]: The brush picks up white paint from the palette, indicating the start of a painting process.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the initial sequence of the video?",
        "time_stamp": "0:00:20",
        "answer": "A",
        "options": [
          "A. A dark screen is followed by a splash screen, then a hand arranges and prepares paint for a painting session.",
          "B. A splash screen is followed by a dark screen, then a hand arranges brushes for a painting session.",
          "C. A white canvas is shown first, followed by a splash screen and paint tubes being arranged.",
          "D. A palette is shown with various colors, followed by a splash screen and a hand preparing brushes."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_139_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:44]: A canvas is positioned in the center of a wooden table with partially painted background featuring a bright blue sky with white clouds on the upper half. The horizon line and a portion below is painted in turquoise. To the right of the canvas, there is a palette with different shades of blue and green paints. There is a piece of masking tape on the canvas which aligns with the horizon. [0:02:44 - 0:02:52]: A hand holding a brush applies more turquoise paint in horizontal strokes, blending it further down the canvas, gradually creating a seamless transition from sky to the sea. [0:02:52 - 0:02:57]: The turquoise paint continues to be blended from the center of the canvas towards the bottom, creating a gradient effect. The hand movement with the brush is smooth and consistent. [0:02:57 - 0:03:00]: The hand is seen reaching towards the palette, mixing and picking up more paint to continue the process. The horizon line remains crisp, with the gradient blending continuing below it.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the masking tape positioned on the canvas?",
        "time_stamp": "00:02:44",
        "answer": "C",
        "options": [
          "A. Along the top edge.",
          "B. Along the bottom edge.",
          "C. Aligning with the horizon.",
          "D. Vertical down the center."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Causal Reasoning",
        "question": "Why did the hand reach towards the palette just now?",
        "time_stamp": "00:03:00",
        "answer": "B",
        "options": [
          "A. To clean the brush.",
          "B. To mix and pick up more paint.",
          "C. To adjust the masking tape.",
          "D. To start a new painting."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_139_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:30]: A hand holding a thin paintbrush is seen working on a painting of an ocean scene. The painting shows waves in various shades of blue with white highlights representing the foam, and a sandy beach in the foreground. There are fluffy white clouds in the sky against a blue background. The hand meticulously adds details to the water with the paintbrush, focusing on creating texture and depth in the waves. [0:05:31 - 0:05:35]: The hand continues to paint fine details on the waves, moving the brush along the top part of the second wave, adding darker and lighter shades to give a sense of movement and realism. The background remains consistent with the previous frames, showing a calm sea meeting the horizon under a partly cloudy sky. [0:05:36 - 0:05:40]: The hand shifts slightly to the right, continuing to add detail to the wave, focusing on the crest where the wave starts to break, creating a foamy texture. The rest of the scene remains unchanged, with the horizon and sky providing a serene backdrop to the detailed ocean waves in the foreground.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the hand focusing its painting efforts right now?",
        "time_stamp": "0:05:38",
        "answer": "C",
        "options": [
          "A. On the sandy beach.",
          "B. On the horizon line.",
          "C. On the the second wave.",
          "D. On the fluffy white clouds."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_139_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:20]: The video shows a close-up view of a hand holding a small brush, delicately painting the details of a seascape on a canvas. The scene being painted consists of vibrant turquoise waves crashing onto a sandy beach, with white foam where the water meets the sand. The hand is positioned near the bottom center of the frame, steadily applying white paint around the area where the waves break. The brush moves gently and precisely to add the frothy details. Surrounding the painted area, the workspace includes a palette with various colors of paint arranged in a semicircle on the right edge of the frame. The background of the workspace is a neatly arranged wooden surface. The top of the scene displays a clear blue sky with a few scattered clouds, and in the distance, a dark silhouette of a mountain range is visible on the horizon.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the content just shown in the video?",
        "time_stamp": "00:08:20",
        "answer": "A",
        "options": [
          "A. A hand painting a vibrant seascape with turquoise waves and white foam.",
          "B. A close-up of an artist sketching a cityscape.",
          "C. A detailed view of a forest scene being painted.",
          "D. An artist mixing colors on a palette."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_139_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:05]: The video opens on a white wall displaying four framed artworks. From left to right, the first frame contains a green abstract design, the second frame shows a blue human figure, the third frame has a solid blue content, and the fourth frame depicts an abstract mix of blue and green colors. All frames are rectangular and aligned horizontally;  [0:00:06 - 0:00:10]: The camera shifts right, moving away from the initial four artworks to another set. Four smaller rectangular frames are visible, each depicting blue orchids against a grayscale striped background. They are arranged in a 2x2 grid; [0:00:11 - 0:00:12]: The scene changes to a white wall with a sign reading \"GALERIE DE’ ARTS 1343 SANTA MONICA, CA\" positioned high on the wall, likely indicating the gallery section; [0:00:13 - 0:00:16]: The camera pans left towards a different display. A white wall holds several framed images of vintage cameras in various designs. One large framed image is in the center, surrounded by smaller frames on the left; [0:00:17 - 0:00:20]: The camera continues to move leftwards, focusing closer on the vintage camera frames. The central large frame shows a detailed camera against an orange backdrop, while the smaller frames showcase cameras with various backgrounds (tan, blue, and green). The video ends with the camera facing directly towards these framed images.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "How are the four smaller rectangular frames arranged?",
        "time_stamp": "00:00:09",
        "answer": "C",
        "options": [
          "A. Horizontally in a single row.",
          "B. Vertically in a single column.",
          "C. In a 2x2 grid.",
          "D. In a diagonal line."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Text-Rich Understanding",
        "question": "What does the sign on the white wall read?",
        "time_stamp": "00:00:15",
        "answer": "A",
        "options": [
          "A. \"GALERIE DE' ARTS 1343 SANTA MONICA, CA\".",
          "B. \"ART GALLERY SANTA MONICA\".",
          "C. \"DE' ARTS GALLERY 1234 SANTA MONICA, CA\".",
          "D. \"SANTA MONICA ART GALLERY\"."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_476_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:47]: The video begins in a gallery exhibition space featuring modern sculptures and artworks. Four abstract, black metallic sculptures of varying shapes and sizes are displayed on white pedestals of different heights. These sculptures are positioned against a white wall. The wall has text saying \"Esquisse gallery\" on the left side. Another text, \"MKRTICHMAZMANIAN,\" runs vertically along the right side of the wall. The floor is concrete, and there is a cluster of planted white flowers near the sculptures. [0:02:48 - 0:02:49]: The camera pans toward an adjacent wall displaying art. A blue sign overhead reads \"ART UNIFIED 1359, VENICE | CA. MODERN - CONTEMPORARY.\" This part of the wall is white and minimalistic, with no displayed artwork visible from this angle. [0:02:50 - 0:02:53]: The video shows a wide, well-lit gallery space with framed artworks hanging on a white partition wall. The artworks include black-and-white and color prints of various subjects. The camera angle tilts up slightly, showing the overhead lighting setup, which consists of several track lights providing bright, focused illumination on the displayed pieces. [0:02:54 - 0:02:56]: As the camera continues to move, more framed artworks on the partition wall come into view. These include a stylized portrait of a man in shades of gray, a boxing-themed piece with a magenta background, and a colorful, abstract print. Above, the blue \"ART UNIFIED\" sign remains visible. [0:02:57 - 0:02:59]: The final frames focus on the individual artworks. A center frame captures three prominent pieces: a divided monochromatic portrait, a portrait of a man standing shirtless with boxing gloves and a magenta background, and another portrait of a man in grayscale with a blue background. The floor of the gallery space and a few visitors in the background are faintly visible, indicating a busy exhibition environment.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What does the blue sign overhead read?",
        "time_stamp": "0:02:52",
        "answer": "A",
        "options": [
          "A. \"ART UNIFIED 1359, VENICE | CA. MODERN + CONTEMPORARY.\".",
          "B. \"ARTISTIC INSPIRATION 123, LA.\".",
          "C. \"MODERN ART SHOW 2024.\".",
          "D. \"VENICE ART EXHIBITION 2023.\"."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the black metallic sculptures positioned in the gallery?",
        "time_stamp": "0:02:47",
        "answer": "B",
        "options": [
          "A. On the floor.",
          "B. On white and blakc pedestals.",
          "C. On a wooden table.",
          "D. Hanging from the ceiling."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Object Recognition",
        "question": "What kind of artworks are displayed on the partition wall?",
        "time_stamp": "0:02:53",
        "answer": "B",
        "options": [
          "A. Sculptures.",
          "B. Framed prints.",
          "C. Digital screens.",
          "D. Posters."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_476_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:18",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_81_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:46",
        "answer": "A",
        "options": [
          "A. 6.",
          "B. 5.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_81_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:02",
        "answer": "C",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 6.",
          "D. 4."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_81_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:01",
        "answer": "B",
        "options": [
          "A. 2.",
          "B. 7.",
          "C. 6.",
          "D. 8."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      },
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:33",
        "answer": "D",
        "options": [
          "A. 5.",
          "B. 8.",
          "C. 6.",
          "D. 7."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_81_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What word is currently displayed in the bottom of the screen?",
        "time_stamp": "00:00:05",
        "answer": "D",
        "options": [
          "A. PORSCHE.",
          "B. TURBO.",
          "C. HYBRID.",
          "D. ONBOARD."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_263_real.mp4"
  },
  {
    "time": "[0:01:32 - 0:01:37]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What brand logo is currently visible on the bridge?",
        "time_stamp": "00:01:34",
        "answer": "D",
        "options": [
          "A. Michelin.",
          "B. Pirelli.",
          "C. Goodyear.",
          "D. Yokohama."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_263_real.mp4"
  },
  {
    "time": "[0:03:04 - 0:03:09]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the current track length displayed on the screen?",
        "time_stamp": "00:03:05",
        "answer": "D",
        "options": [
          "A. 13.567 km.",
          "B. 12.935 km.",
          "C. 18.752 km.",
          "D. 20.832 km."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_263_real.mp4"
  },
  {
    "time": "[0:04:36 - 0:04:41]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the current helmet of the driver?",
        "time_stamp": "00:04:36",
        "answer": "D",
        "options": [
          "A. Red.",
          "B. Blue.",
          "C. Yellow.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_263_real.mp4"
  },
  {
    "time": "[0:06:08 - 0:06:13]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What car model is visible in the information panel right now?",
        "time_stamp": "00:06:08",
        "answer": "D",
        "options": [
          "A. 911 Carrera.",
          "B. Taycan Turbo S.",
          "C. Macan S.",
          "D. Panamera Turbo S E-Hybrid."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_263_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker describe next?",
        "time_stamp": "00:00:30",
        "answer": "A",
        "options": [
          "A. Step-by-step process of long division.",
          "B. History of long division.",
          "C. Applications of long division in real life.",
          "D. Comparison between long division and short division."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_230_real.mp4"
  },
  {
    "time": "[0:02:19 - 0:02:49]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:03:53",
        "answer": "D",
        "options": [
          "A. How to find the remainder in a division problem.",
          "B. How to multiply the divisor by the quotient.",
          "C. How to perform subtraction in long division.",
          "D. How to bring down the next digit in a long division."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_230_real.mp4"
  },
  {
    "time": "[0:04:38 - 0:05:08]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:05:14",
        "answer": "C",
        "options": [
          "A. How to verify the result using multiplication.",
          "B. How to interpret the quotient and remainder.",
          "C. How to solve a similar problem with different numbers.",
          "D. How to check the division result by subtracting."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_230_real.mp4"
  },
  {
    "time": "[0:06:57 - 0:07:27]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What is the next step after dividing 24 by 3?",
        "time_stamp": "00:07:18",
        "answer": "C",
        "options": [
          "A. Write down 8 on the left side.",
          "B. Write down 3 on the right side.",
          "C. Subtract 12 from 24.",
          "D. Add the digits."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_230_real.mp4"
  },
  {
    "time": "[0:09:16 - 0:09:46]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:09:55",
        "answer": "C",
        "options": [
          "A. How to bring down the next digit.",
          "B. How to check the remainder.",
          "C. Subtract 20 from 20.",
          "D. How to estimate the quotient."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_230_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:16",
        "answer": "C",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 0.",
          "D. 4."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      },
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:56",
        "answer": "C",
        "options": [
          "A. 1.",
          "B. 3.",
          "C. 0.",
          "D. 4."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_100_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:01:26",
        "answer": "C",
        "options": [
          "A. 3.",
          "B. 4.",
          "C. 0.",
          "D. 1."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_100_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:26",
        "answer": "C",
        "options": [
          "A. 3.",
          "B. 4.",
          "C. 0.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_100_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:41",
        "answer": "C",
        "options": [
          "A. 2.",
          "B. 8.",
          "C. 0.",
          "D. 3."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_100_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:24",
        "answer": "C",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 0.",
          "D. 7."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_100_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:21]: The video starts with a split-screen frame showing two options for the same exercise. The left side is labeled \"SUBSTITUTION OPTION #1\" and the right side is labeled \"SUBSTITUTION OPTION #2.\" Both images show a person on a bench pressing weights. [0:03:21 - 0:03:26]: The next frames shift focus to a specific exercise titled \"Exercise 2 of 7: Larsen Press: 2 sets X 10 reps.\" The person is lying flat on a bench press, lifting a barbell with weights. Their feet are off the ground, legs extended, and arms fully extended above their chest. This exercise is performed in a gym with various other gym equipment in the background. [0:03:26 - 0:03:35]: The description \"Reduce weight to ~75% of top set weight\" appears on the screen. The person continues to lift the barbell above their chest with slight repetitions. The detailed calculation \"295 lbs x 75% = 225 lbs\" is shown on the screen with the numbers progressively appearing and increasing clarity before fading out. [0:03:36 - 0:03:38]: The person is lying on the bench, holding the barbell up in the same position as the previous frames. The gym equipment, mirrors, and weights are visible in the background, consistent with the setting throughout this part of the video.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What does the split-screen show right now?",
        "time_stamp": "0:03:21",
        "answer": "C",
        "options": [
          "A. Two different people performing the same exercise.",
          "B. A comparison between two types of exercise equipment.",
          "C. Two options for the same exercise.",
          "D. Different stages of the same workout."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      },
      {
        "task_type": "Event Understanding",
        "question": "What exercise is being performed in \"Exercise 2 of 7\"?",
        "time_stamp": "0:03:26",
        "answer": "C",
        "options": [
          "A. Deadlift.",
          "B. Bench Press.",
          "C. Larsen Press.",
          "D. Squat."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_143_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:43]: The video begins with a man in a gym performing a cable chest fly exercise. He holds the handles of a cable machine, extending his arms outward. He stands with a slight forward lean and wears a white T-shirt and shorts. The gym background includes various exercise equipment. [0:06:44 - 0:06:45]: The man continues the exercise, bringing one handle towards the center of his body from the left side, while maintaining his grip on the right handle. His facial expression appears focused as he concentrates on the movement. [0:06:46 - 0:06:48]: The man completes the pulling motion with his left hand and transitions back to the starting position. He adjusts his posture slightly and prepares for the next repetition. The background remains consistent with the previous frames. [0:06:49 - 0:06:52]: The man stands upright, taking a brief break from the exercise. He crosses his arms over his chest, possibly stretching or resting. The gym environment continues to display various machines and weights. [0:06:53 - 0:06:55]: The man continues to rest, with one hand placed on his chest while the other hand hangs by his side. He appears to be catching his breath before resuming the exercise. [0:06:56]: The scene transitions to a thumbnail image of a YouTube video titled \"Why You Should Consider Stretching Between Sets (Science Explained)\" by Jeff Nippard. The thumbnail includes images illustrating optimal muscle growth techniques. [0:06:57]: The YouTube thumbnail remains on screen, providing key points and visual examples to emphasize the importance of stretching between sets for muscle growth. [0:06:58]: The thumbnail continues to be displayed, focusing on the title and the visual examples related to optimal stretching techniques. [0:06:59]: The scene changes to an article with the title \"Interset Stretching vs. Traditional Strength Training: Effects on Muscle Strength and Size in Untrained Individuals.\" The article is accompanied by images depicting various stretching exercises performed during the experimental protocol.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What exercise is the man performing right now?",
        "time_stamp": "0:06:40",
        "answer": "B",
        "options": [
          "A. Bicep curls.",
          "B. Press around.",
          "C. Leg press.",
          "D. Shoulder press."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What is the man wearing during his exercise in the gym?",
        "time_stamp": "0:07:00",
        "answer": "B",
        "options": [
          "A. Black T-shirt and white shorts.",
          "B. White T-shirt and black shorts.",
          "C. Gray T-shirt and white pants.",
          "D. Blue T-shirt and pants."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_143_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:11:00]",
    "captions": "[0:10:00 - 0:10:20] [0:10:00 - 0:10:02]: The screen shows an introduction to exercises 6a and 6b, with instructions for \"Squeeze-Only Pressdown\" and \"Stretch-Only Overhead Extension\" being 3 sets of 8 reps.  [0:10:03 - 0:10:12]: The video transitions to a gym setting where a man in a white shirt and black shorts performs the \"Squeeze-Only Pressdown.\" He stands at a cable machine and repetitively pushes a handle attachment downwards, focusing on his triceps. The background reveals various gym equipment and a spacious exercise area. [0:10:13 - 0:10:18]: The man shifts to the \"Stretch-Only Overhead Extension.\" He positions himself differently, leaning forward with a cable handle behind his head, then straightens up to extend his arms forward. This motion is repeated several times, emphasizing the stretch in his triceps. [0:10:19]: The video ends with a different scene showing a bearded man in a black shirt demonstrating the cable exercise. He stands sideways, facing the camera, against a red and black backdrop. The caption at the bottom gives credit to an Instagram account for hypertrophy teaching.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action is the man in a white shirt performing right now?",
        "time_stamp": "00:10:06",
        "answer": "C",
        "options": [
          "A. Stretch-Only Overhead Extension.",
          "B. Bicep Curl.",
          "C. Squeeze-Only Pressdown.",
          "D. Leg Press."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_143_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The screen starts with the word \"in\" displayed in white text at the center of a black background. [0:00:01 - 0:00:02]: The word transitions to \"video,\" maintaining the same white text and black background. [0:00:02 - 0:00:03]: The video shifts to an image of Earth from space, showcasing various continents, oceans, and cloud patterns on a dark background. [0:00:03 - 0:00:04]: Textual content shifts as the word \"biggest,\" in white with a thick black outline, appears distinctively over the Earth image. [0:00:04 - 0:00:05]: An animated figure of a pixelated sheep appears over the Earth image, aligned with the right side of the screen. The word \"farm\" in white text with black outlining materializes underneath the sheep, while a smaller text at the bottom left reads \"for your comparison.\" [0:00:05 - 0:00:07]: The screen briefly transitions to a black background before a new frame appears. [0:00:07 - 0:00:08]: The scene shifts to a detailed Minecraft setting. A character in detailed purple armor stands on a wooden dock beside a body of water, conveying a first-person perspective. Tall grass, barrels, and a wooden house with lighting fixtures make up the surrounding environment. Text at the bottom reads \"So before we.\" [0:00:08 - 0:00:09]: The character continues to be situated in the same position without much movement. The text at the bottom now reads \"we’re going.\" [0:00:09 - 0:00:10]: The character remains largely in the same location, with the background continuing to showcase a serene lakeside and wooden structures. Text at the bottom reads \"to need to do some.\" [0:00:10 - 0:00:11]: Maintaining the same scenic environment, the character remains central in the frame with text at the bottom shifting to read \"resource gathering.\" [0:00:11 - 0:00:12]: As the scene advances, the viewpoint reorients slightly, focusing more on the surrounding water and land. The text changes to \"because right now.\" [0:00:12 - 0:00:13]: The view continues across the peaceful expanse of water with the text shifting to \"I have hardly anything.\" [0:00:13 - 0:00:14]: The on-screen perspective showcases a floating airship in the distance against the backdrop of the sky and water. The text updates to \"that I need.\" [0:00:14 - 0:00:15]: The view continues to feature the floating structure prominently over the water with text reading \"to actually build this.\" [0:00:15 - 0:00:16]: Transitioning to a more dynamic scene, the viewpoint appears to ascend towards the floating airship. The text changes to \"we are going.\" [0:00:16 - 0:00:17]: The perspective shifts further to an expansive aerial view with greenery below. The text at the bottom changes to \"by heading.\" [0:00:17 - 0:00:18]: The viewpoint continues to ascend swiftly, drawing closer to a structure in the sky with text that reads \"to our creeper farm.\" [0:00:18 - 0:00:19]: The video transitions with a closer view of a circular structure in the sky. The text changes to \"almost run.\" [0:00:19 - 0:00:20]: The final frame zooms in slightly on the structure, with an assortment of user-interface elements visible at the bottom of the screen, detailing items such as baked potatoes and fireworks.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the first word displayed at the start of the video?",
        "time_stamp": "0:00:20",
        "answer": "C",
        "options": [
          "A. Earth.",
          "B. Farm.",
          "C. In.",
          "D. Video."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_195_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:20 - 0:03:40] [0:00:00 - 0:00:01]: From a first-person perspective in a Minecraft game, the scene starts with an open grassy plain. The sky is dark, suggesting either early evening or approaching rain. The player's hand is visible holding a block. There are trees scattered around, with one large tree in the middle and a few more in the background along with some hills. [0:00:02 - 0:00:07]: The player looks slightly to the right while moving forward. The terrain is still a grassy plain, and a large tree is now closer, situated more towards the center. The player's hand is still visible in the field of view. [0:00:08]: The player continues to move forward, approaching a lone tree in the middle of the grassy plain. There is a river visible in the background to the right, flowing calmly. [0:00:09]: The player stops and opens the inventory, which displays various items such as an enchanted pickaxe, armor, and blocks of redstone. The inventory screen covers the view of the game world momentarily. [0:00:10]: The player closes the inventory. The lone tree and river are still in view, with more trees and hills in the distant background. The player's hand reappears holding a block. [0:00:11 - 0:00:14]: The player looks slightly left and forward at the tree, and the area is clearly visible with grassy terrain surrounding the immediate area and the river beyond the tree. [0:00:15]: The player looks up towards the sky, which appears to be entirely dark, potentially indicating nightfall or a storm. [0:00:16]: The player lowers their view back to level, looking straight at the tree. The grassy plain continues stretching out towards the horizon. [0:00:17 - 0:00:19]: The player looks slightly to the right, with the grass and river still in view. The sound of rain starts, as raindrops begin falling on the terrain. [0:00:20]: The player moves forward and opens an inventory menu again, showing items like shovels and picks, indicating a preparation for mining or construction. The inventory screen blocks the overall scene but briefly returns as the player resumes movement in the direction of the river.  [0:00:21]: The player starts moving towards the left, heading away from the tree towards the river which is now more visible. The rain continues to fall as the player progresses in the direction of what appears to be a cave entrance in the distance. [0:00:22]: The player moves downwards, transitioning into a cave. The scene changes to an enclosed, rocky area where the player begins mining, breaking blocks with a netherite pickaxe. [0:00:23 - 0:00:24]: The player continues to mine vigorously, breaking through iron ore blocks. Several pieces of iron ore are visible, indicating a productive mining session. [0:00:25]: The player turns slightly to the right while mining, revealing more blocks including redstone ore behind the broken iron ore. [0:00:26]: The player moves forward to mine the redstone ore with the netherite pickaxe, breaking its blocks efficiently. [0:00:27 - 0:00:28]: The player turns left within the cave, mining more blocks and uncovering lapis lazuli ore alongside the previous blocks. The netherite pickaxe continues to break through the various ores quickly.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What does the player open to display various items such as an enchanted pickaxe and armor?",
        "time_stamp": "00:03:09",
        "answer": "B",
        "options": [
          "A. A crafting table.",
          "B. An inventory.",
          "C. A chest.",
          "D. A furnace."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What is visible in the player's hand right now?",
        "time_stamp": "00:03:22",
        "answer": "B",
        "options": [
          "A. A sword.",
          "B. Nothing.",
          "C. A pickaxe.",
          "D. A torch."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_195_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:42]: The video starts with a first-person view in an outdoor setting, featuring a grassy field with acacia trees scattered around. The focus is on a gray sheep labeled with the numbers \"2\" and \"1\". In the lower part of the screen, an inventory bar displays various items, including tools and blocks. [0:06:43 - 00:06:44]: The camera angle shifts slightly to reveal a fenced area containing multiple animals, including sheep of various colors and cows. One tree stands prominently to the right, while the background features more scattered trees and a body of water to the right. [0:06:45]: The focus moves back to the gray sheep standing in front of the player. The player's hand, holding what appears to be wheat, is visible in the frame. [0:06:46]: The player seems to be guiding the gray sheep toward the fenced area. Another person or character is visible, interacting with the sheep. [0:06:47]: The perspective remains on the gray sheep, which now appears to be nudging against the player, prompting the animals to enter the pen. [0:06:48]: The sheep hop over the fence and enter the pen. The player’s hand continues to hold the wheat, guiding the sheep into their designated area. [0:06:49]: The player ensures the sheep are inside the pen properly, shifting the view to observe the various animals now contained within the fenced space. [0:06:50]: Camera shifts focus, capturing more of the environment, with several sheep visible following the player. [0:06:51]: The scene transitions, zooming out to display a wider view of the surroundings, with lush green terrain and more scattered trees. The text overlay in the frame reads \"stuck into a pen.\" [0:06:52 - 0:06:54]: The video showcases an elevated view of multiple small glass enclosures. A character in purple attire appears to be working with the enclosed sheep. The background features a river and distant forested areas. [0:06:55]: The player continues to interact with the sheep inside the glass enclosures, positioning them correctly. The fields and a river create a peaceful backdrop. [0:06:56]: The sheep are now situated inside individual compartments made of glass and blocks, arranged in a row. The player’s movements focus on organizing these enclosures efficiently. [0:06:57]: The text \"There we go\" appears on the screen as the player finalizes the organizing of sheep within the enclosures. [0:06:58]: The camera captures an overview of the enclosed sheep, with neatly arranged compartments visible. The player can be seen holding a different item from the inventory bar. [0:06:59 - 0:07:00]: The view shifts to display the row of enclosures in greater detail, each containing a different-colored sheep, indicating a completion of the setup. The surrounding scenery remains picturesque with greenery and water visible in the distance. [0:07:01 - 0:07:02]: The player's hand movements indicate final adjustments being made to the setup, ensuring each sheep is securely placed within its designated enclosure. [0:07:03]: The text \"pretty much complete\" appears on the screen while the player adjusts the last few enclosures, showcasing a tidy row of glass compartments secured within the player's construction. [0:07:04 - 0:07:05]: The player performs final checks to ensure all sheep are properly enclosed, with the camera panning slightly to display the completed setup against the natural landscape surrounding it. [0:07:06]: The video concludes with a final overhead shot of the enclosures, indicating a successfully completed project. The surrounding environment of trees, hills, and a body of water enhances the visual appeal of the finished setup.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What was notable about the gray sheep observed just now?",
        "time_stamp": "00:06:42",
        "answer": "B",
        "options": [
          "A. It has a unique color pattern.",
          "B. It is labeled with the numbers \"1\".",
          "C. It is larger than the other sheep.",
          "D. It is standing alone in the field."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_195_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:11:00]",
    "captions": "[0:10:00 - 0:10:20] [0:00:00 - 0:00:02]: The video opens with a view of a multi-level structure made of dirt blocks and wood. The structure has multiple rows of windows with some red objects inside. The camera is focused upward, showing the tall height of the structure. A hand at the bottom right corner indicates the person recording.  [0:00:03 - 0:00:04]: The view shifts downward, revealing a wooden framework at the bottom of the structure. A chest is suspended underneath the framework, held by hoppers. The hand moves toward the chest, indicating interaction. [0:00:05 - 0:00:06]: The chest is now open, displaying its contents. The chest is almost full, with numerous colorful blocks and items organized in rows. Several slots are occupied by stacks of different colors, including red, yellow, green, and blue blocks. The hand remains steady on the screen. [0:00:07 - 0:00:09]: The chest closes, and the view changes to an open grassy area. The background shows a clear sky with some distant hills and trees. The camera starts to move, indicating the person walking. [0:00:10]: A close-up of a spruce log is shown in the hand, revealing the texture and brown color. The background includes a grassy field with red flowers scattered around and a large hill in the distance. [0:00:11 - 0:00:12]: The camera moves toward a dirt block structure. The person begins to place the spruce log against the dirt block, preparing to build upward. The surrounding area includes more grass and distant trees. [0:00:13]: The view shifts upward. The person places another log as the camera rises, indicating upward movement. A nearby river and part of a wooden construction can be seen in the background. [0:00:14]: The camera continues to move up, showing a portion of the taller structure. The view includes several levels of hoppers and chests, connected by frameworks made of wood and dirt. The river and the treeline are visible below. [0:00:15]: The camera stops moving upward, focusing on a section of the structure with chests and hoppers. The person continues the construction, with the view showing the interconnection of the components. [0:00:16]: The camera angle shifts to show the person placing another block. There is a combination of wood and redstone within the frame, indicating complex machine parts. [0:00:17 - 0:00:18]: The view shows more detailed sections of the structure’s framework. The person aligns the next component, carefully positioning it within the system of hoppers, chests, and redstone components. [0:00:19 - 0:00:20]: The camera tilts downward, capturing the entire structure from a different angle. The person starts descending, making adjustments and checking the alignment of the components from a better perspective. The background shows the grassy terrain and distant hills below.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is visible in the background while the person builds upward?",
        "time_stamp": "00:10:14",
        "answer": "A",
        "options": [
          "A. The river and treeline.",
          "B. The grassy terrain and distant hills.",
          "C. The sky and a distant mountain.",
          "D. The forest and a lake."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_195_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What text is visible on the driver's helmet right now?",
        "time_stamp": "00:00:02",
        "answer": "C",
        "options": [
          "A. Red Bull.",
          "B. Monster Energy.",
          "C. Bell.",
          "D. Sparco."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_261_real.mp4"
  },
  {
    "time": "[0:03:24 - 0:03:29]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What brand logo is visible on the driver's sleeve right now?",
        "time_stamp": "00:03:24",
        "answer": "B",
        "options": [
          "A. Red Bull.",
          "B. Monster Energy.",
          "C. TT.",
          "D. Sparco."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_261_real.mp4"
  },
  {
    "time": "[0:06:48 - 0:06:53]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Which way does the road turn now?",
        "time_stamp": "00:06:25",
        "answer": "C",
        "options": [
          "A. there's no road at all.",
          "B. Straight forward.",
          "C. To the right.",
          "D. To the left."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_261_real.mp4"
  },
  {
    "time": "[0:10:12 - 0:10:17]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color are the illuminated gauges on the dashboard right now?",
        "time_stamp": "00:10:14",
        "answer": "A",
        "options": [
          "A. Blue and green.",
          "B. Red and blue.",
          "C. Green and yellow.",
          "D. White and red."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_261_real.mp4"
  },
  {
    "time": "[0:13:36 - 0:13:41]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color are the illuminated gauges on the dashboard right now?",
        "time_stamp": "00:13:39",
        "answer": "A",
        "options": [
          "A. Blue and green.",
          "B. Red and blue.",
          "C. Green and yellow.",
          "D. White and red."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_261_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the alphanumeric code visible on the helicopter's tail right now?",
        "time_stamp": "00:00:01",
        "answer": "C",
        "options": [
          "A. N939HS.",
          "B. N9398H.",
          "C. N939SH.",
          "D. N9399H."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_429_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:02:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the person on the right side of the picture wearing now?",
        "time_stamp": "00:02:02",
        "answer": "A",
        "options": [
          "A. Blue.",
          "B. Yellow.",
          "C. White.",
          "D. Black."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_429_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:04:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color are visible on the man's shirt right now?",
        "time_stamp": "00:04:15",
        "answer": "A",
        "options": [
          "A. Black.",
          "B. White.",
          "C. Red.",
          "D. Blue."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_429_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:06:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the predominant color of the landscape right now?",
        "time_stamp": "00:06:04",
        "answer": "A",
        "options": [
          "A. Green.",
          "B. Blue.",
          "C. Yellow.",
          "D. Brown."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_429_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:08:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the man's shirt right now?",
        "time_stamp": "00:08:02",
        "answer": "A",
        "options": [
          "A. Black.",
          "B. Blue.",
          "C. White.",
          "D. Red."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_429_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the main text on roadside sign right now?",
        "time_stamp": "00:00:07",
        "answer": "B",
        "options": [
          "A. Valentino.",
          "B. Coll d'Ordino, Andorra.",
          "C. Alpes, Spain.",
          "D. Pyrenees, France."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_294_real.mp4"
  },
  {
    "time": "[0:02:35 - 0:02:55]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the feature on the left side of the road right now?",
        "time_stamp": "00:02:35",
        "answer": "A",
        "options": [
          "A. A fence.",
          "B. A river.",
          "C. A mountain.",
          "D. A sidewalk."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_294_real.mp4"
  },
  {
    "time": "[0:05:10 - 0:05:30]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the cyclist currently riding relative to the road?",
        "time_stamp": "00:05:30",
        "answer": "C",
        "options": [
          "A. On the shoulder of the road.",
          "B. In the center of the road.",
          "C. On the left side of the road.",
          "D. In the right side of the road."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_294_real.mp4"
  },
  {
    "time": "[0:07:45 - 0:08:05]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the feature on the right side of the road right now?",
        "time_stamp": "00:07:46",
        "answer": "A",
        "options": [
          "A. A fence.",
          "B. A river.",
          "C. A mountain.",
          "D. A sidewalk."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_294_real.mp4"
  },
  {
    "time": "[0:10:20 - 0:10:40]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the position of the orange car right now?",
        "time_stamp": "00:10:32",
        "answer": "B",
        "options": [
          "A. Directly ahead.",
          "B. On the left side of the road.",
          "C. On the right side of the road.",
          "D. Directly behind."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_294_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why can thieves enter the room without being detected?",
        "time_stamp": "00:02:04",
        "answer": "B",
        "options": [
          "A. Because the security system is broken.",
          "B. Because the masters have all fallen asleep.",
          "C. Because the door was accidentally left unlocked.",
          "D. Because the thieves are using a special device to stay silent."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_235_real.mp4"
  },
  {
    "time": "[0:01:58 - 0:02:28]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does this cat glare viciously at the person in black?",
        "time_stamp": "00:02:23",
        "answer": "C",
        "options": [
          "A. Because the person in black took away the cat's favorite toy.",
          "B. Because the person in black is blocking the cat's path.",
          "C. Because the man in black just stepped on this cat.",
          "D. Because the person in black made a loud noise that startled the cat."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_235_real.mp4"
  },
  {
    "time": "[0:03:56 - 0:04:26]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the person start searching through his wardrobe?",
        "time_stamp": "00:04:07",
        "answer": "B",
        "options": [
          "A. Because he wants to clean his room.",
          "B. Because he is checking if he has lost anything.",
          "C. Because he is preparing to leave the house.",
          "D. Because he cannot find his wallet."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_235_real.mp4"
  },
  {
    "time": "[0:05:54 - 0:06:24]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the man offer his underwear to the woman as a replacement bag?",
        "time_stamp": "00:06:00",
        "answer": "C",
        "options": [
          "A. Because the woman forgot to bring her shopping bag.",
          "B. Because the man is trying to make the woman laugh with a joke.",
          "C. Because the lady's shopping bag is torn.",
          "D. Because the woman mentioned she needed something to carry her things in."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_235_real.mp4"
  },
  {
    "time": "[0:07:52 - 0:08:22]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the character hide behind structures on the roof?",
        "time_stamp": "00:08:10",
        "answer": "C",
        "options": [
          "A. Because he heard a loud noise and got scared.",
          "B. Because he noticed a helicopter approaching and wanted to stay hidden.",
          "C. Because the thief in black clothes spotted him and chased after him.",
          "D. Because he saw the roof starting to collapse and needed to find safety."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_235_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video begins with the back view of a muscular man walking in a gym. The gym is dimly lit, with various pieces of workout equipment such as treadmills and weight machines lining the sides. The man is wearing dark shorts and socks, and his upper body is shirtless. [0:00:01 - 0:00:02]: The scene transitions to a close-up of the same man, now wearing a gray t-shirt, lifting a heavy dumbbell in the gym. The background includes more gym equipment and a large banner with the text \"JEFF NIPPARD PRESENTS.\" [0:00:03]: The man is shown using a weight plate in a gym, lifting it horizontally. The gym is spacious with equipment and machines around. He is wearing a gray t-shirt and black shorts. [0:00:04]: The man takes off his gray t-shirt, revealing his bare torso. He is in the same gym, standing near some equipment, and the lighting remains same. [0:00:05]: A close-up of the man's hands shows him adjusting a weightlifting belt around his waist. The focus is on the hands and belt, with other gym-goers visible in the background. [0:00:06]: The man is now wearing a black t-shirt with the word \"RISE\" on it. He is focused and appears to be in the middle of a weightlifting set, with his eyes downcast. [0:00:07]: The man, now shirtless again, walks confidently through the gym which has rows of treadmills to his left and some weight machines to his right. He appears focused and in a brisk stride. [0:00:08]: Another close-up shot shows hands securing a weight plate onto a barbell. The man's hands are visible, wearing wrist straps for better grip and support. [0:00:09]: The man performs a pull-up on a bar. The camera captures this from behind, displaying his muscular and defined back. The gym's dark setting provides a strong contrast against his skin. [0:00:10]: A different person, wearing a hoodie, is shown stretching on a Smith machine in the gym, holding on to the equipment as they do a leg stretch. The gym background remains visible. [0:00:11 - 0:00:12]: Text on the screen reads \"THE ULTIMATE PUSH PULL LEGS SERIES,\" with a view of the man's back as he flexes his muscles. The setting continues to be the gym, with the background blurred. [0:00:13 - 0:00:16]: The scene transitions to an indoor environment where the man is now sitting at a desk with a laptop in front of him. A globe and some books are beside him. He is wearing a dark sweatshirt and appears to be talking to the camera, conveying information in a direct, engaging manner. [0:00:17 - 0:00:19]: The final segment shows the man back in the gym, this time wearing green shorts and removing a t-shirt over his head. He walks away from the camera toward the gym equipment, indicating that he is about to start or continue his workout.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action does the man perform after taking off his gray t-shirt?",
        "time_stamp": "0:00:05",
        "answer": "A",
        "options": [
          "A. Clap hands to remove the excess powder and adjusts a weightlifting belt.",
          "B. Does a pull-up.",
          "C. Puts on a black t-shirt.",
          "D. Walks through the gym."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_150_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:03]: The video starts with a close-up of a person's hands adjusting the weight on a gym machine by pulling out a pin and inserting it back into a higher weight. The machine's weight stack with the numbers clearly visible is in focus, while the background appears blurred. [0:03:04 - 0:03:10]: The scene transitions to a wider view of the gym. The person is seated on the machine, gripping a pull-down bar and beginning a lat pulldown exercise. The gym's surroundings include other exercise equipment and mirrors reflecting the scene. The person is wearing a dark T-shirt and green shorts. As the exercise continues, the person pulls the bar down towards their chest and then allows it to rise back up. [0:03:11 - 0:03:17]: The person continues performing lat pulldown repetitions. The surroundings are consistent, with various gym machines and mirrors. The person maintains steady control of the bar, pulling it down and letting it rise in a rhythmic pattern. [0:03:18 - 0:03:19]: The scene changes to the person taking a rest, sitting on a bench and adjusting a wrist strap. The gym background remains consistent, populated with various exercise equipment and other gym-goers in the far background. The person appears to be taking a short break before continuing their workout.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person adjusting right now?",
        "time_stamp": "00:03:02",
        "answer": "A",
        "options": [
          "A. The weight on a gym machine.",
          "B. A treadmill speed.",
          "C. A dumbbell rack.",
          "D. A stationary bike seat."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_150_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:03]: The video shows a person exercising in a gym. The scene is divided into two parts, with the left part showing close-up views of him performing vertical and horizontal pull exercises. In the vertical pull exercise (top left), the individual pulls down on a bar, his face showing exertion. In the horizontal pull exercise (bottom left), he pulls a handle towards his torso while seated. The right part of the video shows the person standing with his back to the camera in a flexing pose, highlighting the muscles of his back and arms. [0:06:04 - 0:06:13]: The person shifts focus to another exercise machine. He is performing a seated exercise that targets upper body muscles, involving pushing handles forward and then pulling them back towards his chest. His expression varies, indicating the effort exerted in each rep. He is wearing a dark t-shirt and shorts, and is seated with both feet firmly planted on the ground. The gym in the background is spacious with other equipment visible, though not in use. [0:06:14 - 0:06:19]: The individual continues exercising on a different machine, sitting on a padded seat and adjusting his posture. This segment captures him in various stages of starting the exercise, adjusting his hands, and settling into the correct position. The surroundings remain consistent, depicting a gym environment with multiple pieces of equipment around.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the individual doing right now after shifting focus to the new exercise machine?",
        "time_stamp": "0:06:13",
        "answer": "C",
        "options": [
          "A. Performing vertical pull exercises.",
          "B. Flexing his muscles.",
          "C. Performing a seated exercise targeting upper body muscles.",
          "D. Walking around the gym."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_150_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:04]: The video starts with three panels illustrating different exercises. The left panel shows a person performing a \"Low-to-High\" exercise, involving a diagonal pulling motion upwards. The middle panel demonstrates a \"Mid-to-Mid\" exercise, with the person maintaining a horizontal pulling motion. The right panel displays the \"High-to-Low\" exercise, which involves a downward pulling motion. The background features gym equipment, black benches, and weightlifting stations, all within a spacious, well-lit gym. [0:09:05 - 0:09:13]: The scene transitions to a view of a man in grey shorts and black shoes walking away from the camera. The gym is spacious, with various equipment and benches lined against the walls. The man lifts his hands to his head and then his hips, seemingly adjusting his shorts while continuing to walk forward. His back muscles are prominently visible as he walks. [0:09:14 - 0:09:17]: The man stops walking in the middle of the gym space. He strikes a bodybuilding pose by flexing his back muscles and raising both arms in a double biceps pose. The gym equipment, benches, and walls are consistently visible in the background. [0:09:18 - 0:09:19]: The video then cuts to a close-up of the man performing the \"Mid-to-Mid\" exercise. He stands beside gym equipment, holding a bar with both hands, pulling it towards his chest in a controlled motion. The background remains the gym environment, with the same equipment and flooring visible.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What type of exercise is the person performing in the left panel?",
        "time_stamp": "0:09:01",
        "answer": "C",
        "options": [
          "A. High-to-Low.",
          "B. Mid-to-Mid.",
          "C. Low-to-High.",
          "D. Low-to-Mid."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_150_real.mp4"
  },
  {
    "time": "[0:11:00 - 0:11:41]",
    "captions": "[0:11:40 - 0:11:41] [0:11:40 - 0:11:41]: The video is a first-person perspective of a man sitting at a desk. The scene is indoors, featuring a light gray wall with a potted plant on the left side of the background. His desktop includes a laptop in the center, an articulated microphone stand with a microphone on the right, and a small stack of books on the left. The top book seems related to \"strength training and conditioning.\" Next to the books is a globe with a modern aesthetic. The man, dressed in a dark sweater, appears to be focusing on the laptop screen while leaning slightly to his right. He has short hair and a trimmed beard. The room is well-lit, highlighting the objects on the desk and the man.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is positioned above the books on the man's desk?",
        "time_stamp": "00:11:41",
        "answer": "D",
        "options": [
          "A. A potted plant.",
          "B. A laptop.",
          "C. A microphone.",
          "D. A globe."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_150_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What is the best summary of the actions performed just now?",
        "time_stamp": "0:00:10",
        "answer": "C",
        "options": [
          "A. The individual retrieved a bun, added ketchup, and served it to the customer.",
          "B. The individual prepared a sandwich, added lettuce and turkey, and placed it on a tray.",
          "C. The individual assembled a cheeseburger by retrieving cheese slices and placing them on buns.",
          "D. The individual took a hot dog, added condiments, and wrapped it before serving."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_346_real.mp4"
  },
  {
    "time": "[0:00:57 - 0:01:07]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions performed just now?",
        "time_stamp": "0:01:07",
        "answer": "D",
        "options": [
          "A. The individual retrieved a food tray, added vegetables, and handed it to a colleague.",
          "B. The individual assembled a cheeseburger, prepared the packaging, and wrapped it.",
          "C. The individual prepared a salad, selected dressing, and presented it for immediate delivery.",
          "D. The individual checked order tickets, selected package."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_346_real.mp4"
  },
  {
    "time": "[0:01:54 - 0:02:04]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions performed just now?",
        "time_stamp": "0:02:04",
        "answer": "D",
        "options": [
          "A. The individual prepared a sandwich by taking out bread, adding cheese, and toasting it.",
          "B. The individual organized order tickets, prepared packaging materials, and assembled them for food items.",
          "C. The individual operated the deep fryer to cook fries and then placed them into containers.",
          "D. The individual checked order tickets, prepared the packaging, and organized prepared items."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_346_real.mp4"
  },
  {
    "time": "[0:02:51 - 0:03:01]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions performed just now?",
        "time_stamp": "0:03:01",
        "answer": "B",
        "options": [
          "A. The individual retrieved a bun, added condiments, and wrapped it for serving.",
          "B. The individual assembled a cheeseburger, placing cheese slices.",
          "C. The individual cooked burger patties, added toppings, and handed them to a colleague.",
          "D. The individual prepared the packaging boxes, placed burger patties in them, and added condiments."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_346_real.mp4"
  },
  {
    "time": "[0:03:48 - 0:03:58]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions performed just now?",
        "time_stamp": "0:03:58",
        "answer": "C",
        "options": [
          "A. The individual took a food tray, assembled a hot dog, and handed it to a customer.",
          "B. The individual retrieved burger buns, assembled cheeseburgers, and moved them to the packaging area.",
          "C. The individual collected order tickets, retrieved package, and organized them for next turn.",
          "D. The individual took the order tickets, prepared beverage cups, and filled them with various drinks."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_346_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The screen initially displays a black background. [0:00:01 - 0:00:02]: A person with a distressed expression raises their left hand to their head, with the word \"Forget\" written below them in large white letters. The person wears a white shirt and stands against a plain white background. [0:00:02 - 0:00:03]: The screen transitions to a black background with the word \"ever\" written in small white letters. [0:00:03 - 0:00:04]: The screen then displays the word \"So\" in the center, with a black background and white lettering. [0:00:04 - 0:00:05]: The word \"why\" in small white letters is displayed on a black background. [0:00:05 - 0:00:06]: The word \"HUGE\" appears in the center of a black background in capital white letters. [0:00:06 - 0:00:07]: The word \"it\" in small white letters is displayed on a black background. [0:00:07 - 0:00:09]: The scene shifts to a character in blue armor standing on a wooden platform in a pixelated environment resembling a video game. This character has a distinctive, wide, open-mouthed smile and large white eyes with black pupils. [0:00:09 - 0:00:14]: The camera pans out revealing a detailed setting with lush green trees, a wooden house with barrels and chests, and a flowing river. The character remains centered, facing the viewer, as the background becomes more visible, showcasing a mountainous terrain and blue sky. The character has a menu bar displayed at the bottom of the screen showing various items like tools and health bars, suggesting this is a gameplay recording from a first-person perspective. [0:00:14 - 0:00:16]: The character begins to speak, saying, \"which means,\" while gesturing slightly. The background continues to reveal more details of the environment, such as the wooden beams and stairs leading up to the house. [0:00:16 - 0:00:18]: As the character continues speaking, the words \"we're gonna need\" appear in large white letters at the bottom of the screen. The trees and wooden structures remain prominently in focus. [0:00:18 - 0:00:19]: The character's dialogue continues with the words \"enderpearls\" displayed in white letters on the screen, emphasizing their significance. The camera remains focused on the character standing on the platform. [0:00:19 - 0:00:20]: The character continues speaking, saying, \"and also...\" while the landscape and wooden barrels are still visible in the background. [0:00:20]: The character glances to the side slightly, appearing to ponder something. The setting continues to be the wooden platform surrounded by the greenery and water.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color armor is the character wearing on the wooden platform?",
        "time_stamp": "0:00:29",
        "answer": "C",
        "options": [
          "A. Red.",
          "B. Green.",
          "C. Blue.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Text-Rich Understanding",
        "question": "What does the character mention he need?",
        "time_stamp": "0:00:19",
        "answer": "C",
        "options": [
          "A. Health potions.",
          "B. Tools.",
          "C. Enderpearls.",
          "D. Weapons."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_193_real.mp4"
  },
  {
    "time": "[0:12:00 - 0:13:00]",
    "captions": "[0:12:00 - 0:12:20] [0:12:00 - 0:12:01]: The video begins with a view of a portal frame constructed of stone bricks, embedded with Ender Eyes, surrounding a dark, shimmering portal surface. The location appears to be a stronghold room with stone brick walls and barred windows. In the foreground, a shield with a brown and white pattern is equipped in the left hand, while the right hand holds a purple sword. The inventory bar at the bottom shows various items, including golden apples, potions, and arrows. [0:12:01 - 0:12:02]: The view is centered on the portal, with the eyes and dark surface being the focal point. The perspective moves slightly, creating a sense of imminent action. [0:12:02 - 0:12:03]: The scene transitions to another dimension, known as \"The End,\" characterized by greenish-yellow bricks and an ominous atmosphere. An announcement, \"Advancement Made! The End,\" appears in the top right corner. The character now holds a diamond pickaxe. [0:12:03 - 0:12:04]: The view focuses on a wall of End Stone, with a health bar for the Ender Dragon displayed at the top. The diamond pickaxe is held steadily, preparing to mine. [0:12:04 - 0:12:05]: The perspective shifts slightly downwards, showing the inventory screen. The player's avatar is in full diamond armor with enchanted items. Various items are visible in the inventory, including blocks, potions, and golden apples. [0:12:05 - 0:12:06]: The inventory is briefly inspected, focusing on several slots containing building materials and key resources. The selection hovers over different items quickly, preparing for the imminent task. [0:12:06 - 0:12:07]: With the diamond pickaxe equipped, the player begins mining through the End Stone wall. The perspective narrows as the pickaxe strikes the stone, creating a small passage. [0:12:07 - 0:12:08]: The mining process continues, revealing more of the passage. The sound of breaking blocks underscores the ongoing effort to tunnel through the wall. [0:12:08 - 0:12:09]: The player emerges from the tunnel into a vast, open area of The End. Large obsidian pillars dominate the landscape, each topped with what appears to be a glowing crystal. The Ender Dragon's health bar is still visible at the top. [0:12:09 - 0:12:10]: The landscape is populated with tall, dark Enderman creatures, standing and moving among the various obelisks. The scene radiates a sense of foreboding with the mystical, dark backdrop of The End. [0:12:10 - 0:12:11]: The view shifts towards the right as more of the area and Endermen become visible. The distinctive black towers and dark sky create an eerie environment. [0:12:11 - 0:12:12]: The player character continues to survey the surroundings, focusing on the massive obsidian pillars. Some of the Endermen begin to move towards the player, creating a potential threat. [0:12:12 - 0:12:13]: The camera angle shifts upwards towards the top of one of the towering obelisks, revealing an End Crystal perched at its peak. This crystal emits a faint glow, signifying its importance. [0:12:13 - 0:12:14]: The view narrows further on the End Crystal, preparing to aim and engage. The bow is drawn, ready to fire at the crystal to remove its regenerative influence on the Ender Dragon. [0:12:14 - 0:12:15]: A shot is fired from the bow, targeting the End Crystal atop one of the pillars. The arrow rapidly ascends towards the target. [0:12:15 - 0:12:16]: The arrow strikes the End Crystal, resulting in a brief but intense explosion that dissipates its power. The Ender Dragon's health bar remains partially depleted, indicating progress in the confrontation. [0:12:16 - 0:12:17]: The player continues to aim and fire arrows at the remaining End Crystals atop other obelisks. The arrows are launched swiftly and efficiently, each aiming to neutralize the crystals. [0:12:17 - 0:12:18]: Another End Crystal is hit, causing another explosion. The surrounding environment is momentarily illuminated by the blast, offering a brief glimpse of the towers and the dark sky. [0:12:18 - 0:12:19]: The player adjusts their aim, focusing on the next target with determination. The process of aiming, firing, and destroying continues",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the player holding in the right hand right now?",
        "time_stamp": "0:12:01",
        "answer": "A",
        "options": [
          "A. A purple sword.",
          "B. A diamond pickaxe.",
          "C. A golden apple.",
          "D. A shield."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Event Understanding",
        "question": "What happens when the player shoots the arrow at the pillar?",
        "time_stamp": "0:12:20",
        "answer": "A",
        "options": [
          "A. It causes an explosion.",
          "B. It regenerates the Ender Dragon.",
          "C. It spawns more Endermen.",
          "D. It creates a new portal."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_193_real.mp4"
  },
  {
    "time": "[0:18:00 - 0:19:00]",
    "captions": "[0:18:00 - 0:18:20] [0:18:00 - 0:18:07]: The scene opens with a view of a first-person perspective in a Minecraft game. The player is holding a stone block in their right hand. Their hotbar shows various items including tools, food, and blocks. In front of them is a partially constructed stone structure with a flat rooftop and cobblestone paths. The sky is clear with some scattered cloud patterns. In the background, a lush green hill with trees and a few flying creatures can be seen. The player shifts perspective slightly, walking along the cobblestone path surrounding the structure. [0:18:08]: The player switches from holding a stone block to a firework rocket. The player's view shifts towards a mountainous landscape with snow-capped peaks in the background and dense green forest in the foreground. The player seems to prepare for an action involving the firework rocket. [0:18:09 - 0:18:13]: The player launches the firework rocket and takes off into the sky, revealing an aerial view of the Minecraft world. The terrain includes a variety of elevations, with steep cliffs, flowing rivers, and green valleys. A prominent, large, incomplete stone arch structure is visible, surrounded by dense foliage. The view further expands to show different biomes including a jungle, plains, and savannah in the distance. [0:18:14 - 0:18:19]: The player continues to maneuver in the air, observing the tall stone structure with a large arched window. The structure has a simple design with straight edges and a large opening, and stands on a grassy hill surrounded by trees and bushes. The player appears to hover in place, examining the upper part of the structure. The clear blue sky with scattered clouds frames the background as the large arched window remains the center of focus.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the player holding in the right hand right now?",
        "time_stamp": "00:18:07",
        "answer": "B",
        "options": [
          "A. A firework rocket.",
          "B. A stone block.",
          "C. A wooden plank.",
          "D. A shovel."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_193_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the name appearing on the aircraft?",
        "time_stamp": "00:00:05",
        "answer": "D",
        "options": [
          "A. Rand.",
          "B. Rant.",
          "C. Ren.",
          "D. Raná."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_428_real.mp4"
  },
  {
    "time": "[0:01:20 - 0:01:25]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the primary object right now attached in front of the yellow aircraft?",
        "time_stamp": "00:01:23",
        "answer": "A",
        "options": [
          "A. A rope.",
          "B. A radar.",
          "C. A ladder.",
          "D. A parachute."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_428_real.mp4"
  },
  {
    "time": "[0:02:40 - 0:02:45]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the primary color of the aircraft nose right now?",
        "time_stamp": "00:02:42",
        "answer": "D",
        "options": [
          "A. Blue.",
          "B. Red.",
          "C. White.",
          "D. Brown."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_428_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:04:05]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What physical structure is visible in the background right now?",
        "time_stamp": "00:04:00",
        "answer": "A",
        "options": [
          "A. A road.",
          "B. A lake.",
          "C. A bridge.",
          "D. A factory."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_428_real.mp4"
  },
  {
    "time": "[0:05:20 - 0:05:25]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person in the aircraft wearing on his head right now?",
        "time_stamp": "00:05:22",
        "answer": "A",
        "options": [
          "A. A leather aviator hat.",
          "B. A helmet.",
          "C. A beanie.",
          "D. A cap."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_428_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:05]: The video starts in an open recreational area on a sunny day. There are various people walking in different directions. Three individuals are prominently visible walking away from the camera. One person wears a beige jacket and another person, dressed in black, has a white item draped across their shoulders. The third person wears a gray hoodie and carries a black backpack. The background features tall palm trees, some leafless trees, and various structures like pergolas shading outdoor seating areas. The seating area under beige umbrellas is to the right. [0:00:06]: The view shifts slightly, and now some storefronts and additional seating areas come into view on the right. People are seen sitting and interacting near the stores. The scene retains the casual, sunny setting with clear skies and palm trees in the background. [0:00:07 - 0:00:08]: Moving ahead, the pathway slightly bends to the left. On the left side, there's a long black construction barrier with advertisements. On the right, there's a storefront with people standing and sitting nearby. In the distance, colorful balloons are seen in front of a pointed, castle-like structure. Still clear and sunny, the sky dominates the upper part with wispy clouds. [0:00:09 - 0:00:11]: As the pathway continues, more of the colorful balloons become visible along with more details of the castle-like structure adorned with intricate architectural elements. The left side continues to display the black construction barrier with the Fast & Furious advertisement.  [0:00:12 - 0:00:14]: This section focuses more on the advertisement on the left barrier, displaying the Fast & Furious logo with a car image and smoky effects. A crane is visible overhead, possibly indicating ongoing construction. Meanwhile, on the right, the colorful balloons and part of the crowd are still visible. [0:00:15 - 0:00:20]: The final segment continues along the pathway. The advertisement on the barrier remains prominent on the left, displaying the car and Fast & Furious logo more prominently. The view overhead shows a crane hanging into the scene. The right side showcases the vibrant balloon display fully in front of the castle-like building, with people moving and engaging with the area. A few people, including a couple carrying bags, are visible towards the backend, maintaining the bustling and lively setting of the location.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are people doing near the storefronts?",
        "time_stamp": "0:00:07",
        "answer": "D",
        "options": [
          "A. Running.",
          "B. Playing with balloons.",
          "C. Climbing the structure.",
          "D. Standing and sitting."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_310_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:43]: A cobblestone street is visually filled with various activities. On the right, a bright red Butterbeer cart stands out with two wooden barrels beside it. People mill around the cart, some lining up while others are either walking away or standing and conversing. The grey-stone buildings with snow on the rooftops flank the street. The architecture of the buildings resembles a medieval style, with pointed, triangular roofs and chimneys ascending into the sky. A group of people, including a person in a yellow cap and others pushing a stroller, adds life to the scene. [0:02:44 - 0:02:46]: The frame shifts slightly to the right, revealing a beige umbrella shading a portion of the scene. Underneath, two people are interacting near the Butterbeer cart, while another person waits nearby, holding a bright yellow item. The street continues to bustle with visitors, some walking along, others stopping to look around. Onlookers and customers maintain a lively atmosphere. [0:02:47 - 0:02:49]: The viewpoint continues down the street, focusing more on the general crowd. The number of people increases, many walking towards the camera while others are gathered in small groups. The backdrop of fairytale-like buildings continues to frame the scene, with the sky remaining partly cloudy but generally clear. The architecture of the buildings on both sides of the street creates a tunnel-like effect. [0:02:50 - 0:02:54]: More of the same cobblestone street is visible as the camera angle shifts, showing the busy environment filled with tourists. Buildings on the right are more prominent, revealing details of their constructed stonework and snow-capped roofs. People lean against a stone wall on the left, enjoying the scene. A few people with strollers and others chatting create a dynamic and bustling atmosphere. [0:02:55 - 0:02:59]: The camera continues to move forward, gradually focusing on a large snow-capped building with a wooden entrance arch at the end of the street. People in various attire, including casual clothing and hats, are scattered throughout the frame, adding to the busy, vibrant feel of the location. The building's entrance is an apparent focal point as more visitors gather around it, likely indicating its importance or attractiveness.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the architectural style of the buildings along the street?",
        "time_stamp": "00:02:49",
        "answer": "D",
        "options": [
          "A. Victorian.",
          "B. Modern.",
          "C. Gothic.",
          "D. Medieval."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_310_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:21]: The scene opens at an outdoor setting in front of a shop labeled \"OWL POST.\" The shop features a vintage-looking glass window and a partially open wooden door with glass panels. A crowd is visible in the background, including various people walking and interacting. The architecture suggests a quaint, possibly medieval theme, with stone buildings visible. [0:05:22 - 0:05:23]: The camera starts to move towards the slightly open door of the \"OWL POST\" shop. The crowd remains visible, though somewhat blurred as the focus shifts towards the interior of the shop. [0:05:24 - 0:05:25]: Entering the shop, the camera reveals wooden flooring and shelves lined with various items. The interior is warmly lit, and the shop's décor includes wooden beams and vintage-style lighting fixtures. The shelves hold stuffed animals resembling owls and other decorative items. [0:05:26 - 0:05:30]: As the camera progresses further inside, more details of the shop become visible. A large wooden counter is on the right, behind which are more shelves stacked with parcels and boxes. Some shelves hold a variety of owls and other themed merchandise. The lighting creates a cozy ambiance, highlighting the shop’s detailed, rustic design. [0:05:31 - 0:05:33]: The camera pans upward, focusing on the shelves above. These shelves are filled with owl figurines and other items, meticulously arranged. The high wooden beams and ceiling lights contribute to the warm and inviting atmosphere of the shop. [0:05:34 - 0:05:36]: The camera lowers back down and shifts focus to the adjacent room. The camera passes through a doorway with green-painted trim, leading to a section of the shop displaying clothing. Sweaters with initials \"H\" and \"G\" are visible, neatly arranged on wooden shelves. [0:05:37 - 0:05:38]: Entering the clothing section, the camera captures more details of the displayed items. The warm lighting continues to enhance the rich wooden textures of the shelves and walls. Two customers are seen walking deeper into the shop. [0:05:39 - 0:05:40]: The video concludes with a focused view of the clothing display. Sweaters with different initials are neatly arranged, emphasizing the shop’s theme and merchandise variety. The scene ends with the camera facing the interior further into the shop, capturing the cozy and thematic appeal of the \"OWL POST.\"",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the shop labeled in the outdoor setting?",
        "time_stamp": "0:05:21",
        "answer": "D",
        "options": [
          "A. OWL NEST.",
          "B. OWL SHOP.",
          "C. OWL HOUSE.",
          "D. OWL POST."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What initials are visible on the sweaters displayed in the clothing section now?",
        "time_stamp": "0:05:35",
        "answer": "D",
        "options": [
          "A. A and B.",
          "B. M and N.",
          "C. L and P.",
          "D. H and G."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_310_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: The video begins with a view of a room with blue walls and ceilings decorated with stars. Several items of clothing, possibly uniforms or costumes, are displayed on hangers and shelves on the right. In the center, there is a lit showcase containing various items. Two people can be seen walking away in the background, near a lit window. [0:08:01 - 0:08:02]: The scene shifts to another room with shelves filled with many colorfully stacked small rectangular boxes. The shelves are divided into various sections, each with differently colored boxes, including orange, blue, purple, and silver. The flooring is wooden, and a metal staircase is visible on the right side. [0:08:02 - 0:08:06]: The camera moves slightly forward, focusing more on the shelves filled with colorful boxes. The detailed arrangement of the boxes is visible, with a noticeable pattern in their stacking. The background remains consistent with different shades of boxes, and some decorative elements can be seen on the wall. [0:08:06 - 0:08:09]: The movement continues, highlighting the shelves on both sides filled with boxes. The lighting in the room is warm, with multiple light sources visible on the ceiling. The right side of the room features a painting or decorative panel beside another set of shelves. [0:08:09 - 0:08:15]: The shot transitions to a closer view of the boxes on the right side, showcasing the intricate details and arrangements of the boxes. The colors of the boxes range widely, including shades of grey, pink, blue, and gold, arranged in a somewhat orderly fashion on the shelves.  [0:08:15 - 0:08:16]: The perspective continues to shift slightly, capturing more details of the boxes and the room layout. The top shelves contain longer, flatter boxes, while the lower shelves hold smaller, cubical ones. [0:08:16 - 0:08:19]: As the video progresses, focus narrows on the right side of the shelves, showing the assorted boxes in their patterned arrangement. The shelves' precise organization remains visible, and additional items, like a stack of books, are seen on the lower right shelf. [0:08:19]: The final frame shows the camera angle slightly shifted similar shelves filled with neatly arranged boxes on both sides. The background maintains the wooden flooring and greenish wall hues, consistent with the previous frames.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What colors are visible on the boxes stacked on the shelves right now?",
        "time_stamp": "0:08:12",
        "answer": "D",
        "options": [
          "A. Green, yellow, black, and red.",
          "B. White, purple, orange, and silver.",
          "C. Red, blue, green, and white.",
          "D. Grey, pink, blue black, and gold."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_310_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "How can the actions just observed in the recent video segment be best summarized?",
        "time_stamp": "0:00:10",
        "answer": "B",
        "options": [
          "A. A shopper selected and evaluated various fruit snacks before placing them in their basket, focusing on price comparison for different brands.",
          "B. A worker stocked fruit cups on supermarket shelves, arranging boxes and checking labels to ensure proper placement.",
          "C. A chef prepared a detailed shopping list, meticulously inspecting each item before selecting fruits and vegetables for a gourmet recipe.",
          "D. A child picked out colorful fruit snacks from a wide selection, expressing excitement over new flavors and brands."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_414_real.mp4"
  },
  {
    "time": "[0:02:19 - 0:02:29]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:02:29",
        "answer": "A",
        "options": [
          "A. An individual restocks shelves with canned fruits while organizing other products.",
          "B. An individual maneuvers through a crowded farmers market, selecting fresh produce.",
          "C. An individual stacks canned soups on shelves while arranging sales tags.",
          "D. An individual organizes breakfast cereals on the shelves, ensuring everything is properly placed."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_414_real.mp4"
  },
  {
    "time": "[0:04:38 - 0:04:48]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following summaries best describes the recent actions carried out in the footage?",
        "time_stamp": "00:04:48",
        "answer": "D",
        "options": [
          "A. The individual browsed through the cereal aisle, examined various boxes, and placed one in their cart.",
          "B. The individual navigated through the chocolate syrup section, compared brands, and added a specific syrup to their basket.",
          "C. The individual walked through the tea aisle, checked prices, and picked up a box of herbal tea.",
          "D. The individual strolled through the cereal aisle, checked prices, and appeared to be restocking shelves."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_414_real.mp4"
  },
  {
    "time": "[0:06:57 - 0:07:07]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the action that just now took place?",
        "time_stamp": "00:07:07",
        "answer": "C",
        "options": [
          "A. An individual sorted through various condiments, selecting and organizing jars of relish on the shelf.",
          "B. An individual restocked ketchup bottles, ensuring they were properly aligned and accessible for customers.",
          "C. An individual unpacked and restocked mayonnaise jars on the shelves, organizing them neatly in a supermarket aisle.",
          "D. An individual evaluated different brands of jam, comparing prices and features before making a purchase."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_414_real.mp4"
  },
  {
    "time": "[0:09:16 - 0:09:26]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:09:26",
        "answer": "C",
        "options": [
          "A. An individual browses through a section of fresh produce, selecting and bagging different vegetables and fruits.",
          "B. An individual stocks up on cleaning supplies, checking labels for ingredients, and placing several items in a cart.",
          "C. An individual browses through the salad dressing aisle, picks up and examines bottles, places one in a box, and continues down the aisle.",
          "D. An individual navigates through a hardware store, comparing different brands of paint and supplies before selecting and purchasing a few items."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_414_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "Which coffee shop logo is visible in the background right now?",
        "time_stamp": "00:00:03",
        "answer": "D",
        "options": [
          "A. Dunkin'.",
          "B. Costa Coffee.",
          "C. Tim Hortons.",
          "D. Starbucks."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_392_real.mp4"
  },
  {
    "time": "[0:02:11 - 0:02:16]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What kind of structure is visible in the distance right now?",
        "time_stamp": "00:02:15",
        "answer": "D",
        "options": [
          "A. A tunnel.",
          "B. A skyscraper.",
          "C. A park.",
          "D. A bridge."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_392_real.mp4"
  },
  {
    "time": "[0:04:22 - 0:04:27]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What type of clothing are most people wearing right now?",
        "time_stamp": "00:04:25",
        "answer": "D",
        "options": [
          "A. Heavy coats.",
          "B. Business suits.",
          "C. Rain jackets.",
          "D. Casual summer outfits."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_392_real.mp4"
  },
  {
    "time": "[0:06:33 - 0:06:38]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What brand is the black SUV driving in front right now?",
        "time_stamp": "00:06:36",
        "answer": "D",
        "options": [
          "A. Toyota.",
          "B. Ford.",
          "C. BMW.",
          "D. Benz."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_392_real.mp4"
  },
  {
    "time": "[0:08:44 - 0:08:49]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which transportation service's bus is visible right now?",
        "time_stamp": "00:08:45",
        "answer": "B",
        "options": [
          "A. Greyhound.",
          "B. Flixbus.",
          "C. Megabus.",
          "D. BoltBus."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_392_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video begins with a black screen. [0:00:01]: Abstract and colorful background with horizontal lines and spots. [0:00:02 - 0:00:07]: A logo of \"Gravity Throttle Racing\" appears on a yellow oval shape with red and blue accents. The background remains abstract with a mix of dark and purple hues. [0:00:08 - 0:00:10]: Scene transitions to a miniature landscape of a rocky mountain, small trees, and a tunnel. A red model car is nearby the train track with a train approaching from the tunnel. [0:00:11 - 0:00:12]: The train, a black steam locomotive with the number 765, moves out of the tunnel, creating steam. [0:00:13]: The steam locomotive continues moving with the number 765 clearly visible. [0:00:14 - 0:00:15]: A black train car with the \"Gravity Throttle Racing\" logo and the text \"ARE YOU TRACKIN'?\" visible. [0:00:16]: Followed by a red and white boxcar with \"SOO LINE\" and graffiti art on it. [0:00:17]: A flatcar with minimal details. [0:00:18 - 0:00:19]: Three miniature figurines in white uniforms and black belts are standing in front of a miniature gas station named \"ESSOCO\".",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What happens right after the train moves out of the tunnel?",
        "time_stamp": "0:00:15",
        "answer": "D",
        "options": [
          "A. The train stops.",
          "B. The logo \"Gravity Throttle Racing\" appears.",
          "C. A red and white boxcar appears.",
          "D. The steam locomotive number 765 becomes visible."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_499_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:26]: The video shows a leaderboard displayed against a background of a raceway, labeled \"SHAVANO MOUNTAIN RACEWAY.\" There are firework animations in the corners. Three car images are displayed with names and times: BLR (22.350s), Flip (23.030s), and Jonabi (23.093s). The term \"FAST TRACK!\" appears on the left side in an angled orientation. The large desert-like background displays winding roads and sparse vegetation. [0:02:27 - 0:02:29]: The scene shifts to a different view featuring a starting gate on a snowy track indoors. Five toy cars are arranged at the starting line, colored red, black, blue, gray, and another gray car. A small green toy vehicle and some obstacles are positioned to the left side in the background. [0:02:30 - 0:02:32]: The cars remain at the starting line under the race signal. The white and snowy setup continues with the track. The display on the left shows \"RACE 2\" with contenders' names: Fat Dad, Numbskull, Iceman, Champagne Papi. [0:02:33 - 0:02:35]: The race signal changes, lighting up the track. The toy cars start moving forward. The setup is the same as before in this detailed indoor snowy race track. [0:02:36 - 0:02:39]: The cars move swiftly down the icy path. The blue car leads, followed by the black, red, and then the two gray cars. The track has curves, banks, and snowy mounds to navigate. The camera shows different angles, capturing the intensity and speed of the competitors on the winding track. Various elements such as a small green object and snow-covered trees are seen along the sides of the track.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which car has the fastest time displayed on the leaderboard right now?",
        "time_stamp": "0:02:26",
        "answer": "B",
        "options": [
          "A. Flip.",
          "B. BLR.",
          "C. Jonabi.",
          "D. Fast Track."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which of the following names is not listed as a contender in \"RACE 2\"?",
        "time_stamp": "0:02:32",
        "answer": "D",
        "options": [
          "A. Fat Dad.",
          "B. Numbskull.",
          "C. Iceman.",
          "D. Flip."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_499_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:46]: The video starts with a first-person perspective overlooking a modeled racetrack. The track is elevated, with a gray concrete path bordered by grassy terrain. There are small bushes and some rocks along the sides. A timer display on the left side shows a blue screen with white text and a red section reading “ARE YOU TRACKIN’?”. Two miniature cars are visible on the track, positioned near the right side of the frame, moving down the slope. [0:04:47 - 0:04:53]: A graphical overlay appears on the screen, showing racing standings for \"Ultimate EVO-Stage 2.\" This overlay details the teams, drivers, and their scores across several rounds. The overlay remains for several seconds, obscuring part of the track and the timer on the left. [0:04:54 - 0:04:59]: The perspective shifts to a different section of the track, now in a snowy environment. Four small, race-ready cars are lined up at the start line, ready to race. The background is a snowy landscape with a blue wall and some terrain features. The text “RACE 4” appears in the top left corner. The names of the racers are listed below: “Iceman, Champagne Papi, Fat Dad, Numbskull.” The rest is occupied by the white, snow-like terrain.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Which car won first place in this competition?",
        "time_stamp": "0:04:51",
        "answer": "C",
        "options": [
          "A. The blue car.",
          "B. The dark purple car.",
          "C. The white car with red decorations on it.",
          "D. The red car."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_499_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:01]: The scene features what appears to be a model landscape, depicting a dry, desert-like area with a large rock formation in the foreground and mountains in the background. Several model vehicles, including cars and a train, are positioned on a road and train tracks surrounding the rock formation. The sky above is partly cloudy with blue skies visible. [0:07:01 - 0:07:04]: A close-up view of a single-lane road is shown, bordered by mossy and sandy terrain. A small blue car is seen driving along this road, and a black structure resembling a ladder is present to the right side of the road.  [0:07:04 - 0:07:04]: The same road is visible, but now a red car is driving along it, moving quickly from left to right. The black ladder-like structure remains in view. [0:07:04 - 0:07:05]: A different car, possibly red as well, is shown traveling along the same road. [0:07:06]: A transition occurs to a scoreboard showing points and rankings of racing teams. The scores for different rounds (R1, R2, R3, R4) and total points (Final Total) for each team (Champagne Papi, Blue Line Racing, Iceman Racing, Numbskull Racing, and Fat Dad Racing) are displayed. Additional text and two logos are present at the bottom of the screen, indicating associations with racing organizations. [0:07:07 - 0:07:10]: The scoreboard becomes more evident with different team names and scores clearly visible. Confetti and colorful streamers appear, signifying celebration or victory. The black ladder-like structure is still faintly visible in the background. [0:07:11]: Confetti continues to fall around the scoreboard, signifying the festive environment. The text \"ULTIMATE EVO - STAGE 2\" is prominently displayed above the scoreboard. Various team logos are present along with their respective scores and round results. [0:07:12]: The scene transitions to a new frame showing an infographic titled \"SHAVANO MOUNTAIN RACEWAY.\" It depicts three different cars and their respective lap times. The backgrounds show a racetrack with cars driving along, complemented by mountainous surroundings. [0:07:13 - 0:07:15]: The infographic highlights three racing teams. The text \"FAST TRACK!\" is also displayed. The cars are labeled as BLR (Blue Line Racing), Flip, and Jonabi, with their respective lap times (22.350, 23.030, and 23.093). [0:07:16 - 0:07:20]: The infographic updates with altered lap times and added graphics, including fireworks to emphasize racing achievements. The depicted cars remain consistent. The model racetrack and its surrounding mountainous landscape continue to serve as the background.",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many different racing teams' names are displayed on the scoreboard?",
        "time_stamp": "00:07:11",
        "answer": "B",
        "options": [
          "A. Three.",
          "B. Four.",
          "C. Five.",
          "D. Six."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_499_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video starts with a view of a light wooden surface. A shadow is cast toward the top-left corner, indicating a light source from the top right. [0:00:01 - 0:00:07]: A pair of hands appear, holding a white box labeled \"Mac Studio\" in black text. The box has a handle on top, and the hands are in the process of opening the box by pulling the handle. The box is rotated to reveal different angles, and it is then lifted vertically to remove the lid from the base. [0:00:07 - 0:00:08]: As the lid is lifted, a white tray inset becomes visible within the box. The tray holds the Mac Studio device, which is covered in white paper with an Apple logo faintly visible through it. [0:00:08 - 0:00:12]: The hands begin to remove the device from the tray. The box flaps are fully opened to the sides, and the Mac Studio is lifted out of its packaging. [0:00:12 - 0:00:13]: The Mac Studio, still covered in protective paper, is being held up by the hands. The hands are positioned on either side of the device as it is moved out of the box. [0:00:14 - 0:00:17]: The hands proceed to peel off the white protective paper around the Mac Studio. The paper is carefully unwrapped starting from one side. [0:00:17 - 0:00:19]: The protective paper is fully unfolded from the Mac Studio, revealing the device's metallic surface and ports. The hands hold the device securely while removing the last pieces of wrapping.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What text is visible on the white box being handled right now?",
        "time_stamp": "00:00:07",
        "answer": "B",
        "options": [
          "A. \"Apple Device\".",
          "B. \"Mac Studio\".",
          "C. \"Tech Gear\".",
          "D. \"Studio Box\"."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_120_real.mp4"
  },
  {
    "time": "[0:03:40 - 0:04:00]",
    "captions": "[0:03:40 - 0:04:00] [0:03:40 - 0:03:42]: A person in a green shirt is sitting at a desk, holding a large black box with his left hand and a smaller silver box with his right hand. The scene is well-lit with a dark background. The person appears to be speaking or explaining something. [0:03:43 - 0:03:50]: The person has shifted to sitting in front of a computer monitor with a keyboard on the desk. The monitor displays a colorful, abstract pattern. The lighting is consistent, and the person continues to speak or explain, occasionally using hand gestures for emphasis. [0:03:51 - 0:03:54]: A close-up of an open computer case with various internal components visible, including a cooling fan and a motherboard. The hand adjusts elements inside the case, while a small, closed, silver computer is positioned to the left. [0:03:55 - 0:03:58]: The focus shifts to a new scene showing a partially assembled computer case with a mesh front panel. A person is seen in the background, interacting with a tablet and moving items on the desk, which includes tools and additional computer parts under bright blue lighting. [0:03:59]: A close-up of computer hardware, including an RTX 3090 graphics card and a GTX 1080 Ti graphics card, placed on a dark surface with a green background. The lighting highlights the details of the hardware components.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What specific hardware components are highlighted right now?",
        "time_stamp": "00:03:59",
        "answer": "A",
        "options": [
          "A. RTX 3090 and GTX 980 Ti graphics cards.",
          "B. Cooling fan and motherboard.",
          "C. Large black box and smaller silver box.",
          "D. Computer monitor and keyboard."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_120_real.mp4"
  },
  {
    "time": "[0:07:20 - 0:07:40]",
    "captions": "[0:07:20 - 0:07:40] [0:07:20 - 0:07:21]: The video starts by showing a close-up view of the inside components of a computer case, prominently featuring a component marked with the text \"GEFORCE RTX\" that is illuminated. The background appears mostly dark, highlighting the text. [0:07:22]: The perspective widens slightly to reveal the edge of the computer case, showing some of its white metallic structure and fans that are also illuminated. [0:07:23 - 0:07:24]: The camera angle shifts slightly to show the entire side of the computer case with its transparent panel, now including more of the surroundings with a focus on the computer components visible within the case. [0:07:25 - 0:07:29]: The video then transitions to showing a digital display with a bar graph titled \"mask tracking,\" comparing two systems: \"Mac Studio | M2 Ultra\" and \"7800X3D + 4090 PC\". The Mac system shows a time of 13.9 seconds, while the PC shows 15.2 seconds. [0:07:30 - 0:07:31]: The scene switches to a man seated at a desk, in front of a large monitor. He is engaged in an activity on the computer, appearing to be discussing or explaining something while using gestures, including hand movements. [0:07:32 - 0:07:34]: The man continues to talk and gesture, clearly emphasizing points in his explanation. The monitor shows a user interface that could be a video editing or production software with multiple track layers and elements. [0:07:35 - 0:07:38]: The perspective changes again to a close-up view of the computer setup on the desk, showing a large monitor displaying editing software, a compact desktop computer, a camera, and a keyboard and mouse, highlighting a high-end, organized work environment. [0:07:39]: The final frame provides a more detailed view of the monitor, focusing on the user interface showing the rendering progress of a project, with several thumbnails at the bottom representing different media files or video clips being edited in the software.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the title shown on the digital display with a bar graph right now?",
        "time_stamp": "00:07:26",
        "answer": "C",
        "options": [
          "A. System Analysis.",
          "B. Performance Metrics.",
          "C. Mask Tracking.",
          "D. Data Comparison."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_120_real.mp4"
  },
  {
    "time": "[0:11:00 - 0:11:20]",
    "captions": "[0:11:00 - 0:11:20] [0:11:00 - 0:11:01]: The video begins with a view of the back of a silver desktop computer monitor placed on a white desk. The monitor has the Apple logo, and cables are connected to the ports on the back. An Apple Mac Mini is positioned on the right side of the desk. In the background, there's a vertical orange light strip and a portion of the room wall, which appears dark. [0:11:02 - 0:11:04]: The viewpoint shifts to the front of the monitor, displaying a vibrant blue and purple gradient wallpaper. The lower part of the monitor's bezel and the desktop stand are visible, revealing a clean and minimalistic workspace setup. [0:11:05 - 0:11:06]: A person wearing a green t-shirt is interacting with the monitor. They are seated at the desk, holding the side of the monitor with one hand and pointing at the screen with the other. There is a small keyboard placed in front of them, and the background shows a dark-colored wall and part of an open door. [0:11:07 - 0:11:10]: The monitor now displays a video editing timeline with multiple video and audio tracks represented in different colors. The person's hand gestures toward the screen, indicating an interaction with the video editing software. [0:11:11 - 0:11:15]: The person, still seated, is engaging in a conversation, explaining something while gesturing with their hands. The video editing timeline remains displayed on the monitor, with various clips and tracks visible in the editing software. [0:11:16 - 0:11:19]: The camera switches back to focus solely on the monitor's screen, detailing the video editing interface. Different sections of the timeline are shown, with various clips, tracks, and timelines indicating an ongoing video editing process. The Mac Mini remains visible on the left side of the desk.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What type of wallpaper is displayed on the monitor screen right now?",
        "time_stamp": "0:11:03",
        "answer": "D",
        "options": [
          "A. Green and yellow abstract design.",
          "B. Photo of a natural landscape.",
          "C. Black and white geometric pattern.",
          "D. Vibrant blue and purple gradient."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_120_real.mp4"
  },
  {
    "time": "[0:14:40 - 0:14:54]",
    "captions": "[0:14:40 - 0:14:54] [0:14:40 - 0:14:44]: A person in a green shirt is seated at a table in a dimly lit room. On the table are two objects: a silver device, resembling a compact computer, on the left, and an opened black computer case with visible internal components, including a fan, on the right. The person gestures with their right hand towards the compact computer. [0:14:45 - 0:14:49]: The scene changes to a close-up of the person's hands as they position the silver compact computer under a monitor. The monitor displays a colorful desktop screen. The person seems to be adjusting the position and connections of the compact computer, making sure it is properly placed. [0:14:50 - 0:14:53]: The scene returns to the previous setup with the person in the green shirt at the table. They continue to gesture towards the black computer case, holding up one of the components, and explaining something about it while looking at the camera. The compact computer remains on the left side of the table.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the person in the green shirt gesturing towards right now?",
        "time_stamp": "00:14:51",
        "answer": "B",
        "options": [
          "A. The compact computer on the left.",
          "B. The opened black computer case on the right.",
          "C. The monitor displaying a colorful desktop screen.",
          "D. The table itself."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_120_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:06]: The video begins with a view of two framed art pieces hanging on a white wall. The top piece features a black and white graphic resembling an emblem, while the bottom piece depicts a colorful rooster. As the camera moves, a tall, colorful sculpture comes into view. The sculpture consists of multiple rectangular boxes stacked on each other at various angles. Each box has distinct patterns and text, including polka dots, stripes, and words like \"POW!\" and \"Brillo.\" The background shows other people walking around and observing the artwork in the gallery. [0:00:07 - 0:00:10]: The camera continues to move closer to the tall sculpture, providing different angles and emphasizing the text and patterns on each box. The gallery space is spacious and well-lit, with white walls and various colorful art pieces hanging on them. People are seen interacting with each other and looking at the displayed art. [0:00:11 - 0:00:15]: The focus shifts to a large wall-mounted artwork that reads \"LIFE I LOVE YOU MORE.\" The letters are filled with colorful images of flowers. The camera moves from left to right, capturing the entire piece. The background includes various art pieces and frames, along with people walking around and examining the exhibits. [0:00:16 - 0:00:20]: The camera continues to focus on the artwork with the text \"IN MY LIFE I LOVE YOU MORE,\" providing a detailed view of the vibrant colors and flowers within each letter. The gallery remains busy, with visitors appreciating the art and engaging in conversations. The video concludes with this detailed examination of the artwork.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What are the patterns and text on the rectangular boxes of the tall sculpture?",
        "time_stamp": "00:00:10",
        "answer": "B",
        "options": [
          "A. Stripes, polka dots, \"BANG!\", \"Soup\".",
          "B. Polka dots, stripes, \"POW!\", \"Brillo\".",
          "C. Chevron, zigzags, \"WOW!\", \"Coca-Cola\".",
          "D. Dots, lines, \"BOOM!\", \"Pepsi\"."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Text-Rich Understanding",
        "question": "What does the large wall-mounted artwork say right now?",
        "time_stamp": "00:00:27",
        "answer": "C",
        "options": [
          "A. \"LIVE LIFE TO THE FULLEST\".",
          "B. \"ART IS LIFE\".",
          "C. \"IN MY LIFE I LOVE YOU MORE\".",
          "D. \"LOVE MAKES LIFE BEAUTIFUL\"."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_475_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:44]: Several pieces of wall-mounted art are visible. Three artworks, all composed of black, textured materials, hang against a white wall. Two square pieces are positioned at the top, side by side, and one larger rectangular piece is below them. To the right of the larger piece is a small QR code sign and an information placard. [0:02:45 - 0:02:46]: The camera pans to the right, revealing additional artwork and signage. A QR code sign is prominently displayed on the left side of the frame. Two people are visible further down the gallery, standing in front of more artwork. [0:02:47 - 0:02:48]: The camera continues to move right, showing more of the gallery's interior. Various colorful paintings hang on the white walls. The two people stand closer to the wall art, which includes vibrant, textured pieces. The gallery is identified as the \"Bruce Lurie Gallery\" with signage, including the location \"1315”. [0:02:49 - 0:02:50]: A blue sign indicating \"Bruce Lurie Gallery 1315\" is visible near the ceiling, along with the gallery's location details. The ceiling has recessed lighting and visible ventilation elements. [0:02:51 - 0:02:52]: The camera tilts downward, and additional paintings and the general layout of the gallery space come into view. Artworks are scattered on the walls, and a group of people can be seen conversing and browsing the gallery. [0:02:53 - 0:02:54]: Three artworks are clearly visible. One detailed and colorful mixed media piece, with the word \"GUERIN,\" a figure, and layers of text and patterns, is vibrant and prominently displayed. [0:02:55 - 0:02:57]: The camera focuses on the same vibrant piece titled \"GUERIN,\" now with additional surrounding artworks becoming visible. Adjacent is an abstract art piece with black and colorful splashes hinting at layered composition and detail.  [0:02:58 - 0:03:00]: More artworks are displayed, showing a mix of different styles and mediums. Abstract art and figurative elements blend, with the eclectic mix contributing to the gallery's dynamic visual experience. Several framed pieces are spaced evenly along the white walls, each unique in style and subject matter.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action does the camera perform after showing the \"Bruce Lurie Gallery 1315\" sign?",
        "time_stamp": "0:02:54",
        "answer": "D",
        "options": [
          "A. Pans left.",
          "B. Zooms in.",
          "C. Pans upward.",
          "D. Tilts downward."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_475_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:21]: Several pieces of artwork are displayed on a white wall. The leftmost artwork is a canvas with a blue background featuring a black palm tree and five toucans. Next to it, there are framed paintings depicting varied scenes, including an abstract landscape and a pool scene. Below these are two more framed pieces, one with a red flower in a green vase and another with various logos and cartoon characters in bright colors. [0:05:22 - 0:05:23]: The camera focuses on the framed painting of a pool scene, which depicts a person in a red jacket standing at the edge of a pool while a person swims underwater beneath the rippling surface. The background shows hilly scenery with green foliage. [0:05:24 - 0:05:28]: The camera continues to focus on the pool scene painting, revealing more details such as the texture of the water and the brushstrokes. The scenery in the background includes a mountain range and lush greenery, with the person observing the swimmer. [0:05:29]: The camera starts to zoom in on the lower part of the pool scene painting, showing closer detail of the person's legs and the deck they are standing on. [0:05:30 - 0:05:34]: The camera shifts to the right side of the pool scene painting, focusing on the small information placard next to it. The placard contains details about the artist and the context of this piece but is not entirely readable from this frame. [0:05:35 - 0:05:36]: The camera pans downward, showing the lower edge of the pool scene painting where the artist’s signature is visible. Below this painting, a colorful artwork featuring multiple cartoon characters and logos can be seen partially. [0:05:37 - 0:05:39]: The camera moves downward and to the left, providing a close-up of the colorful artwork with a collage of logos, characters, and symbols such as Mickey Mouse, a Ferrari logo, Apple logo, and Mario. The artwork's vivid colors and varied graphical elements stand out against the white wall.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the artist's signature located in the pool scene painting?",
        "time_stamp": "00:05:36",
        "answer": "C",
        "options": [
          "A. At the top of the painting.",
          "B. On the left side of the painting.",
          "C. At the lower edge of the painting.",
          "D. In the center of the painting."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_475_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:50]",
    "captions": "[0:09:40 - 0:09:50] [0:09:40 - 0:09:42]: The video shows three photographs hung on a white wall. The photographs feature distorted images of a person submerged underwater. The first photograph, in the top left corner, shows a person swimming left, intertwined with abstract shapes. The second photograph is centrally located and shows the same person in a dynamic pose, again with the distortion of water. The third is positioned at the bottom right and also depicts the person underwater with similar distortion and dynamic poses. [0:09:43 - 0:09:44]: The perspective shifts to a sign above the wall, which reads \"PULA\" and \"1341\" with \"REYKJAVÍK | ICELAND\" below. The background is plain white, and part of a black structure is visible at the top. [0:09:45 - 0:09:49]: The view changes to a broader area of the gallery. A large white wall is visible with additional artworks. The left side shows a painted figure with blue and black hues, resembling an abstract human form. Other smaller images are hung next to it, including an image of a human figure shown in neon blue against a dark background. Some people and other elements of the gallery can be seen in the periphery.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is written on the sign above the wall just now?",
        "time_stamp": "00:09:46",
        "answer": "B",
        "options": [
          "A. \"PULA\" and \"1341\" with \"NEW YORK | USA\" below.",
          "B. \"PULA\" and \"1341\" with \"REYKJAVÍK | ICELAND\" below.",
          "C. \"PULA\" and \"1341\" with \"TOKYO | JAPAN\" below.",
          "D. \"PULA\" and \"1341\" with \"PARIS | FRANCE\" below."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_475_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins with a logo of the FC Barcelona club placed centrally on a gradient blue background. The logo is colorful and distinct, featuring the traditional colors and elements of the FC Barcelona crest. An animation introduces a \"SUBSCRIBE\" button below the logo, which starts as partly obscured and then becomes fully visible. [0:00:03 - 0:00:07]: The scene transitions to a live action where several individuals are clapping in what appears to be a stadium or sports field at night. Bright floodlights illuminate the background. The group is in the foreground, and they are dressed in formal or semi-formal attire with medals around their necks. The people are arranged in a loosely spaced line facing the camera.  [0:00:08 - 0:00:15]: The camera continues to focus on the different individuals, one by one. It captures close-up shots of women applauding, all wearing medals. There is a mix of attires, including blazers and more casual outfits. The mood appears celebratory, and the background shows bright stadium lights against a dark night sky. [0:00:16 - 0:00:20]: In the final frames, the scene shifts slightly to show a woman in a black dress holding a silver trophy. Beside her is another woman in a white outfit. Both are standing on a grassy field, likely the stadium pitch, and surrounded by other individuals. There are also quick close-up shots of the trophy being held aloft.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What appears at the very beginning of the video?",
        "time_stamp": "0:00:08",
        "answer": "B",
        "options": [
          "A. A football match.",
          "B. A logo of FC Barcelona.",
          "C. A silver trophy.",
          "D. A live concert."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "What is the position relationship between the woman in a black dress and the woman in a white outfit?",
        "time_stamp": "0:00:20",
        "answer": "A",
        "options": [
          "A. They are standing side by side.",
          "B. They are standing back to back.",
          "C. The woman in black is in front of the woman in white.",
          "D. The woman in white is holding the trophy."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_3_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:02]: The video begins with a close-up shot from a football match. A player in a red and blue striped jersey is about to head a ball toward the goal, while another player in a white jersey observes. The goalkeeper in a green outfit dives to try to block the shot. Surrounding the field, there are advertisements and a crowd in the background. [0:01:02 - 0:01:09]: The view shifts to a wider perspective of the football field, capturing multiple players. The red and blue striped team is on the offensive, dribbling the ball near the right wing toward the opposing team's goal. The defenders in white are positioning themselves to block any potential shots. The goalkeeper stands alert in front of the goalposts, wearing a bright yellow kit. The surroundings include advertising boards and vibrant green turf. [0:01:09 - 0:01:12]: The next scenes show a close-up behind the goal net. The goalkeeper dives to the left attempting to make a save as the ball approaches the goal line. Several players in red and blue are closely following up for a potential rebound, while defenders in white try to block any imminent chances. The crowd can be seen in the background through the net. [0:01:13 - 0:01:15]: The scene changes once more to a wider shot from another part of the field. The white jersey team is attacking now, moving down the left wing. They are passing the ball in an attempt to break through the defensive line of the red and blue striped team. The defense and midfielders from the defending team are trying to intercept the play, positioning themselves strategically around the penalty box. [0:01:15 - 0:01:20]: The final frames display the white team attempting a cross into the box from the left wing. Multiple players from both teams inside the box are anticipating the ability to control the ball. The red and blue defenders are prepared to clear the ball, and the goalkeeper is alert, ready to react to any incoming shots. Advertising boards, the field’s vibrant green color, and the stadium seating filled with spectators are visible in the background.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the goalkeeper's kit when standing in front of the goalposts?",
        "time_stamp": "0:01:09",
        "answer": "C",
        "options": [
          "A. Red.",
          "B. Green.",
          "C. Yellow.",
          "D. Blue."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_3_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:03]: A player in a blue and red uniform prepares to kick a soccer ball within the penalty area. An opposing player in a white uniform with the number 9 on his back watches closely. The red advertisement boards in the background display \"ESTRELLA DAMM\", and a cameraman is visible behind the boards. [0:02:03 - 0:02:06]: The player in the blue and red uniform kicks the ball towards the goal. Another opposing player in a white uniform with the number 22 on his back is moving towards the ball. The focus is primarily on the action near the goal area. [0:02:06 - 0:02:08]: The ball is being saved by a goalkeeper in a yellow kit, stretching to his left to block the shot. In front of the goal, an opposing player in a white uniform with the number 26 on his back watches the action unfolding.  [0:02:08 - 0:02:12]: The goalkeeper in yellow successfully blocks the shot and holds the ball while lying on the ground. The players in blue and red as well as the player in the white uniform with the number 26 stand near the goal, watching. [0:02:12 - 0:02:20]: The viewpoint changes, showing a wider angle of the soccer field. A set-piece or corner kick is happening, with several players from both teams positioned inside the penalty box. The attacking team, wearing blue and red, has several players clustered near the goal, while the defending team in white is positioned to cover them. The advertisements around the field show \"AMBILIGHT TV\" and several other sponsors. The crowd in the stands watches intently, and a few players outside the box are preparing for the play.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What advertisement is displayed on the red boards behind the goal?",
        "time_stamp": "0:02:03",
        "answer": "C",
        "options": [
          "A. AMBILIGHT TV.",
          "B. NIKE.",
          "C. ESTRELLA DAMM.",
          "D. ADIDAS."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Action Recognition",
        "question": "What are the two people in the video frame doing now?",
        "time_stamp": "0:02:51",
        "answer": "A",
        "options": [
          "A. handshaking.",
          "B. Hugging.",
          "C. High-fiving.",
          "D. Fist bumping."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_3_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:07]: The video starts in an art gallery setting, showcasing various pieces of art on white walls. To the immediate left, a large photographic artwork of a vibrant red and orange abstract landscape is visible. Moving rightwards, there is a large photograph of a giraffe, closely framed. Next to it, another large photograph, this one depicting a leopard in grass, hangs on the wall. Several people are visible in this scene, including a woman with curly long gray hair to the right, and a man with a nametag engaging with two women near the center. Background details suggest the gallery is well-lit with spotlights focused on the artwork. [0:00:08 - 0:00:15]: As the camera moves along, more artwork comes into view. To the left, a large vibrant photograph of marine life, featuring a shark, catches the eye. Various other pieces, including one depicting aquatic life in blue tones and another with vividly colored flowers, fill the walls. More people are present, some taking photos with their smartphones. The woman in a bright pink sweater stands out as she appears to be photographing the artwork or the scene. The art pieces are mounted in frames and seem to belong to an exhibit focusing on nature and wildlife. [0:00:16 - 0:00:19]: The camera pans to the right, showcasing another large image of abstract shapes and intense light patterns, featuring reds, oranges, and yellows. The gallery space is spacious with white walls and strategic lighting, enhancing the visual appeal of the displayed artworks. More visitors are seen observing and discussing the pieces, giving a vibrant and engaging atmosphere. The video concludes with a closer look at the abstract artwork filled with intense, flowing color patterns, emphasizing the dynamic range and vividness of the exhibit.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are some people doing with their smartphones?",
        "time_stamp": "00:00:15",
        "answer": "D",
        "options": [
          "A. Making calls.",
          "B. Texting.",
          "C. Playing games.",
          "D. Taking photos."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best describes the overall content of the video clip?",
        "time_stamp": "00:00:19",
        "answer": "A",
        "options": [
          "A. A tour of an art gallery showcasing nature and wildlife photography.",
          "B. A walkthrough of a historical museum.",
          "C. A visit to a contemporary art exhibition.",
          "D. A day at an amusement park."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_467_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:42]: The video begins with a close-up view of three vibrant and colorful paintings on a white wall. The largest painting on the left depicts a person wearing a detailed, ornate jacket with gold accents and vibrant colors surrounding them. To the right, an artwork is visible with a Union Jack backdrop and features famous figures in a crosswalk scene. Above it is another artwork, featuring a person in grayscale with a contemplative expression. [0:02:43 - 0:02:44]: The camera then pans to the right, revealing more of the exhibition space. The area is spacious and well-lit, with other visitors visible. Some stand in groups, examining various pieces of art, while others walk through the space. [0:02:45 - 0:02:52]: The camera focuses on a section featuring portraits of women. The first portrait is of a woman with blonde hair, painted in a glamorous and stylized manner. She has bright red lips and a captivating gaze. Adjacent to it is a monochromatic portrait of another woman, also with blonde hair, styled in classic waves. The third portrait shows a young woman with auburn hair and striking red lipstick, set against a background filled with scattered images and objects. [0:02:53 - 0:02:54]: A closer view of the first portrait allows the viewer to appreciate its intricate details and the artist's technique. The woman's blue eyes, red lips, and the nuanced textures in her hair and surroundings are more visible. [0:02:55 - 0:02:57]: The camera zooms further into the painting, focusing intensely on the woman's face. The details of her expression, the shading within her skin, and the highlights in her eyes become more pronounced. [0:02:58 - 0:02:59]: The final moments of the clip feature an extreme close-up of the subject's eyes, capturing the intricate details and reflective highlights, providing a very intimate view of the artwork.",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many portraits of women are shown on the walls right now?",
        "time_stamp": "00:02:52",
        "answer": "C",
        "options": [
          "A. Two.",
          "B. Four.",
          "C. Three.",
          "D. Five."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_467_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:24]: The video showcases a wall with vibrant artwork under the sign \"P B G print bakery\". The artwork is a surreal painting featuring a mix of indoor and outdoor settings. It includes two large arches on the left, revealing a lush, green landscape with a waterfall. In the foreground, there is a checkered black and white floor, and a red and green rectangular object that appears like a mattress or pad. There is also a tree with a peculiar branch extending into a yellow box on the right side of the painting. The overall background of the painting is vibrant yellow. [0:05:25 - 0:05:35]: As the video progresses, the camera moves to the left, providing a closer view of the detailed surreal painting. Another painting on the adjacent wall gradually comes into view. This second painting continues the surrealistic theme, featuring a similar checkered floor pattern, a tree, and a landscape extending into the distance. There is some overlap in the imagery, suggesting a continuous theme or connected spaces. [0:05:36 - 0:05:39]: The focus shifts to the artwork on the left wall. A large piece is prominently displayed, depicting a scene framed by an archway with an open door revealing a lush, green landscape beyond. The black and white checkered floor extends outside, creating an almost seamless transition between indoor and outdoor worlds. Several smaller objects are displayed beside it, including paintbrushes in a white container and other artworks.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What words are written on the sign above painting shown right now?",
        "time_stamp": "00:05:24",
        "answer": "A",
        "options": [
          "A. P B G print bakery.",
          "B. L M N art studio.",
          "C. Q R S gallery.",
          "D. T U V design hub."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_467_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:49]",
    "captions": "[0:09:40 - 0:09:49] [0:09:40 - 0:09:43]: The video begins with a first-person perspective inside an art gallery. On the white walls, a series of paintings are displayed. The camera focuses on a prominently large, red-colored painting depicting an abstract or partially obscured image of a muscular torso. This painting is mounted in a silver frame. To the left of this painting is another artwork that appears to be an abstract piece with predominantly blue and white colors. [0:09:44 - 0:09:45]: The camera shifts its perspective slightly to the right, revealing more of the gallery space. The background shows other visitors in the gallery, with some walking past artworks. A sign with the name \"Hazel's Story\" in blue neon lights is partially visible in the background, along with other displayed artworks. [0:09:46 - 0:09:49]: As the camera continues moving to the right, another small artwork comes into view. This artwork appears to be abstract with red coloring and is placed in a silver frame similar to the previous ones. To the right of this red painting is another larger piece, predominantly gray and abstract in nature.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the name on the sign partially visible in the background wall?",
        "time_stamp": "00:09:45",
        "answer": "A",
        "options": [
          "A. \"Hazel's Story\".",
          "B. \"Gallery Night\".",
          "C. \"Art Showcase\".",
          "D. \"Modern Art\"."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_467_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the frog appear from the toilet?",
        "time_stamp": "0:00:17",
        "answer": "D",
        "options": [
          "A. Because the frog was hiding there to avoid being caught by the rabbit.",
          "B. Because the frog was attracted by the water and jumped in.",
          "C. Because the toilet leads to a secret underground pond where the frog lives.",
          "D. Because the chicken that was just thrown into the toilet is wearing a frog's disguise."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_232_real.mp4"
  },
  {
    "time": "[0:01:22 - 0:01:52]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the rabbit dressed in red and white start to become restless?",
        "time_stamp": "0:01:26",
        "answer": "A",
        "options": [
          "A. Because the fly just disturbed him, he is swatting the fly.",
          "B. Because he suddenly remembered he left something important outside.",
          "C. Because he heard a strange noise coming from the next room.",
          "D. Because he can't find the key to the door and is getting frustrated."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_232_real.mp4"
  },
  {
    "time": "[0:02:44 - 0:03:14]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why is this cartoon chicken very shocked now?",
        "time_stamp": "0:02:54",
        "answer": "A",
        "options": [
          "A. Because just now another small cartoon chicken was eaten by a frog.",
          "B. Because it just saw a mirror and didn't recognize its own reflection.",
          "C. Because a loud noise suddenly scared it from behind.",
          "D. Because it saw a large shadow pass overhead, thinking it was a predator."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_232_real.mp4"
  },
  {
    "time": "[0:04:06 - 0:04:36]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the character encounter a heavy legacy robot in the room?",
        "cause": "The character reads a comic book",
        "effect": "The character encounters a heavy legacy robot in the room",
        "time_stamp": "0:04:11",
        "answer": "D",
        "options": [
          "A. Because the character turns on a machine.",
          "B. Because the character follows someone into the room.",
          "C. Because the character activates a hidden switch.",
          "D. Because the character reads a comic book."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_232_real.mp4"
  },
  {
    "time": "[0:05:28 - 0:05:58]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why is this red rabbit standing in front of the iron gate now?",
        "time_stamp": "00:04:13",
        "answer": "D",
        "options": [
          "A. Because the red rabbit is trying to figure out how to open the gate.",
          "B. Because the iron gate suddenly started glowing, and the rabbit is curious.",
          "C. Because the cartoon character is seeking amusement.",
          "D. Because the cartoon character on the iron gate has been whistling just now."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_232_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What best summarizes the process just shown?",
        "time_stamp": "00:00:10",
        "answer": "C",
        "options": [
          "A. The vendor prepared a burger, toasted the bun, added condiments, and served it to the customer.",
          "B. The vendor selected hot dogs, prepared a bun, added relish and mustard, and served it to the customer.",
          "C. The vendor selected several hot dogs, placed them in a container, heated them, and arranged other ingredients.",
          "D. The vendor cooked hot dogs, placed them in buns, added ketchup and mustard, and served them to the customer."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_373_real.mp4"
  },
  {
    "time": "[0:02:17 - 0:02:27]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions just now?",
        "time_stamp": "00:02:27",
        "answer": "C",
        "options": [
          "A. The vendor selected condiments, placed them on a tray, and began packaging hot dogs.",
          "B. The vendor heated hot dogs, added toppings, and handed them to the customer.",
          "C. The vendor picked up a hot dog bun, retrieved hot dogs from the warmer.",
          "D. The vendor prepared a salad, added dressings, and served it to the customer."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_373_real.mp4"
  },
  {
    "time": "[0:04:34 - 0:04:44]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions just now?",
        "time_stamp": "00:04:44",
        "answer": "C",
        "options": [
          "A. The vendor cleaned the workstation, organized utensils, and added new items to the menu.",
          "B. The vendor adjusted equipment, reviewed the counter area, and greeted a customer.",
          "C. The vendor retrieved a hot dog bun, used tongs to grab a hot dog, and prepared the bun for assembly.",
          "D. The vendor heated up condiments, chopped vegetables, and arranged them for a salad."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_373_real.mp4"
  },
  {
    "time": "[0:06:51 - 0:07:01]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions just now?",
        "time_stamp": "00:07:00",
        "answer": "C",
        "options": [
          "A. The vendor cooked a hot dog, added mustard and ketchup, wrapped it, and served it to the customer.",
          "B. The vendor grilled a hot dog, added ketchup, placed it on a bun, and served it to the customer without wrapping.",
          "C. The vendor retrieved a cooked hot dog, added toppings, wrapped it in paper, put it in a bag, and handed it to the customer.",
          "D. The vendor made a sandwich, added vegetables, wrapped it, and handed it to the customer."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_373_real.mp4"
  },
  {
    "time": "[0:09:08 - 0:09:18]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the process just shown?",
        "time_stamp": "00:09:22",
        "answer": "C",
        "options": [
          "A. The vendor grilled a hot dog, placed it in a bun, and added vegetables before serving it to the customer.",
          "B. The vendor cooked a hot dog, added ketchup and mustard, and wrapped it for the customer.",
          "C. The vendor prepared a hot dog, added ketchup and onion topping, and handed it to the customer.",
          "D. The vendor prepared a hot dog, added mayonnaise and mustard, and served it to the customer."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_373_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:11]: The video begins showing a first-person perspective focused on a computer screen displaying an in-game interface. The character on screen is wearing a dark green hoodie and a dark gray cap with a red emblem. The interface shows items for sale such as wax, car polish, and lock picks. There is also a shopping cart on the right-hand side of the screen showing selected items. During this segment, the game interface changes momentarily, showing other items like blueprints and engine parts before reverting to the original screen. The chat window on the right side of the screen is active with messages from other users. [0:00:11 - 0:00:12]: The in-game interface displays car-related items again, including wax and car polish. The chat on the right continues to update with user interactions. [0:00:13 - 0:00:15]: The interface remains focused on the same items without significant changes. The chatter on the right continues as before. [0:00:16 - 0:00:20]: The camera view shifts from the first-person screen to an outdoor scene in a video game environment. A character wearing a green \"Air Force\" hoodie and gray cap interacts with another character wearing a reflective work vest and headphones. The environment includes a building with a cracked wall and multiple cars parked outside around a courtyard, indicating a repair shop or similar setting.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the character wearing right now?",
        "time_stamp": "00:00:12",
        "answer": "B",
        "options": [
          "A. Blue jacket and black hat.",
          "B. Dark green hoodie and dark gray cap.",
          "C. Red hoodie and blue cap.",
          "D. Black jacket and white cap."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_285_real.mp4"
  },
  {
    "time": "0:02:20 - 0:02:40",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:23]: The video begins with a person walking inside a room. The room has a drab concrete floor and beige walls. To the right, there is a metal shelf filled with various boxes and containers. Alongside the shelf is a water cooler. A table with blue chairs is positioned against the wall with a window, and a door is visible beside it. The video is shown from a first-person perspective where the viewer can see the person's back, who is wearing a yellow-green safety vest and headphones. [0:02:24 - 0:02:27]: The person heads towards the door in the room. The door is white with a dark frame, and just left of the metal shelf stacked with items. The viewer can see the person's arm extended as they reach towards the door. [0:02:28 - 0:02:30]: Entering a larger space, the individual moves through another section of the building, which appears to be a garage. The garage door, which is partially rolled up, is visible ahead and to the right. Stacks of tires and red tool chests are scattered around the space. [0:02:31 - 0:02:33]: Continuing to move through the garage, the environment is filled with industrial elements. The cement floor has visible cracks and stains, suggesting heavy use. Various maintenance equipment and tools are arranged around the area. The person walks towards the open garage door. [0:02:34 - 0:02:38]: The person heads towards the entrance of the garage where sunlight is pouring in through the partially open door, casting shadows on the ground. Visible in the background are more storage racks, workstations, and stacked equipment. [0:02:39 - 0:02:42]: Stepping outside the garage onto a concrete ground area, there is a scattered assortment of tires, tools, debris, and a large, weathered old tire mechanic frame. The sunlight brightens the scene considerably. [0:02:43 - 0:02:46]: The individual approaches an old rusty engine lifter along with scattered debris and car parts around it. Farther ahead, several blue cars and a few people are visible, working or loitering around. Palm trees and several buildings loom in the distance. [0:02:47 - 0:02:51]: Progressing towards the open space, the person walks around and past a taller mechanic frame. A blue car with its trunk visible is straight ahead. Another blue car and a group of people gathered around it, chatting or doing some activity, come into focus. [0:02:52 - 0:02:55]: The person moves closer to the group standing near the blue car. The individuals are dressed in casual attire and seem engaged with each other. The cars in the vicinity seem parked randomly, and there are cones and boxes scattered around too. [0:02:56 - 0:02:59]: As the person reaches the group, it becomes clearer that they are in an outdoor area with more visible cars and persons. The group stands casually with some carrying objects like boxes and tools. The person stops and appears to join or observe the group conversation. [0:03:00 - 0:03:03]: Facing a wall with a blue car parked along it, the person stands near two others who are engaged in conversation. There are visible boxes, tires, and car parts arranged next to the people. [0:03:04 - 0:03:07]: The individual in the safety vest looks at the men standing by the blue car for a moment, turning their head slightly, providing a better view of all the participants' actions and gestures. [0:03:08 - 0:03:11]: Moving again slightly closer to the two men by the car, more detail is noticeable. One of the men holds a brown box, and there are tires near their feet. The other groups of cars and items scattered around become more distinct. [0:03:12 - 0:03:16]: Gesturing while observing the actions of the person in the green shirt, the viewer's perspective changes slightly, taking in the opening to a garage behind them and other surrounding details. The setting reveals it is a functional working area utilized for car repairs or maintenance. [0:03:17 - 0:02:20]: Walking towards the man in the green shirt, the viewer gets a clear view of his face and attire. The man is wearing a green hoodie and a cap with visible text insignia. The person interacts, likely initiating a conversation, and a close-up of the man's face concludes the scene. [0:03:21 - 0:02:24]: The person likely engages directly, as the camera angle tightens on the individual's",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the individual in the safety vest doing right now?",
        "time_stamp": "00:02:33",
        "answer": "A",
        "options": [
          "A. Observing the group conversation.",
          "B. Operating a piece of machinery.",
          "C. Cleaning the area.",
          "D. Fixing a car engine."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_285_real.mp4"
  },
  {
    "time": "0:04:40 - 0:05:00",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:42]: A person wearing a reflective vest is walking in a parking lot. There are multiple parked cars around, including a blue car directly ahead and a larger gray van in the background. The person appears to be moving towards the cars. [0:04:43 - 0:04:45]: The person continues to walk towards a blue car parked near the building's entrance in the background. Two cardboard boxes are seen beside the wall of the building. The ground appears to be a mixture of concrete and gravel. [0:04:46 - 0:04:48]: The person stops in front of the blue car. The front end of another vehicle is partially visible on the right. The person seems to be interacting with the blue car, possibly preparing to clean it. [0:04:49 - 0:04:50]: The person, still in the same position, is seen closer to the blue car. A contextual menu appears in front of the person, providing options to interact with the vehicle, such as washing it. [0:04:51 - 0:04:53]: The person's right arm is raised, likely choosing an option from the menu. The view is focused on the front and side of the blue car. [0:04:54 - 0:04:55]: The person begins the action of washing the blue car. The individual is holding a yellow cleaning cloth and appears to be cleaning the hood area. The process bar for cleaning is visible in the interface. [0:04:56 - 0:04:57]: The person continues to clean the car methodically. The cloth is moving over the surface of the vehicle. A progress bar showing the cleaning status is displayed on the screen. [0:04:58 - 0:04:59]: The person in the reflective vest is seen from above, still cleaning the car. The interface still shows the cleaning options and progress status. There is also a small double-arrowed icon at the top of the screen. [0:05:00]: The person continues cleaning, with nearly half of the cleaning process complete. The progress bar shows the cleaning percentage increasing. The scene captures an enthusiastic expression from the individual at the top left corner of the screen.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:04:50",
        "answer": "C",
        "options": [
          "A. Walking towards a parking lot.",
          "B. Choosing an option from a contextual menu.",
          "C. Cleaning the blue car.",
          "D. Holding a cardboard box."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_285_real.mp4"
  },
  {
    "time": "0:07:00 - 0:07:20",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:05]: The scene is set in an outdoor area adjacent to a building with faded paint and visible wear. There is a man wearing a black hat, dark coat, white necktie, and dark shoes, standing near a black vehicle with the driver's door open. He appears to be engaged in conversation with another individual who is wearing a light green vest, gray shirt, and headphones. A dumpster and a ladder leaning against the building's wall can be seen in the background, along with other assorted objects and yellow bollards. [0:07:06 - 0:07:09]: The man in the dark coat slightly turns his head and body as he continues his conversation with the other person. The background and positioning remain similar, with the dumpster, ladder, and building still visible. [0:07:10 - 0:07:12]: The man in the dark coat seems to be emphasizing his point, as he gestures with his right hand. The other person remains in place, looking at him attentively. The ambient details stay consistent with prior frames. [0:07:13 - 0:07:17]: The video continues with both individuals maintaining their positions. The person in the black hat appears to be speaking, while the individual in the light green vest listens. The background elements, including the building, ladder, and vehicle, do not change substantially. [0:07:18 - 0:07:20]: The conversation seems to draw toward a close as the man in the dark coat stands casually, with both individuals seemingly reaching a point of mutual understanding. The position of the individuals, the vehicle, and the surroundings remain consistent with prior frames.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "How did the man in the dark coat emphasized his point just now?",
        "time_stamp": "00:07:16",
        "answer": "B",
        "options": [
          "A. By changing his position.",
          "B. By gesturing with his left hand.",
          "C. By raising his voice.",
          "D. By walking away."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_285_real.mp4"
  },
  {
    "time": "0:08:40 - 0:08:49",
    "captions": "[0:08:40 - 0:08:49] [0:08:40 - 0:08:44]: A man with blonde hair is wearing a reflective safety vest and headphones, standing in front of a black car on an industrial-looking property. He is looking into the driver's window of the car. In the background, there is a garage door, some scattered debris on the ground, and a concrete wall with various signs. Another person is visible inside the black car, with only part of their body seen through the window. [0:08:45 - 0:08:49]: The man in the reflective vest changes his position slightly but remains focused on the person inside the car. A \"No Parking\" sign is visible on the wall directly in front of the camera. An additional individual stands in the background near the wall. Above this scene is a small window displaying a person with blond hair and wearing large headphones, presumably streaming the video live with an active chat window on the screen showing various emojis and comments from the viewers.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the man with the reflective safety vest doing right now?",
        "time_stamp": "00:08:40",
        "answer": "B",
        "options": [
          "A. Adjusting his headphones.",
          "B. Looking into the driver's window of the car.",
          "C. Speaking with another individual near the wall.",
          "D. Picking up the scattered debris on the ground."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_285_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The scene starts with a colorful toy auto rickshaw visible on a white surface. The rickshaw is constructed with plastic building blocks in shades of yellow, orange, green, blue, and red. It has distinguishable features resembling an auto rickshaw, including a roof and wheels. The background is plain and white, and the text \"Auto Rickshaw\" is written in the top right corner in yellow. [0:00:02 - 0:00:02]: The toy auto rickshaw is no longer visible, leaving an empty white surface. [0:00:03 - 0:00:07]: A hand, positioned on the right side of the frame, places a blue, rectangular plastic building block with eight pegs on the white surface. The block is oriented horizontally. [0:00:08 - 0:00:10]: The hand remains in the frame, adjusting and pressing down on the blue building block to secure its position. [0:00:11 - 0:00:11]: The hand is briefly out of the frame, leaving the blue rectangular block alone on the white surface. [0:00:12 - 0:00:14]: The hand reappears, adding a yellow triangular block adjacent to the blue block, aligning it on top of the blue block's left side. [0:00:15 - 0:00:19]: Next, the hand starts constructing on top of the blue block by placing an orange rectangular block with four pegs. The hand firmly presses the orange block down to secure it, adjusting the position slightly to ensure a proper fit.  [0:00:19 - 0:00:19]: The hand adjusts the orange block one more time before the video ends.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What was written in the top right corner at the start of the video?",
        "time_stamp": "00:00:08",
        "answer": "B",
        "options": [
          "A. Toy Car.",
          "B. Auto Rickshaw.",
          "C. Building Blocks.",
          "D. Colorful Toy."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      },
      {
        "task_type": "Event Understanding",
        "question": "What sequence of actions happened after the blue rectangular block is placed on the surface?",
        "time_stamp": "00:00:19",
        "answer": "A",
        "options": [
          "A. The hand adds a yellow block, then an orange block, and adjusts the orange block.",
          "B. The hand adjusts the blue block, then adds a red block, and adjusts it again.",
          "C. The hand places a green block, then a yellow block, and adjusts the yellow block.",
          "D. The hand removes the blue block, places it again, and then adds a yellow block."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_210_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:02]: The video begins with a hand reaching towards a set of colorful interlocking plastic building blocks placed on a white surface. The blocks are arranged in uneven layers, predominantly showing yellow, orange, and blue colors. The hand holds a yellow block and begins to place it beside another yellow block on the surface. [0:01:02 - 0:01:04]: The hand aligns the yellow block firmly as it starts creating a larger structure. There are several orange blocks visible in the middle, surrounded by blue blocks. The background is plain and white, ensuring focus on the building blocks. [0:01:04 - 0:01:06]: The hand continues working with the yellow block, securing its position next to the previously placed one, enhancing the yellow section above the orange and blue blocks. [0:01:06 - 0:01:08]: The hand makes a slight adjustment to ensure the blocks are firmly attached. It then reaches out to pick up another yellow block. [0:01:08 - 0:01:10]: The hand picks up another yellow block and positions it over the existing structure. The lower layers of blue and orange blocks remain the same, acting as the foundation. [0:01:10 - 0:01:12]: The hand continues placing the block, aligning it carefully to maintain the structure's form. The blocks on the surface are starting to form a multi-leveled build. [0:01:12 - 0:01:14]: The hand releases the block, moving slightly to ensure it is securely in place. The background remains consistently plain and white. [0:01:14 - 0:01:16]: The hand reaches for another yellow block, repeating the previous actions to enhance the structure further. The newly added block is placed adjacent to the last one. [0:01:16 - 0:01:18]: The hand aligns the block carefully, making sure it fits well with the surrounding ones. The multi-colored foundation remains consistent beneath the yellow blocks. [0:01:18 - 0:01:20]: The hand securely positions the block and then moves out of the frame. The structure now appears larger, showing increments in yellow block placement above the foundational blue and orange blocks.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the predominant color of the blocks being placed by the hand right now?",
        "time_stamp": "00:01:06",
        "answer": "D",
        "options": [
          "A. Blue.",
          "B. Orange.",
          "C. Green.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_210_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:03]: A hand is arranging colorful interlocking building blocks on a plain white surface. The blocks are stacked to form a structure primarily consisting of blue, yellow, and orange blocks. The hand is placing a blue block onto the left side of the structure. [0:02:04]: After placing the blue block, the hand moves away momentarily, showing a completed segment of the structure with predominantly blue, yellow, and orange blocks. [0:02:05 - 0:02:07]: The hand picks up and begins to place a green block onto the structure, adding another layer to the existing arrangement of blocks. [0:02:08 - 0:02:09]: The green block is adjusted to sit correctly on the structure, aligning with the blue, yellow, and orange blocks. [0:02:10 - 0:02:11]: The hand moves away again, displaying the arrangement which now includes the added green block. [0:02:12 - 0:02:14]: The hand picks up another green block and starts positioning it onto the top of the left side of the structure. [0:02:15 - 0:02:16]: The new green block is placed and adjusted to fit well with the existing ones, creating a tiered effect on the left side. [0:02:17 - 0:02:19]: The hand moves back to reposition the newly placed green block, ensuring it is securely attached to the rest of the structure, with the last adjustments making the structure stable and complete.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What color is the block being placed on the left side of the structure right now?",
        "time_stamp": "00:02:02",
        "answer": "B",
        "options": [
          "A. Red.",
          "B. Blue.",
          "C. Green.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_210_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:03:50]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:02]: An arm is holding a yellow and red toy piece and placing it on a structure made of large, colorful toy blocks. The structure is predominantly composed of yellow, blue, orange, and green blocks and is positioned on a white surface against a light background. [0:03:03 - 0:03:06]: The hand continues to press the yellow and red toy piece onto the top of the block structure. Gradually, it appears to attach firmly. [0:03:07 - 0:03:10]: After securing the piece, the hand moves to adjust another section of the structure, which has a combination of yellow and red parts. This section is then slightly pushed down to secure its position. [0:03:11 - 0:03:12]: The structure is flipped to reveal the underside where more blocks are visible, mainly yellow and blue, along with some red parts. The structure is then turned upright again. [0:03:13 - 0:03:15]: The hand adjusts the structure, ensuring it is stable on the white surface. The block structure appears to be a vehicle, as it has wheels attached to the bottom. [0:03:16 - 0:03:17]: The block structure remains stationary on the white surface, showcasing its full form, which resembles a colorful toy vehicle. [0:03:18 - 0:03:19]: The hand returns and makes final adjustments, moving the vehicle slightly forward on the white surface. The vehicle remains brightly colored with green, yellow, blue, and orange blocks and red and yellow wheels.\n[0:03:40 - 0:03:50] [0:03:40 - 0:03:43]: A first-person perspective showing a colorful construction toy made from large interlocking plastic blocks. The toy is on a white surface. A hand is moving the toy slightly forward. The toy consists of yellow, blue, red, and green blocks and appears to be a small vehicle with a yellow front, blue bottom with red wheels, and a green top. The hand is positioned on the top of the toy. [0:03:44 - 0:03:45]: The hand is no longer in the frame, and the toy is stationary on the white surface. [0:03:46 - 0:03:49]: The video transitions to a bright orange screen with black doodle-like lines at the top and bottom. White text in the center reads, \"Thanks for watching!\" Below that, in smaller white text inside a black outlined box, is the message, \"DON'T FORGET TO SUBSCRIBE!\"",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the block structure identified as?",
        "time_stamp": "00:03:17",
        "answer": "B",
        "options": [
          "A. A house.",
          "B. A vehicle.",
          "C. A spaceship.",
          "D. A robot."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_210_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which company's logo is visible on the advertising board to the right?",
        "time_stamp": "00:00:05",
        "answer": "B",
        "options": [
          "A. Pirelli.",
          "B. AsahiKASEI.",
          "C. Shell.",
          "D. Goodyear."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_266_real.mp4"
  },
  {
    "time": "[0:01:03 - 0:01:08]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What colors primarily dominate the gloves of the driver right now?",
        "time_stamp": "0:01:03",
        "answer": "C",
        "options": [
          "A. Red and blue.",
          "B. Black and blue.",
          "C. Black, white and red.",
          "D. Red and green."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_266_real.mp4"
  },
  {
    "time": "[0:02:06 - 0:02:11]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What number is currently displayed on the central screen in the car?",
        "time_stamp": "0:02:09",
        "answer": "A",
        "options": [
          "A. 3.",
          "B. 4.",
          "C. 5.",
          "D. 6."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_266_real.mp4"
  },
  {
    "time": "[0:03:09 - 0:03:14]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which brand's logo is visible on the bottom part of the steering wheel right now?",
        "time_stamp": "0:03:12",
        "answer": "B",
        "options": [
          "A. Logitech.",
          "B. Fanatec.",
          "C. Sony.",
          "D. Microsoft."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_266_real.mp4"
  },
  {
    "time": "[0:04:12 - 0:04:17]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the current predominant color of the markings on the car's dashboard gauges?",
        "time_stamp": "0:04:12",
        "answer": "D",
        "options": [
          "A. Blue.",
          "B. Green.",
          "C. White.",
          "D. Red."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_266_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:00:10",
        "answer": "B",
        "options": [
          "A. The individual brewed a fresh cup of espresso and served it to a customer.",
          "B. The individual steamed milk using an espresso machine, frothing it to create foam.",
          "C. The individual prepared a cappuccino by adding steamed milk and coffee.",
          "D. The individual cleaned the espresso machine and prepared it for the next use."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_371_real.mp4"
  },
  {
    "time": "[0:01:57 - 0:02:07]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "0:02:07",
        "answer": "C",
        "options": [
          "A. The individual brewed a cup of espresso and added a stencil for decoration.",
          "B. The individual cleaned the espresso machine and prepared to shut down the station.",
          "C. The individual prepared a cappuccino, added a stencil design.",
          "D. The individual organized the counter, placed cups and lids in position, and cleaned up the area."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_371_real.mp4"
  },
  {
    "time": "[0:03:54 - 0:04:04]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions just shown?",
        "time_stamp": "00:04:04",
        "answer": "D",
        "options": [
          "A. The individual adjusted the settings on a coffee machine, printed a receipt, and placed an order for a beverage.",
          "B. The individual cleaned the counter, arranged cups, and made the station more organized.",
          "C. The individual steamed milk, brewed coffee, and served a cappuccino.",
          "D. The individual adjusted coffee machine settings, added coffee to cups while organizing printed receipts."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_371_real.mp4"
  },
  {
    "time": "[0:05:51 - 0:06:01]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the content just shown?",
        "time_stamp": "00:06:01",
        "answer": "D",
        "options": [
          "A. The individual brewed a fresh pot of coffee and served it to a customer.",
          "B. The individual steamed milk, brewed coffee, and arranged cups for service.",
          "C. The individual cleaned the counter, organized receipts, and served a latte.",
          "D. The individual prepared a latte, added a design to the foam, and sealed the cup with a lid."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_371_real.mp4"
  },
  {
    "time": "[0:07:48 - 0:07:58]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:08:04",
        "answer": "B",
        "options": [
          "A. The individual adjusted the coffee machine settings, brewed a fresh cup of coffee, and served it to a customer.",
          "B. The individual cleaned the coffee portafilter, filled another portafilter with fresh coffee grounds.",
          "C. The individual steamed milk, added it to a cup, and placed it on the counter for serving.",
          "D. The individual cleaned the glasses, organized the workspace."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_371_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why is the character startled?",
        "time_stamp": "00:00:11",
        "answer": "C",
        "options": [
          "A. Because the character is afraid of the dark.",
          "B. Because the character finds the teddy bear.",
          "C. Because a person enters the scene unexpectedly.",
          "D. Because the flashlight stops working."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_244_real.mp4"
  },
  {
    "time": "[0:02:09 - 0:02:39]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why is Mr. Bean holding two iron pots in his hands?",
        "time_stamp": "00:02:31",
        "answer": "C",
        "options": [
          "A. Because Mr. Bean is preparing to cook a meal.",
          "B. Because Mr. Bean was trying to catch a mouse.",
          "C. Because Mr. Bean wanted to wake up the old lady who was sleepwalking onto his bed.",
          "D. Because Mr. Bean thought they would make a funny noise."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_244_real.mp4"
  },
  {
    "time": "[0:04:18 - 0:04:48]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the person enter the building to inquire about rooms for rent?",
        "time_stamp": "0:04:33",
        "answer": "D",
        "options": [
          "A. Because he is looking for a place closer to his workplace.",
          "B. Because he needs a temporary place to stay while his house is being renovated.",
          "C. Because the person have no money.",
          "D. Because he was evicted by his previous landlord."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_244_real.mp4"
  },
  {
    "time": "[0:06:27 - 0:06:57]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why do landlords now take the money and run away?",
        "time_stamp": "0:06:46",
        "answer": "B",
        "options": [
          "A. Because they are in a hurry and promise to return later.",
          "B. Because she is a fraud, the house is fake.",
          "C. Because they misunderstood and thought the tenant didn't need a receipt.",
          "D. Because they were threatened by someone and had to leave quickly."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_244_real.mp4"
  },
  {
    "time": "[0:08:36 - 0:09:06]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Who is the person in the video now?",
        "time_stamp": "00:08:49",
        "answer": "D",
        "options": [
          "A. It's Mr. Bean's twin brother.",
          "B. It's a character from a different show.",
          "C. It's a lookalike impersonating Mr. Bean.",
          "D. It's Mr. Bean in disguise."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_244_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins with a person running on a track, captured from the front. The runner is wearing sunglasses, a blue tank top, and black shorts, with a lush green park and trees in the background. The sky is clear, and the track is reddish-brown with small orange cones arranged on the side.  [0:00:03 - 0:00:04]: The scene transitions to an indoor gym. The same person, still in their running attire, is performing box jumps on a red plyometric box with the label \"16\" on it. A large American flag hangs on the wall in the background. [0:00:05 - 0:00:07]: The next segment shows the individual in the gym performing a barbell clean and press. He lifts a barbell from chest level to overhead, with other gym equipment and people in the background. The gym has bright lights and motivational posters on the walls. [0:00:08 - 0:00:09]: The text \"Part 1: Track Workout\" appears on the screen, set against a backdrop of the outdoor track. The sky is bright blue with a few scattered clouds, and the green grass contrasts with the track. Cones are spaced along the track. [0:00:10 - 0:00:14]: The video shows another individual running on the track, wearing a hat, a gray shirt, and black shorts with white accents. The runner is captured mid-stride from different angles, displaying smooth, controlled movements. The background remains the same with trees, grass, and a soccer goal visible. [0:00:15 - 0:00:19]: The video continues with the runner on the outdoor track, taken from various distances and angles. The runner maintains a consistent pace, with the park scenery and green vegetation providing a picturesque setting for the workout. The sky remains clear and bright.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What exercise is the individual performing in the gym after the box jumps?",
        "time_stamp": "0:00:07",
        "answer": "C",
        "options": [
          "A. Squats.",
          "B. Deadlifts.",
          "C. Barbell clean and press.",
          "D. Push-ups."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_145_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:24]: A person wearing a blue tank top, black shorts, and white sneakers is performing a lift inside a gym. The person is lifting a barbell with weights attached, holding it at shoulder height. Various gym equipment, such as weight plates and a bench, is visible in the background. An American flag is hanging on the wall alongside posters. The gym has a high ceiling with lights and a ceiling fan. [0:03:24 - 0:03:31]: The person lowers the barbell back to the floor, pauses, and then holds it at thigh level in a standing position. Multiple weight plates and gym equipment are visible around the area. There is a person in a tan shirt and red shorts in the background, walking across the gym toward the right. [0:03:31 - 0:03:35]: The person begins to lift the barbell again, bringing it back to shoulder level. Following this, the person lowers the barbell to the floor, steps over it, and begins to walk away from the lifting platform. [0:03:35 - 0:03:39]: The scene shifts to another part of the gym with red boxes stacked near an American flag on the wall. The person from earlier is seen in the right side of the frame, preparing to perform a box jump. They jump onto a red box and stand up straight upon landing. The lighting and layout of the gym are consistent with the previous frames.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color are the sneakers worn by the person performing the lift right now?",
        "time_stamp": "00:03:33",
        "answer": "B",
        "options": [
          "A. Black and yellow.",
          "B. White and black.",
          "C. Blue and white.",
          "D. Red and white."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the American flag located in relation to the person performing the box jump?",
        "time_stamp": "00:03:39",
        "answer": "C",
        "options": [
          "A. Directly behind him.",
          "B. To his left.",
          "C. To his right.",
          "D. In front of him."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_145_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:45]: A man is standing in a gym holding two weight plates. He is dressed in a blue sleeveless shirt, black shorts, and white athletic shoes. Behind him is gym equipment including a squat rack with a barbell and weight plates, and other gym apparatus. The floor is black matting, and there is also a green artificial turf area. The man begins by lifting the weight plates in an upward motion, performing a lateral raise. He continues this motion for several seconds. [0:06:46]: The man begins to walk towards the camera, carrying the two weight plates with both arms lowered by his sides. [0:06:47 - 0:06:49]: The view shifts to a low-angle shot showing the man setting up a barbell in a squat rack. He places himself under the barbell to get into position for squats. The setting is the same gym, with weights and gym equipment visible around, including more athletes seen in the background. [0:06:50 - 0:06:59]: The man proceeds to perform squats. The camera remains at a low angle, providing a clear view of the proper squat form, including the barbell resting on his shoulders and his legs bending at the knees. The environment remains constant, with the gym's lighting and equipment being consistent throughout the exercise.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the man who is lifting wearing right now?",
        "time_stamp": "00:07:00",
        "answer": "B",
        "options": [
          "A. Red sleeveless shirt, black shorts, and white athletic shoes.",
          "B. Blue sleeveless shirt, black shorts, and white athletic shoes.",
          "C. Green sleeveless shirt, black shorts, and white athletic shoes.",
          "D. Blue sleeveless shirt, red shorts, and white athletic shoes."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_145_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:11:00]",
    "captions": "[0:10:00 - 0:10:20] [0:10:00 - 0:10:01]: The scene takes place inside a spacious gym. Prominently displayed on the left are large red rectangular softboxes against a light gray wall. Above the boxes on the wall is a large American flag. The floor is partially covered with green artificial turf. [0:10:02 - 0:10:09]: A person wearing a blue sleeveless shirt and black shorts enters from the right side of the frame. He begins performing box jumps, landing with both feet on top of the red box with \"16\"\" written on it. The individual swings their arms upward during the jump for momentum. After landing, he steps down to the left side of the boxes. [0:10:10 - 0:10:13]: The individual walks out of the frame. The gym equipment remains, including the red boxes stacked against the wall and the American flag above. There is no movement in the frame during this period. [0:10:14 - 0:10:14]: The person re-enters the frame from the right side and prepares to jump again. [0:10:15 - 0:10:18]: He executes another box jump, landing on the top of the red box stack. After a moment, he repeats the action of stepping down off the box to the left side. [0:10:19]: The person begins leaving the frame while the red boxes and American flag are prominently displayed in the background.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action is the individual performing right now?",
        "time_stamp": "00:10:27",
        "answer": "C",
        "options": [
          "A. Running on the treadmill.",
          "B. Lifting weights.",
          "C. Performing box jumps.",
          "D. Stretching."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_145_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: In todays video; [0:00:03 - 0:00:05]: to make really really; [0:00:06 - 0:00:06]: subscribed; [0:00:07-0:00:09]: So for us to; [0:00:10]: Our build; [0:00:11-0:00:12]: a bunch of; [0:00:13]: Collect weird vine things; [0:00:14-0:00:17]: diamonds; [0:00:17 - 0:00:20]: And I don't actually know or a silk Some shears",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the focus of today's video as mentioned at the beginning?",
        "time_stamp": "0:00:20",
        "answer": "B",
        "options": [
          "A. Cooking recipes.",
          "B. Building something.",
          "C. Making a tutorial.",
          "D. Crafting objects."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_186_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:09]: The video begins with an aerial view of a large, partially constructed structure surrounded by a dense forest. The structure consists of a circular, zigzagging wall made of brown blocks, with an inner, incomplete rectangular building at the center. A few characters are visible, represented as small blocky figures, positioned near the walls of the structure. As the video progresses, the perspective remains at a high altitude, showcasing the expanding construction process of the structure. The brown blocks form additional walls, creating a more complete circular formation. The land around the structure is green and appears to be lush with vegetation. In the background, there is a mix of flat terrain and hilly areas. [0:03:10 - 0:03:13]: The video continues to follow the construction progress from a top-down view. The circular outer wall becomes more defined and nearly complete. The central rectangular structure remains apparent, yet unchanged. The green vegetation around the structure remains consistent, and characters continue to be visible, executing construction tasks on the outer walls.  [0:03:14 - 0:03:16]: The video highlights the final stages of the construction. The outer circular wall is entirely closed, and the inner space within the structure becomes more defined. The central rectangular structure remains constant, with a noticeable square opening in the middle of the space. The characters now appear on top of the circular wall, indicating the completion of the base structure. [0:03:17]: At this point, a close-up view shows two characters on top of the wall. One character is dressed in blue and seems to be actively working, while the other character, dressed in purple, stands nearby. The ground inside the structure is visibly coarse, resembling gravel. [0:03:18 - 0:03:19]: The video ends with a high-angle view of the now fully completed circular structure. The walls are sturdily built with stone and have a uniform height. The central square opening still remains, with the ground around it showing some green patches due to the surrounding vegetation. The dense forest and hilly terrain in the background remain visible, framing the impressive construction.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What material are the walls of the structure made from?",
        "time_stamp": "0:03:10",
        "answer": "C",
        "options": [
          "A. Wood blocks.",
          "B. Metal blocks.",
          "C. Brown soil blocks.",
          "D. Glass blocks."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_186_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:01 - 0:06:02]: The video shows an interior room made predominantly of wooden planks. Chests are positioned to the right, one on top of the other, and a workstation consisting of various Minecraft blocks stands in the left-center. The ceiling is also wooden with a plank design. A red and black block sits above a gray and white block, likely a furnace. The camera’s first-person perspective reveals a character holding a pickaxe. [0:06:02]: The next frame reveals a slight rotation to the right, now focusing more on the chests and workstation. An anvil is also visible to the left, a crucial tool in Minecraft for repairing and enchanting items. The player’s inventory bar, located at the bottom of the screen, shows various tools and items, including pickaxes and food items. [0:06:03]: The scene transitions to an upgrade screen. The upgrade interface displays two input slots and one output slot. Various items in the player's inventory, including a diamond pickaxe, a furnace fuel, gold ingots, and a netherite ingot, are visible. The text overlay reads, “So now what.” [0:06:04]: The player moves the cursor to highlight specific items in their inventory. The text overlay continues, “I am going,” indicating the player is about to perform an action or upgrade on an item. [0:06:05]: The cursor moves towards the diamond axe in the inventory. The diamond axe’s attributes are visible and show it has several enchantments: Efficiency IV, Unbreaking III, Silk Touch, and Mending. The text overlay just confirms the player's intention with, “is do.” [0:06:06]: The player scrolls through their inventory and highlights the diamond shovel. The shovel also has numerous enchantments: Efficiency IV, Unbreaking III, Silk Touch, and Mending. The text overlay reads, “my shovel.” [0:06:07]: The player selects the shovel, and it is moved into the upgrade slot along with a netherite ingot. The text overlay completes the action being described: “first.” [0:06:08]: The shovel's upgrade panel displays, indicating an upgrade to a netherite shovel. The efficiency, attack damage, and durability statistics are shown for the newly upgraded tool. The text overlay explains the importance: “because that,” indicating the player's reasoning. [0:06:09]: The view stays on the upgrade panel. The text overlay reads, “for some reason,” continuing the player's commentary. [0:06:10]: Clarity on the upgrading process is displayed with the text overlay reading, “hold on back.” [0:06:11]: The player confirms the upgrade of the shovel to netherite and the text overlay states, “The shovel was used the most.” [0:06:12]: The panel shows the upgraded shovel, highlighting its superior stats. The text overlay reads, “And oh,” indicating a realization or satisfaction. [0:06:13]: The cursor hovers over the newly upgraded netherite shovel, continuing with the text overlay: “I need to actually.” [0:06:14]: The same netherite shovel remains highlighted in the upgrade panel. The text overlay shifts to “Upgrade everything else that is nice.” [0:06:15]: Returning to the main room, the player turns left to head towards the door. The view shifts to the right, showing more of the wooden room interior. [0:06:16]: The player exits the house, now standing on a wooden deck. The scenic background includes trees, a hill, and a lake. The text overlay reads, “really good now.” [0:06:17]: The player’s avatar in the foreground details the character's armor and tools, turning slightly to face the camera directly. The text overlay reads, “Well, almost.” [0:06:18]: A close-up of the avatar, giving a clearer view of the character model and purple armor. The text overlay continues: “to find that.” [0:06:19]: The camera pans out slightly, capturing more of the environment, including a hill, a river, and some foliage. The text overlay states, “but yes.” [0:06:20]: The frame reveals a fuller view of the surroundings, focusing on the exterior of a wooden house, a small garden, and surrounding landscape.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What tool is the character holding in the first-person perspective right now?",
        "time_stamp": "00:06:01",
        "answer": "B",
        "options": [
          "A. A sword.",
          "B. A pickaxe.",
          "C. A shovel.",
          "D. An axe."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which enchantment is NOT on the diamond shovel?",
        "time_stamp": "00:06:06",
        "answer": "D",
        "options": [
          "A. Efficiency IV.",
          "B. Unbreaking III.",
          "C. Silk Touch.",
          "D. Fortune III."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_186_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:04]: The video begins with a view overlooking a vibrant and colorful structure, vividly painted with alternating bands of orange and yellow blocks, crowned with blue blocks. The structure features two tall towers on either side, connected by a fence of iron bars and surrounded by lush greenery and distant trees. As the perspective shifts slightly to the right, more of the surroundings come into view, revealing a landscape with patches of green and yellow flowers scattered across a grassy expanse. The scene is sunlit, bringing out the bright colors of the structure prominently. [0:09:05 - 0:09:07]: The camera viewpoint continues to move slightly to the right, and raindrops begin to fall, creating a soft pattering sound. The vibrant structure remains the focal point, with its towers and iron fence clearly visible. The landscape in the background includes dense greenery and mountainous terrain, but the rain starts to obscure the distant view. [0:09:08 - 0:09:11]: The rain intensifies, adding a layer of mist to the scene. The colorful structure remains in the center of the frame, but the view of the landscape becomes more limited due to the rain. Visibility is slightly reduced as the raindrops create a blurred effect on the lens. The smooth transition of the camera provides a continuous first-person perspective, enhancing the immersive experience. [0:09:12]: The camera angle moves slightly upward, showing more of the blue-topped towers against the gray, overcast sky created by the rain. The iron fence panels, painted in bold orange and yellow colors, continue to stand out strongly despite the deteriorating weather conditions. [0:09:13]: The video shifts focus to the vacant interior space below the colorful structures, filled only with a green blocky surface. Raindrops keep falling, indicating that the weather persists in this manner. [0:09:14 - 0:09:19]: The rain stops, and the scene transition reveals a clearer, bright day. The camera turns away from the structure, showing an expansive grassy field sprinkled with small yellow flowers and bordered by varying terrains. [0:09:15]: The focus shifts to another structure in a different location. The camera captures an orange and yellow blocky figure within the iron \"fence,\" giving a strong pictorial impact. The background reveals high mountain peaks, covered with patches of snow. [0:09:16 - 0:09:17]: The camera moves outwards, depicting intricate structures and a better view of the nested fences. The sunny weather casts sharp shadows, accentuating the vibrant colors of the blocks and the clear blue sky adorned with fluffy white clouds. [0:09:18]: More perspectives unfolding the complex patterns and designs of the colorful fortress. The bright skies and slightly moving clouds accentuate the visual allure. [0:09:19]: The camera continues the journey with a slight left turn showing the lush landscape and occasional trees while tracking above the grassy surface. The scene is serene and seems to embody a peaceful journey through the vibrant landscape. The emphasis on the terrain and geographical diversity enriches the scene’s visual appeal.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What colors are the blocks used in the structure shown right now?",
        "time_stamp": "0:09:04",
        "answer": "B",
        "options": [
          "A. Red, green and brown.",
          "B. Orange, yellow, blue and green.",
          "C. Blue and white.",
          "D. Purple and pink."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_186_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type of vehicle is visible from the cockpit view right now?",
        "time_stamp": "00:00:04",
        "answer": "D",
        "options": [
          "A. Helicopter.",
          "B. Paraglider.",
          "C. Hot air balloon.",
          "D. Airplane."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_432_real.mp4"
  },
  {
    "time": "[0:01:48 - 0:01:53]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the plane flying above now?",
        "time_stamp": "00:01:12",
        "answer": "D",
        "options": [
          "A. Above the islands.",
          "B. Above the field.",
          "C. Above the mountain.",
          "D. Above the lake."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_432_real.mp4"
  },
  {
    "time": "[0:03:36 - 0:03:41]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type of structure can be seen on the hilltop right now?",
        "time_stamp": "00:04:12",
        "answer": "D",
        "options": [
          "A. Lighthouse.",
          "B. Windmill.",
          "C. Observation Deck.",
          "D. A red and white tower."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_432_real.mp4"
  },
  {
    "time": "[0:05:24 - 0:05:29]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the dominant color of the fields visible right now?",
        "time_stamp": "00:05:26",
        "answer": "A",
        "options": [
          "A. Green.",
          "B. Yellow.",
          "C. Brown.",
          "D. Blue."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_432_real.mp4"
  },
  {
    "time": "[0:07:12 - 0:07:17]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What object is the pilot's right hand holding right now?",
        "time_stamp": "00:07:14",
        "answer": "D",
        "options": [
          "A. A map.",
          "B. A throttle.",
          "C. A compass.",
          "D. A joystick."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_432_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the man's shirt right now?",
        "time_stamp": "00:00:05",
        "answer": "D",
        "options": [
          "A. Blue.",
          "B. Red.",
          "C. Black.",
          "D. White."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_427_real.mp4"
  },
  {
    "time": "[0:02:13 - 0:02:18]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the man in the video holding right now?",
        "time_stamp": "00:02:14",
        "answer": "B",
        "options": [
          "A. A wrench.",
          "B. An air pump.",
          "C. A screwdriver.",
          "D. A flashlight."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_427_real.mp4"
  },
  {
    "time": "[0:04:26 - 0:04:31]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color are the man's shorts right now?",
        "time_stamp": "00:04:27",
        "answer": "C",
        "options": [
          "A. Black.",
          "B. Red.",
          "C. Blue.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_427_real.mp4"
  },
  {
    "time": "[0:06:39 - 0:06:44]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the main source of light illuminating the area right now?",
        "time_stamp": "00:06:42",
        "answer": "D",
        "options": [
          "A. Street lamp.",
          "B. Campfire.",
          "C. Flashlight.",
          "D. Tent light."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_427_real.mp4"
  },
  {
    "time": "[0:08:52 - 0:08:57]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the man setting up right now?",
        "time_stamp": "00:08:55",
        "answer": "A",
        "options": [
          "A. A tent.",
          "B. A table.",
          "C. A chair.",
          "D. A blanket."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_427_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: It has been; [0:00:01]: a long time; [0:00:03]: a let's play series; [0:00:04]: So; [0:00:05]: hopefully you guys; [0:00:06]: will end up enjoying this one; [0:00:07]: So... anyway; [0:00:08 - 0:00:15]: The perspective shows a blocky, pixelated landscape characteristic of the Minecraft video game. A game menu is briefly shown. The viewpoint is positioned facing out towards rolling hills covered in green blocks representing foliage, a large river, and various block formations creating the terrain. The scene then shifts to show dense tree cover ahead, with the player standing on what appears to be a cliff or ledge. As the camera pans across the landscape, it captures the extensive green terrain, bending river, and various elevations formed by the Minecraft blocks. Heart and hunger indicators appear at the bottom of the screen, confirming the first-person viewpoint within the game; [0:00:16 - 0:00:20]: Really; The person in the first-person camera begins interacting with the environment, punching at the blocky dirt and grass ahead.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What type of landscape is shown in the video?",
        "time_stamp": "00:00:15",
        "answer": "C",
        "options": [
          "A. Realistic forest and mountains.",
          "B. Cartoon-style desert.",
          "C. Blocky, pixelated landscape.",
          "D. Detailed urban cityscape."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the player standing as the scene shows dense tree cover ahead?",
        "time_stamp": "00:00:18",
        "answer": "B",
        "options": [
          "A. In a valley.",
          "B. On a cliff or ledge.",
          "C. Next to a river.",
          "D. Inside a cave."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_194_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:20 - 0:04:40] [0:04:20 - 0:04:24]: The initial frames show a serene water body, surrounded by steep rocky cliffs on both sides. The cliffs are textured with various shades of gray and are irregularly shaped, jutting out and creating a rugged landscape. The scene is lit by daylight, evidenced by the bright blue sky in the background, interspersed with a few clouds. The near edge of the water shows a slight ripple, indicating gentle movement. From the first-person perspective, the viewer's hands holding a series of tools and items are visible at the bottom center of the screen; these tools include a pickaxe, a shovel, a sword, and food items in the inventory bar. [0:04:24 - 0:04:27]: As the camera moves forward, the viewer approaches the edge where the water meets a rocky shore leading into a dark cave. The water remains calm, reflecting some light from the entrance of the cave. The surrounding rock walls are high, casting shadows into the cave. The inventory bar and player hand holding a block are still visible. [0:04:28]: The next set of frames transition to an upward view of the inside of a lush green tree canopy. The player's hand, holding an axe, is shown actively chopping down a tree trunk. Green leaves encompass the view, allowing fragments of the blue sky to peek through. The inventory bar indicates progression, reflecting new items obtained, including wooden logs. [0:04:29 - 0:04:32]: The frames highlight the chopping action from a different angle, displaying progress in felling the tree. The block of wood becomes a central focus, floating iconographically in mid-air as it detaches from the tree. The background remains filled with vibrant green foliage and glimpses of the sky. [0:04:32]: The player's first-person view shifts to standing on grassy terrain, adjacent to a dirt block, with distinctly outlined smaller patches of grass and dirt distinguishing the ground texture. The player's hand interacts with these blocks, demonstrating actions of placing or removing dirt.  [0:04:33]: The next scene shows a broader view of the surrounding environment with green grass, a few dirt patches, and towering trees. A notable characteristic is the presence of various man-made structures, such as a furnace and a chest, placed on the terrain, close to the water's edge. The clear sky and daylight illuminate the scene, emphasizing the texture and colors of the blocks.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What does the inventory bar change after the player chops the tree trunk?",
        "time_stamp": "0:04:29",
        "answer": "A",
        "options": [
          "A. No change has occurred.",
          "B. Losing items.",
          "C. Damaging tools.",
          "D. Gaining health."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_194_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:40 - 0:09:00] [0:08:41 - 0:08:42]: The scene is dark, with dim light illuminating a cave filled with greyish stone blocks. In the foreground, a character's inventory is visible at the bottom of the screen, displaying tools like a sword, pickaxes, a shield, and food items. The surroundings suggest a Minecraft game environment with jagged rock formations all around. [0:08:43 - 0:08:44]: The view slightly shifts forward, maintaining focus on the same cave setting. A central crosshair indicates the player's point of aim, and the illuminated path ahead is leading further into the dark cave. [0:08:45]: The player moves closer to what appears to be a waterfall cascading from an elevated position within the cave. The surrounding walls and floor continue to display the blocky, pixelated textures typical of Minecraft. [0:08:46]: The player pauses as the waterfall becomes more prominent. The cascading water falls into a pool at the bottom, its source not fully visible but presumed to be higher up within the cave network. [0:08:47]: A wider perspective of the cave is shown, with more of the waterfall visible now. The dark cavern walls stretch further into the background, enhancing the depth. [0:08:48]: The player looks upward, further revealing the vastness of the cave ceiling, where faint light sources provide minimal illumination. [0:08:49]: A portion of the screen goes dark, indicating perhaps a transition or an in-game environmental change such as entering a more shadowed area. [0:08:50]: The cave view brightens, now showing an adjacent area with more stone formations, punctuated by patches of light and the upper part of the waterfall. [0:08:51]: The perspective shifts to a different section of the cave, with the same grey stone blocks and shadowy ambiance. The player's directional focus changes, indicating movement within the cave. [0:08:52]: The scene shifts dramatically, revealing a broader view of the cave's entrance. Fresh green blocks, blue sky, and daylight are visible, contrasting the dark cave interior. [0:08:53]: The angle adjusts to show the bedrock floor with various items dropped, likely remnants of mining activities. The player seems to be preparing for an interaction or decision within the cave. [0:08:54 - 0:08:55]: The player holds a yellow bed, visible in the hotbar at the bottom of the screen, suggesting preparation for setting up a rest spot. The environment remains consistent with the dark, cave setting interspersed with lighter areas. [0:08:56]: The player moves to a higher vantage point, looking down at the flowing water which continues to highlight the cavern's verticality. [0:08:57]: The flowing water becomes more central as the player's view tracks the fall of the water, almost inviting a dive or further exploration downwards. [0:08:58]: The player's inventory shows a switch to an iron pickaxe, likely preparing for mining. The cave's consistent grey texture with sectional lights persists. [0:08:59]: The player is underwater, indicated by wave textures and decreased visibility. The inventory interface at the bottom can still be seen faintly through the water. [0:09:00]: Back above water, the player appears to exit the pool, now looking around a broader open space within the cave that has better visibility. [0:09:01]: Another shift happens as the player's inventory now shows a stone axe, and more of the cave is visible, suggesting further exploration. [0:09:02]: The player places the yellow bed on the cave floor, signaling a pause or planned rest in this section of the cave. Grey stone blocks form the ground and walls around. [0:09:03]: The player returns focus to their surroundings, looking at possible directions to explore further or to gather resources, indicative of ongoing spelunking activities. [0:09:04]: The inventory indicates the player switching to a stone sword, showing preparedness as movement continues to explore what appears to be an increasingly elaborate cave system. [0:09:05]: The player is seen expressing relief, indicated by the added text \"i'm so lucky,\" suggesting a moment of finding something valuable or narrowly avoiding danger within the cave.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What item is the player holding in his hand right now",
        "time_stamp": "0:09:11",
        "answer": "B",
        "options": [
          "A. A Pickaxe.",
          "B. A bed of white and yellow.",
          "C. A Shield.",
          "D. A red Bed."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_194_real.mp4"
  },
  {
    "time": "[0:13:00 - 0:14:00]",
    "captions": "[0:13:00 - 0:13:20] [0:13:01 - 0:13:05]: The video begins with a view of a partially constructed wooden house from a first-person perspective. The interior walls are made of oak planks, with the ceiling composed of log blocks and some spaces still open to the sky. The player is holding a crafting table in their right hand, as indicated by the inventory hotbar at the bottom. To the left is a shield, indicating the player is equipped with it;  [0:13:05 - 0:13:07]: Moving closer to what appears to be a wall under construction, the player switches from holding the crafting table to holding oak planks. The view is tilted slightly upwards, focusing on the upper part of the structure where the wall meets the ceiling;  [0:13:07 - 0:13:08]: The player places an oak plank block into what seems to be a gap in the wall, completing the section and transitioning the view towards the ground where more block placement could occur. The environment outside the open roof reveals a clear blue sky and some trees in the immediate vicinity; [0:13:08 - 0:13:10]: Redirecting focus downward and towards the left, the player hovers over a placed yellow bed adjacent to the wall. The ground inside the structure is made of grass with a few items scattered around, indicative of ongoing construction and organization; [0:13:10 - 0:13:11]: With a turn towards the right, the motion reveals a chest and a furnace positioned against a dirt wall. The chest is open, suggesting recent interaction. Behind the furnace, there's a clear view of a river outside the base, enhancing the scene with a sense of natural surroundings; [0:13:11 - 0:13:12]: The perspective zooms in on the furnace, which has a fire burning inside, denoting that it is in use, possibly for smelting items or cooking food. The player’s shield still remains in the lower left, representing readiness for any potential threats; [0:13:12 - 0:13:13]: The player’s view shifts back to the crafting table, showing an interface where items in the inventory are displayed. The player has various items like oak logs, oak planks, saplings, and tools such as a pickaxe and a sword, all organized in the inventory slots; [0:13:13 - 0:13:16]: Focusing on crafting, the player moves oak logs from the inventory to the crafting grid, converting them into oak planks. These planks appear in the resultant area in the crafting window, increasing the player's resources; [0:13:16 - 0:13:17]: The player continues crafting, this time placing multiple oak planks in the crafting grid in a specific arrangement to produce another item, enhancing building capabilities. The process is done methodically, showcasing the crafting mechanics in the game; [0:13:17 - 0:13:21]: Post crafting, the player exits the crafting interface and resumes focus inside the house. The camera turns slightly towards the right, keeping the bed and chest in view. The player now holds a different item, bread, denoting preparation to maintain health or hunger levels; [0:13:21 - 0:13:22]: The player then reverts to interacting with the wooden structure, holding an oak trapdoor which is placed to cover a small section in the wall, revealing a meticulous approach to detail in the construction of the base; [0:13:22 - 0:13:23]: Adjusting the trapdoor placement, the player ensures it fits snugly within the designated area, signifying completion of a minor yet vital building task. The trapdoor appears functional, blending seamlessly with the existing wooden theme of the house; [0:13:23 - 0:13:25]: A shift back outside showcases a view of the external environment where the dirt-covered hillside meets the wooden structure. The scenery is lush with greenery as the player moves around continually to integrate different areas of the house with the surroundings; [0:13:25 - 0:13:27]: Moving forward, the player circles around the house structure, pausing near another open section where additional trapdoors are being placed. The focus remains steady on ensuring every part of the house has a consistent look and functionality; [0:13:27 - 0:13:29]: From the outside, the player's view shifts to a completed wall section now adorned with stacked trapdoors, providing a finished look to that segment. The alignment and aesthetic choice of adding trapdoors hint at practical and design-driven decisions in building.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the player do after placing the oak plank block into the gap in the wall?",
        "time_stamp": "0:13:08",
        "answer": "B",
        "options": [
          "A. Turned towards a furnace.",
          "B. Opened the oak plank.",
          "C. Opened a chest.",
          "D. Placed a yellow bed."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_194_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What object is mounted in front of the pilot on the left side?",
        "time_stamp": "00:00:02",
        "answer": "C",
        "options": [
          "A. A map.",
          "B. A compass.",
          "C. A tablet.",
          "D. A camera."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_430_real.mp4"
  },
  {
    "time": "[0:02:06 - 0:02:11]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What type of footwear is the pilot wearing right now?",
        "time_stamp": "00:02:07",
        "answer": "C",
        "options": [
          "A. Sandals.",
          "B. Dress shoes.",
          "C. Sneakers.",
          "D. Boots."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_430_real.mp4"
  },
  {
    "time": "[0:04:12 - 0:04:17]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What object is the man in the gray shirt holding right now?",
        "time_stamp": "00:04:15",
        "answer": "C",
        "options": [
          "A. A backpack.",
          "B. A helmet.",
          "C. A purple bag.",
          "D. A clipboard."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_430_real.mp4"
  },
  {
    "time": "[0:06:18 - 0:06:23]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color are the man's sunglasses's reflection right now?",
        "time_stamp": "00:06:21",
        "answer": "B",
        "options": [
          "A. Red.",
          "B. Blue.",
          "C. Green.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_430_real.mp4"
  },
  {
    "time": "[0:08:24 - 0:08:29]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the predominant color of the buildings in the background right now?",
        "time_stamp": "00:08:29",
        "answer": "A",
        "options": [
          "A. Red.",
          "B. Blue.",
          "C. Green.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_430_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is located to the right side of the road with a blue sign?",
        "time_stamp": "00:00:04",
        "answer": "B",
        "options": [
          "A. Petrol Station.",
          "B. 24Hrs park.",
          "C. Bank.",
          "D. Restaurant."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_400_real.mp4"
  },
  {
    "time": "[0:02:12 - 0:02:17]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the car directly in front?",
        "time_stamp": "00:02:13",
        "answer": "A",
        "options": [
          "A. Yellow.",
          "B. Black.",
          "C. Green.",
          "D. Red."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_400_real.mp4"
  },
  {
    "time": "[0:04:24 - 0:04:29]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the traffic light now?",
        "time_stamp": "00:03:56",
        "answer": "B",
        "options": [
          "A. Red.",
          "B. Green.",
          "C. Yellow.",
          "D. Orange."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_400_real.mp4"
  },
  {
    "time": "[0:06:36 - 0:06:41]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the common texture on the road surface visible right now?",
        "time_stamp": "00:06:39",
        "answer": "D",
        "options": [
          "A. Dry.",
          "B. Cracked.",
          "C. Snow-covered.",
          "D. Wet."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_400_real.mp4"
  },
  {
    "time": "[0:08:48 - 0:08:53]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the bus line number seen right now?",
        "time_stamp": "00:09:00",
        "answer": "C",
        "options": [
          "A. M15.",
          "B. Q32.",
          "C. M42.",
          "D. B60."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_400_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:04]: A person is at a gas station, wearing a yellow safety vest and white eyeglasses. They have headphones on. Behind the person, there are cars and a lit building visible in the background. The person moves towards a gas pump on their right;  [0:00:05 - 0:00:06]: The person is facing the gas pump, holding the nozzle in one hand. They appear to be interacting with the pump, indicated by a green interaction menu overlay visible on the screen;  [0:00:07 - 0:00:08]: The person continues looking at the gas pump, holding the nozzle while additional text appears on the screen, likely continuing interaction with the pump;  [0:00:09]: The person stands still, still facing the gas pump while holding the nozzle. The environment around them stays the same;  [0:00:10 - 0:00:14]: The person continues standing by the gas pump. The interaction with the pump seems to proceed, as the person is still facing it;  [0:00:15 - 0:00:16]: The person's attention switches from the pump to a parked vehicle nearby, indicated by their movement towards it;  [0:00:17 - 0:00:19]: The person moves towards a brown van parked nearby, facing the rear side of the vehicle;  [0:00:20 - 0:00:21]: Standing by the rear side of the brown van, the person is interacting with the vehicle, while still holding the nozzle. The environment remains consistent with the previous frames, showing a busy gas station setup;  [0:00:22 - 0:00:24]: The person moves along the side of the van towards the front, interacting with the vehicle. The surroundings, including the gas station, poles, and cones, remain visible;  [0:00:25 - 0:00:26]: The perspective shifts slightly as the person continues to move, now towards the front passenger door of the van, which they appear to open;  [0:00:27]: The person partly gets into the van, visible through the driver's side window;  [0:00:28 - 0:00:29]: The perspective changes to an overhead view of the vehicle as the person is now inside. The entirety of the van is visible along with the gas station;  [0:00:30 - 0:00:35]: The van begins to drive away from the gas pump. The area around, including the station and other vehicles, remain in view. The building and underpass in the background become more visible;  [0:00:36]: The van drives through the gas station. The vehicle's movement is smooth, and the setting remains consistent with a city environment;  [0:00:37 - 0:00:40]: The vehicle exits the gas station area, moving towards a street. The station and other cars become less visible, and more of the street and surrounding underpass start to appear;  [0:00:41 - 0:00:44]: The van continues to drive on the street, passing under an overpass. The environment continues to display a city setting with various elements like sidewalks and street poles;  [0:00:45 - 0:00:48]: The van navigates through the street, maintaining a steady speed. The buildings and city aspects are still visible, with a slight view of parked cars and other structures;  [0:00:49 - 0:00:51]: The van is shown driving alongside the road, with the overpass and surrounding buildings visible in the frame. The movement indicates traveling through the cityscape;  [0:00:52 - 0:00:54]: The van moves towards a corner, seemingly navigating a turn. The environment remains primarily urban, with signs of city infrastructure like sidewalks and crosswalks;  [0:00:55 - 0:00:20]: The van continues its steady movement through the city road, approaching a street intersection. Various details of the street and buildings remain in the frame, indicating consistent urban surroundings.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:00:09",
        "answer": "A",
        "options": [
          "A. Interacting with a parked vehicle.",
          "B. Walking towards a gas pump.",
          "C. Driving away from the gas station.",
          "D. Holding the nozzle at the gas pump."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_291_real.mp4"
  },
  {
    "time": "0:02:20 - 0:02:40",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:23]: The video is from a first-person perspective showing two individuals standing in an alleyway. One person wears a high-visibility vest and ear protection, while the other wears a dark sweatshirt and sunglasses. They are near a large van with the word \"grim\" on it. [0:02:24 - 0:02:30]: The individual wearing the vest makes a few gestures with his hands while the other individual stands still, facing him. The environment is industrial, with concrete walls and a metal gate in the background. [0:02:31 - 0:02:33]: The individual wearing the vest continues gesturing, and both appear engaged in conversation. The second person remains stationary. [0:02:34 - 0:02:35]: The person in the vest looks more animated in their gestures, suggesting an ongoing conversation or explanation. The other person maintains their position. [0:02:36 - 0:02:39]: The two individuals remain in their positions, with the first person continuing to gesture with his hands. The van remains parked beside them. [0:02:40]: The person wearing the high-visibility vest turns to face the van as if preparing to move towards it, while the other person still stands in place.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person in the high-visibility vest doing right now?",
        "time_stamp": "00:02:12",
        "answer": "A",
        "options": [
          "A. Turning to face the van as if preparing to move.",
          "B. Standing still and listening.",
          "C. Walking towards the metal gate.",
          "D. Gesturing animatedly towards the concrete walls."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_291_real.mp4"
  },
  {
    "time": "0:04:40 - 0:05:00",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:43]: At the beginning of the video, the camera shows a first-person perspective of a nighttime scene on a city street. The street is mostly empty, with a few cars visible ahead. Overhead, there's a traffic sign detailing directions to \"SOUTH DOCKS\" and other locations. The perspective from inside a vehicle, showing a dashboard displaying various game-related indicators. There's also a smaller window in the top left corner displaying a live feed of the player sitting and wearing headphones, with various chat messages visible on the right side of the screen. [0:04:44 - 0:04:53]: The vehicle begins moving forward very slowly, as evidenced by the changing scenery and the dashboard indicators. The road is flanked by streetlights and buildings in the background, with the sky showing a dim post-sunset or dawn hue. Traffic is minimal, with only a car or two seen in the oncoming lane. [0:04:54 - 0:05:00]: As the vehicle continues moving straight, the chat messages keep updating rapidly on the right side of the screen. There are several emoji reactions in the chat. The in-game messages and menu options on the right remain visible, indicating that the player might be interacting with them. The vehicle's movements are smooth, with no sudden turns or speed changes evident. The streetlights continue to illuminate the road, and buildings and street infrastructure like barriers and poles remain consistently visible with more cars appearing on the road.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the player doing right now?",
        "time_stamp": "00:04:25",
        "answer": "A",
        "options": [
          "A. Moving the vehicle slowly forward.",
          "B. Turning the vehicle sharply.",
          "C. Speeding up rapidly.",
          "D. Stopping the vehicle completely."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_291_real.mp4"
  },
  {
    "time": "0:07:00 - 0:07:20",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:02]: The video begins with a first-person perspective, showing the viewer moving from behind a large black van. The sky is transitioning from sunset to dusk with an orange hue visible on the horizon, and streetlights are beginning to turn on in the background. [0:07:03 - 0:07:04]: The viewer starts running, moving past the van, towards a small group of people gathered near a building. The ground is made of concrete, and the area appears to be some sort of loading dock or industrial zone. [0:07:05 - 0:07:07]: The viewer approaches the group, consisting of three to four individuals, standing near the corner of a building made of brick and metal. One person in the group seems to be in a postal uniform, indicating his employment. [0:07:08 - 0:07:11]: The viewer comes up closer to the individual in the postal uniform, who is standing against the brick wall of the building. Two other individuals are observed casually standing or leaning nearby. [0:07:12]: The perspective shows a close-up of the postal worker's face and uniform. A text interface appears on the screen, displaying the name \"Frank Miller\" and options for interaction, underlining that this is an interaction in a video game. [0:07:13 - 0:07:17]: The viewer selects an option, and a larger interface appears overlaying the view. The interface is related to \"Trucking Progression,\" showing various delivery destinations and related information.  [0:07:18 - 0:07:20]: The interface details various delivery points such as \"Les Lumieres Courthouse,\" \"Highway Convenience Store,\" and related payout information. The entire display seems to provide an overview of possible in-game missions or tasks related to trucking. The video then continues to show this overlay, providing detailed statistics and mission criteria.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the viewer doing right now?",
        "time_stamp": "00:07:14",
        "answer": "A",
        "options": [
          "A. Interacting with an interface.",
          "B. Running towards a van.",
          "C. Standing next to a group of people.",
          "D. Observing the postal worker."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_291_real.mp4"
  },
  {
    "time": "0:09:00 - 0:09:08",
    "captions": "[0:09:00 - 0:09:08] [0:09:00 - 0:09:01]: The view is from a first-person perspective, looking at a monitor displaying a car driving simulation game. The video shows a blue compact car positioned at an intersection in a residential area with palm trees and streetlights. The in-game time appears to be dusk or dawn, with a reddish sky visible in the background. [0:09:01 - 0:09:02]: The car is now moving forward along the street. Houses with lights on are visible in the background, and there are sidewalks on both sides of the road. The car's headlights are illuminating the street in front. [0:09:02 - 0:09:03]: The car continues to drive straight, passing more houses and a parked car on the right side of the road. The environment remains consistent with residential features and some greenery. [0:09:03 - 0:09:04]: The car slows down and prepares to make a turn. A fenced yard and a house with illuminated windows are seen on the right side. The streetlights are now more prominent as the car curves slightly. [0:09:04 - 0:09:05]: The car stops on the road, and a broader view of the street intersection is visible. Multiple apartment buildings surround the junction, and a trash bin is placed near the sidewalk. [0:09:05 - 0:09:06]: The car is now making another turn, and the perspective changes to show more of the neighborhood. Low-rise buildings and parked vehicles become more visible. The street remains illuminated by the car's headlights. [0:09:06 - 0:09:07]: The car proceeds to drive down the street with a dumpster visible on the left side, and more residential buildings appear on both sides. The overall environment remains consistent, with illuminated windows and palm trees in the distance. [0:09:07 - 0:09:08]: The car continues straight, moving past more houses and parked cars. The camera captures a broad view of the surroundings, maintaining its focus on the car's movement along the street. The road remains clear, with streetlights illuminating the path ahead.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the car doing right now?",
        "time_stamp": "00:09:08",
        "answer": "A",
        "options": [
          "A. Driving around on the roads at night.",
          "B. Making a U-turn.",
          "C. Stopping at a crosswalk.",
          "D. Reversing into a parking spot."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_291_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:16",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_94_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:01:15",
        "answer": "A",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 1."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_94_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:32",
        "answer": "C",
        "options": [
          "A. 3.",
          "B. 4.",
          "C. 6.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_94_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:18",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 4.",
          "C. 3.",
          "D. 7."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_94_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:40",
        "answer": "B",
        "options": [
          "A. 2.",
          "B. 10.",
          "C. 9.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_94_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:00:23",
        "answer": "D",
        "options": [
          "A. Basic addition and subtraction.",
          "B. Algebraic expressions.",
          "C. Properties of equality.",
          "D. The order of operations rules."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_220_real.mp4"
  },
  {
    "time": "[0:01:59 - 0:02:29]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might be the next topic the speaker will address?",
        "time_stamp": "00:02:28",
        "answer": "B",
        "options": [
          "A. The order of operations for addition and subtraction.",
          "B. The highest priority operator symbols.",
          "C. The importance of following the PEMDAS rule.",
          "D. Practice problems involving exponents."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_220_real.mp4"
  },
  {
    "time": "[0:03:58 - 0:04:28]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:04:28",
        "answer": "C",
        "options": [
          "A. Simplify the expression.",
          "B. Subtract the results.",
          "C. Add the simplified terms.",
          "D. Multiply the results."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_220_real.mp4"
  },
  {
    "time": "[0:05:57 - 0:06:27]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:06:26",
        "answer": "B",
        "options": [
          "A. How to integrate functions.",
          "B. The operator symbols for 'Then' and 'last' precedence.",
          "C. Application of quadratic equations.",
          "D. Solving basic arithmetic."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_220_real.mp4"
  },
  {
    "time": "[0:07:56 - 0:08:26]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:08:07",
        "answer": "D",
        "options": [
          "A. The importance of parentheses in operations.",
          "B. The commutative property of multiplication.",
          "C. How to handle exponents in an equation.",
          "D. The order of operations between the multiplication sign and the division sign."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_220_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: A person stands in front of a stainless steel double-door refrigerator, which is centered against a white tiled wall. The individual wears a dark blue T-shirt and has short, blonde hair. Their hands are clasped comfortably in front of them, and they are looking directly at the camera; [0:00:01 - 0:00:02]: The individual begins to speak, gesturing slightly with their right hand while their left hand remains positioned near their abdomen. Their facial expression remains calm; [0:00:02 - 0:00:03]: Continuing to speak, the person's left hand now gesticulates while their right hand is lowered. They maintain direct eye contact with the camera; [0:00:03 - 0:00:04]: The individual smiles broadly, raising their left hand slightly as if emphasizing a point. Their posture remains relaxed and inviting; [0:00:04 - 0:00:05]: The person continues to engage with the camera, now clasping their hands together again while speaking; [0:00:05 - 0:00:06]: Raising both arms gently outward, the individual gives the impression of presenting or explaining something important. Their facial expressions are dynamic and engaging; [0:00:06 - 0:00:07]: The individual brings their hands together again, speaking and maintaining an enthusiastic expression. The white tiled wall and stainless steel refrigerator background remain unchanged; [0:00:07 - 0:00:08]: The person's posture relaxes slightly, with hands back in the clasped position. They continue to engage with the audience through direct eye contact; [0:00:08 - 0:00:09]: The individual gestures with their right hand while speaking, the left hand positioned lower, near the torso, emphasizing their points with clear expressions; [0:00:09 - 0:00:10]: Holding both hands together, the person appears to be emphasizing key points. They continue to look at the camera directly, maintaining a sincere facial expression; [0:00:10 - 0:00:11]: The individual raises their right hand, fingers forming a pinching gesture, possibly indicating a small or precise amount. Their expression becomes more instructional; [0:00:11 - 0:00:12]: The person resumes the stance with clasped hands at the front and continues speaking. The background remains consistent with previous frames; [0:00:12 - 0:00:13]: Their facial expression softens, and they briefly close their eyes, possibly for emphasis. Their hands remain together as they continue to address the camera; [0:00:13 - 0:00:14]: With a slight lowering of their hands, the individual maintains a calm and explanatory demeanor, speaking directly to the camera; [0:00:14 - 0:00:15]: The individual continues to smile slightly, their body language relaxed yet engaged as they communicate their message; [0:00:15 - 0:00:16]: Maintaining a consistent speaker's demeanor, the person appears focused and holds their hands together in front of them; [0:00:16 - 0:00:17]: The camera captures the individual standing with a composed posture. Their eyes continue to lock with the camera lens as they finish their statement; [0:00:17 - 0:00:18]: The screen transitions to a new image with the text \"RAMSAY in 10\" in bold red and white letters, and a man with folded arms wearing a white chef's coat appears to the right; [0:00:18 - 0:00:19]: The final frame maintains the same \"RAMSAY in 10\" graphic and the man in the chef's coat, with no visible changes.\n[0:00:20 - 0:00:40] \n[0:00:40 - 0:01:00] ",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What text appears on the screen right now?",
        "time_stamp": "0:00:19",
        "answer": "A",
        "options": [
          "A. \"RAMSAY in 10\".",
          "B. \"RAMSAY in 5\".",
          "C. \"COOKING in 10\".",
          "D. \"CHEF in 10\"."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_23_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:00 - 0:01:20] [0:00:00 - 0:00:03]: A pair of hands are seen holding a stainless steel bowl above a pot, scooping chopped yellow pieces with a spoon into the pot. The pot is placed on a kitchen stove, with a silver backsplash visible in the background. [0:00:04 - 0:00:07]: The kitchen's countertop is visible, showcasing a variety of items. From left to right, there are three small glass bowls, a large white onion, a wooden pepper grinder, and a tall glass bottle of oil. A woman in a black off-shoulder top is stirring the pot with a spoon while holding a small bowl. [0:00:08 - 0:00:11]: The woman, still in the same position, is seen speaking and stirring the contents of the pot with a spoon, while holding a small bowl in her left hand. Behind her are stainless steel refrigerators and a white-tiled wall. [0:00:12 - 0:00:16]: Close-up shots show a pair of hands holding a knife and cutting an onion into small, even cubes on a wooden cutting board. The knife moves back and forth, swiftly dicing the onion. [0:00:17 - 0:00:19]: The woman in the black top is back in the wider kitchen setting. She continues to dice the onion on the wooden cutting board, with the neatly arranged kitchen items still in place in the background.\n[0:01:20 - 0:01:40] \n[0:01:40 - 0:02:00] ",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What item is the woman holding while stirring the pot?",
        "time_stamp": "0:01:02",
        "answer": "D",
        "options": [
          "A. A wooden spoon.",
          "B. A glass bottle of oil.",
          "C. A pepper grinder.",
          "D. A small bowl."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_23_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:04]: In the first frame, a woman wearing a black off-the-shoulder top is standing in a kitchen in front of a counter. The counter has various kitchen items, including a large chopping board, an onion, several glass bowls, a pepper grinder, a bottle of olive oil, and a large pot on a stove. She is in the process of cutting something on the chopping board. The kitchen background consists of stainless steel appliances and white tiled walls. As the video progresses, she continues cutting the ingredients.  [0:02:04 - 0:02:07]: Moving to the next frame, she puts down the knife for a moment and looks towards the pot on the stove, appearing to be in contemplation or about to shift her focus. Her hands are seen resting on the counter. The kitchen setup remains consistent with the previous frames. [0:02:07 - 0:02:12]: Subsequently, she turns her gaze back to the pot on the stove and begins to interact with it, stirring the contents inside. Her attention is fully captured by the cooking process. The camera captures her scooping up some ingredients with a spoon and adding them to the pot. These scenes show her methodical process of cooking. [0:02:12 - 0:02:19]: Finally, the last few frames showcase her repeatedly stirring the contents in the pot. In these frames, she is more engaged with the cooking process and ensures everything is well-mixed. Towards the end, a close-up shot of the pot reveals its bubbling contents as she stirs with a spoon, providing a detailed view of the cooking process in the kitchen.\n[0:02:20 - 0:02:40] \n[0:02:40 - 0:03:00] ",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the woman wearing while she is cooking?",
        "time_stamp": "00:02:12",
        "answer": "A",
        "options": [
          "A. A black off-the-shoulder top.",
          "B. A white apron.",
          "C. A red shirt.",
          "D. A blue dress."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_23_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:04]: A first-person view shows a black frying pan being held over a stove. Inside the pan, diced onions and pieces of meat are being sautéed. The pan is being moved around to toss the ingredients. Beside it, there is a smaller frying pan with a sunny-side-up egg being cooked. [0:03:05 - 0:03:09]: The camera angle changes to a kitchen scene featuring a woman with long, dark hair, wearing an off-the-shoulder black top. She is standing behind a counter with various cooking ingredients and utensils laid out. A shiny metal pot is on the stovetop. She seems to be explaining something, gesturing with her hands. Her focus then shifts to arranging green vegetables on a cutting board. [0:03:10 - 0:03:14]: Close-up shots of the woman's hands show her opening a plastic container and taking out fresh, green stalks. She places them carefully on the wooden cutting board, aligning them neatly. A knife and other utensils are visible on the countertop nearby. [0:03:15 - 0:03:19]: The scene switches back to the woman, who starts chopping an onion on the cutting board. She uses a large chef’s knife to make precise cuts. The kitchen setup includes a pepper grinder, olive oil bottle, and several small bowls, creating an organized and clean cooking environment. She continues talking, likely giving instructions or explanations as she works.\n[0:03:20 - 0:03:40] \n[0:03:40 - 0:04:00] ",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is being sautéed in the black frying pan?",
        "time_stamp": "00:03:51",
        "answer": "D",
        "options": [
          "A. Diced onions and green vegetables.",
          "B. Sliced potatoes and pieces of meat.",
          "C. Diced onions and mushrooms.",
          "D. Diced onions and pieces of meat."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_23_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:02]: The scene starts with a black frying pan positioned on the stovetop, which contains diced onions or vegetables being sautéed. To the left of the pan is a shiny stainless steel pot. A hand appears at the upper section of the frame, seemingly adding an ingredient to the pan. [0:04:03 - 0:04:07]: The camera angle shifts to a wider view of a kitchen featuring white subway tile walls and a window with a pot of green plants on the nearby counter. A woman standing in front of the stovetop shakes pepper into the pot while her other hand handles the stovetop controls. Additional kitchen tools, a bottle of oil, and some ingredients are organized on the countertop.  [0:04:08 - 0:04:12]: The focus shifts back to the stovetop where another frying pan appears, this one containing an egg being fried. The egg whites are nearly set, while the yolk remains intact and bright yellow.  [0:04:13 - 0:04:15]: Returning to the wider kitchen view, the woman tends to the stovetop operations. She uses a spatula to stir the contents of the frying pan before adding an ingredient from a bowl. She occasionally looks down at her tasks. [0:04:16 - 0:04:18]: The woman lifts the frying pan at a slight angle and starts stirring it by shaking. She ensures the onions or vegetables inside are moved around consistently, likely for even cooking. [0:04:19]: The camera zooms in on the frying pan, capturing the contents as they are being sautéed. The pan is held over another burner, displaying the woman’s steady hand as she skillfully tosses the ingredients in motion.\n[0:04:20 - 0:04:40] \n[0:04:40 - 0:05:00] ",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the woman lift the frying pan at a slight angle and shake it?",
        "time_stamp": "00:04:18",
        "answer": "C",
        "options": [
          "A. To add more oil to the pan.",
          "B. To check if the contents are cooked.",
          "C. To ensure the ingredients are moved around for even cooking.",
          "D. To cool down the pan."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_23_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video starts inside a small, colorful room with wooden details. The floor is covered with a green, patterned carpet. The room has a diamond-patterned wallpaper in shades of red and beige. The window ahead shows a kitchen scene, and there are four brown posts with ropes creating a small barrier in the room. [0:00:01 - 0:00:07]: The video focuses on the window, providing a clear view of the kitchen on the other side. The kitchen has a white background and wooden cupboards, with various kitchen utensils, colorful cabinets, and shelves filled with items like bottles and boxes. The window frame is wooden and styled with carved details. [0:00:07 - 0:00:08]: The view starts shifting to the right side of the room. The red and beige diamond-pattern wallpaper continues, and a partially visible door appears. [0:00:08 - 0:00:12]: The view further shifts to the right, revealing more of the room. Now, decorative columns painted in red and brown stripes frame an entrance leading to another room. The video captures a group of people standing in the next room, which shares the same green patterned carpet. The second room has a counter with shelves decorated with various items, like figurines and screens showing information. [0:00:12 - 0:00:14]: The camera moves towards the entrance of the next room. More of the people can be seen—some of them are waiting or looking at the displays behind the counter. The ceiling in this room is wooden with round light fixtures. [0:00:14 - 0:00:16]: The view continues to move towards the counter, where several people are standing. The counter has wooden detailing and is attended by staff. The overall decor in the room continues with the diamond-patterned wallpaper and green patterned carpet. [0:00:16 - 0:00:17]: The camera pans left, moving back to the initial room and focusing on a different corner. This corner has a small shelf with unknown items displayed. [0:00:17 - 0:00:19]: The camera then moves closer to a fireplace that is located in the corner of the room. Above the fireplace is a framed picture of a character in a red mushroom-topped hat. The fireplace is built with brown stones and wooden carvings. [0:00:19 - 0:00:20]: The focus is on the fireplace and the picture above it, showing the details of the intricate wooden carvings and the cheerful character in the illustration, wrapped up by the green patterned carpet meeting at the fireplace’s wooden base.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the pattern of the wallpaper in the room?",
        "time_stamp": "0:00:20",
        "answer": "B",
        "options": [
          "A. Striped pattern in red and beige.",
          "B. Diamond pattern in shades of yellow ,orange and beige.",
          "C. Floral pattern in shades of red and green.",
          "D. Geometric pattern in shades of green and beige."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_321_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:45]: The scene shows an amusement park area with a theme resembling a video game. There are multiple levels made to look like grassy platforms with brown brick supports, reminiscent of a classic video game. Numerous people are congregated in front of the display. There are several elements scattered around, including green pipes labeled \"POW\" and characters that resemble those from a video game series, placed on the different levels. Two large umbrellas provide shade to some of the visitors, and there are ropes to organize the queue. [0:02:46 - 0:02:49]: The video perspective shifts slightly to the right, displaying more of the themed environment. Two people, one holding a child, are standing close to a castle-like structure with a stone facade. Visitors walk towards the entrance of the castle. Another green pipe labeled \"POW\" is visible along with a light post adorned with star decorations from the game theme. [0:02:50 - 0:02:54]: The focus moves towards the castle entrance, showcasing the stonework more clearly. Overhead, blue and yellow blocks can be seen suspended against a blue sky backdrop. The pathway leads deeper into the themed area, and the castle predominantly fills the frame. [0:02:55 - 0:02:58]: The view continues through the stone structure, with the camera moving inside. The interior reveals a wooden panel design with subdued lighting, enhancing the themed atmosphere. Lamps mounted on the walls cast a warm glow over the stone and wood surfaces. Stairs lead upwards, following the interior pathway. [0:02:59]: The video culminates with a view up the staircase, following a person ascending the steps. The lamps along the walls continue to illuminate the path upwards into the themed environment of the amusement park.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is visible on the green pipes in the amusement park area?",
        "time_stamp": "00:02:45",
        "answer": "C",
        "options": [
          "A. Coins.",
          "B. Stars.",
          "C. \"POW\".",
          "D. \"1UP\"."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_321_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:25]: The video starts with a first-person perspective of someone descending a set of stairs. The surroundings are designed to resemble a stone or brick cave, with walls made of large stones in brown, beige, and tan. The lighting is provided by large, lantern-style lamps hanging from the ceiling. The steps descend slightly to the right, with metal railings on either side.  [0:05:26 - 0:05:30]: As the person continues descending the stairs, they approach an opening that leads outside. The end of the stone passageway reveals a contrasting bright exterior. The outside area is colorful and has a theme reminiscent of a playful, cartoon-like environment, judging by the bright colors and playful structures visible. [0:05:31 - 0:05:39]: The person exits the stone passageway and enters a vibrant, crowded amusement park environment. To the left, large mushroom-shaped structures can be seen, and to the right, there's a large, castle-like structure with people gathered around. The ground is yellow and green, and various booths and attractions are visible. There are many visitors, some standing in line at various attractions. The atmosphere is lively and colorful, with the walls and decorations designed to resemble a whimsical, fantastical world.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the metal railings located while descending the stairs?",
        "time_stamp": "00:05:25",
        "answer": "C",
        "options": [
          "A. On the left side.",
          "B. On the right side.",
          "C. On both sides.",
          "D. At the bottom only."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_321_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: The video starts with a first-person view of what looks like an amusement park or attraction area with colorful, cartoonish designs. In the foreground, three people are approaching a white archway adorned with pink and white cloth, and a small fountain with a light blue lamp post is visible to the left. On the left side, a person in a red outfit is partially seen holding an umbrella. The ground is covered with artificial green grass. [0:08:02 - 0:08:03]: The view broadens to show more people in a queue on the left side. The queue is defined by red ropes held up by golden posts, and people are standing in a line waiting for their turn. Beside the line, there is another structure to the left that has colorful decorations and a few umbrellas providing shade. [0:08:04 - 0:08:06]: The camera pans slightly to the left, revealing a larger view of the attraction area. The surroundings are vividly colored, with various structures and elements that resemble features from a cartoon or video game landscape. Large mushroom-shaped structures are in the middle of the area, and a towering, multi-level structure with trees and clouds can be seen in the background. [0:08:07 - 0:08:09]: As the camera continues to move, a crowd of visitors is visible enjoying the area. The atmosphere is lively, with various groups of people taking photos, exploring the area, and queuing for different attractions. The tall structure in the center continues to be a prominent feature in the background, with its numerous levels and decorations. [0:08:10 - 0:08:11]: The camera view changes slightly, focusing more on the central multi-level structure. Numerous elements, such as trees and clouds, are part of the structure, making it look dynamic and vibrant. More people appear to be moving around and taking in the attraction. [0:08:12 - 0:08:13]: The scene shifts to reveal more details of the amusement park's features, including a red and white checkered structure and a towering, fortress-looking building with castle-like elements in the distance. The overall design is whimsical and eye-catching. [0:08:14 - 0:08:15]: The camera zooms in a bit closer to the central area, highlighting a large, menacing flower with sharp teeth sprouting from one of the structures. Additional details, such as colorful blocks and more decorations, become more evident, adding to the area's theme. [0:08:16 - 0:08:17]: The view moves slightly to reveal an \"EXIT\" sign pointing to the left. People are seen walking towards this exit, and the overall area maintains its vibrant, animated atmosphere. Families, groups of friends, and park staff are visible throughout. [0:08:18 - 0:08:19]: The final seconds capture the central structure prominently, with its various levels, colorful decorations, and animated elements like the sharp-toothed flower being focal points. The area remains busy with visitors, and the whimsical, cartoonish theme continues to stand out as the video concludes.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What does the \"EXIT\" sign point to in the amusement park?",
        "time_stamp": "0:08:17",
        "answer": "D",
        "options": [
          "A. Right.",
          "B. Up.",
          "C. Down.",
          "D. Left."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_321_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:43]",
    "captions": "[0:09:40 - 0:09:43] [0:09:40 - 0:09:42]: As the video begins, the viewer's perspective starts at the bottom of an upward-moving escalator, surrounded by a transparent curved glass structure that forms an arched roof revealing a clear blue sky. The escalator on the left features greenish steps, while on the right is a parallel stationary escalator. A large building wall is visible in the background sporting a significant logo and a large dinosaur mural. To the right of the stationary escalator, a row of plants is seen. [0:09:42 - 0:09:43]: As the viewer ascends, the parallel escalator and the large wall with the logo become more prominent. The plants on the right side remain visible.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What becomes more prominent as the viewer ascends the escalator right now?",
        "time_stamp": "00:09:43",
        "answer": "B",
        "options": [
          "A. The plants on the left side.",
          "B. The large wall with the dinosaur logo .",
          "C. The transparent curved glass structure.",
          "D. The parallel stationary escalator."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_321_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The scene begins with a first-person perspective of a road underneath an overpass. A blue car is driving straight down the road. In the top left corner, there is a small window showing a person reacting with excitement or surprise. They are wearing a headset and are in a room with various lights and decorations. The road is empty and the buildings on the sides are visible.  [0:00:04 - 0:00:05]: As the blue car continues forward, a motorcyclist appears on the right side of the road, crossing the car's path. [0:00:06 - 0:00:08]: The blue car strikes the motorcyclist, causing them to fall off their bike and land on the road. Debris, including the motorcycle, sprawls across the road. [0:00:09 - 0:00:12]: The car stops immediately after hitting the motorcyclist, who is now lying motionless on the road. The person in the top left window is reacting with a mixture of surprise and amusement. [0:00:13 - 0:00:14]: The camera angle changes to an aerial view, showing the blue car stopped next to the fallen motorcyclist and the motorcycle’s debris. The driver seems to be looking around, assessing the situation. [0:00:15 - 0:00:17]: The car performs a slight maneuver, possibly reversing or turning to ensure it is not on top of the fallen motorcyclist or the debris. The reaction of the person in the top left window appears more animated and engaged. [0:00:18 - 0:00:19]: As the scene unfolds, a pedestrian begins to cross the street, approaching the intersection near where the car and the motorcyclist are located. The person in the top left window continues to react energetically to the unfolding events. [0:00:20]: The video concludes with the car still partially blocking the intersection, the fallen motorcyclist on the road, and the pedestrian continuing to cross the street. The reactions of the person in the top left window remain lively and expressive.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the blue car do just now?",
        "time_stamp": "00:00:08",
        "answer": "A",
        "options": [
          "A. It struck the motorcyclist.",
          "B. It started moving.",
          "C. It stopped at the intersection.",
          "D. It swerved to avoid debris."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_276_real.mp4"
  },
  {
    "time": "0:02:40 - 0:03:00",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:46]: The video shows a city street scene in what looks like early evening. Three men are in the center of the frame. The man in the middle, with his back to the camera, has blonde hair and is wearing a vertically-striped short-sleeve shirt and light shorts. He appears to be looking at his phone. To his left, a man with dark hair and glasses is kneeling down, tending to or looking at another man lying on the ground in front of them. The man on the ground is wearing a Hawaiian shirt and dark pants. Cars and buildings are in the background. [0:02:47 - 0:02:52]: The blonde-haired man continues to look at his phone while the man with dark hair checks on the person lying on the ground. The surrounding cityscape remains the same, with the cars and buildings in the background. [0:02:53 - 0:02:55]: The camera angle remains relatively constant, closely capturing the three individuals. The man in the Hawaiian shirt remains motionless on the ground. [0:02:56 - 0:02:59]: The blonde-haired man stands and observes his phone, while the dark-haired man maintains his attention on the man on the ground. The background, consisting of buildings and distant traffic, does not show significant changes. [0:03:00]: The focus is still on the three men, with the blonde-haired man standing and the other dark-haired man kneeling beside the person on the ground. The camera shows details such as the textures of their clothing and the pavement.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the blonde-haired man doing right now?",
        "time_stamp": "00:02:39",
        "answer": "A",
        "options": [
          "A. Looking at his phone.",
          "B. Kneeling beside the person on the ground.",
          "C. Talking to the man with dark hair.",
          "D. Walking away from the scene."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_276_real.mp4"
  },
  {
    "time": "0:05:20 - 0:05:40",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:26]: A man in a striped shirt and shorts stands on a road. Another man wearing a patterned shirt is walking near a parked scooter and a blue car. In the background, the street is flanked by large buildings and has an underpass with some greenery, and the sky is overcast. There are elevated roads overhead and traffic lights at the intersection. [0:05:26 - 0:05:30]: The man in the striped shirt stands still while the man with the patterned shirt moves closer to the scooter. Another man, in darker clothing, appears in the frame, walking away from the camera. [0:05:30 - 0:05:32]: The camera follows the movement of the man in dark clothes as he walks past the scooter. The man in the patterned shirt is now closer to the scooter. [0:05:32 - 0:05:34]: The view shifts slightly as the man in the striped shirt turns his gaze towards the man in dark clothes. Vehicles start to appear in the background, driving on the opposite side of the road. [0:05:34 - 0:05:38]: The man in the striped shirt continues standing at the intersection. Cars pass by, and the man in dark clothes has crossed the road. The man in the patterned shirt is stationary near the scooter. [0:05:38]: The man in the striped shirt starts moving forward as if following the man in dark clothes. [0:05:38 - 0:05:42]: The view changes direction, showing the man in the striped shirt walking towards the blue car and parked scooter. [0:05:42 - 0:05:46]: The man’s attention is directed towards the parked blue car. Another man with dark clothes appears again, walking across the road. [0:05:46 - 0:05:50]: The man reaches the car while the camera pans to the man in the patterned shirt who is interacting with the scooter. [0:05:50 - 0:05:56]: As the man in the striped shirt approaches the car, he looks at it closely. The man in the patterned shirt starts to position himself on the scooter as though preparing to ride it. [0:05:56 - 0:06:00]: The man in the striped shirt stands near the blue car, and the man in the patterned shirt is now seated on the scooter, looking towards the road. The man in dark clothes is visible in the background.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the man in the patterned shirt doing right now?",
        "time_stamp": "00:05:56",
        "answer": "A",
        "options": [
          "A. Sitting on the scooter, looking towards the road.",
          "B. Walking away from the scooter.",
          "C. Standing still near the scooter.",
          "D. Interacting with the blue car."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_276_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:05]: Two men are standing in a parking lot next to a blue car. One man is facing away from the camera with short blonde hair, wearing a striped shirt, while the other, in a blue jacket, approaches from the front. A large yellow truck with \"CLUCKIN' BELL 110% MEAT\" printed on it is parked behind a metal barrier in the background. [0:08:06 - 0:08:08]: The man in the striped shirt remains by the blue car, and the man in the blue jacket moves closer, appearing to be engaged in conversation. Industrial buildings and trailers are visible in the background. [0:08:09 - 0:08:12]: The camera focus shifts more towards the man in the blue jacket who seems to be gesturing with his hands while speaking. The man in the striped shirt continues to stand by the blue car. [0:08:13]: The focus briefly returns to the blue car and the man in the striped shirt. [0:08:14 - 0:08:16]: The perspective shifts to the back of the blue car, revealing both men still near it.  [0:08:17 - 0:08:19]: The man in the striped shirt begins to walk away from the blue car. In the background, there are people near a building labeled \"Post Op.\" [0:08:20]: The camera pans to follow the man in the striped shirt as he walks further into the lot.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the man in the striped shirt doing right now?",
        "time_stamp": "00:08:00",
        "answer": "A",
        "options": [
          "A. Walking away from the blue car.",
          "B. Standing by the blue car.",
          "C. Talking to the man in the blue jacket.",
          "D. Gesturing with his hands."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_276_real.mp4"
  },
  {
    "time": "0:10:00 - 0:10:19",
    "captions": "[0:10:00 - 0:10:19] [0:10:00 - 0:10:06]: The video showcases a first-person perspective of driving a car. The scene starts on a wide, deserted street with an overpass structure ahead and several palm trees lining the road. The sky is a combination of purple and pink hues, indicating early morning or late evening. The road appears to be cracked and worn, Suggesting the area might be old or less maintained. [0:10:07 - 0:10:10]: As the car moves forward, it veers slightly to the right while approaching a curved section. The surrounding environment includes more palm trees, streetlamps, and a mix of industrial and residential buildings. [0:10:11 - 0:10:19]: The perspective shifts inside the vehicle, showing the dashboard and steering wheel, which displays a logo, and the driver’s hands. The driver navigates through moderately empty streets. The setting sun casts long shadows on the pavement. Various buildings can be seen to the side, including stores and possibly houses, with some street signs and traffic signals visible ahead. The car maintains a steady pace as it travels through predominantly empty intersections and streets.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the driver doing right now?",
        "time_stamp": "00:10:19",
        "answer": "A",
        "options": [
          "A. Navigating through moderately empty streets.",
          "B. Adjusting the rearview mirror.",
          "C. Honking the horn.",
          "D. Switching on the headlights."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_276_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the older woman appear annoyed or angry?",
        "time_stamp": "00:00:26",
        "answer": "B",
        "options": [
          "A. Because the character shouted loudly.",
          "B. Because the character disturbed her.",
          "C. Because the character knocked on the door.",
          "D. Because the character broke something."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_248_real.mp4"
  },
  {
    "time": "[0:02:15 - 0:02:45]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the taxi drive off?",
        "time_stamp": "0:02:31",
        "answer": "C",
        "options": [
          "A. Because the passenger instructs the driver to leave.",
          "B. Because the person just got out of another vehicle.",
          "C. Because the person gets inside the taxi.",
          "D. Because the taxi has completed its repairs."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_248_real.mp4"
  },
  {
    "time": "[0:04:30 - 0:05:00]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why Mr. Bean can receive money now?",
        "time_stamp": "00:06:17",
        "answer": "A",
        "options": [
          "A. Because he impersonated a taxi driver and took the lady to her destination.",
          "B. Because he found a wallet on the street and returned it for a reward.",
          "C. Because he won a small lottery prize earlier in the day.",
          "D. Because he sold some of his belongings at a yard sale."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_248_real.mp4"
  },
  {
    "time": "[0:06:45 - 0:07:15]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why Mr. Bean can receive money now?",
        "time_stamp": "00:09:01",
        "answer": "A",
        "options": [
          "A. Because he impersonated a taxi driver and took the man to his destination.",
          "B. Because he found a wallet on the street and returned it for a reward.",
          "C. Because he won a small lottery prize earlier in the day.",
          "D. Because he sold some of his belongings at a yard sale."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_248_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:30]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why Mr. Bean now only pays 0.85 pounds?",
        "time_stamp": "0:09:19",
        "answer": "A",
        "options": [
          "A. Because Mr. Bean cleared his mileage before.",
          "B. Because he used a discount coupon for the purchase.",
          "C. Because he convinced the cashier to give him a special deal.",
          "D. Because he found some extra change in his pocket to cover the cost."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_248_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the displayed speed of the leading cyclist right now?",
        "time_stamp": "00:00:08",
        "answer": "C",
        "options": [
          "A. 40 km/h.",
          "B. 39 km/h.",
          "C. 38 km/h.",
          "D. 37 km/h."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_295_real.mp4"
  },
  {
    "time": "[0:00:41 - 0:01:01]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the position of the yellow sign and black arrow right now?",
        "time_stamp": "00:00:54",
        "answer": "D",
        "options": [
          "A. Directly behind the cyclists.",
          "B. On the right side of the road.",
          "C. Directly ahead of the cyclists.",
          "D. On the left side of the road."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_295_real.mp4"
  },
  {
    "time": "[0:01:22 - 0:01:42]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the position of the yellow traffic cones right now?",
        "time_stamp": "00:01:26",
        "answer": "B",
        "options": [
          "A. Directly in front of the cyclist.",
          "B. To the left of the cyclist.",
          "C. Directly behind the cyclist.",
          "D. To the right of the cyclist."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_295_real.mp4"
  },
  {
    "time": "[0:02:03 - 0:02:23]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is to the left of the cyclists right now?",
        "time_stamp": "00:02:23",
        "answer": "C",
        "options": [
          "A. Open farmland.",
          "B. Dense forest.",
          "C. Fence bordering trees.",
          "D. River."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_295_real.mp4"
  },
  {
    "time": "[0:02:44 - 0:03:04]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the cyclist positioned in the lane right now?",
        "time_stamp": "00:02:56",
        "answer": "B",
        "options": [
          "A. On the far left side.",
          "B. On the left side of the lane.",
          "C. On the far right side.",
          "D. On the shoulder of the road."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_295_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins with the FOX SPORTS logo. The logo is in silver, bold typeface and is centrally positioned against a dark blue background with curved, layered graphical elements. A bright light glows from behind the \"O\" in \"FOX\", creating a lens flare effect that shifts slightly in intensity and position. [0:00:03 - 0:00:05]: A football player wearing a white jersey with the number 26 and several logos stands on the field. He appears focused, with a serious expression, looking forward. The crowd in the background is slightly blurred, emphasizing the player.  [0:00:06 - 0:00:10]: The player wearing number 26 is now in motion, running towards the football placed on the ground for a penalty shot. He swings his right leg back in preparation to kick. The goalpost, goalkeeper, and a lively crowd filled with red-clad fans are visible. [0:00:11 - 0:00:14]: The goalkeeper, wearing a black jersey with the name \"DIOGO COSTA\" and the number 22 on his back, walks back towards his goal, looking attentive and composed as he prepares for the next play.  [0:00:15 - 0:00:18]: The video switches to a different scene showing the crowd. People in the crowd, wearing red and with expressions of exuberance, are cheering energetically, raising their arms, and clenching their fists. The vibrant colors of the crowd's attire and the background banners create a lively atmosphere. [0:00:19 - 0:00:20]: An overlay of the Euro 2024 trophy logo with colorful ribbons appears on the screen. The background displays fans continuing to celebrate energetically with smiles and raised fists.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color and number of the football player's jersey who is preparing for a penalty kick?",
        "time_stamp": "00:00:05",
        "answer": "D",
        "options": [
          "A. Red, 10.",
          "B. Blue, 14.",
          "C. Black, 22.",
          "D. White, 26."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What name and number are on the goalkeeper's jersey righ now?",
        "time_stamp": "00:00:14",
        "answer": "D",
        "options": [
          "A. COSTA, 26.",
          "B. RONALDO, 7.",
          "C. MESSI, 10.",
          "D. DIOGO COSTA, 22."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_8_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:20 - 0:01:40] [0:01:20 - 0:01:22]: Two fans are shown in the stands, cheering enthusiastically. The fan on the left is draped in a red scarf and is wearing a white shirt, while the fan on the right is wearing a jacket with a red and green design and has his arms raised in excitement. Both have exuberant expressions on their faces. [0:01:23]: A graphic of the tournament trophy appears with colorful segments radiating from it, transitioning between scenes. [0:01:24 - 0:01:25]: The camera shifts to a wide shot of the football field. The view is from behind the goal towards the corner of the field, showing the goalkeeper in a yellow uniform, a player in white preparing to take a penalty kick, and the referee standing nearby. The stadium is filled with spectators, and there are \"Booking.com\" advertisements around the perimeter of the field. [0:01:26 - 0:01:31]: The view remains centered on the penalty area. The player in white is seen preparing to take the shot, with the referee watching closely and the goalkeeper ready in position. Various angles capture the anticipation before the kick, and the scoreboard showing 1-0 in favor of Portugal is visible. [0:01:32 - 0:01:34]: A close-up of the player in a white jersey with the number 3 on it as he prepares for the penalty kick. The player appears focused, and the close-up emphasizes the intensity of the moment. [0:01:35 - 0:01:36]: The close-up shifts slightly, showing the player moving his head down as he takes a deep breath. His expression remains focused and determined, showing his concentration before the decisive moment. [0:01:37]: The player begins his run-up to take the penalty kick, leaning forward with intent. The background shows the crowd watching with bated breath. [0:01:38 - 0:01:39]: The wide shot of the penalty area returns. The player is in motion, about to strike the ball. The referee and goalkeeper are in their positions, and the crowd in the background is visible.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What advertisement is visible around the perimeter of the field right now?",
        "time_stamp": "00:01:25",
        "answer": "C",
        "options": [
          "A. \"Airbnb\".",
          "B. \"Expedia\".",
          "C. \"Booking.com\".",
          "D. \"TripAdvisor\"."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_8_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:41]: A football player is seen in a red uniform with a serious expression on his face, presumably preparing for an action on the field. The background shows a blurred crowd with a scoreboard displaying the scores. [0:02:42 - 0:02:43]: The scene shifts to an excited crowd of fans, waving flags and scarves, celebrating enthusiastically. One fan is wearing a red shirt with green trimming, holding up a green and red scarf. [0:02:44 - 0:02:46]: The scene changes to a football field where the player in red approaches the ball placed near the penalty spot. The goal and goalkeeper, clad in yellow, are in the background. [0:02:47 - 0:02:49]: The player kicks the ball towards the goal, and the goalkeeper dives in an attempt to save it. The ball is on its way towards the net. [0:02:50 - 0:02:52]: The scene cuts to the crowd again, showing a fan with face paint and a green and white shirt appearing anxious and then extremely joyful. Other fans around him are equally animated and celebrating. [0:02:53 - 0:02:56]: The footage returns to the football player in red, who is shown shouting or celebrating after the goal. The expressive celebration continues with the camera focusing on his face. [0:02:57 - 0:03:00]: The final cut shows another player in white, possibly preparing for a penalty kick, with the scoreboard displaying 2:0 in favor of Portugal. The referee and goalkeeper are in position, ready for the next kick with players and fans in the background.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What colors are present on the shirt of the fan holding the scarf?",
        "time_stamp": "0:02:43",
        "answer": "D",
        "options": [
          "A. Red with white trimming.",
          "B. Green with red trimming.",
          "C. Yellow with red trimming.",
          "D. Red with green trimming."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_8_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:03]: A player wearing a red jersey with the number 10, labeled \"Bernardo,\" is bent over on the field, adjusting the position of a soccer ball. The background shows an audience and banners. [0:04:04 - 0:04:05]: The camera shifts to a close-up of a goalkeeper wearing a yellow jersey, standing in front of the goal. The netting, along with some flags and spectators, is visible in the background. [0:04:06 - 0:04:07]: A group of players in red jerseys from the same team as Bernardo stand in a line with their arms around each other's shoulders. They are on the left side of the field, facing the action. [0:04:08 - 0:04:09]: The camera returns to a close-up of Bernardo walking forward, preparing for the penalty. He looks focused as the background shows a blurred glimpse of the crowd. [0:04:10]: A coach on the sideline, dressed in a dark suit, stands with his arm around an assistant wearing a white jacket. Both are observing the game with serious expressions. [0:04:11 - 0:04:12]: Bernardo is seen from behind, running up towards the ball to take the penalty kick, while the goalkeeper prepares to make a save. The crowd and stadium backdrop are clearly visible. [0:04:13]: Bernardo strikes the ball, and the goalkeeper dives to the left. The ball is in mid-air, heading towards the right side of the goal. [0:04:14 - 0:04:15]: The ball hits the back of the net, indicating a successful goal. Bernardo runs towards the left in celebration, while the goalkeeper remains on the ground. [0:04:16 - 0:04:17]: A close-up shows Bernardo embraced by a teammate in celebration near the goal. The scene captures the joy and excitement among players and spectators. [0:04:18 - 0:04:19]: Another player in a red jersey, smiling broadly and with open arms, runs towards the celebrating players. His facial expression shows joy as the team continues to celebrate the successful goal.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the outcome of Bernardo's penalty kick?",
        "time_stamp": "00:04:16",
        "answer": "D",
        "options": [
          "A. The ball hits the post.",
          "B. The ball goes out of bounds.",
          "C. The goalkeeper saves the ball.",
          "D. The ball hits the back of the net."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_8_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video begins with a muscular man standing against a black wall. He is wearing black shorts and black socks. The text \"HOW TO TRAIN\" appears prominently on the left side of the screen. An additional subheading, \"QUADS, HAMSTRINGS & GLUTES,\" appears beneath the main text. The man, facing the camera, starts with his arms down by his sides and gradually raises them into a posed stance, flexing his muscles. [0:00:04 - 0:00:05]: The man maintains a flexed pose, showing his muscular build. A white arrow appears pointing at his knee, indicating focus on that section. [0:00:06 - 0:00:09]: The man turns to his right, giving a side view of his body while continuing to flex. Additional arrows appear, indicating different muscles such as the hamstrings and quadriceps. [0:00:10 - 0:00:14]: The scene shifts to a gym setting. The man is now wearing a grey hoodie and is using a treadmill. He starts walking or jogging on the treadmill, which is shown from different angles. [0:00:15 - 0:00:17]: The man continues his warm-up in the gym. The text \"WARM-UP\" appears, followed by \"5 MINUTES TREADMILL OR STAIRMASTER\". The man is seen doing front-to-back leg swings and side-to-side leg swings using a gym rack for support.  [0:00:18 - 0:00:20]: The frame zooms in on the man, who is wearing a hooded sweatshirt while performing leg swings. The text \"FRONT-TO-BACK LEG SWINGS\" and \"SIDE-TO-SIDE LEG SWINGS\" appears, each followed by \"12 REPS EACH LEG.\" This segment outlines a gym workout routine focusing on lower body muscles, providing visuals of exercises and corresponding instructions.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What text appears prominently on the left side of the screen at the start of the video?",
        "time_stamp": "0:00:03",
        "answer": "A",
        "options": [
          "A. \"HOW TO TRAIN\".",
          "B. \"WORKOUT ROUTINE\".",
          "C. \"TRAINING SCHEDULE\".",
          "D. \"GYM TIPS\"."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Text-Rich Understanding",
        "question": "How many repetitions are suggested for each leg for both front-to-back and side-to-side leg swings?",
        "time_stamp": "0:00:20",
        "answer": "A",
        "options": [
          "A. 12 reps.",
          "B. 10 reps.",
          "C. 15 reps.",
          "D. 20 reps."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_149_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:01]: The video begins with a view of a man standing near a barbell loaded with 45-pound weight plates on either side, preparing for a deadlift. The man is wearing a dark gray t-shirt, black shorts, and knee-high socks. There is text overlay reading \"USE -50-60% OF YOUR DEADLIFT TOP SET WEIGHT.\" The setting is a gym with various weightlifting equipment visible in the background. [0:02:01 - 0:02:02]: The man turns around and begins to position himself near the barbell, which is placed on the ground in front of him. He checks his stance and prepares for the lift. [0:02:02 - 0:02:07]: The man adopts the starting position for a deadlift by bending down and gripping the barbell with both hands. His back is straight, and he is looking slightly forward. He begins to lift the barbell by straightening his legs and standing up. [0:02:07 - 0:02:09]: The man successfully lifts the barbell to an upright position, with the barbell resting at his hips. He maintains this position for a moment, ensuring his posture is correct. [0:02:10 - 0:02:12]: The man lowers the barbell back down in a controlled manner, bending his knees and hips while keeping his back straight. He prepares for the next repetition. [0:02:13 - 0:02:15]: The man repeats the deadlift motion, lifting the barbell to an upright position and then lowering it back down again. The entire movement is executed with proper form. [0:02:16 - 0:02:17]: He lifts the barbell back to an upright position once more. The gym environment remains consistent, with various equipment such as benches, squat racks, and dumbbells visible in the background. [0:02:18 - 0:02:19]: After completing another repetition, the man stands upright, resting the barbell on the ground momentarily. He looks around briefly before bending down to perform another set. The surroundings, lighting, and equipment remain the same, emphasizing the focused and consistent gym setting.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the man wearing during the deadlift exercise?",
        "time_stamp": "0:02:14",
        "answer": "B",
        "options": [
          "A. A white t-shirt and black shorts.",
          "B. A dark gray t-shirt and black shorts.",
          "C. A blue t-shirt and black shorts.",
          "D. A dark gray t-shirt and blue shorts."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_149_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:04]: A man in athletic attire is performing an exercise using a gym machine designed for hyperextensions. The gym is spacious and well-lit, with numerous pieces of equipment positioned systematically. The man is initially in a bent position with his upper body parallel to the machine seat and his legs anchored. His elbows are bent, and he gradually lifts and lowers his torso, engaging his back and hamstring muscles. Various pieces of gym equipment, including weights and machines, are seen in the background, along with other gym-goers engaged in their workouts. [0:04:04 - 0:04:09]: The man continues his exercise routine on the hyperextension machine, repeatedly lifting and lowering his torso. During this segment, a partially translucent yellow arrow overlays the frames, pointing in the direction of his movement. This addition emphasizes the exercise motion, directing attention to the lifting phase. He maintains a consistent rhythm, emphasizing proper form and control. [0:04:10 - 0:04:19]: The scene transitions to a different exercise focusing on \"Slow-Eccentric Leg Extension.\" The man is seated on a leg extension machine, and the captions at the bottom of the screen provide specific instructions: \"Exercise 5 of 7,\" \"Slow-Eccentric Leg Extension: 3 sets x 8-10 reps.\" The man performs the exercise by pushing the padded bar upwards with his shins and then slowly lowering it back to the starting position. The controlled upward and downward movements target the quadriceps. He is focused and methodical, ensuring each repetition is executed with precision. The gym's dark backdrop and organized arrangement of workout apparatus remain consistent.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the man doing during the \"Slow-Eccentric Leg Extension\" exercise?",
        "time_stamp": "00:04:19",
        "answer": "D",
        "options": [
          "A. Pulling the bar upwards quickly with his legs.",
          "B. Jumping off the machine.",
          "C. Swinging his legs back and forth.",
          "D. Pushing the padded bar upwards and lowering it slowly with his legs."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_149_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:07:50]",
    "captions": "[0:07:40 - 0:07:50] [0:07:40 - 0:07:41]: The video begins with a person standing against a black wall, positioned to the right side of a poster that reads \"The Ultimate Push Pull Legs System.\" The person is flexing their muscles with their arms bent, hands near their waist, and showing a front double bicep pose. The individual is wearing black shorts and black socks.  [0:07:42]: The poster is no longer visible, and the person's pose remains static, continuing to flex while standing in the same position. [0:07:43]: The person lowers their arms, and both hands come together in front of their body as if preparing for the next pose. Their legs and overall stance remain unchanged. [0:07:44]: The person now positions their arms in front of them, right elbow bent, clasping their left wrist, looking slightly downwards as if focusing on their arm muscles. [0:07:45]: The individual reverts to a relaxed stance with arms down by the sides, readying for another pose, looking straight ahead. [0:07:46 - 0:07:47]: The person returns to the flexed pose like earlier, this time flexing their biceps and looking off to the right side, displaying their muscle definition. [0:07:48 - 0:07:49]: The person continues holding the flexed pose, but their gaze shifts forward, maintaining the same body position and muscle flex.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is written on the poster next to the person right now?",
        "time_stamp": "00:07:41",
        "answer": "D",
        "options": [
          "A. \"The Ultimate Bodybuilding Guide\".",
          "B. \"The Ultimate Fitness Program\".",
          "C. \"The Ultimate Muscle Flex System\".",
          "D. \"The Ultimate Push Pull Legs System\"."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_149_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:06]: A close-up view shows three colorful discs placed on a smooth, light grey surface. The disks are circular with glossy finishes and have text on them in white, cursive script. The first disc on the left is red with some golden patterns, featuring the words \"SIX months.\" This disc is partially placed on top of a white hexagonal object which is underneath it. The second disc to the right of the red one is green with lighter areas and the words \"TWO months.\" The third disc, which is located below the first two, is purple with gold and light purple patterns and has the text \"NINE months.\" [0:00:07 - 0:00:10]: The text \"Alcohol Ink Baby Milestone Discs\" appears centered on a black background in a simple, white font. [0:00:11]: The screen is completely black. [0:00:12 - 0:00:16]: The text \"SOME OF THE MATERIALS YOU WILL NEED\" appears in a bold, uppercase font centered in the frame. Below this text, there is smaller text that reads \"*More information in description*\". The background is light pink with a decorative leaf pattern along the bottom edge. [0:00:17 - 0:00:19]: Two hands are holding a stack of white, round acrylic blanks against a light grey background. There are five discs in the stack, and the top one is slightly lifted to show the rounded shape and smooth texture. The nails of the person are painted in a shiny, brownish-red color. The text \"4” (10cm) Acrylic Blanks\" appears at the top of these frames, describing the objects being held.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is being shown just now?",
        "time_stamp": "00:00:18",
        "answer": "B",
        "options": [
          "A. Three colorful discs on a grey surface.",
          "B. Hands holding a stack of acrylic blanks.",
          "C. The phrase \"SIX months\" on a red disc.",
          "D. A light pink background with decorative leaves."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_51_real.mp4"
  },
  {
    "time": "0:01:20 - 0:01:40",
    "captions": "[0:01:20 - 0:01:40] [0:01:20 - 0:01:22]: A pair of hands, one with a pink sleeve and black gloves, holds a circular object that appears to be a transparent dish with swirling pink and white liquid. Three bottles of different colored liquids are placed at the top of a white sheet of paper; one gold-toned, one white, and one red. A black bottle cap is lying to the right of the paper. [0:01:23 - 0:01:26]: The dish is tilted slightly as a paintbrush applies a fresh layer of the pink liquid, enhancing the swirling pattern. The colors appear more vibrant as the brush moves across the surface. [0:01:27 - 0:01:29]: The pink liquid pool spreads as the brush continues to swirl gently around the surface. The hands adjust the angle slightly to ensure even coverage. [0:01:30 - 0:01:32]: The dish is held over the white paper, revealing more of the swirling pattern being formed. The hands reposition the paintbrush and start to mix in another color from one of the bottles. [0:01:33]: The hands apply another hue, introducing a faint red along with the existing vibrant pink and white, creating a complex pattern of swirling colors. [0:01:34 - 0:01:36]: The colors continue to blend harmoniously, and the paintbrush is dipped and swirled to enhance the design. [0:01:37 - 0:01:39]: The circular object becomes more vivid as additional strokes are added, further defining the intricate pattern emerging on its surface.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are the hands doing right now?",
        "time_stamp": "0:01:32",
        "answer": "C",
        "options": [
          "A. Holding a black bottle cap over the paper.",
          "B. Tilting the dish slightly and applying pink liquid with a brush.",
          "C. Mixing in another color from one of the bottles.",
          "D. Swirling the brush inside a gold-toned bottle."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_51_real.mp4"
  },
  {
    "time": "0:02:40 - 0:03:00",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:48]: A person wearing black gloves is holding a round clear disc in their left hand, and the disc is partially covered with purple ink. The disc is positioned at approximately a 45-degree angle towards the person. There are several splashes and blobs of purple ink spreading across the surface, mainly concentrated in the center. The hand's motion seems to be rotating the disc slightly. There are three bottles to the right of the disc: a dark purple one at the top, a gold one in the middle, and a white one at the bottom, all positioned vertically. On the left side of the workspace, a person’s left hand is resting, wearing a maroon knitted sleeve. [0:02:49 - 0:02:56]: The person continues to manipulate the disc with their left hand while also holding a paintbrush in their right hand. The brush is being used to spread and move the purple ink around the surface, creating smoother and more even coverage. The amount of ink on the disc increases, and its intensity of purple color grows. The bottles remain in their positions on the right side of the workspace. [0:02:57 - 0:02:59]: A close-up reveals the person using the brush to refine the purple ink application. The circular motions of the brush create fluid patterns within the ink. The ink on the disc starts to appear more uniform, with fewer areas of transparency. Additional white areas on the disc speak to the dabbing or mixing techniques being applied. The workspace remains largely unchanged with the three ink bottles still aligned vertically on the right side.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:03:00",
        "answer": "A",
        "options": [
          "A. Applying blue ink on a disc using a brush.",
          "B. Mixing different inks in the three bottles.",
          "C. Drawing circles on paper with a pencil.",
          "D. Spreading glue on a piece of paper."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_51_real.mp4"
  },
  {
    "time": "0:04:00 - 0:04:20",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:01]: A hand wearing black gloves and a pink sleeve holds a circular object filled with various shades of green ink on it. Three bottles of ink are placed on a white sheet beneath the object, with one bottle having a golden cap, the second a white cap, and the third an open bottle with no cap visible. [0:04:02 - 0:04:06]: The camera, aligned with the person’s perspective, captures a close-up view of a gloved hand gently applying colors onto a circular surface using a thin brush. The hand slightly tilts the object, showcasing the applied ink designs. The colors on the circular surface appear to be blending and creating new patterns with each brush stroke. [0:04:07 - 0:04:11]: The circular object is now adorned with golden-colored spots along with the green ink. The gloved hand appears to rotate the object slowly, revealing the shimmering effects of the gold ink under the light. The three bottles of ink on the white surface remain undisturbed. [0:04:12 - 0:04:15]: The hand, holding a brush with multi-colored ends, continues to work diligently on the circular object. The application of more golden ink is evident as the intensity and spread of the gold color increase. The green tones on the object contrast with the gold, creating an engaging dual-tone effect. [0:04:16 - 0:04:19]: The gloved hand remains focused on perfecting the piece, adding final touches with a cautious and steady brushstroke. The interplay of green and gold on the circular surface becomes more pronounced, giving a sense of intricate detailing and careful craftsmanship. No changes are noted in the position of the three ink bottles on the white sheet below the artwork.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:04:19",
        "answer": "C",
        "options": [
          "A. Rotating the circular object under the light.",
          "B. Cleaning the brushes with water.",
          "C. Adding final touches to the circular object with a steady brushstroke.",
          "D. Placing a golden cap on an ink bottle."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_51_real.mp4"
  },
  {
    "time": "0:05:20 - 0:05:40",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:21]: Two hands are holding a piece of transfer tape with the word \"Cricut\" printed on it. The hands are applying the transfer tape onto a white surface. Three circular disks are placed side by side above the hands. One disk is dark purple, another one is red, and the third one is green with light green and beige patterns. [0:05:22 - 0:05:23]: The right hand has now moved to grab a green disk with swirling patterns. The white surface and transfer tape are still visible below, with the other two disks at the top. [0:05:24 - 0:05:25]: The right hand holds a pick tool near the edge of the green disk, lifting some excess plastic. The white surface, transfer tape, and remaining two disks are still clearly visible. [0:05:26 - 0:05:29]: Using the pick tool held by the right hand, excess plastic around the green disk is carefully peeled back. The left hand holds the disk steady while the tool works its way around the edge. [0:05:30 - 0:05:32]: The plastic cover is almost completely removed from the green disk, revealing its full design. The transfer tape and other disks remain in the background. [0:05:33 - 0:05:36]: The clear plastic is fully peeled off the green disk, which is now prominently displayed with its swirling patterns clearly visible. The left hand rotates the disk to show its shiny, clean surface. [0:05:37 - 0:05:38]: The green disk is set down, and the left hand reaches for the red disk. The pick tool is still in the right hand, ready to be used on the red disk next. [0:05:39]: The left hand holds the red disk while the right hand prepares to use the pick tool to start lifting the plastic cover from the new disk. The green disk and the other items remain in the background.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the left hand doing right now?",
        "time_stamp": "00:05:40",
        "answer": "D",
        "options": [
          "A. Holding the pick tool.",
          "B. Peeling back the plastic cover from the green disk.",
          "C. Rotating the green disk.",
          "D. Holding the red disk."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_51_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What did the individual do just now?",
        "time_stamp": "00:00:14",
        "answer": "C",
        "options": [
          "A. The individual washed their hands, applied hand sanitizer, and dried their hands with a towel.",
          "B. The individual washed their hands, used a dryer, and sanitized the sink.",
          "C. The individual washed their hands, dried them with paper towels, and disposed of the towels.",
          "D. The individual prepared food, cleaned the sink, and disposed of trash."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_339_real.mp4"
  },
  {
    "time": "[0:01:18 - 0:01:28]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the content of the video just now?",
        "time_stamp": "00:01:28",
        "answer": "A",
        "options": [
          "A. The individual pressed and stretched pizza dough into a baking pan, ensuring it was evenly spread.",
          "B. The individual prepared a salad, adding various toppings and dressing.",
          "C. The individual assembled a sandwich, layering different meats and cheeses.",
          "D. The individual applied sauce and added cheese to a cooked pizza crust."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_339_real.mp4"
  },
  {
    "time": "[0:02:36 - 0:02:46]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:02:46",
        "answer": "B",
        "options": [
          "A. The individual washed and stored kitchen utensils in a cabinet.",
          "B. The individual opened a tray to reveal dough balls, adjusted them, and he took one out and put flour on it.",
          "C. The individual prepared a pizza by adding toppings and placing it in the oven.",
          "D. The individual chopped vegetables and placed them in a container."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_339_real.mp4"
  },
  {
    "time": "[0:03:54 - 0:04:04]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:04:04",
        "answer": "D",
        "options": [
          "A. The individual was preparing a sandwich, adding vegetables and condiments.",
          "B. The individual was slicing vegetables and arranging them on a platter.",
          "C. The individual was chopping toppings and placing them in separate containers.",
          "D. The individual was placing pepperoni slices on a pizza and then moving it aside."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_339_real.mp4"
  },
  {
    "time": "[0:05:12 - 0:05:22]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:05:20",
        "answer": "B",
        "options": [
          "A. The individual prepared a pizza by adding toppings and placing it in the oven.",
          "B. The individual was pressing and stretching pizza dough inside a baking pan.",
          "C. The individual was washing kitchen utensils and placing them in a drying rack.",
          "D. The individual was organizing ingredients in the refrigerator for future use."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_339_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:11]: A man in a white shirt with the number 10 and red shorts is walking on a soccer field. He gradually approaches a referee in a red shirt and black shorts, who is holding a soccer ball. The crowd in the background is large, with fans cheering and some wearing team colors and waving flags. Multiple security personnel in yellow vests are also visible near the stands. Around the stadium, various sponsors’ banners, such as Wanda and Vivo, are displayed prominently. [0:00:12 - 0:00:14]: A line of soccer players, wearing white shirts and red shorts, stand side by side facing away from the camera, looking towards the goal. One player in a white shirt with the number 20 is kneeling on the ground. All seem focused on the events on the field. [0:00:15 - 0:00:17]: From a wider angle, the same man in the white shirt with the number 10 appears to be preparing for a penalty kick. He is positioned on the penalty mark, facing the goalkeeper wearing a bright green kit. The referee stands to the side, holding the soccer ball. The audience watches from the stands in the background. [0:00:18 - 0:00:20]: Close to the goal, the player places the ball on the penalty mark, preparing to take the kick. The goalkeeper stands on the goal line, focused and ready. The referee observes the scene closely, ensuring the player is ready for the penalty kick. The vibrant crowd continues to watch attentively.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action is the player in the white shirt with number 20 doing?",
        "time_stamp": "0:00:14",
        "answer": "D",
        "options": [
          "A. Walking on the field.",
          "B. Preparing for a penalty kick.",
          "C. Waving to the crowd.",
          "D. Kneeling on the ground."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_6_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:25]: A football player in a white and red uniform stands on the grass near the penalty spot, preparing for a penalty kick. The goalkeeper, in a green uniform, stands in front of the goal. The crowd behind the goal is full of spectators, with some stewards in yellow vests positioned near the barriers. There are \"Kia Motors\" advertisements on the boards behind the goal. [0:02:26 - 0:02:28]: The camera shifts to a close-up of the crowd. A woman with emotional expressions has painted red marks on her face and is surrounded by other spectators who appear engaged with the game. [0:02:29 - 0:02:30]: The camera returns to the player on the field. The football player now appears focused, looking at the ball in preparation for the kick. The crowd in the background is still visible, and the yellow-vested stewards remain in position. [0:02:31 - 0:02:32]: The camera focuses again on the crowd, showing two men in red shirts with their hands clasped, seemingly in prayer or anticipation. [0:02:33 - 0:02:36]: The camera angle shifts to show a wider view of the field. The player, still standing near the ball, looks ready to take the penalty while the referee and the goalkeeper await. The stadium is elaborately decorated, and the full crowd can be seen. [0:02:37 - 0:02:39]: The perspective shifts to the view from behind the goal, showing the goalkeeper preparing for the shot. The netting of the goal is visible, and the lights of the stadium shine brightly. The player is seen in the distance, preparing to strike the ball.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What brand is advertised on the boards behind the goal?",
        "time_stamp": "00:02:25",
        "answer": "D",
        "options": [
          "A. Adidas.",
          "B. Nike.",
          "C. Pepsi.",
          "D. Kia Motors."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Event Understanding",
        "question": "What is the crowd's reaction during the player's preparation for the penalty kick?",
        "time_stamp": "00:02:33",
        "answer": "D",
        "options": [
          "A. Celebrating wildly.",
          "B. Leaving the stadium.",
          "C. Booing the player.",
          "D. Seemingly in prayer or anticipation."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_6_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:40 - 0:05:00] [0:04:40]: The video begins with the close-up view of a soccer player, wearing a black jersey with a checkered badge, walking with his head down and touching the back of his neck. The background shows a blurry crowd and some FIFA branding. [0:04:41 - 0:04:43]: The camera shifts to a wider view of a large soccer stadium packed with spectators. The focal point is on the penalty area, where a player in a black jersey with red socks stands near the penalty spot, preparing for a kick. [0:04:44 - 0:04:47]: The player in black moves slightly closer to the ball, getting ready to take the shot. The goalkeeper in a yellow jersey is positioned near the goal line, and there are additional figures behind the goal net. [0:04:48 - 0:04:49]: The player in black is now standing very close to the ball, tilting his body forward with one arm raised. The crowd in the background remains intensely engaged in the moment. [0:04:50 - 0:04:52]: The focus tightens on the scenario: the player in black jersey appears to have taken the shot, bending down. The goalkeeper, clad in a bright yellow uniform, is seen behind the goal line with arms outstretched. [0:04:53 - 0:04:54]: A close-up of the goalkeeper, who looks focused and is gesturing with his hands. He seems ready to react to the incoming ball. [0:04:56 - 0:04:57]: The goalkeeper, wearing the number '1' jersey, keeps his eyes on the ball, maintaining a defensive stance in the goal. [0:04:58 - 0:04:59]: Finally, the scene shows the goalkeeper from behind, standing firmly at the goal line with arms raised high, looking toward the field.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the player in the black jersey doing just now?",
        "time_stamp": "0:04:53",
        "answer": "D",
        "options": [
          "A. Raising both arms.",
          "B. Moving away from the ball.",
          "C. Running toward the crowd.",
          "D. Bending down to adjust the football."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_6_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:05]: A soccer player, dressed in a black and dark red uniform, is about to take a penalty kick. The goalkeeper, in a yellow uniform, stands in front of the goal preparing for the shot. The penalty taker begins his run towards the ball as the referee, dressed in red, observes near the penalty area. The goal's net and an advertisement banner are visible behind the keeper, and the crowd in the background intently watches the scene. [0:07:05 - 0:07:07]: The player strikes the ball towards the goal. The goalkeeper dives to his right side, fully extended, attempting to save the penalty. The ball is in mid-air heading towards the corner of the goal. The referee and audience continue to observe closely as the action unfolds. [0:07:07 - 0:07:09]: The ball narrowly misses the goal, and the goalkeeper starts to get up from his dive. The penalty taker reacts in disappointment while another player, wearing a dark-colored uniform, covers his face with his elbow in a gesture of disbelief or frustration. The crowd behind the goal shows a mixture of reactions - some cheering, while others are seemingly tense. [0:07:10 - 0:07:11]: In a close-up frame, a man in a black shirt inside the spectators’ area reacts with an excited expression. He is surrounded by other individuals who seem equally engaged in the match, showing varying displays of emotion. Another spectator pats him on the back in a congratulatory manner. [0:07:12 - 0:07:13]: The focus shifts to the stands where a man dressed in a red and white checkered shirt appears distraught. His expression suggests disappointment, probably aligning with the missed penalty. He holds his head and looks down as the crowd around him exhibits mixed emotions. [0:07:14 - 0:07:15]: Another man dressed similarly in red and white colors is seen with a noticeable look of concern. He has his hands on the sides of his face, possibly shocked at the penalty miss.  [0:07:16 - 0:07:19]: Returning to the field, the focus is back on the penalty taker, seen from behind, with his name and number (\"PIVARIC 22\") on his jersey. He is making another attempt at scoring as he runs up to kick the ball. The goalkeeper, again in yellow, prepares to defend the goal. The ball is kicked firmly towards the net, and the goalkeeper makes another dive to save it. The player watches anxiously as the ball heads toward the goal.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is written on the back of the goalkeeper's jersey?",
        "time_stamp": "0:07:05",
        "answer": "C",
        "options": [
          "A. SMITH 10.",
          "B. JONES 15.",
          "C. SCHEMEICHEL 1.",
          "D. MARTIN 8."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_6_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What brand of the car's equipment is visible right now?",
        "time_stamp": "00:00:04",
        "answer": "A",
        "options": [
          "A. Sparco.",
          "B. ACME.",
          "C. OMP.",
          "D. MOMO."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_252_real.mp4"
  },
  {
    "time": "[0:01:20 - 0:01:25]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What are the current colors on the car's dashboard right now?",
        "time_stamp": "00:01:23",
        "answer": "D",
        "options": [
          "A. Blue and yellow.",
          "B. Red and green.",
          "C. White and blue.",
          "D. Black and red."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_252_real.mp4"
  },
  {
    "time": "[0:02:40 - 0:02:45]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What equipment brand is visible on the driver's gloves right now?",
        "time_stamp": "00:02:43",
        "answer": "A",
        "options": [
          "A. Sparco.",
          "B. MOMO.",
          "C. OMP.",
          "D. HANS."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_252_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:04:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What colors are visible on the gloves the driver is wearing right now?",
        "time_stamp": "00:04:04",
        "answer": "D",
        "options": [
          "A. Red and black.",
          "B. Blue and white.",
          "C. Beige and black.",
          "D. White and black."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_252_real.mp4"
  },
  {
    "time": "[0:05:20 - 0:05:25]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What brand is visible on the driver's gloves right now?",
        "time_stamp": "00:05:23",
        "answer": "A",
        "options": [
          "A. Sparco.",
          "B. Simpson.",
          "C. Alpinestars.",
          "D. OMP."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_252_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:00:10",
        "answer": "B",
        "options": [
          "A. Prepared a cup of coffee, added milk, and served it with biscuits on the side.",
          "B. cleaned the coffee machine, and wiped down the counter.",
          "C. Cleaned a cup of coffee, added sugar and cream, and handed it to a customer.",
          "D. Assembled a coffee machine, ground coffee beans, and served coffee to a customer."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_370_real.mp4"
  },
  {
    "time": "[0:01:42 - 0:01:52]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:01:52",
        "answer": "C",
        "options": [
          "A. Filled a pitcher with almond milk, prepared a blender, and blended a smoothie.",
          "B. Poured milk into a jug, steamed it, and prepared a latte for a customer.",
          "C. Poured milk into a pitcher, cleaned the workspace, and dumped the used coffee grounds.",
          "D. Selected multiple milk alternatives, tasted each, and served samples to customers."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_370_real.mp4"
  },
  {
    "time": "[0:03:24 - 0:03:34]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:03:34",
        "answer": "B",
        "options": [
          "A. Poured water into a cup, added instant coffee, and stirred it.",
          "B. Poured espresso into a cup, added steamed milk, and created a latte art.",
          "C. Brewed tea, added sugar, and placed the cup on a tray.",
          "D. Poured espresso into a cup, cleaned the area, and served the drink without milk."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_370_real.mp4"
  },
  {
    "time": "[0:05:06 - 0:05:16]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:05:16",
        "answer": "C",
        "options": [
          "A. The individual poured milk into a cup, added sugar, and mixed it, then served the drink to a customer.",
          "B. The individual poured milk into an espresso shot, added syrup, and served a flavored coffee drink to a customer.",
          "C. The individual steamed milk, poured it into an espresso shot, and created a latte art before serving it.",
          "D. The individual brewed coffee, added cream and sugar, then served it to a customer."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_370_real.mp4"
  },
  {
    "time": "[0:06:48 - 0:06:58]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:06:58",
        "answer": "B",
        "options": [
          "A. The individual cleaned the coffee machine, refilled the water tank, and wiped the counter.",
          "B. The individual ground the coffee, tamped it, locked the portafilter, and started brewing a shot of espresso.",
          "C. The individual steamed milk, poured it into a pitcher, and created latte art.",
          "D. The individual packaged pastries, displayed them, and organized the counter."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_370_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:04]: The video starts with a man standing against a black background. He is shirtless, wearing black shorts, and is facing slightly to his left, with one hand resting on his head as if scratching it. He shifts his position to a more central stance, standing straight and looking forward, and begins a muscle pose by flexing his arms and chest. [0:00:05 - 0:00:07]: The scene changes to a collage of 20 small images, depicting the same man performing various exercises in a gym environment. Each small image is numbered from 1 to 20. [0:00:08 - 0:00:10]: The collage transitions to a tier list diagram with rows labeled from top to bottom as \"S,\" \"A,\" \"B,\" \"C,\" and \"D,\" colored in red, orange, yellow, and green, respectively. There is a blank gray section in the background. [0:00:11 - 0:00:12]: The scene shifts to an interior setting, where the same man is sitting at a desk with a laptop. He is talking into a microphone. Shelves with various objects, such as books, anatomical models, and decor, are visible behind him. [0:00:13 - 0:00:15]: The tier list diagram returns, now renamed, with an additional \"S+\" tier at the top and an \"F-\" tier at the bottom, which is depicted as engulfed in flames. [0:00:16 - 0:00:19]: The final segment returns to a gym setting, where the man is performing exercises with equipment. A list on the left side of the screen has points listed next to numbers 1, 2, and 3, with the bold text reading \"STRETCH & TENSION\" next to number 1.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What does the man do at the start of the video?",
        "time_stamp": "0:00:08",
        "answer": "A",
        "options": [
          "A. He is scratching his head.",
          "B. He is tying his shoes.",
          "C. He is drinking water.",
          "D. He is adjusting his watch."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "How is the tier list diagram colored?",
        "time_stamp": "0:00:10",
        "answer": "A",
        "options": [
          "A. Red, orange, yellow, and green.",
          "B. Blue, purple, yellow, and red.",
          "C. Green, yellow, orange, and red.",
          "D. Yellow, blue, green, and orange."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_148_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:03]: The video begins with an image displaying a ranking or grading system chart. The chart is divided into several horizontal bands with labels S, A, B, C, D, and F from top to bottom. A picture of a person performing a squat with a barbell on their shoulders is positioned in the top-left corner of the chart within the 'S' rank band. Below the chart, there are smaller images of the same person performing different exercises. [0:03:04 - 0:03:10]: The scene transitions to a gym environment where a person is performing a barbell front squat. The individual is wearing a blue shirt, black shorts, and black shoes. They are seen lifting the barbell from a squatting position to a standing position and then returning to the squat position. The background includes various gym equipment, such as weights and machines. The caption \"BARBELL FRONT SQUAT\" appears at the bottom of the screen. [0:03:11 - 0:03:17]: The person continues exercising in the gym, performing a sequence of front squats with the barbell. The camera angle changes slightly to provide a better view of the movement. The gym setting includes more visible details of the equipment and layout, including weight racks and a mirrored wall. [0:03:18 - 0:03:19]: The video transitions to a diagram comparing the biomechanics of a back squat and a front squat. The left half of the diagram illustrates the back squat with a red line sketching the movement and indicating specific angles and measurements. The right half shows the front squat in a similar manner. The diagrams include numerical data and labels highlighting different metrics and measurements associated with each squat type.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What text appeared at the bottom of the screen just now?",
        "time_stamp": "00:03:10",
        "answer": "C",
        "options": [
          "A. \"BARBELL BACK SQUAT\".",
          "B. \"DEADLIFT\".",
          "C. \"BARBELL FRONT SQUAT\".",
          "D. \"BENCH PRESS\"."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_148_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:11]: A man is exercising on a leg press machine at the gym. He starts with his knees bent as he pushes the platform on the machine away from him, extending his legs. He is wearing a blue shirt, black shorts, and black socks. The background includes various gym equipment such as free weights, benches, and a few people using other machines. The gym has a modern look with a monochrome color palette, and the overall ambiance is clean and organized. [0:06:12 - 0:06:16]: There is a close-up view of the man as he continues his exercise. His focused expression is visible, and details of the machine, like the black padding and silver framework, are clear. The man's beard and hairstyle are well-groomed, and he remains in a consistent rhythm. [0:06:17 - 0:06:18]: The video shifts to an image ranking grid, where pictures of the man performing different exercises are placed in different categories labeled S, A, B, C, D, and F. The grid appears to be evaluating the difficulty or effectiveness of various exercises. [0:06:19 - 0:06:20]: The final frame focuses on the man performing a horizontal leg press, with text at the bottom describing the exercise. He continues with the same attire and demeanor, placed in a well-equipped gym environment that appears professional and tidy.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the man using right now?",
        "time_stamp": "00:06:23",
        "answer": "D",
        "options": [
          "A. Treadmill.",
          "B. Dumbbells.",
          "C. Elliptical machine.",
          "D. Leg press machine."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_148_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:07]: In a well-lit gym with various exercise equipment and machines, a person wearing a dark blue shirt, black shorts, and black sneakers is seen performing a weighted squat. He is holding a dumbbell close to his chest with both hands. The gym has a high ceiling with numerous overhead lights, and there are large windows at the far end, allowing some natural light in. The person is positioned in the center of the gym, with exercise machines and weights surrounding him. The sequence starts with him standing upright holding the dumbbell and then squatting down to a low position and returning to a standing position. [0:09:08 - 0:09:14]: The person continues performing weighted squats while holding the dumbbell close to his chest. The camera angle shifts to his side, providing a view of his form and technique during the squats. The background remains consistent with the previous segment, showing the various exercise machines and equipment in the gym. [0:09:15 - 0:09:19]: The scene transitions to a different display, which seems to be ranking or categorizing various exercises or workout forms. The display features several small images of the person performing different exercises, organized in a tiered format labeled \"S\" through \"D.\" The focus remains on a large image of the same person in a squatting position, holding a dumbbell close to his chest. The categorization display continues to be shown from different angles, highlighting the organizational structure of the exercise rankings.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What exercise is the person performing in the gym right now?",
        "time_stamp": "00:09:14",
        "answer": "C",
        "options": [
          "A. Deadlift.",
          "B. Bench press.",
          "C. Weighted squat.",
          "D. Push-up."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_148_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: A person is seen from the back, wearing black shorts and a belt, walking down an aisle in what appears to be a gym, surrounded by various exercise equipment, including treadmills on the left side. [0:00:01 - 0:00:02]: The video transitions to a close-up of a muscular person with text overlaying the frames, reading \"JEFF NIPPARD PRESENTS\". The background shows more gym equipment and some windows. [0:00:03]: The same person is performing an exercise, lifting a weight plate in front of them while standing in the gym. The gym equipment and walls are visible in the background. [0:00:04]: The person removes a shirt to reveal their muscular physique. They are positioned in front of treadmills, possibly in the same gym as in previous frames. [0:00:05]: A close-up shot focuses on the person adjusting a lifting belt around their waist, probably preparing for an exercise. [0:00:06]: The person, now wearing a black shirt, is preparing to lift a barbell. A serious expression is visible on their face, emphasizing focus. The background includes large windows and gym equipment. [0:00:07]: The person, now shirtless again, is seen walking in the gym area, with treadmills to the left and other gym equipment on the right. The environment appears to be the same throughout the video. [0:00:08]: A close-up image of a person’s hands as they load weights (6.75 kg) onto a barbell. The gym equipment is visible in the blurry background. [0:00:09]: A muscular person performs a pull-up exercise in front of a black background, holding onto a pull-up bar with both hands. [0:00:10]: Another individual, dressed in a hoodie, is doing a stretch exercise in the gym. They have one leg raised and resting on a weight-lifting rack. [0:00:11 - 0:00:12]: The frame shows text overlaying a muscular person flexing their arms, with the text reading \"THE ULTIMATE PUSH PULL LEGS SERIES\". [0:00:13 - 0:00:16]: A person is seated at a desk, with a laptop, a globe, a microphone, and books nearby. They are talking and gesturing with their hands while looking at the camera. [0:00:17 - 0:00:19]: The person is performing walking lunges with dumbbells in each hand, moving down an aisle in the gym filled with various exercise machines and equipment.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What series is being presented according to the text overlay during the video?",
        "time_stamp": "0:00:15",
        "answer": "D",
        "options": [
          "A. \"THE ULTIMATE BODYBUILDING SERIES\".",
          "B. \"JEFF NIPPARD'S WORKOUT SERIES\".",
          "C. \"THE ULTIMATE GYM SERIES\".",
          "D. \"THE ULTIMATE PUSH PULL LEGS SERIES\"."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_144_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:04]: In a gym with multiple exercise machines and weights in the background, a person wearing a gray shirt, black knee supports, and dark shorts is performing squats in a power rack. They are holding a barbell across their shoulders and descend into a squat position, maintaining an upright posture. The gym is equipped with various weightlifting equipment, and the setting appears professional with proper lighting. [0:03:05 - 0:03:09]: The individual continues their squats, reaching the bottom of the squat and beginning to ascend. Their facial expression indicates the exertion involved. The power rack, barbell, and additional gym equipment remain prominent in the background, with the surroundings unchanged. [0:03:10]: The frame transitions to another exercise labeled as \"Exercise 2 of 6.\" The person is now preparing to perform a Romanian deadlift. They are positioned near a barbell on the floor, dressed similarly to earlier but with a different view that focuses more on the back and sides of the gym. [0:03:11 - 0:03:15]: The individual is executing the Romanian deadlift, lifting the barbell while keeping their back straight and knees slightly bent. The setting remains consistent, emphasizing the proper form for the exercise. The gym's detailed environment, with various equipment and neatly organized weights, continues to form the backdrop. [0:03:16 - 0:03:20]: The person is performing reps of the Romanian deadlift, returning the barbell to the ground and repeating the motion. The focus stays on their precise form and the consistency of their movements. The gym environment, with its modern equipment and clean organization, remains visible throughout the sequence, showcasing a professional workout setting.",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many exercises are indicated in the sequence the person is following?",
        "time_stamp": "00:03:10",
        "answer": "C",
        "options": [
          "A. 4.",
          "B. 5.",
          "C. 6.",
          "D. 7."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_144_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:02]: The video begins in a gym setting with a person performing lunges while holding dumbbells. The person is wearing a grey shirt, navy shorts, and a cap. The gym is well-lit and equipped with various machines and weights.  [0:06:03 - 0:06:09]: The person is now squatting down and adjusting the grip on their dumbbells. The environment remains consistent, displaying the gym setup with other machinery in the background and a few people working out. The text overlay reads, \"3. Don't let your grip strength limit the load you can use,\" emphasizing proper grip technique for lifting weights. [0:06:10]: The person is standing up again, holding the dumbbells firmly, and preparing to continue their exercise. [0:06:11 - 0:06:13]: The person resumes performing lunges while holding the dumbbells, moving steadily forward. The gym's atmosphere remains active, with other individuals in the background focusing on their workouts. [0:06:14 - 0:06:17]: The person continues their lunging exercise with consistent form, maintaining a steady pace. The background gym environment remains the same with various exercise machines, weights, and a few active gym-goers. [0:06:18 - 0:06:20]: The video concludes with the person completing another lunge, focusing intently on their movement and maintaining their form while holding the dumbbells firmly. The gym's setup and lighting remain constant throughout.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What exercise is the person performing right now?",
        "time_stamp": "0:06:02",
        "answer": "B",
        "options": [
          "A. Squats.",
          "B. Lunges.",
          "C. Push-ups.",
          "D. Deadlifts."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Event Understanding",
        "question": "What action does the person take after squatting down and adjusting their grip on the dumbbells?",
        "time_stamp": "0:06:17",
        "answer": "D",
        "options": [
          "A. He put the dumbbells down.",
          "B. He start doing push-ups.",
          "C. He switch to another exercise machine.",
          "D. He begin performing lunges again."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_144_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:05]: The video starts with an anatomical illustration of the lower leg muscles, specifically the medial and lateral gastrocnemius, positioned on the left side of the screen. Next to this illustration, on the right side of the screen, is a chart with data points, labeled sections for medial and lateral gastrocnemius, and a blue arrow indicating a specific data point for medial gastrocnemius. Below the chart is a caption detailing the figure and definitions of symbols, with blue and black V shapes. [0:09:06 - 0:09:13]: The two arrows (blue and green) appear overlaid on the muscles on the left side of the screen. The chart on the right now has a green arrow pointing to a specific data point for the lateral gastrocnemius. The rest of the background remains the same, maintaining the anatomical figure and chart data. [0:09:13 - 0:09:17]: The setting transitions to a gym environment. A person dressed in dark shorts, dark socks, and black shoes is walking away from the camera. They are surrounded by various gym equipment with black and red detailing on the frames. The gym floor appears to be made of dark, slip-resistant material. The person briefly stops and touches the back of their calf muscle with their hand while slightly bending forward. [0:09:17 - 0:09:19]: The person continues walking, then stops and sits down on a bench. They bend forward with their elbows resting on their knees, seemingly prepared to exercise or stretch. The gym equipment and backdrop remain consistent.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is shown on the left side of the screen right now?",
        "time_stamp": "0:09:05",
        "answer": "A",
        "options": [
          "A. An anatomical illustration of the lower leg muscles.",
          "B. A person exercising in a gym.",
          "C. A chart with data points.",
          "D. A blue and green arrow."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_144_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The scene starts with a view of a city intersection on a rainy day. The road is wet and shiny. There are multi-story buildings lining all sides of the street. A few pedestrians are standing under umbrellas, facing the crosswalk. On the left, there is a traffic light showing a red signal for pedestrians. Some of the buildings have glass windows, while others are covered in brick or concrete.  [0:00:04 - 0:00:07]: As pedestrians begin to cross the street, the camera viewpoint gradually moves forward. Several people carrying umbrellas walk on the crosswalk. Brightly colored signs, including one that is yellow with red text, hang from buildings on the right side. The traffic light now shows green for pedestrians.  [0:00:08 - 0:00:11]: The camera continues moving closer to the crosswalk. More umbrellas are visible, and the weather seems to remain consistently rainy. The temperature is displayed as 14 degrees Celsius. Flowerbeds with colorful blooms are positioned along the edges of the sidewalk near the crosswalk. [0:00:12 - 0:00:15]: The camera moves past the crosswalk now and into the intersection. Pedestrians are still visible under their umbrellas, and there are street signs and banners on the sidewalks. Bright red banners with black and white text can be seen on the left side. [0:00:16 - 0:00:20]: The camera viewpoint continues down the street, showing more of the buildings ahead. Some pedestrians are still standing by the crosswalk, others are crossing. The roads are slick with rain, reflecting the buildings and lights around. A cyclist appears on the far side of the intersection, waiting under the traffic light.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the temperature displayed on the scene right now?",
        "time_stamp": "0:00:11",
        "answer": "C",
        "options": [
          "A. 10 degrees Celsius.",
          "B. 12 degrees Celsius.",
          "C. 14 degrees Celsius.",
          "D. 16 degrees Celsius."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_323_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:02]: A wet street scene captured from a first-person perspective shows a sidewalk with alternating grey and beige tiles, with the road to the left. Modern buildings line both sides of the street, and it is raining lightly. Several people hold umbrellas while walking along the sidewalk and crossing the street. A white van and a few other vehicles are driving on the road. A covered bus stop or telephone booth occupies the right side of the frame. [0:03:03 - 0:03:05]: The camera continues moving forward along the sidewalk. In the background, buildings further along the street create a corridor perspective. Pedestrians, mainly on the right side of the walkway, use umbrellas due to the rain. Vehicles, including a white car and van, keep moving along the wet street. A black signboard with red and white signage is visible on the right. [0:03:06 - 0:03:08]: The view remains consistent as the camera holder moves further along the sidewalk. A larger group of pedestrians becomes visible. Buildings on the right side show additional signage. The street and sidewalk remain wet from the rain, and additional vehicles appear on the road. [0:03:09 - 0:03:11]: Pedestrians are more dispersed; some are wearing coats and holding umbrellas while walking on the sidewalk. The wet conditions of the sidewalk and road are more evident, with puddles forming. More buildings with large windows and signage continue to line the street. [0:03:12 - 0:03:14]: The camera captures additional details of the urban environment, showing more intricate patterns of the wet sidewalk tiles. Various signs and storefronts are increasingly visible as the view progresses. More individuals with umbrellas appear, and the reflection on the wet surfaces becomes more pronounced. [0:03:15 - 0:03:17]: Pedestrians, vehicles, and buildings continue to populate the scene. A small tree and a bicycle rack can be seen on the right side of the sidewalk. Additional storefronts are noticed with their lights on. The setting continues to depict a rainy urban day with a consistent walking pace. [0:03:18 - 0:03:19]: The final frames show the continuity of the urban street with additional pedestrians under umbrellas walking to their destinations. Vehicles continue driving along the wet street. The sidewalk's alternating tile pattern becomes more defined, showing the mixture of grey and beige tiles as the camera perspective maintains the forward motion.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What occupies the right side of the frame right now?",
        "time_stamp": "00:03:02",
        "answer": "B",
        "options": [
          "A. A tree.",
          "B. A telephone booth.",
          "C. A black car.",
          "D. A bench."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_323_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:02]: The video shows a wet urban street, likely after raining, with the ground reflecting the city lights. A man in a dark hoodie is walking on a sidewalk to the right, while various individuals with umbrellas walk ahead. White vans and cars are on the road. [0:06:03 - 0:06:06]: As the video progresses, the man in the hoodie moves out of frame, and two individuals with an umbrella walk on the right side, closer to the camera. They appear to be a couple, as they stay close to each other. Traffic continues on the wet road. [0:06:07 - 0:06:09]: The couple under the umbrella walk past the camera, and the right side of the sidewalk becomes empty. A lone pedestrian with an umbrella enters the frame on the right side of the street, walking towards the camera. [0:06:10 - 0:06:12]: The road and sidewalk continue to appear wet, reflecting lights from nearby buildings and vehicles. Several cars and vans continue driving on the street, while multiple pedestrians with umbrellas cross the street in the distance. [0:06:13 - 0:06:15]: Pedestrians with umbrellas are seen crossing the street in the distance. A blue street sign indicating pedestrian crossing is visible, along with a brown sign showing restroom facilities. Vehicles, predominantly white, move along the road beneath an overhead structure. [0:06:16 - 0:06:18]: The camera perspective showcases people using umbrellas to shield themselves from the rain, continuing to walk on the wet sidewalks and cross the street. The traffic flows smoothly, with vehicles proceeding cautiously due to the wet conditions. The background displays a multitude of buildings, likely office or residential, indicating an urban setting. [0:06:19 - 0:06:20]: More pedestrians are visible on the sidewalk and street, with a person in a skirt carrying an umbrella walking towards the camera. The scene remains consistent with the previous frames, with wet conditions and an urban environment surrounding the area.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What happens to the man in the black hoodie as the video progresses?",
        "time_stamp": "00:06:06",
        "answer": "C",
        "options": [
          "A. He runs towards the camera.",
          "B. He stops walking.",
          "C. He moves out of frame.",
          "D. He drops his umbrella."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the lone pedestrian with an umbrella located relative to the camera?",
        "time_stamp": "00:06:12",
        "answer": "C",
        "options": [
          "A. To the left of the street.",
          "B. Directly behind the camera.",
          "C. On the right side of the street, walking towards the camera.",
          "D. In the middle of the street."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_323_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:03]: The video starts at a crosswalk on a rainy day in an urban area. Many people holding umbrellas of different colors are waiting to cross the street. There are tall buildings in the background with numerous windows, one of which is grey and white while the other is predominantly glass with a blue tint. The pedestrian signal is visible and shows a red light. There are a few vehicles on the road. [0:09:04 - 0:09:07]: The pedestrian light turns green and people begin to cross the street. A man in a black jacket is seen from behind walking towards the crosswalk. Other pedestrians, including individuals with umbrellas, also start to step onto the zebra crossing. [0:09:08 - 0:09:09]: More people continue to cross the street. A woman wearing a pink jacket and holding an umbrella is visible in the front right part of the frame. The video follows a first-person perspective, suggesting that the camera is carried by someone who is also crossing. [0:09:10 - 0:09:11]: As the crossing progresses, the video captures a group of seven to eight people under umbrellas walking ahead. The crosswalk lines are clear, and the wet pavement glistens from the rain. Several green and red traffic lights are visible. [0:09:12 - 0:09:14]: The camera continues moving forward, showing more of the crosswalk and pedestrians already halfway through. The alignment and movement of the crowd are organized as everyone heads toward the opposite sidewalk. Buildings continue to line the street offering a typical cityscape view. [0:09:15 - 0:09:18]: Nearing the other side of the crosswalk, the camera catches more detail of the people and surroundings. Some pedestrians are holding umbrellas while others, possibly wearing raincoats, are not. A white vehicle is parked by the curb on the right, and some greenery is noticeable near the buildings. [0:09:19 - 0:09:20]: The perspective changes slightly upward as the camera reaches the other side of the road, capturing the height of the skyscrapers and additional pedestrians continuing their journey along the sidewalk. More details of the city, such as signs and street lights, are visible.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What was the main activity happening just now?",
        "time_stamp": "00:09:20",
        "answer": "B",
        "options": [
          "A. People shopping in a market.",
          "B. People crossing a street on a rainy day.",
          "C. Cars driving through a busy intersection.",
          "D. Pedestrians waiting at a bus stop."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_323_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What activity is being performed just now?",
        "time_stamp": "00:00:10",
        "answer": "A",
        "options": [
          "A. Grinding coffee beans and weighing the ground coffee.",
          "B. Brewing a pot of tea using loose leaves and boiling water.",
          "C. Preparing a smoothie with various fruits and a blender.",
          "D. Making a sandwich with multiple layers of ingredients."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_366_real.mp4"
  },
  {
    "time": "[0:02:23 - 0:02:33]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the activity performed just now?",
        "time_stamp": "00:02:33",
        "answer": "A",
        "options": [
          "A. A person is cleaning a metal surface with a cloth and checking its condition.",
          "B. A person is painting a metal surface with a brush.",
          "C. A person is repairing an electronic device using tools.",
          "D. A person is assembling parts of a mechanical device."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_366_real.mp4"
  },
  {
    "time": "[0:04:46 - 0:04:56]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the activity performed just now?",
        "time_stamp": "00:04:56",
        "answer": "A",
        "options": [
          "A. Preparing an espresso by grinding coffee beans, measuring the coffee, and extracting the shot.",
          "B. Cleaning and sanitizing a workspace in preparation for a new task.",
          "C. Operating kitchen equipment to bake a batch of cookies.",
          "D. Conducting a detailed inspection and maintenance of a coffee machine."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_366_real.mp4"
  },
  {
    "time": "[0:07:09 - 0:07:19]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the activity performed just now?",
        "time_stamp": "00:07:19",
        "answer": "A",
        "options": [
          "A. Preparing an espresso and stirring it.",
          "B. Brewing a pot of tea and adding sugar.",
          "C. Mixing ingredients for a cake batter.",
          "D. Cleaning and sanitizing a kitchen workspace."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_366_real.mp4"
  },
  {
    "time": "[0:09:32 - 0:09:42]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the activity performed just now?",
        "time_stamp": "00:09:42",
        "answer": "A",
        "options": [
          "A. Weighing coffee grounds and tamping them into a portafilter.",
          "B. Cleaning the coffee machine and setting it up for a new brew.",
          "C. Preparing a tray of cookies for baking in the oven.",
          "D. Cleaning and arranging the kitchen workspace."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_366_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video opens with a view from above showing several items on a table. At the top, there's a stack of white cartons, each labeled \"JO\" in black text. Below these, there's a stack of circular yellow-lidded containers labeled \"Mild Yogurt.\" Some boxes and containers are strewn on the floor. [0:00:01 - 0:00:02]: The camera angle shifts slightly downward, showing a larger section of the storage room. Various stacks of boxed items are visible, including some more of the \"JO\" cartons. The green boxes labeled \"Bregott\" are more clearly visible. There are cartons, trays with bottles, and other packages around. [0:00:02 - 0:00:03]: The perspective shifts downward towards the floor, where several more sturdy-looking trays and packages, including more stock items, are visible. The floorage shows marks and slight debris, indicating a working area.  [0:00:03 - 0:00:04]: The first-person perspective moves towards the stacked items on the table. The large green box labeled \"Bregott\" stands out prominently. There is a plastic-wrapped pallet with more cartons and circular containers around it. [0:00:04 - 0:00:05]: The camera continues to move in a specific direction; you can see a person pointing towards the stacks of items on the table, likely examining them or organizing them. The person's hand is gloved, suggesting a work environment that requires protective gear. [0:00:05 - 0:00:06]: The camera angle transitions to show more of the room, including shelves packed with various items on the left side. There is a clutter of cardboard boxes and items on the floor that look like they are in the process of being organized or stored.  [0:00:06 - 0:00:07]: In this frame, the person who is handling the objects is seen lifting a carton labeled \"JO.\" The hand appears in close proximity to the carton, giving a clearer view of the shelf arrangement and the workspace.  [0:00:07 - 0:00:08]: The video shows a person placing down the carton labeled \"JO\" among other similar cartons. The surrounding shelving units and different kinds of packaged items become more apparent, providing an understanding of the storage setup. [0:00:08 - 0:00:09]: The view shifts back a little to offer a broader view of the workspace, including stacks of \"Bregott\" boxes and \"JO\" cartons arranged systematically. There is a metal trolley on the side, likely used for moving the items. [0:00:09 - 0:00:10]: The video continues to show the organized arrangement of dairy products and other items with plastic wraps upon close inspection. The workspace is becoming neater as the camera moves to the side. [0:00:10 - 0:00:11]: The focus zooms in on a person's hands as they handle the \"JO\" cartons, placing them neatly on top of each other. The gloves and the precision suggest careful handling of the products. [0:00:11 - 0:00:12]: The camera stays focused on the placement of the \"JO\" cartons, showing how they stack properly. The gloved hands are handling each item delicately to ensure proper placement. [0:00:12 - 0:00:13]: The sequence continues to showcase methodical stacking of the cartons. The workspace is coming together as items are being organized systematically. [0:00:13 - 0:00:14]: The gloved hands remain the focal point, handling products efficiently. The well-lit environment reveals the details on the cartons and the careful placement of each. [0:00:14 - 0:00:15]: An angle shift reveals more of the storage area, showing how items are being arranged. The green \"Bregott\" boxes contrast against the otherwise monochromatic boxes and packages. [0:00:15 - 0:00:16]: The room full of organized crates and items on shelves suggests a busy work environment, focused on systematically handling and organizing products. Cardboard boxes are placed judiciously in corners. [0:00:16 - 0:00:17]: The camera moves back to reveal more shelf space and the individual systematically placing the last few cartons. The boxes are stacked efficiently, maximizing space. [0:00:17 - 0:00:18]: It shows a person handling the final few items, ensuring all products are neatly arranged and accessible. The carton stacks look consistent in height and arrangement. [0:00:18 - 0:00:19]: The video showcases a wider view",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is prominently visible on top of the table at the start of the video?",
        "time_stamp": "0:00:01",
        "answer": "B",
        "options": [
          "A. A stack of green boxes.",
          "B. A stack of white cartons labeled \"JO\".",
          "C. A stack of red cartons labeled \"YO\".",
          "D. A stack of blue containers labeled \"Mild Yogurt\"."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_446_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:41]: The video shows a person wearing gloves organizing items on a table covered partially with plastic wrap. Several stacks of containers, possibly yogurt or cream, are visible on the left side in blue and white packaging. The person is handling a box containing multiple smaller blue items. [0:02:41 - 0:02:42]: The video continues with the person placing the box of small blue items next to the stacks of containers. Nearby, there are several cartons labeled ‘Bregott’ in green with white text. [0:02:42 - 0:02:43]: The person moves another box of small blue items, preparing to place it on the table. More boxes and cartons are scattered around the area, some unpacked, some still wrapped in plastic. [0:02:43 - 0:02:44]: The items are being organized onto a cart positioned next to the table. The cart is filled with various stacked items, including containers labeled ‘KESO’ and more boxed goods. [0:02:44 - 0:02:45]: The person adjusts the position of several items on the cart, ensuring they are organized and secure. They move another box toward the cart. [0:02:45 - 0:02:46]: The cart is now fully loaded with various goods, including more 'KESO' containers, boxed items, and what appears to be plastic-wrapped goods placed on top. [0:02:46 - 0:02:47]: The person moves away from the cart, surveying the surrounding area. Nearby boxes and cartons still need to be organized. [0:02:47 - 0:02:48]: The person bends down to pick up more items, focusing on a stack of flattened cardboard boxes under some plastic wrap. [0:02:48 - 0:02:49]: More cardboard boxes are picked up and adjusted. The surroundings reveal more of the storage area, with shelves nearby stocked with various items. [0:02:49 - 0:02:50]: The person handles the cardboard boxes, stacking them strategically near other organized goods. The area is becoming more organized. [0:02:50 - 0:02:51]: The focus shifts back to the table with more containers and boxed goods. The Bregott cartons remain visible, showing a partially organized section. [0:02:51 - 0:02:52]: More Bregott cartons are moved, adding to the organized rows of items on the table. The stacks of containers are neatly arranged in rows. [0:02:52 - 0:02:53]: The person now holds a Bregott carton, lifting it to place it among other neatly arranged items. The storage area looks more organized. [0:02:53 - 0:02:54]: The view shifts to show a larger portion of the storage room. Various goods, carts, and shelves are set in an organized manner. [0:02:54 - 0:02:55]: The person continues handling and organizing the Bregott cartons along with other items onto the cart, preparing for further organization. [0:02:55 - 0:02:56]: The Bregott carton is placed onto the cart, joining other items. More cartons and containers are stacked near the cart. [0:02:56 - 0:02:57]: The person returns to the table, checking and adjusting the already organized items, ensuring everything is in order. [0:02:57 - 0:02:58]: The person continues placing and adjusting items on the table. Multiple cartons and containers are visible, sorted neatly in rows. [0:02:58 - 0:02:59]: The person works with more cartons and containers. The table shows a neatly arranged collection of goods ready for storage or transport.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What brand name appears on the green cartons being holding by the person?",
        "time_stamp": "00:02:52",
        "answer": "B",
        "options": [
          "A. KESO.",
          "B. Bregott.",
          "C. Arla.",
          "D. Lurpak."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_446_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:21]: The video starts with a first-person perspective view of a shopping aisle. The person is looking down at a variety of products on shelves, including multiple brands and types of goods. On the left, there is a metal rack in the image and another one on the right, laden with various packaged items. The shelves appear to be well-stocked.   [0:05:22 - 0:05:31]: The shopper reaches out towards a refrigerator shelf containing different yogurt containers. The individual is wearing gloves and moves their right hand towards a set of white yogurt cups with blue lids on the middle shelf. They start to rearrange these yogurt cups, picking one from the back and stacking it toward the front. To the left, there are bottled beverages like yellow-bottled juices or possibly dressings next to the yogurts. [0:05:32 - 0:05:33]: The gloved individual continues to adjust the placement of the yogurt containers on the middle shelf, moving about the stacked yogurt cups. They add another yogurt container to the stacked cups, ensuring a neat arrangement. [0:05:34 - 0:05:35]: The person moves away from the refrigerator shelf, pushing a restocking cart with more yogurt items. They start maneuvering the cart through the aisle, which is filled with products on both sides of the shelves, and head back toward the shelf containing the yogurts. [0:05:36 - 0:05:38]: The individual is seen grabbing a set of yogurt containers from the cart and starting to place them onto the shelf. They continue with the restocking process meticulously, ensuring the yogurts are correctly positioned among others on the refrigeration unit. [0:05:39]: The shopper reaches to place the last couple of yogurt containers on the shelf, organizing them precisely next to the previously placed items. The shelf appears to be fully stocked with a variety of yogurt containers, each neatly aligned.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the individual wearing on their hands while handling the yogurt containers?",
        "time_stamp": "0:05:31",
        "answer": "A",
        "options": [
          "A. Gloves.",
          "B. Mittens.",
          "C. Bare hands.",
          "D. Bandages."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_446_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:11]: A person wearing black gloves is stocking shelves in a store. They handle beverage cartons labeled \"mellanmjölk\" and appear to be arranging them on the shelf. The worker wears a long-sleeve black shirt and has a blue wristband on the left wrist and a gold bracelet on the right wrist. There is a white box with “Valio” printed on it by their knees, which they seem to be holding as they arrange the cartons. The shelves in the background are white, and some cartons are already placed neatly on the left top shelf, while others are being adjusted on the right middle shelf. [0:08:12 - 0:08:14]: The individual opens a cardboard box filled with green cartons and prepares to stock these on the shelves. Each carton has a white cap, and the box fits snugly between their knees and the shelf. [0:08:15 - 0:08:19]: The person begins placing the green cartons from the box onto the lower shelf diligently. Their hands move back and forth between the box and the shelf as they arrange the cartons neatly beside other products. The shelves are well-lit, and there are other product labels visible, indicating different prices. There is precise care in the arrangement of the items, ensuring they are lined up correctly and that gaps on the shelf are filled.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What brand is printed on the white box near the worker's knees?",
        "time_stamp": "00:08:11",
        "answer": "B",
        "options": [
          "A. Arla.",
          "B. Valio.",
          "C. Alpro.",
          "D. Oatly."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_446_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:57]",
    "captions": "[0:09:40 - 0:09:57] [0:09:40 - 0:09:41]: The video begins in what appears to be a storage or warehouse area, facing shelving units filled with boxes and green-white packaged items. A gloved hand reaches for a box on a lower shelf. [0:09:41 - 0:09:42]: As the hand pulls the box from the shelf, the view shifts, showing more shelving along the wall with various items and equipment. [0:09:42 - 0:09:43]: Continuing the motion, the camera angle changes to show a view of a retail shelf filled with milk cartons and some empty spaces. The gloved hand holds the box. [0:09:43 - 0:09:45]: The person places the box on the floor and begins opening it. Milk cartons are visible on the shelf, and cardboard boxes are scattered on the floor. [0:09:45 - 0:09:46]: The person starts placing items from the box onto the shelf. They appear to be organizing or restocking. [0:09:46 - 0:09:47]: More items are taken out of the box and placed on the shelf. The red and white milk cartons are clearly visible, along with some green-packaged items. [0:09:47 - 0:09:49]: The person continues restocking, aligning the items neatly on the shelf, filling the empty spots below the milk cartons. [0:09:49 - 0:09:53]: The camera angle shows the milk shelf closer as more items are placed. The focus is on ensuring the products are correctly positioned. [0:09:53 - 0:09:54]: The person removes the empty box, revealing an organized shelf with red and white milk cartons and green packages neatly arranged. [0:09:54 - 0:09:57]: The angle shifts slightly to show a broader view of the shelves, indicating the task is nearly complete. The scene pans slightly, showing the remaining scattered boxes and items on the floor, while the person finalizes the organization.",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "What indicates that the restocking task is nearly complete?",
        "time_stamp": "00:09:54",
        "answer": "B",
        "options": [
          "A. The person leaves the area.",
          "B. Everything items in the box has been put on the shelves.",
          "C. The items are being packed into the box.",
          "D. The camera shows an empty warehouse."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_446_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: A man wearing a gray beanie and an olive-green hoodie is seated and looking directly at the camera. He has a beard and is wearing a light-colored T-shirt with a graphic of Mickey Mouse. The background features dim lighting with a teal and yellow hue, a desk with various items, including a houseplant, and some equipment, likely a camera, on his right. [0:00:03 - 0:00:05]: The man begins to speak, moving his mouth and occasionally blinking. His expression is focused and slightly serious. The ambient lighting and backdrop remain consistent. [0:00:05 - 0:00:07]: The man shifts his gaze to his right, continuing to speak. His body posture remains relatively still, but his head turns slightly. The background and lighting remain unchanged. [0:00:07 - 0:00:09]: The man returns his gaze to the front, maintaining his conversation. His facial expression is attentive, and his gestures are minimal. The environment around him stays the same with steady lighting. [0:00:09 - 0:00:11]: The man reaches out with his left hand towards a piece of cloth or clothing, picking it up from out of frame. He remains seated and continues speaking, glancing briefly towards the item he is holding. [0:00:11 - 0:00:14]: Holding the cloth or clothing in his left hand, the man looks back at the camera, as though explaining or discussing the item. His expression is calm and composed, and he maintains an upright posture. [0:00:14 - 0:00:16]: The man resumes his direct gaze at the camera, still holding the cloth or clothing. There is a slight nodding motion, indicating he might be emphasizing a point in his speech. His facial expression remains neutral. [0:00:16 - 0:00:19]: The man turns his head and looks to his left while still speaking. He then looks back at the camera, continuing his conversation with slight gestural movements of his hands. The background remains a consistent teal-lit space with various items on the desk.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the man in the video doing so far?",
        "time_stamp": "00:00:19",
        "answer": "A",
        "options": [
          "A. Speaking and gesturing while seated.",
          "B. Cooking in a dimly lit kitchen.",
          "C. Reading a book quietly.",
          "D. Writing notes on a paper."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_124_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:27]: A white bottle labeled \"GAC 100\" with \"Acrylic primer and extender\" stands upright. The brand name \"GOLDEN\" is visible in blue vertical text. The bottle has a black cap. The background is slightly blurred and indicates an indoor setting with various objects, including colorful items and lights, positioned on a surface. [0:02:27 - 0:02:29]: The cap is removed from the bottle, and the bottle is now tilted down above a clear plastic lid placed on a piece of light denim fabric. A small amount of white liquid begins to dispense from the bottle onto the lid. [0:02:29 - 0:02:31]: More white liquid is dispensed, forming a small puddle on the plastic lid. [0:02:31 - 0:02:33]: The puddle of white liquid expands as additional liquid is dispensed. The liquid forms a round, opaque white shape on the lid. [0:02:33 - 0:02:34]: A paintbrush with a flat, wide bristle is dipped into the white liquid on the lid. The bristles pick up some of the liquid. [0:02:34 - 0:02:37]: The paintbrush, loaded with white liquid, moves towards a section of the denim fabric bordered by yellow tape. The brush starts to apply the white liquid onto the denim within the taped-off section. [0:02:37 - 0:02:40]: The brush continues to spread the white liquid evenly over the section of the fabric, filling the area neatly within the yellow tape. The strokes cover the denim with a light, white coat.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What brand is mentioned on the white bottle labeled \"GAC 100\"?",
        "time_stamp": "00:02:40",
        "answer": "A",
        "options": [
          "A. GOLDEN.",
          "B. SILVER.",
          "C. BRONZE.",
          "D. PLATINUM."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Causal Reasoning",
        "question": "Why is the yellow tape used on the denim fabric?",
        "time_stamp": "00:02:59",
        "answer": "B",
        "options": [
          "A. To secure the fabric to the table.",
          "B. To create a border for the white liquid application.",
          "C. To measure the fabric size.",
          "D. To mark the sections for different colors."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_124_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:45]: The video begins by focusing on a hand holding a paintbrush close to a canvas. The canvas displays a detailed portrait of a person wearing a dark hoodie; the person's face is partially covered by the hood. The paintbrush is being used to apply a reddish-orange color to the area beneath the subject's eye, accentuating facial features and shadows. The setting appears to be well-lit, highlighting the brushstrokes and textures of the painting. [0:04:46 - 0:04:50]: The hand continues to work meticulously on different facial aspects, moving the brush to adjust the shading and details around the nose and mouth. The palette of colors being used includes various shades of red, brown, and white, blending into the canvas to create a realistic depiction of the face. The proximity of the brush to the canvas implies a focus on refining finer details. [0:04:51 - 0:04:57]: The continuous movement of the paintbrush demonstrates subtle adjustments being made to the cheeks and lips area. The painter adds layers of white paint to parts of the face, particularly around the mouth, creating highlights and enhancing the dimensionality of the portrait. The video gradually captures the richness of textures formed by the layered paint. [0:04:58 - 0:05:00]: The final seconds of the video show the artist making the final touches with the paintbrush around the eyes, enhancing the expression and intensity of the subject's gaze. The intricate detailing and careful brushwork emphasize the artist's skill and the lifelike quality of the painted portrait.",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the artist add layers of white paint to the portrait?",
        "time_stamp": "00:04:57",
        "answer": "A",
        "options": [
          "A. To create highlights and enhance dimensionality.",
          "B. To cover mistakes made earlier.",
          "C. To change the color of the background.",
          "D. To outline the figure in the portrait."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_124_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:09]: The frames depict a close-up painting of an elderly man with a white beard, wearing a brown cloak with a hood. The man holds a blue sword-like object, creating a striking contrast in the painting. The painting appears to be on a textured canvas, emphasizing the brush strokes and details in the man's facial expression and attire. The background is a mix of dark and light colors, possibly suggesting a scene with a mystical or otherworldly theme. [0:07:10 - 0:07:19]: The scene transitions to a first-person view of someone adjusting their denim jacket. The individual's arms and hands are visible, with the focus on the process of fastening and rolling up the jacket sleeves. The person appears to be indoors, as indicated by the presence of a plant and various objects blurred in the background. The lighting is soft, creating a casual and somewhat cozy atmosphere.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the painting shown in the clip?",
        "time_stamp": "00:07:09",
        "answer": "D",
        "options": [
          "A. A young warrior playing a musical instrument.",
          "B. A landscape with a forest and a gardener.",
          "C. An abstract painting with vibrant colors.",
          "D. An elderly man with a blue sword-like object."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_124_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What instrument is visible on the pilot's lap right now?",
        "time_stamp": "00:00:03",
        "answer": "D",
        "options": [
          "A. An altimeter.",
          "B. A compass.",
          "C. A throttle.",
          "D. A joystick."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_435_real.mp4"
  },
  {
    "time": "[0:01:15 - 0:01:20]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "Where is the airplane preparing to land?",
        "time_stamp": "00:01:18",
        "answer": "D",
        "options": [
          "A. At the airport.",
          "B. On the mountain.",
          "C. In the field.",
          "D. On the grassland."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_435_real.mp4"
  },
  {
    "time": "[0:02:30 - 0:02:35]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the handgrip the pilot's left hand is holding right now?",
        "time_stamp": "00:02:31",
        "answer": "D",
        "options": [
          "A. Red.",
          "B. Yellow.",
          "C. Green.",
          "D. Blue."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_435_real.mp4"
  },
  {
    "time": "[0:03:45 - 0:03:50]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the handgrip on the left side right now?",
        "time_stamp": "00:03:47",
        "answer": "D",
        "options": [
          "A. Red.",
          "B. Yellow.",
          "C. Green.",
          "D. Blue."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_435_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:05:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the aircraft's side window frame right now?",
        "time_stamp": "00:05:00",
        "answer": "D",
        "options": [
          "A. Black.",
          "B. Grey.",
          "C. Blue.",
          "D. White."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_435_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:04]: The initial frames present a close-up view of a computer's internal components. Illuminated by blue lighting, a large fan on the left side has a prominently visible glowing circular ring. Two vertically aligned glowing strips are positioned to the right of the fan. In the center, several black, braided cables are visible, with one curving from the foreground to the background. [0:00:05]: The focus shifts to the front view of a computer case, situated on a wooden surface. The case, labeled \"NZXT,\" features a mesh design with a luminous blue circular ring in the center. [0:00:06 - 0:00:08]: A person is working on the computer's interior. The case, still featuring the \"NZXT\" label, is set horizontally on the table. The individual’s hand, visible in the top center of the frame, is adjusting components. Within the case, two fans with white rings are visible. [0:00:09 - 0:00:10]: The perspective changes, shifting to a side view of the computer case, where the interior components, lit by blue lights, are seen through a clear panel. The room's background is dimly lit. [0:00:11 - 0:00:20]: The scene transitions to a well-lit workspace. A man stands behind a desk, facing the camera while gesturing towards items on the table. To his right, there are two purple boxes and additional equipment including a camera on a tripod. Dressed in a black shirt and beige pants, he continues speaking and gesturing as the video progresses, seemingly explaining something about the components or setup on the desk.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What label is prominently visible on the computer case right now?",
        "time_stamp": "00:00:06",
        "answer": "D",
        "options": [
          "A. ASUS.",
          "B. Corsair.",
          "C. Cooler Master.",
          "D. NZXT."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_114_real.mp4"
  },
  {
    "time": "[0:03:40 - 0:04:00]",
    "captions": "[0:03:40 - 0:04:00] [0:03:40 - 0:03:43]: The video begins with a close-up shot focused on a pair of hands holding a small, rectangular, black component with the \"Crucial\" label. The component is inside a transparent plastic case. The background shows a partially assembled computer motherboard lying flat on a wooden surface. Off to the left, part of a blue and white box, also labeled \"Crucial,\" is visible. [0:03:43 - 0:03:50]: The hands remove the component from its plastic case and turn it around, displaying different angles of the component. The motherboard remains visible in the background. [0:03:50 - 0:03:52]: The hands continue to handle the component, orienting it to be ready for installation. The background remains the same, with the computer motherboard laying flat on the wooden surface and the blue and white \"Crucial\" box visible on the left. [0:03:53 - 0:03:58]: The view shifts to a different perspective, now showing the hands holding a screwdriver near the motherboard. The screwdriver is being used to install the component onto the motherboard, specifically into one of the slots. The focus is on the precise insertion, highlighting the detailed work involved in the assembly process. [0:03:59]: The final scene shows a broader view of the work area from above. The hands are now fixing the component into place securely with the screwdriver. The computer motherboard is fully visible, along with several tools and additional components laid out on the table.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What label is visible on the memory bank just now?",
        "time_stamp": "00:04:07",
        "answer": "D",
        "options": [
          "A. Essential.",
          "B. Major.",
          "C. Primary.",
          "D. Crucial."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_114_real.mp4"
  },
  {
    "time": "[0:07:20 - 0:07:40]",
    "captions": "[0:07:20 - 0:07:40] [0:07:20 - 0:07:23]: The video starts with a close-up of a computer case's side panel. The panel is black and perforated with many small circular holes. Two hands are visible, removing the panel to reveal the inside of the computer case. The case interior has components and cables partially visible through the perforations. [0:07:24 - 0:07:35]: The scene changes to a view of a wooden desk with a natural, light brown finish. A pair of hands is holding and expanding several boxes of NZXT AER RGB 2 fans in a fan-like arrangement. The boxes are primarily white and purple, with images of the fans on the front showing multicolored RGB lighting. To the right, there is a blue-handled box cutter and a blue plastic tool. [0:07:36 - 0:07:39]: The video transitions back to the computer case. The view is now from the top, showing the inside with two mounted black fans labeled \"NZXT.\" The fans are connected with black wires. The background of the case interior includes perforated metal panels and visible components, such as cables and possibly other computer parts.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which brand name appears on the black fans inside the computer case right now?",
        "time_stamp": "0:07:39",
        "answer": "D",
        "options": [
          "A. Corsair.",
          "B. Cooler Master.",
          "C. Thermaltake.",
          "D. NZXT."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_114_real.mp4"
  },
  {
    "time": "[0:11:00 - 0:11:20]",
    "captions": "[0:11:00 - 0:11:20] [0:11:00 - 0:11:02]: The video starts with a close-up of a black desktop computer case placed on a wooden surface. The case has a transparent side panel revealing internal components illuminated with white and blue LED lights. A person's hand appears to be interacting with the computer, with one hand hovering above the case while the other is below and slightly inside the case. [0:11:03 - 0:11:07]: The perspective changes, showing the computer case situated on the floor next to a desk and a chair. The computer components continue to be illuminated, demonstrating internal lighting and the arrangement of parts. The lighting in the room is dim, with blue and white lights primarily highlighting the computer. [0:11:08 - 0:11:10]: The camera angle shifts again to a top-down view of the computer case, clearly showing a GIGABYTE GeForce RTX graphics card and other hardware components inside the case. The internal lighting makes the components more visible, particularly the branding and LEDs. [0:11:11 - 0:11:15]: Another close-up angle highlights the bottom part of the computer case. The graphics card and internal fans are visible, with LED lighting emphasizing the GIGABYTE logo and other details. The camera captures the details of the components with a slightly different lighting angle. [0:11:16 - 0:11:19]: The scene switches to a screen displaying performance statistics for the RTX 4070 Ti graphics card across various video games. The data is presented in bar graph format, listing average frames per second (FPS) for each game: DOOM Eternal, F1 22, Assetto Corsa Competizione, Shadow of the Tomb Raider, Horizon Zero Dawn, Forza Horizon 5, Control, God of War, COD: Modern Warfare, Dying Light 2, Cyberpunk 2077, and Red Dead Redemption 2.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which CPU's performance are displayed right now?",
        "time_stamp": "00:11:20",
        "answer": "D",
        "options": [
          "A. RTX 4070 Ti SUPER.",
          "B. RTX 4090.",
          "C. RTX 4080.",
          "D. RTX 4070Ti."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_114_real.mp4"
  },
  {
    "time": "[0:14:00 - 0:14:16]",
    "captions": "[0:14:00 - 0:14:16] [0:14:00 - 0:14:03]: The video begins with a close-up view of a black computer case placed on a wooden desk. The case has a transparent left side panel, revealing internal components including three illuminated cooling fans at the front, a central LED ring with a logo, and two RGB illuminated RAM sticks. An AIO liquid cooler with illuminated fans and tubes runs over the CPU. [0:14:04 - 0:14:08]: The camera transitions to a man sitting next to the computer on a desk with a backdrop of dark grey walls and some ambient lighting. He is wearing a black shirt and sits on the left, smiling while speaking. The setup behind him is visible, including additional lighting equipment. [0:14:09 - 0:14:11]: The camera shifts back to the close-up of the computer case, focusing more on the illuminated internal components. The lights and fans continue to glow, showcasing the RGB lighting inside the tempered glass. [0:14:12 - 0:14:15]: The camera returns to the man, who continues to speak while occasionally smiling. The surroundings remain the same: the computer case on the desk, dark walls, and ambient lighting fixtures.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What words can be seen inside the computer case now",
        "time_stamp": "00:14:16",
        "answer": "D",
        "options": [
          "A. GPU.",
          "B. RTX 4070 Ti SUPER.",
          "C. FAN.",
          "D. GIGABYTE."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_114_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:06]: The scene shows a first-person perspective of someone walking on a bridge. The bridge has metal railings on both sides and a pathway marked by a yellow line. On the left side, there are two people walking ahead, both facing away from the camera. The bridge has a structure with overhead beams and cables. The background consists of a cityscape with buildings, including a prominent structure that appears to be a church or historical building with a dome. The sky is clear with some scattered clouds. [0:00:07 - 0:00:12]: The camera continues moving forward on the bridge. A couple walking towards the person holding the camera comes into view, passing by on the left. The man is wearing a red shirt and sunglasses, while the woman has long hair and is wearing a white top. The cityscape in the background remains visible, including the mixture of modern and traditional buildings. The water body along the right side of the scene is also clearly visible. [0:00:13 - 0:00:15]: Another couple comes into view, both looking at something in the man's hands, possibly a map or a phone. The man is wearing a black shirt and shorts, with a mask partially covering his face. The woman beside him is wearing a light top and shorts. They seem to be enjoying a leisurely walk on the bridge. [0:00:16 - 0:00:20]: The focus shifts forward as the camera passes by the couple. The path ahead is now mostly clear of people, with a few individuals seen at a distance walking in the same direction as the camera. The background and surroundings remain consistent, showing the bridge structure, the cityscape, and the water body on the right. The end of the video is marked by the bridge continuing into the distance.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are the man doing as he walk on the bridge?",
        "time_stamp": "0:00:15",
        "answer": "A",
        "options": [
          "A. Looking at something in his hands.",
          "B. Holding hands.",
          "C. Talking to each other.",
          "D. Taking pictures."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_327_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:43]: A yellow and gray tram is moving along a bridge, occupying the right side of the scene. The bridge has a pathway on the left side with railings, where two individuals, a man in a navy shirt and shorts and a woman in a white dress, are walking towards the camera. A historic building with a dome is visible in the background on the left, along with some other buildings built into a hillside. The sky is clear and blue. [0:02:44 - 0:02:46]: Another tram appears on the left side of the bridge, moving towards the right, while the initial tram continues its journey to the right side of the frame. The path on the left remains clear as the two trams pass each other. [0:02:47 - 0:02:49]: The first tram has nearly exited the frame to the right, while the second tram from the left is centrally positioned. The pathway on the left side of the bridge remains unobstructed, showing clear railings and a yellow safety line. [0:02:50 - 0:02:52]: Both trams are now exiting the frame, revealing more details of the distant surroundings, including additional buildings and architectural features on the hillside. The sky remains clear with a couple of faint clouds. [0:02:53 - 0:02:59]: Only the tracks and bridge structure remain in the frame as the tram has moved further away into the distance. More details of the surrounding cityscape become visible, including more buildings in the background. The bridge's pathway and railings are empty, and the structure contrasts against the clear blue sky.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What color is the tram?",
        "time_stamp": "00:03:16",
        "answer": "D",
        "options": [
          "A. Red and white.",
          "B. Blue and white.",
          "C. Green and yellow.",
          "D. Yellow and gray."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_327_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: The video begins with a view of a relatively large, curved multi-story residential building with balconies. Several vehicles move along the street, and pedestrians walk on the pedestrian path. A red stop sign stands in the foreground on the right. Several businesses are noted on the ground floor, including one with a blue sign and another with a red sign. [0:08:01 - 0:08:04]: As the camera moves, the perspectives of the buildings, businesses, and street increase. A few more pedestrians are visible, crossing the street or walking beside the buildings. The stop sign remains prominent, and the ground is covered with a light-colored tile pedestrian path. [0:08:04 - 0:08:07]: The video continues with a closer view of the building. The camera is now closer to the pedestrian walkway, capturing more details of the businesses, including a storefront with large glass windows facing the street and various signs. The backdrop includes a clear blue sky, and it appears to be a bright and sunny day. [0:08:07 - 0:08:10]: As the camera progresses, the backdrop continues to show a clear blue sky. In this segment, the view begins to shift toward an interlocking pathway leading to another building. Pedestrians continue walking in and out of the frame, and vehicles remain parked along the street. Detailed outdoor seating is visible through the glass storefront windows. [0:08:10 - 0:08:12]: The pathway near the camera continues to be prominent with several turns into other parts of the area. The building in the previous frame passes the view and more features of the new building come into focus. There is a row of cars parked on the side street, and a few more pedestrians cross the road. [0:08:12 - 0:08:14]: The camera now transitions towards an orange-colored building with square windows and decorative shutters. A row of pedestrians is present, crossing the street. The backdrop includes green trees and more buildings. The stop sign is no longer in focus. [0:08:14 - 0:08:17]: The camera moves further along toward the orange building. The camera captures people walking on the sidewalk with more buildings aligning the street. The right sidewalk features a noticeable lining of cars parked along the road. [0:08:17 - 0:08:20]: The view widens and captures more of the street. The orange building appears extensively, showing more details such as windows and a decorative roofline. On the far left, a 'No Parking' sign is visible, and more pedestrians fill the sidewalk. Vehicles remain parked along the right-hand side of the street, leading to more residential buildings and greenery in the distance.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What sign is prominently visible right now?",
        "time_stamp": "0:08:01",
        "answer": "D",
        "options": [
          "A. No Parking sign.",
          "B. Yield sign.",
          "C. Speed limit sign.",
          "D. Stop sign."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_327_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:57]",
    "captions": "[0:09:40 - 0:09:57] [0:09:40 - 0:09:45]: The scene begins on a cobblestone street lined with parked cars on both sides. The video is from a first-person perspective, moving steadily forward. On the left side of the street is a multi-story red and white building, followed by several other buildings, some of which are cream-colored. Directly in view is a small, two-story tan building with a few windows. On the right side, there's a car partially in view with a tree just above it, and to the right side of the street is a green space with trees and a lamp post. Trash bins are located along the sidewalk. The sky is clear with scattered clouds. [0:09:46 - 0:09:52]: As the camera moves forward, more of the street becomes visible, showing additional cars parked along both sides. On the left-hand side of the small tan building, more details like balconies with flowerpots can be seen. A person is visible near the right side of the street, appearing to walk along the sidewalk adjacent to the green space. The white van on the right-hand side becomes more prominent, and the layout and positioning of the trash bins are clearer. [0:09:53 - 0:09:57]: Continuing down the street, the perspective straightens to show the center of the pathway. The skyline of a distant cityscape is visible in the background, showing various rooftops and a few taller buildings. The buildings on the left side include more windows and architectural details. The street curves slightly to the right ahead, leading towards more urban scenery. The white van seen earlier becomes more centered in the frame before passing fully out of view. Additional cars, houses, and infrastructure become apparent, enhancing the urban atmosphere.",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "What does the person near the right side of the street appear to be doing?",
        "time_stamp": "0:09:46",
        "answer": "B",
        "options": [
          "A. Sitting on a bench.",
          "B. Walking along the sidewalk.",
          "C. Standing still.",
          "D. Entering a building."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the trash bins located in the scene?",
        "time_stamp": "0:09:52",
        "answer": "A",
        "options": [
          "A. Along the sidewalk on the left side.",
          "B. Along the left side of the street.",
          "C. In the green space with trees.",
          "D. Next to the multi-story red and white building."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_327_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the actions performed just now?",
        "time_stamp": "00:00:10",
        "answer": "B",
        "options": [
          "A. The individual took vegetables out of the refrigerator, cleaned them, and arranged them on a plate.",
          "B. The individual retrieved a container with liquid, uncovered it, and began stirring its contents.",
          "C. The individual operated a deep fryer, cooked food, and placed it into a serving dish.",
          "D. The individual brewed coffee, poured it into cups, and served them."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_376_real.mp4"
  },
  {
    "time": "[0:00:38 - 0:00:48]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the actions performed just now?",
        "time_stamp": "00:00:46",
        "answer": "A",
        "options": [
          "A. The individual placed food on a grill, removed gloves, and discarded them in the trash.",
          "B. The individual cleaned the grill, prepared a sandwich, and discarded waste.",
          "C. The individual assembled a cooking station, put on gloves, and set up kitchen tools.",
          "D. The individual operated a fryer, removed food, and placed it into a serving dish."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_376_real.mp4"
  },
  {
    "time": "[0:01:16 - 0:01:26]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the actions performed just now?",
        "time_stamp": "00:01:39",
        "answer": "B",
        "options": [
          "A. The individual placed raw meat on a grill and monitored the cooking process.",
          "B. The individual retrieved cooked meat from a tray and placed it into a serving container.",
          "C. The individual prepared a sandwich by slicing bread and adding condiments.",
          "D. The individual operated a coffee machine and served fresh coffee to customers."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_376_real.mp4"
  },
  {
    "time": "[0:01:54 - 0:02:04]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the actions performed just now?",
        "time_stamp": "00:02:06",
        "answer": "C",
        "options": [
          "A. The individual deep-fried some meat, drained the excess oil, then garnished it with seasoning.",
          "B. The individual prepared a pot of stew, added various ingredients, and simmered it.",
          "C. The individual placed meat in a container, put cooked chicken in the oven, and then prepared buns.",
          "D. The individual made a salad by adding lettuce, tomatoes, and dressing into a bowl."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_376_real.mp4"
  },
  {
    "time": "[0:02:32 - 0:02:42]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the actions performed just now?",
        "time_stamp": "00:02:42",
        "answer": "C",
        "options": [
          "A. The individual grilled a burger patty, added cheese, and placed it on a bun with pickles and onions.",
          "B. The individual breaded chicken, fried it, and served it with a side of vegetables.",
          "C. The individual prepared a chicken sandwich by adding BBQ sauce and placing it on a bun with pickled cucumber and onions.",
          "D. The individual made a dessert by assembling ingredients and placing them into a dessert cup."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_376_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:04]: The video begins on a large grass soccer field with visible goal posts. The perspective is from the player's view, feet visible with the ball close. Another player, dressed in dark sports gear, runs toward the camera from a distance, preparing to challenge. The camera wearer moves forward, dribbling the ball. [0:00:05 - 0:00:06]: As the wearer approaches the goal area, a referee in fluorescent attire with a flag stands near the goal post. The ball is closer to the goal, and some of the wearer's teammates and opponents are visible in the background. [0:00:07]: The player slightly lifts the ball with their foot. The ball is mid-air, and the player’s foot and shadow are visible. [0:00:08 - 0:00:11]: The camera wearer regains control of the ball and continues dribbling. An opposing player attempts to challenge and intercept the ball, positioned close to the camera, making brief contact. [0:00:12 - 0:00:17]: The opposing player, in blue and yellow sports gear, continues to defend fiercely. The wearer maneuvers the ball swiftly, ultimately managing to bypass the opponent. Another player is visible further ahead, raising his arms in celebration or excitement as the camera wearer progresses. [0:00:18 - 0:00:20]: The video concluding scene shows the field broadly, with players spread out. One prominent individual in a numbered jersey raises his arms while walking away from the camera, indicating a potential successful play or goal. The light is clear, suggesting a daytime match.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is visible near the goal post just now?",
        "time_stamp": "0:00:06",
        "answer": "D",
        "options": [
          "A. A coach in a fluorescent vest.",
          "B. A player with a blue and yellow jersey.",
          "C. A spectator with a hat.",
          "D. A referee with a flag."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_255_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:44]: The video begins with a group of people on a soccer field, focused on a goal area. In the background, there is a large industrial-looking building and some construction equipment. Several players dressed in soccer uniforms are visible near the goalposts. The grass is bright green, and the sky is clear. [0:04:45 - 0:04:48]: A wide view of the soccer field shows players scattered around. Some players are walking towards the middle of the field, while others appear to be standing and talking. The field is well-maintained with the grass closely trimmed. [0:04:49 - 0:04:53]: The perspective of the video shifts slightly, with the shadow of the person holding the camera evident on the ground. More players in uniform are visible, and some appear to be preparing for a break. The high temperature is indicated by the subtitle stating it is time for a water break. [0:04:54 - 0:04:57]: A clearer view of several soccer players in a cluster shows one of them, possibly a referee, wearing a bright yellow shirt. The heat is emphasized by the players' relaxed postures and the ongoing break. [0:04:58 - 0:05:00]: The video registration explicitly mentions the hot weather in Thailand, with players walking on the field. Trees and a structure in the background are visible, with players appearing to regroup and discuss their next steps. [0:05:01 - 0:05:04]: The game resumes with players running across the field. One player is kicking the ball to restart play, with others in various positions ready to continue. The grass remains vibrant, and the clear markings on the field are visible. [0:05:05 - 0:05:08]: A close-up perspective shows the player who restarted the game dribbling the ball. Other players are seen moving to strategic positions, preparing for the next phase of the game. [0:05:09 - 0:05:12]: The soccer game is in full swing, with players actively running and positioning themselves. The field's boundaries are clear, and the video continues to showcase the active play. [0:05:13 - 0:05:16]: Some players are now closer to the camera, actively engaging in the game. There are visible attempts to maneuver the ball, and the game's intensity is evident from the players' movements. [0:05:17 - 0:05:20]: The video continues with active gameplay, showing players focusing on the ball. The interaction between the players is dynamic, indicating teamwork and strategy. The field's condition and clear weather remain consistent throughout.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What indicates that the players are taking a break?",
        "time_stamp": "00:04:44",
        "answer": "D",
        "options": [
          "A. The players are lying down.",
          "B. The players are leaving the field.",
          "C. The game is stopped by the referee.",
          "D. The subtitle mentions it is time for a water break."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_255_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:20 - 0:09:40] [0:09:20 - 0:09:22]: A group of players is seen running towards a goal. The grass field is brightly lit, suggesting a sunny day. A few players are wearing white shirts and dark shorts, and one player stands near the goal. [0:09:23 - 0:09:24]: The players, in darker attire, continue to approach the goal area. The field remains the same, with buildings and fencing visible in the background. [0:09:25 - 0:09:26]: A player in a white shirt is running towards the ball, which is heading towards the goal. Another player in darker clothing is keeping close. [0:09:27 - 0:09:28]: The ball is getting closer to the goal, with a player in dark clothing preparing to make a play. [0:09:29 - 0:09:30]: More players are moving towards the goal, and the ball is near the net. [0:09:31 - 0:09:32]: A player in dark clothing kicks the ball towards the goal while others watch. [0:09:33]: The ball is in the net, and a player raises their arm in celebration. Another player shows a gesture of excitement. [0:09:34 - 0:09:35]: The ball is inside the goal. Three players show excitement while another player in the background also raises their hand. [0:09:36]: The ball remains in the goal, and the players are moving away, celebrating the goal. [0:09:37]: A player in dark clothing is raising their hands up while others are starting to gather. [0:09:38 - 0:09:39]: Players begin congregating, celebrating the goal. Shadows on the field show the low angle of the sun. [0:09:40]: The players are shown further congregating, celebrating the recent equalizing goal, with some looking towards the camera.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What event are the players celebrating right now?",
        "time_stamp": "00:09:40",
        "answer": "D",
        "options": [
          "A. Scoring the first goal.",
          "B. Winning the game.",
          "C. A penalty kick.",
          "D. Equalizing the score."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_255_real.mp4"
  },
  {
    "time": "[0:14:00 - 0:15:00]",
    "captions": "[0:14:00 - 0:14:20] [0:14:00 - 0:14:02]: In a soccer match, players are seen running on a grass field. One player in a black jersey is sprinting towards the goal with a defender close by. The goalkeeper and other players are visible in the background.  [0:14:03 - 0:14:04]: The ball is now closer to the goal, with players in pursuit. A player in black is still running towards the ball, while another player in a white and red striped jersey is running alongside him. [0:14:05 - 0:14:06]: The ball rolls towards the goal, with the goalkeeper positioned close to the goal line. The player in black is preparing to take a shot. [0:14:07 - 0:14:08]: The player approaches the ball and seems ready to kick it. The goalposts and net are visible, with the goalkeeper in position to block the shot. [0:14:09 - 0:14:10]: The shot is taken, and it seems the goalkeeper makes a save. Other players react, and a teammate in a blue jersey runs towards the celebration. [0:14:11 - 0:14:12]: The players celebrate the save or the miss, with some kneeling in excitement. The shot-taker and others react to the moment. [0:14:13 - 0:14:14]: The goal celebration continues with the players in black and white jerseys gathering around the net. The goalkeeper walks away from the goal. [0:14:15 - 0:14:16]: More players join the scene, and some start walking back towards the field. The celebration slows down, hinting at the end of the excitement. [0:14:17 - 0:14:18]: The players start clearing the field, preparing for the next play. The camera pans to show more of the field and players resetting their positions. [0:14:19]: The video cuts to a wide-angle view showing a replay of the game, focusing on the action that led to the counterattack by the Korean team. The field, players, and some buildings in the background are visible.  [0:14:20]: The replay continues, showing the movements of the players and the progress of the ball. The setting remains the same, with the action taking place in the central part of the field.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What was the player recording this video prepared to do just now?",
        "time_stamp": "0:14:08",
        "answer": "D",
        "options": [
          "A. Pass the ball.",
          "B. Tackle the defender.",
          "C. Block the ball.",
          "D. Take a shot."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_255_real.mp4"
  },
  {
    "time": "[0:17:00 - 0:17:43]",
    "captions": "[0:17:40 - 0:17:43] [0:17:40 - 0:17:43]: The end screen of the video displays a grassy field, likely part of a sports stadium, as the background. In the top center of the screen, there is a text in bold red font reading \"Please subscribe and like :)\". Below this, three outlined boxes are seen. The box on the left contains the label \"Recommended Video,\" while the box on the right has the label \"Recently upload video,\" both in plain white font. The central circular box is labeled \"Click!\" in a bold aqua font. Near the bottom, the text \"Thanks for watching!\" appears in a bright yellow font. This layout remains consistent across the frames displayed.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the label on the box on the left?",
        "time_stamp": "0:17:43",
        "answer": "D",
        "options": [
          "A. Recently upload video.",
          "B. Click!.",
          "C. Thanks for watching!.",
          "D. Recommended Video."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_255_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why are the two fries in the plate in front of the teddy bear gone?",
        "time_stamp": "00:01:46",
        "answer": "A",
        "options": [
          "A. Because Mr. Bean ate it.",
          "B. Because the teddy bear knocked them off the plate.",
          "C. Because the fries were moved to another plate.",
          "D. Because the fries were given to a passing bird."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_243_real.mp4"
  },
  {
    "time": "[0:02:09 - 0:02:39]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why Mr. Bean was just flipping the sofa.",
        "time_stamp": "00:02:12",
        "answer": "C",
        "options": [
          "A. Because Mr. Bean was looking for loose change.",
          "B. Because Mr. Bean wanted to clean underneath the sofa.",
          "C. Because the remote control fell into the gap of the sofa.",
          "D. Because Mr. Bean thought something valuable was hidden inside."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_243_real.mp4"
  },
  {
    "time": "[0:04:18 - 0:04:48]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why Mr. Bean is watching a sofa advertisement.",
        "time_stamp": "0:03:23",
        "answer": "C",
        "options": [
          "A. Because Mr. Bean is curious about the latest furniture trends.",
          "B. Because Mr. Bean is helping a friend choose a new sofa.",
          "C. Because Mr. Bean's sofa is broken, he wants to buy a new one.",
          "D. Because Mr. Bean enjoys watching TV commercials."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_243_real.mp4"
  },
  {
    "time": "[0:06:27 - 0:06:57]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the character take the teddy bear from the girl and give her money in exchange?",
        "time_stamp": "0:06:34",
        "answer": "C",
        "options": [
          "A. Because the teddy bear is a rare collectible that Mr. Bean wants to keep.",
          "B. Because Mr. Bean needs the teddy bear for a special project.",
          "C. Because Mr. Bean is selling things to pay off the money to buy a sofa.",
          "D. Because the girl insisted on having the money instead of the teddy bear."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_243_real.mp4"
  },
  {
    "time": "[0:08:36 - 0:09:06]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why did Mr. Bean drive back to the sofa shop again?",
        "time_stamp": "0:09:03",
        "answer": "A",
        "options": [
          "A. Because his TV remote control fell into the old sofa.",
          "B. Because he realized he forgot to measure the space for the new sofa.",
          "C. Because he wanted to exchange the sofa for a different color.",
          "D. Because he left his wallet at the shop earlier."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_243_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:04]: The video begins with an image of a large white number '10' on the left side. To the right of the number, a round plate with cutlery at its center forms a clock design. The knife acts as the hour hand and the fork as the minute hand, both pointing towards the ten o'clock position. This image has a plain background with a gradient of blue shades. [0:00:01 - 0:00:04]: The video transitions to a person wearing a dark blue shirt standing in front of the same background. The text \"RAMSAY in 10\" appears behind the person in large blue and white letters, with the same plate-clock graphic replacing the \"0\" in \"10\". The setup is brightly lit, with a slight sparkle effect around the text. A smile is visible on the individual's face. [0:00:05 - 0:00:09]: The scene changes to a kitchen setting where the individual from the previous segment is talking. The person is now wearing a grey shirt and appears to be explaining something. The kitchen has white brick walls, and shelves filled with various utensils, including jars, dishes, and knives arranged neatly. The word \"COOK\" in large red letters is prominently displayed on the top shelf. On the lower shelf, there is a sign that reads \"HOT\". Throughout these frames, the individual maintains eye contact with the camera, making expressive hand movements. [0:00:10 - 0:00:12]: The video provides a side view of the individual as they continue speaking, showing more details of the kitchen. There are several stacked ovens with glass doors on the left side, and more jars filled with ingredients can be seen placed along the shelves. The lighting in the kitchen is bright, highlighting the clean and organized environment. [0:00:13 - 0:00:19]: Returning to the frontal view of the kitchen, the individual appears to be wrapping up their explanation. The expression on the face changes from focused to slightly inquisitive, and then back to a calm demeanor. The individual maintains a relaxed posture, with hands clasped together or gesturing gently. The background remains consistent with the earlier kitchen scene, emphasizing the central positioning of the speaker within the frame.\n[0:00:20 - 0:00:40] [0:00:20 - 0:00:21]: In a kitchen, a person stands in front of a white-tiled wall with wooden shelves holding various items, including dishes, jars, and bottles. A large red \"COOK\" sign is prominently displayed on the top shelf. Wearing a dark green shirt, the person is positioned behind a counter with folded arms, and a striped dish towel is visible hanging from their waistband. [0:00:21 - 0:00:22]: The person looks down, hands clasped. The background remains consistent with shelves filled with a variety of kitchen items. [0:00:22 - 0:00:25]: The angle shifts to a close-up of the person's right side profile, emphasizing some kitchen shelves, including what appears to be storage jars with various contents and a part of a blue kitchen cabinet. The person is talking, their mouth is open, and they are gesturing with their hands. [0:00:25 - 0:00:26]: The scene returns to the original angle, showing the person from the front. They are gesturing with their right hand, fingers extended toward the camera, possibly to emphasize a point being made. [0:00:26 - 0:00:27]: They are seen with hands clasped again, talking while slightly bending forward. The continuity of the background shelves is maintained, featuring dishes and various kitchen utensils. [0:00:27 - 0:00:28]: The camera moves closer to a view of the person's hands, which are now hovering over a baking tray, aligned with two pieces of dough. The counter displays several kitchen items, such as a wooden cutting board, a piece of rhubarb, and a stove. Part of the green shirt is visible at the top of the frame. [0:00:28 - 0:00:31]: The person’s hands are moving to position the pieces of dough on the baking tray. A close-up reveals more kitchen items: a rhubarb stick placed on a wooden tray along with a stove on the side, containers of ingredients, and another preparation area in the background. [0:00:31 - 0:00:33]: The scene continues to show detailed movements of the hands adjusting the dough on the tray, maintaining the previous kitchen layout, with a stove containing pans, bowls, and containers of various ingredients. [0:00:33 - 0:00:34]: The camera returns to a wider shot, showing the person from the front while indicating the dough with their hand. They explain something, gesturing toward the tray on the counter. [0:00:34 - 0:00:36]: The view changes to a different perspective where the person leans over the counter, reaching for ingredients. Behind the counter are kitchen shelves with dishes, jars, and knives. [0:00:36 - 0:00:37]: The person is still reaching for ingredients, now touching a jar with one hand. The background remains the same, with kitchen tools and ingredients aligning the counter. [0:00:37 - 0:00:38]: A close-up of the counter area shows multiple ingredients, including a block of butter on a plate, a small honey bottle, a bowl of almonds, and a variety of utensils. The baking tray with the dough is also visible near the edge of the counter. [0:00:38 - 0:00:39]: The camera pulls back to show the person standing behind the counter. The person is talking, hands resting on the counter, while various kitchen items are in the foreground, including a grater, bowls, honey, and blocks of butter. The wall and shelves in the background display a variety of kitchen utensils, dishes, and books.\n[0:00:40 - 0:01:00] [0:00:40 - 0:00:46]: The video shows a kitchen area with a person preparing food. The kitchen features white brick walls and a shelving unit with various kitchen items like bowls, jars, and utensils. Cabinets in the background are blue. The person is wearing a green t-shirt and is standing at a countertop that has multiple kitchen utensils and ingredients arranged neatly, including pans, bowls, and ingredients for cooking. The person appears to be explaining or discussing something while gesturing with their hands and looking at different items on the counter. [0:00:47 - 0:00:48]: A close-up view of the preparation area shows a person's hands arranging ingredients. There is a cutting board with two rectangular dough pieces, rhubarb stalks, and various small bowls containing butter, sugar, and other ingredients. The person is picking up a small bowl and adding it to the setup. [0:00:49 - 0:00:52]: The camera returns to the broader view of the person in the kitchen. They are continuing to explain steps in the preparation and moving around the countertop, occasionally picking up and showing ingredients to the camera. The person appears focused and is using hand gestures while speaking. Various kitchen tools and ingredients remain visible and well-organized on the counter. [0:00:53 - 0:00:56]: The person is now standing near the stovetop area, continuing to explain and demonstrate certain aspects of the cooking process. Their motions are deliberate, highlighting specific tools and ingredients laid out on the counter. The surrounding environment remains consistent with various kitchen items and a modern aesthetic. [0:00:57 - 0:00:58]: The video captures the person in a slightly different angle, showing more of the kitchen's interior. The person is now showing more active engagement with the stovetop, possibly indicating the transition to a cooking stage. The backdrop includes green plants near the window, adding a fresh look to the kitchen area.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What shirt color is the individual wearing right now?",
        "time_stamp": "00:00:21",
        "answer": "D",
        "options": [
          "A. Dark blue.",
          "B. Grey.",
          "C. White.",
          "D. Dark green."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_13_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:02]: In a kitchen setting, a person grates an orange over a bowl of ingredients with a grater. Ingredients and utensils are arrayed on a countertop, including butter, ginger, and a whisk. [0:07:03 - 0:07:04]: The person, wearing a grey shirt, continues grating the orange with concentration. There is a blue cabinet and an oven in the background. [0:07:05 - 0:07:06]: The grating process continues over the bowl. The orange's peel is finely grated. The bowl sits on a wooden cutting board, surrounded by various kitchen tools. [0:07:07]: The person maintains focus while grating. Several knives are arranged on a magnetic strip on the wall behind them, accompanied by neatly organized shelves of kitchenware. [0:07:08]: Momentarily pausing, the individual glances to the side, perhaps checking their progress or considering the next step. [0:07:09]: The person resumes the task, rubbing the orange back and forth over the grater. The countertop below holds a pot with a wooden spoon and oil. [0:07:10 - 0:07:11]: The person looks to their side, possibly preparing for the next step or ensuring all necessary ingredients and tools are within reach. [0:07:12 - 0:07:13]: An overhead shot reveals the countertop arrangement: a bowl of grated components, nearby dishes, cooking utensils, and a stovetop burner. The person holds the grater over a small frying pan. [0:07:14 - 0:07:16]: Close-up of the grater releasing orange zest into a frying pan already containing some oil, the zest falling steadily. [0:07:17]: The person's action shifts as they step away from the stove briefly. The frying pan is left slightly unattended but still visible, along with another orange sitting on the cutting board. [0:07:18]: In a wider view of the kitchen, the person is in action again, attending to a pan on the counter. Above, large red letters spell the word \"COOK\". [0:07:19]: They lift a pan of sliced food, likely vegetables or meat, indicating readiness to integrate this into the recipe in progress.\n[0:07:20 - 0:07:40] [0:07:20 - 0:07:21]: A stovetop with four burners is visible from a top-down perspective. The background appears to be a kitchen counter with a textured surface. There are two pans placed on the left-most and right-most burners. The left pan has remnants of food, while the right pan is cooking some ingredients that appear to be sautéed. [0:07:22]: A hand appears in the frame, reaching towards the right pan. This hand is preparing to interact with the ingredients in the pan. [0:07:23 - 0:07:24]: A close-up shows two hands holding the right pan filled with sautéing ingredients. The hands are bare and slightly muscular. The background includes a part of the countertop and items like bowls and utensils, some of which are metal and some are glass. [0:07:25 - 0:07:26]: An individual is visible, wearing a grayish-green shirt and standing in front of a kitchen counter. This person is closely attending to the pan on the stovetop, handling either the pan or its contents with focus. The background reveals a kitchen with white brick tiles, open shelves holding cups and plates, and a blue cabinet. [0:07:27]: The individual is now moving to their right, reaching out towards the counter. Their attention is likely shifted to another task on the countertop to their right. [0:07:28 - 0:07:29]: The camera captures a broader view of the kitchen space. The individual is standing in front of a wide counter that stretches across the frame. Various cooking utensils and containers (including bowls and knives) are organized on the counter and shelves. The individual is picking up an item from the countertop, possibly an orange or another piece of fruit, as their left hand is holding one and bringing it towards a bowl. [0:07:30 - 0:07:33]: The individual continues the task of prepping the fruit, peeling and slicing it. The person is completely focused on this activity, using both hands to handle the fruit and knife. The background elements include several bowls with ingredients, assorted utensils, and the ongoing cooking setup. [0:07:34 - 0:07:38]: A close-up perspective shows the hand slicing through the fruit on a wooden cutting board. A transparent mixing bowl is partially visible on the right side of the frame, containing what looks like a white mixture. The individual carefully slices and segments the fruit, placing some of the pieces into the bowl. [0:07:39]: The camera captures an overhead view of the countertop, highlighting the process of preparing ingredients for a recipe. Various items, including a whisk, measuring spoons, a jar of honey, chopped nuts, and other smaller bowls containing different components are present. The individual’s hand steadily slices through the fruit, continuing to assemble the dish.\n[0:07:40 - 0:08:00] [0:07:40 - 0:07:42]: On a wooden chopping board centered within the frame, various ingredients are arranged systematically. At the bottom center is an orange being peeled and sliced by a hand holding a serrated knife. Just above the chopping board to the left are several ingredients in separate bowls, including whipped cream in a clear blue bowl, and small quantities of powdered ingredients in other white bowls. To the right, a frying pan sits on an active stove, its surface showing residues of previously cooked food. Assorted kitchen utensils such as spatulas and ladles are organized neatly to the left of the board. [0:07:42 - 0:07:45]: A man, dressed in a green t-shirt, is seen working methodically in a modern kitchen with a mix of blue and wooden cabinetry. He stands directly at the chopping board, focusing intently on slicing the orange. He occasionally shifts his attention to other ingredients on the board, working with steady and practiced movements. [0:07:45 - 0:07:51]: The video switches to close-up shots of hands skillfully cutting an orange. The knife meticulously follows the curve of the fruit, peeling away the outer skin and revealing the bright, juicy segments beneath. The orange peel, partially removed, lies beside the fruit on the wooden chopping board. A glass bowl in close proximity to the cutting area occasionally blurs into the foreground, indicating its relative position on the counter. [0:07:51 - 0:07:53]: Returning to the broader kitchen view, the man continues to precisely cut and peel the orange. Around him, the kitchen’s aesthetically pleasing design is evident, featuring white brick walls, wooden shelves lined with kitchenware, and a variety of cooking tools mounted on the wall. The ambient light ensures clear visibility while providing a warm and inviting atmosphere. [0:07:53 - 0:07:56]: The man picks up a frying pan from the stovetop, which contains partially sautéed ingredients. He appears engaged and focused, tilting the pan slightly to allow the contents to shift, revealing assorted browned and sizzling components. His movements suggest a step-by-step culinary process, ensuring everything is cooked evenly. [0:07:56 - 0:07:59]: The video shifts perspective, capturing the man from a slight distance. He begins arranging freshly cut orange slices on a plate on the countertop beside the stove. In the background, additional kitchen elements like ovens integrated into the blue cabinetry and various plants by the windows can be seen. [0:07:59 - 0:08:00]: The man reaches upwards to retrieve a plate from a shelf. His kitchen is well-organized, with items cleanly arranged on the shelves, including various bowls, plates, and jars. The word \"COOK\" in bold red letters decorates the upper shelf, adding a personal and thematic touch to the decor.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What was the person doing while holding the grater over the bowl?",
        "time_stamp": "0:07:06",
        "answer": "D",
        "options": [
          "A. Slicing the orange.",
          "B. Mixing ingredients.",
          "C. Washing the orange.",
          "D. Grating the orange peel."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_13_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: The scene opens in a well-organized kitchen with blue cabinetry. A man in a green t-shirt and striped apron reaches for a white plate on a counter. To his right, a stove with a frying pan and other cooking utensils is visible. Above the stove, a red sign reads \"COOK,\" and various kitchen items are neatly arranged on shelves. [0:08:01 - 0:08:02]: The man slightly turns to his left while holding the white plate, looking at the stove. The kitchen layout, including the blue cabinetry and organized shelves, remains consistent. [0:08:02 - 0:08:03]: The man positions the white plate near the stove, preparing to cook. The background continues to display the neat arrangement of items on the shelves. [0:08:03 - 0:08:04]: The man, still facing the stove, holds the frying pan with his right hand while listening to someone off-camera. Text on the screen reads, \"Producer: 7 minutes gone.\" [0:08:04 - 0:08:05]: He maintains his focus on the frying pan, listening attentively to the producer's instruction. [0:08:05 - 0:08:06]: The man moves the frying pan, possibly adjusting its position on the stove. [0:08:06 - 0:08:07]: Tilting the frying pan with his right hand, he seems to be closely monitoring its contents. [0:08:07 - 0:08:08]: The man turns back towards the counter. [0:08:08 - 0:08:09]: He places the frying pan down and steps away from the stove, reaching for something off-screen. The background, adorned with various kitchen tools and ingredients, remains unchanged. [0:08:09 - 0:08:10]: The man bends slightly over the counter, focusing on an orange which he begins to peel. [0:08:10 - 0:08:11]: He continues to peel the orange attentively. [0:08:11 - 0:08:12]: The man slices the orange into segments, deftly maneuvering the knife around the fruit. [0:08:12 - 0:08:13]: With precision, he continues slicing, ensuring each piece is evenly cut. [0:08:13 - 0:08:14]: An extreme close-up of the man's hands showcases the detailed slices of the orange, emphasizing his careful cutting technique. [0:08:14 - 0:08:15]: The intense close-up still captures the orange slices and the man's hands, focusing on the knife cutting through the fruit. [0:08:15 - 0:08:16]: The slicing continues in close detail, highlighting the man's proficiency with his knife. [0:08:16 - 0:08:17]: He tilts his head downwards, concentrating on his work as he slices through another piece of the orange. [0:08:17 - 0:08:18]: The man remains focused, his hands carefully cutting and arranging the orange pieces on the cutting board. [0:08:18 - 0:08:19]: Maintaining his dedicated attention to detail, he completes the slicing of the orange pieces meticulously.\n[0:08:20 - 0:08:40] [0:08:20 - 0:08:22]: The man stands at a kitchen counter, focused intently on the task in front of him. He wears a dark green t-shirt and holds a knife in his right hand, which he uses to carefully slice an orange into thin segments. The kitchen background features white brick walls, with shelves holding various bowls, jars, and a few visible utensils. A large red sign reading \"COOK\" decorates the top shelf, adding a splash of color to the otherwise muted tones. His concentration is evident as he cuts the orange peel into small pieces. [0:08:23 - 0:08:24]: In a close-up view, the man continues slicing the orange into thin strips, with the knife blade gliding smoothly through the fruit peel. Pieces of the orange peel are scattered on the wooden cutting board beneath, emphasizing the precision of his slicing technique. [0:08:25 - 0:08:28]: The frame widens again, showing the man placed centrally in the kitchen as he maintains his focus on slicing. To his left, a frying pan sits on the stovetop, ready for use. The counter has a wooden cutting board with orange peel strips and various utensils. [0:08:29]: He starts to move his right hand closer to the bowl on his left, indicating preparation for the next step while maintaining a steady grip on the orange. [0:08:30 - 0:08:31]: The knife cuts through the orange peel meticulously as the man holds the fruit firmly in his left hand. His actions suggest he is making precise cuts to avoid wasting any part of the orange. [0:08:32]: With the orange still in his hand, he positions the knife to make further incisions. The detailed close-up displays the vibrant red and orange hues of the fruit contrasted against the shiny blade of the knife. [0:08:33 - 0:08:34]: The view zooms in slightly more, showing the final slices being made. The man's hands remain steady, emphasizing his focus and expertise. The kitchen stove is visible in the background. [0:08:35 - 0:08:38]: The man moves away from the cutting board towards the stove while still holding the orange segments in his hand. He positions them above the frying pan, ready to proceed with the next cooking step. He slowly lowers the orange pieces into the pan, showing care in his placement. The kitchen atmosphere remains consistent, with a tidy and organized appearance.  [0:08:39]: The last close-up highlights the freshly cut orange segment poised above the pan, ready to be added to the meal. The man's fingers, slightly glistening with juice, maintain a firm hold on the fruit.\n[0:08:40 - 0:09:00] [0:08:40 - 0:08:41]: In a kitchen, a person is holding a tomato in their left hand and slicing it with a knife in their right hand. A stove with black grates is visible in the background. [0:08:41 - 0:08:42]: The person continues slicing the tomato, and small slices fall onto the knife blade. The stove remains in the background. [0:08:42]: The person, with short hair and wearing a green shirt, is focused on cooking. There are various cooking utensils and ingredients on a kitchen counter. Behind the person, shelves with plates and glassware are mounted on a white tiled wall. [0:08:43]: A close-up of a frying pan on a stove, a few pieces of sliced tomato are sizzling in oil. [0:08:44 - 0:08:45]: The person is cooking, adding ingredients to the frying pan while holding a kitchen utensil in their right hand. The countertop is cluttered with various bowls and cooking items. [0:08:45 - 0:08:46]: The person, still wearing a green shirt, is now handling a clear glass bowl on the counter. The background includes shelves with more kitchen items and a white brick wall. [0:08:46 - 0:08:47]: The person is utilizing a grater over the glass bowl, likely adding seasoning or zest to the bowl's contents. [0:08:47 - 0:08:48]: The person persists in grating ingredients into the bowl, showing concentration on their task. [0:08:48 - 0:08:49]: The person is squeezing juice from a small lemon into the bowl. The bowl contains a white mixture, possibly a sauce or dressing. [0:08:49 - 0:08:50]: More lemon juice is being squeezed into the bowl, enhancing the mixture inside. [0:08:50]: The lemon squeezing has stopped, and the bowl is now left in view on the counter along with other small bowls and a partially sliced lemon on a plate. [0:08:51]: The person, still present in the kitchen, is turning and looking away from the bowl on the counter. [0:08:52]: The person moves towards the sink positioned against the white brick wall, indicating a transition in their activity. [0:08:53]: The person is washing their hands at the sink with their back turned, emphasizing cleanliness during food preparation. [0:08:54]: The person dries their hands with a cloth towel, still standing by the sink. [0:08:55]: The person walks towards a blue oven on the opposite side of the kitchen to check on food inside. [0:08:56]: The person continues to attend to the oven, possibly adjusting or removing food from it. [0:08:57]: The person's focus remains on the oven, maintaining the position as they handle its contents. [0:08:58]: Maintaining their position near the oven, the person keeps checking inside. [0:08:59]: The person turns to face the camera, holding a dish towel and possibly explaining something while standing in the kitchen.",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the man bend over the counter?",
        "time_stamp": "0:08:17",
        "answer": "B",
        "options": [
          "A. To pick up a dropped item.",
          "B. To peel an orange.",
          "C. To check the stove.",
          "D. To clean the counter."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_13_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:05]: In a modern kitchen with a white brick backsplash and blue cabinets, a person stands at a countertop, wearing a grey t-shirt. Several knives are mounted on a magnetic strip on the wall behind, and various kitchen items are neatly arranged on wooden shelves. There are two saucepans on the stove to the left of the person. The individual drapes a towel over a clear glass mixing bowl placed on a wooden chopping board. Additional kitchen utensils and food ingredients are seen around the mixing bowl, including a white ceramic bowl, a small metallic sieve, butter, and a bunch of cut vegetables.  [0:09:05 - 0:09:10]: The person, working with focused movements, begins to mix ingredients in the bowl with a spoon. The actions emphasize combining the contents vigorously, ensuring thorough mixing. The camera angle shifts closer, providing a detailed view of the bowl and the hands at work. Nearby utensils and ingredients remain visible, with a carefully arranged assortment on the countertop. [0:09:10 - 0:09:15]: As the mixing continues, the view alternates between a close-up from above, showing the bowl from a top-down perspective, and wider shots that include parts of the kitchen. The ingredients in the glass bowl become more thoroughly mixed as the person works the spoon around the sides. The background remains consistent with the kitchen setup, highlighting the ergonomic arrangement of tools and materials. [0:09:15 - 0:09:19]: The person's focused expression suggests meticulous attention to detail while preparing the mixture. Attention to hand movements and actions is key, as they ensure a smooth and properly combined mixture. The movement transitions to grating an orange, adding zest to the bowl's contents. The bright orange peel contrasts against the mixed ingredients, adding visual appeal. The backdrop remains unchanged, with the orderly kitchen environment continuing to provide context to the culinary task at hand.\n[0:09:20 - 0:09:40] [0:09:20 - 0:09:25]: In a kitchen setting, a person wearing a dark green shirt is seen grating an orange into a glass bowl containing a mixture that appears white and creamy. The scene includes a stovetop with a pan nearby, and several bowls on a wooden countertop, positioned on the right. The person holds an orange with their left hand and a metal grater with their right, consistently grating the orange zest into the bowl. There are various kitchen utensils and ingredients scattered around. [0:09:26 - 0:09:27]: The video transitions to an overhead view, showing a close-up of the grater and orange. The individual’s hands are in focus as they grate. Surrounding the bowl, there are additional ingredients like butter, sliced oranges, and different condiments. The person continues to grate the orange in quick, consistent motions. [0:09:28 - 0:09:31]: The perspective shifts back to the previous angle focusing on the person’s torso and face. The individual continues grating the orange into the bowl, concentrating on their task. Shelves with various kitchen items and utensils are visible in the background, with the white tiled wall providing a contrast. Red letters spelling \"COOK\" are seen on the shelf. [0:09:32 - 0:09:33]: The scene switches back to a close-up view of the bowl on the wooden countertop. The grating continues, with the orange zest falling into the creamy mixture. The stovetop and other kitchen tools are clearly visible on the left, indicating an ongoing cooking process. [0:09:34 - 0:09:36]: The camera angle shifts to show the entire kitchen counter. The person is still grating the orange, but they shortly set the grater down and inspect the orange. The countertops are filled with various utensils, dishes, and ingredients, showing a busy cooking environment. [0:09:37 - 0:09:39]: The focus moves to an extreme close-up of the hands, showing the last portions of orange zest being grated. The person uses a knife to scrape off the remaining zest from the grater, ensuring none of it is wasted. The creamy mixture in the bowl is now more visibly speckled with the orange zest.\n[0:09:40 - 0:10:00] [0:09:40 - 0:09:42]: A person wearing a grey t-shirt is standing in a kitchen with white brick walls and blue cabinets. He is in the process of grating something into a clear glass mixing bowl on the countertop using a microplane. Various kitchen utensils and ingredients including bowls, a saucepan, and an orange appear on the countertop. [0:09:42 - 0:09:44]: In the next frame, the person's focus is still on the grating activity, the camera angle changes to capture a close-up, showcasing his hand movements more clearly. [0:09:44 - 0:09:45]: The camera zooms out slightly to capture a frontal view of the person, still focused on grating into the mixing bowl on the countertop. [0:09:45 - 0:09:46]: The person starts stirring the contents inside the mixing bowl while standing in the kitchen.  [0:09:46 - 0:09:48]: A close-up clearly highlights the mixing process, showing hands stirring a creamy mixture inside the glass bowl with a wooden spatula which contains small orange fragments that were recently added. [0:09:48 - 0:09:50]: The close-up continues capturing the mixing process from another angle, providing a clear view into the bowl where the ingredients are being combined thoroughly. [0:09:50 - 0:09:52]: The stirring continues, showing the glass bowl's contents getting smoother and better mixed. The camera zooms out to show the person’s torso in focus. [0:09:52 - 0:09:54]: The person resumes stirring while standing at the kitchen counter. A variety of utensils and ingredients are visible around, meanwhile two burners of the stove are visible in the background, indicating the workspace is active. [0:09:54 - 0:09:56]: Reaching beside the bowl, he picks up a small ingredient bowl, possibly containing salt or sugar, while still mixing. [0:09:56 - 0:09:58]: Without missing a beat, the camera angle shifts back, capturing the person adding the contents of the small ingredient bowl into the mixture while maintaining control of the large mixing bowl. [0:09:58 - 0:09:59]: The glass mixing bowl is placed back down on the counter. The person turns to the side to possibly reach for another ingredient or utensil. His actions and surrounding setup suggest a cooking or baking process.  [0:09:59]: The person then bends down looking closely into what appears to be a double oven positioned against the blue cabinets, appearing to check or monitor something inside. The word \"COOK\" appears in red letters on a shelf above him.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What tool is the person using to grate the orange?",
        "time_stamp": "0:09:35",
        "answer": "D",
        "options": [
          "A. Cheese grater.",
          "B. Knife.",
          "C. Peeler.",
          "D. Microplane."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_13_real.mp4"
  },
  {
    "time": "[0:11:00 - 0:11:51]",
    "captions": "[0:11:00 - 0:11:20] [0:11:00 - 0:11:05]: A person is seen lifting a piece of pastry topped with red fruit from a baking tray using both hands. The piece of pastry is being carefully placed onto a white plate resting on a countertop. The countertop has a rough, stone-like surface, and there is a stainless steel stovetop with a black burner and a small pan. The background shows shelves with kitchen utensils, spices, and decorative items. [0:11:06 - 0:11:09]: The person, wearing a grey shirt, slightly bends forward, concentrating on placing the piece of pastry neatly on the plate. Several items are visible on the counter, including a large bowl, a knife, a partially visible wooden cutting board, and various small bowls and utensils. The background has a white brick wall with hanging shelves holding plates, jars, and bottles. [0:11:10 - 0:11:12]: The person shifts to the left side of the countertop, still focused on the task. The person reaches for another piece of the pastry. The kitchen background continues, showcasing blue cabinets, a white sink with a modern faucet, and more kitchen tools. Additionally, there is another plate with food items placed on the stove. [0:11:13 - 0:11:14]: From an aerial view, the countertop reveals a variety of kitchen items, including a bowl with cream, a small bowl with sliced almonds, a chunk of butter, a wire mesh strainer, and a plate. A wooden spatula and a spoon rest beside these items.  [0:11:15 - 0:11:16]: The person tilts the bowl to scoop out the creamy mixture onto the pastry. The focus remains on the smooth muscle movement as the food is being prepared. The background remains consistent with the prior scenes, detailing the kitchen’s neat and well-organized arrangement. [0:11:17]: The person is seen adding a dollop of cream onto the plate with the piece of pastry. The baking tray and small frying pan remain on the stove, while more items lie prepared on the counter. [0:11:18 - 0:11:19]: In these final moments, the person hurriedly reaches out for additional toppings to place on the pastry, maintaining a detailed focus on presentation. The kitchen background, with its organized setting and vibrant utensils, continues to frame the scene.\n[0:11:20 - 0:11:40] [0:11:20 - 0:11:21]: A man, wearing a green t-shirt, is shown sprinkling seasoning onto a dish. The scene takes place in a kitchen equipped with both modern and rustic elements. He is slightly bent over the counter, with focused attention on the dish in front of him, which appears to be a dessert. Various kitchen utensils and pots are visible in the background, along with a blue cabinet. [0:11:21 - 0:11:22]: The man holds the dish with both hands, presenting it towards the camera. The kitchen backdrop, filled with neatly arranged kitchenware, remains consistent, adding a homely, organized ambiance to the scene. [0:11:22 - 0:11:24]: The man stands upright in the kitchen. He holds the plated dessert up and appears to be explaining or presenting it. An array of kitchen items, such as knives, bowls, and containers, are systematically placed on the shelves in the background. \"LOOK\" in red letters is prominent on the left side of the frame. [0:11:24 - 0:11:25]: He continues to present the dish with a smile on his face. The kitchen setting remains constant with all utensils and decorative elements in place. [0:11:25]: Two dessert items are presented on a cooling rack and a white plate with a scoop of vanilla ice cream on the side. They sit on a wooden counter. The desserts are rectangular, showcasing layers of what appears to be pastry, garnished with neatly aligned, thin slices of fruit. [0:11:26 - 0:11:28]: Close-up shots of the dessert reveal the detailed texture of the pastry and the thin slices of pink fruit, possibly rhubarb. The ice cream scoop alongside with almond slices adds to the presentation. Each layer and droplet of syrup on the fruit is clearly visible, indicating careful preparation. [0:11:29 - 0:11:30]: The camera angle switches to an overhead view of the two dessert plates. The alignment of the rectangular pastries is obvious, with the slanted edges and uniform layering of the fruit strips. The vanilla ice cream on the round white plate has almond slices scattered around. [0:11:31 - 0:11:32]: The scene transitions back to the man in a green t-shirt. He is now speaking to the camera, using hand gestures to emphasize his points. The backdrop remains his kitchen with various kitchen items on the shelves. [0:11:32 - 0:11:33]: The man continues speaking, his hands are more open in a welcoming or explaining gesture. [0:11:33 - 0:11:34]: He changes his gestures to a more relaxed position, continuing his explanation. The backdrop remains constant with the neatly arranged utensils and kitchenware. [0:11:34 - 0:11:36]: The man displays a reassuring smile, clasping his hands together. The view doesn't change, retaining the same elements in the background. [0:11:36]: He has an attentive expression, possibly wrapping up the presentation or explanation with a pointed look towards the camera.  [0:11:37 - 0:11:39]: The viewpoint shifts slightly, showing the man from a different angle still wearing a green t-shirt, standing by the blue cabinet area. The double oven is visible behind him along with a wooden shelf holding jars, emphasizing the well-equipped and organized kitchen.\n[0:11:40 - 0:11:51] [0:11:40 - 0:11:47]: The video starts with a middle-aged man in a kitchen. The background consists of a white brick wall with two wooden shelves filled with various kitchen items. On the top shelf, there are green bottles, white bowls, and a small white clock. The bottom shelf holds more bowls, some glass containers, and a set of knives in a knife block. To the left of the frame, a part of a cutting board and some more jars are visible. The man is wearing a green t-shirt and is speaking directly to the camera, gesturing with his hands. He is making various hand movements and expressions while speaking, shifting his gaze from the camera occasionally. The lighting is bright and natural, clearly illuminating the scene. The man looks focused and engaged with what he is saying. [0:11:48 - 0:11:49]: The frame switches to a close-up of a large cookbook titled \"Gordon Ramsay in 10: Delicious Recipes Made in a Flash.\" The book is placed on a wooden cutting board, and the surface beneath it appears to be a concrete countertop. The book cover features several images of food dishes and the author. The title of the book is prominently displayed in large white letters against a blue background. The lighting in this shot is consistent with the previous frames, bright and natural.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the man sprinkling onto the dish in the kitchen?",
        "time_stamp": "0:11:26",
        "answer": "D",
        "options": [
          "A. Sugar.",
          "B. Flour.",
          "C. Cocoa powder.",
          "D. Sliced nut."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_13_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Which structure is visible overhead right now?",
        "time_stamp": "00:00:13",
        "answer": "D",
        "options": [
          "A. A traffic signal.",
          "B. A restaurant sign.",
          "C. A store roof.",
          "D. A pedestrian bridge."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_299_real.mp4"
  },
  {
    "time": "[0:01:55 - 0:02:15]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is directly ahead of the motorcyclists right now?",
        "time_stamp": "00:02:01",
        "answer": "D",
        "options": [
          "A. A construction crane.",
          "B. A green building.",
          "C. A hospital.",
          "D. A Michelin sign."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_299_real.mp4"
  },
  {
    "time": "[0:03:50 - 0:04:10]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the yellow and black barriers located right now?",
        "time_stamp": "00:04:06",
        "answer": "D",
        "options": [
          "A. Only on the left side of the road.",
          "B. Only on the right side of the road.",
          "C. In the middle of the road.",
          "D. On both sides of the road."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_299_real.mp4"
  },
  {
    "time": "[0:05:45 - 0:06:05]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the red and white barriers located right now?",
        "time_stamp": "00:05:47",
        "answer": "D",
        "options": [
          "A. On both sides of the road.",
          "B. Only on the right side of the road.",
          "C. In the middle of the road.",
          "D. Only on the left side of the road."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_299_real.mp4"
  },
  {
    "time": "[0:07:40 - 0:08:00]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the bridge in relation to the rider right now?",
        "time_stamp": "00:07:54",
        "answer": "D",
        "options": [
          "A. Above the rider.",
          "B. To the right of the rider.",
          "C. In front of the rider.",
          "D. Behind the rider."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_299_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the food preparation process just now?",
        "time_stamp": "00:00:10",
        "answer": "A",
        "options": [
          "A. The cook added sautéed greens, a spoonful of sauce, and toppings from the counter to a plate for serving.",
          "B. The cook boiled pasta, seasoned it with herbs, and added it to the sauce.",
          "C. The cook prepared a salad by chopping vegetables and adding a vinaigrette.",
          "D. The cook grilled a steak, seasoned it with spices, and placed it on the plate."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_411_real.mp4"
  },
  {
    "time": "[0:02:05 - 0:02:15]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the content of the video just shown?",
        "time_stamp": "0:02:15",
        "answer": "A",
        "options": [
          "A. The chef grilled vegetables using a blowtorch.",
          "B. The chef prepared a stir-fry dish on a stove, adding various ingredients and seasoning.",
          "C. The chef fried items in a deep fryer and then drained them on a rack.",
          "D. The chef prepared a salad by chopping fresh vegetables and mixing them in a bowl."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_411_real.mp4"
  },
  {
    "time": "[0:04:10 - 0:04:20]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the recent food preparation process?",
        "time_stamp": "00:04:20",
        "answer": "A",
        "options": [
          "A. The individual cooked a vegetable mix, plated it, and added some finishing touches for garnish.",
          "B. The individual sliced fruits and arranged them decoratively on a platter.",
          "C. The individual prepared a meat dish, seasoned it with various spices, and placed it on the grill.",
          "D. The individual mixed ingredients for a smoothie and poured it into a glass."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_411_real.mp4"
  },
  {
    "time": "[0:06:15 - 0:06:25]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken by the individual just now?",
        "time_stamp": "00:06:24",
        "answer": "A",
        "options": [
          "A. The individual cleaned a tray, prepared vegetables and add cooking oil to the vegetables.",
          "B. The individual prepared a salad, dressed it with sauces, and served it to the customer.",
          "C. The individual grilled a meat patty, added cheese, and placed it in a bun.",
          "D. The individual was teaching a cooking class, demonstrating how to prepare different dishes."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_411_real.mp4"
  },
  {
    "time": "[0:08:20 - 0:08:30]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions just shown?",
        "time_stamp": "00:08:30",
        "answer": "A",
        "options": [
          "A. The individual is arranging bok choy and other greens on a cooking surface and cooking them.",
          "B. The individual is mixing a salad with various dressings and plating it.",
          "C. The individual is marinating meat with different sauces and preparing it for grilling.",
          "D. The individual is seasoning fish and placing it onto a grill."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_411_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:00:13",
        "answer": "C",
        "options": [
          "A. The concept of the division.",
          "B. The process of multiplying numbers.",
          "C. The concept of subtraction.",
          "D. How to solve an equation."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_226_real.mp4"
  },
  {
    "time": "[0:02:03 - 0:02:33]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:02:16",
        "answer": "A",
        "options": [
          "A. The meaning of the dividend.",
          "B. The process of multiplying numbers.",
          "C. The concept of subtraction.",
          "D. How to solve an equation."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_226_real.mp4"
  },
  {
    "time": "[0:04:06 - 0:04:36]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:04:16",
        "answer": "A",
        "options": [
          "A. How to determine the remainder.",
          "B. How to find the remainder.",
          "C. The concept of partial quotients.",
          "D. The relationship between divisor and dividend."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_226_real.mp4"
  },
  {
    "time": "[0:06:09 - 0:06:39]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:06:34",
        "answer": "D",
        "options": [
          "A. Why dividing cookies by people is useful.",
          "B. The importance of remainders in division.",
          "C. Use real-life example to explain the concept of remainders.",
          "D. How to distribute the cookie to people."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_226_real.mp4"
  },
  {
    "time": "[0:08:12 - 0:08:42]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "Based on the current explanation, what might the speaker do next?",
        "time_stamp": "00:08:41",
        "answer": "A",
        "options": [
          "A. Explain the next division step using the tabular.",
          "B. Talk about prime numbers.",
          "C. Introduce a new mathematical operation.",
          "D. Discuss the properties of multiplication."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_226_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:03]: A person is seen standing at a kitchen counter. They are holding a small bowl of green ingredients in their right hand and using a spoon in their left hand to scoop the contents into a larger glass bowl containing ground meat. The larger bowl is positioned on a wooden chopping board placed on the counter. The countertop has a gas stovetop with a frying pan, and there is a wall-mounted cabinet and utensil holder in the background. [0:01:04 - 0:01:09]: The person steps back from the counter, holding an object, and addresses the camera. They use hand gestures while talking, emphasizing their points. There is a white-tiled backsplash with floating wooden shelves filled with various kitchen items, including plates and jars. The person then reaches for a small bottle and begins pouring its contents into the glass bowl with the ground meat. [0:01:10 - 0:01:11]: The person uses their hands to mix the ingredients thoroughly in the glass bowl. The countertop features several small bowls with spices and a sprig of rosemary next to the mixing bowl. [0:01:12 - 0:01:15]: The person continues mixing the ingredients with their hands, ensuring even distribution of the green mixture and seasoning. Their focus remains on the task at hand, showing careful attention to the mixing process. [0:01:16 - 0:01:19]: The person wipes their hands with a cloth and grabs a lime from the counter. They zest the lime directly over the bowl, adding the citrus flavor to the meat mixture, then they discard the lime rind into another bowl on the counter. During this time, the kitchen's blue and wooden cabinets become more prominent in the background.\n[0:01:20 - 0:01:40] [0:01:20 - 0:01:22]: A person wearing a black shirt is grating the zest off a green lime into a clear glass bowl using a fine grater. The grated lime zest is falling into the bowl, which already contains a mixture of ingredients. The action takes place on a wooden chopping board placed on a kitchen countertop. On the left side of the image, part of a gas stove with a frying pan can be seen. [0:01:23 - 0:01:27]: The person continues to grate the lime zest into the bowl with meticulous attention. The grated zest accumulates in the bowl with the mixture of ingredients. Behind the chopping board, the kitchen setup includes various jars, bottles, and utensils organized on shelves against a white brick wall. A red sign with the letters 'Coor' is mounted on the wall above the shelves. The kitchen cabinets are blue with metal handles, and a cutting board is visible on the counter. [0:01:28]: A close-up view shows the fine grater with some lime zest stuck to its surface held above the bowl. A part of the mixture inside the bowl is visible, focusing on the grater and the texture of the lime zest. [0:01:29]: The grater moves out of the frame, and the clear glass bowl with the mixture is left in focus. The background remains blurred, emphasizing the ingredients in the bowl. [0:01:30 - 0:01:32]: The person's hands are now mixing the ingredients inside the bowl. The hands are working the mixture thoroughly. The bowl is half-full, and the mixture's texture appears dense and slightly moist. [0:01:33 - 0:01:34]: The person kneads and mixes the ingredients with both hands, ensuring an even blend. The kitchen background remains constant with various cooking tools and ingredients laid out on the counter and shelves. Green leafy vegetables and various bottles are visible on the counter. [0:01:35 - 0:01:37]: The person continues mixing the ingredients with focused attention, occasionally moving their hands to get a better grip on the mixture inside the bowl. The consistent actions demonstrate the thorough blending required for the preparation.  [0:01:38 - 0:01:39]: The hands keep moving in the bowl, mixing the ingredients with precision. The person's expression suggests concentration on the task at hand. The entire sequence showcases the methodical preparation involved in blending the ingredients.\n[0:01:40 - 0:02:00] [0:01:40 - 0:01:44]: A person is seen wearing a black shirt, captured from a first-person perspective in a kitchen setting. The background features a white brick wall with shelves holding various kitchen items, such as jars, containers, and decorative pieces. The person is working with their hands, appearing to shape or mold something. The shelves in the background hold items like a large red 'COOK' sign, a brown basket, glass containers, and green bottles. Several knives are visible hanging on the wall to the right;  [0:01:45 - 0:01:48]: The person moves towards a countertop with a stove. Their hands are shown molding a mixture near a glass bowl, with two stainless steel frying pans placed on the stove to the left. The stove is built into a kitchen island with a blue backdrop and wooden countertop visible in the background. The scene is very detailed and focused on the actions of shaping the mixture with their hands. The lighting is bright and natural, illuminating the hands and kitchen items clearly;  [0:01:49 - 0:01:54]: The view shifts to an overhead angle, focusing on a transparent glass bowl placed on a wooden chopping board. The bowl contains a meat mixture, and the person's hands continue shaping or rolling the mixture into a specific form. The chopping board is surrounded by various kitchen items, such as a blue-handled knife, a green lime, and several small glass bowls containing different spices. A stainless steel stovetop is visible to the right, hinting at the preparation for cooking; [0:01:55 - 0:01:57]: The person places a rolled meatball onto the chopping board, then uses their hands to scoop more of the mixture from the glass bowl. The movement is continuous and smooth, suggesting experienced familiarity with the task. The hands expertly shape another portion of the mixture while the already rolled meatball rests beside the bowl on the chopping board. The focus remains on the person's dexterous hand movements and the detailed kitchen setup;  [0:01:58 - 0:01:59]: A close-up shot showcases the person's hands molding the meat mixture, with the background slightly blurred, keeping the focus on the preparation process. The clear glass bowl can be seen in the foreground, containing the remaining mixture. The kitchen setting remains consistent, with bright lighting emphasizing the hands' precise movements while shaping the meatballs.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What does the person do right after zesting the lime over the bowl?",
        "time_stamp": "00:01:32",
        "answer": "D",
        "options": [
          "A. Mixes the ingredients with a spoon.",
          "B. Adds another ingredient from a bottle.",
          "C. Places the lime on the countertop.",
          "D. Mixes the ingredients by his hand."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_15_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:02]: A pair of hands is seen shaping a round object made of a brown, malleable material. The fingers are pressed around the object, smoothing it out;  [0:02:03]: The camera zooms out, revealing a kitchen setting with white brick walls and wooden shelves with various kitchen items. A man, wearing a black t-shirt, is focused on the task at hand as he shapes the object;  [0:02:04 - 0:02:05]: The man continues shaping the object, occasionally glancing down at it. On the countertop in front of him are a glass bowl, a wooden cutting board, and some finished round objects of similar material;  [0:02:06 - 0:02:10]: The camera shifts to an overhead view, showing the man's hands working with the brown material inside a glass bowl. He shapes another round object, placing it next to the others on the wooden cutting board;  [0:02:11 - 0:02:13]: The man continues to meticulously shape the brown material in his hands. The rounded objects begin to form uniform shapes, lined up neatly on the cutting board;  [0:02:14 - 0:02:15]: The man adds another finished round object to the group on the cutting board and then reaches back into the glass bowl for more material;  [0:02:16 - 0:02:19]: The camera switches back to a side view. The man is seen continuing his task, shaping the brown material with swift, practiced movements. Three round objects lie in the glass bowl, and several more sit on the cutting board. In the background, a stove and additional kitchen items can be seen.\n[0:02:20 - 0:02:40] [0:02:20 - 0:02:26]: A pair of hands is shaping a piece of ground meat into a patty over a wooden cutting board. A clear glass bowl containing more ground meat is placed on the board. Next to the board, there are two round pans on a stovetop with one of the pans slightly closer to the edge of the kitchen counter. The background reveals teal-colored cabinets and wooden sections, with parts of the wall decorated with white tiles. [0:02:21 - 0:2:26]: A person wearing a black t-shirt is seen patting pieces of ground meat between their hands. Behind them, a counter with a white tile backsplash hosts numerous shelves stocked with various kitchen items, such as bowls, books, and jars. The scene contains typical kitchen elements: a sink, a cutting board with pieces of shaped ground meat, a lime, and fresh greens on the counter. [0:02:27 - 0:02:29]: The person is placing the glass bowl into the sink with their back facing the camera. A set of knives is mounted on the tiled wall above the countertop, while two round pans and other kitchen items remain on the counter. [0:02:30 - 0:02:31]: A close-up shot of neatly shaped ground meat pieces resting on a wooden cutting board is displayed. The cutting board is positioned next to a stovetop's cast iron grate. [0:02:32 - 0:02:33]: The person is holding a striped cloth while standing by the counter. Kitchen shelves stocked with dishes and various kitchenware are visible in the background, and the word \"COOK\" is artfully displayed in large red letters on the wall. [0:02:34 - 0:02:34]: The person continues to hold the cloth and reaches toward one of the two pans on the stovetop. [0:02:35 - 0:02:37]: The scene shows the person standing with a striped cloth and gesturing towards the stovetop. There are visible cutting boards and knives in the background, along with items on the shelves, such as bottles and ceramic bowls. The word \"COOK\" remains prominently featured on the wall. [0:02:38 - 0:02:39]: The person holding the striped cloth appears to be handling the dishes and preparing the equipment for the next cooking steps. Kitchen elements including pans, countertops, utensils, and decorative items like the word \"COOK\" continue to be visible in the background.\n[0:02:40 - 0:03:00] [0:02:40 - 0:02:41]: In a well-lit and modern kitchen, a person wearing a dark T-shirt stands in front of a stove with two frying pans. Above the white-tiled backsplash, shelves hold various kitchen items, including green and glass bottles, plates, and spice jars. A large, red sign reading \"COOK\" is mounted on the wall. The person is holding a bottle of olive oil, and they are in the process of pouring the oil into one of the frying pans which are located on the right side of the stove. [0:02:42 - 0:02:43]: The camera captures a close-up of the two frying pans from an angle above the stovetop. The left frying pan has a thin layer of olive oil spread out in a circular pattern, and a similar pattern appears in the right frying pan. Kitchen utensils and items, including a red pepper mill, are visible on the countertop, along with the edge of a wooden cutting board holding several round, uncooked meatballs. [0:02:44]: The close-up view of the stovetop continues, focusing on the frying pans as the person’s hand hovers over the pan on the right. [0:02:45]: The person is seen lifting the right frying pan, tilting it slightly, which causes the oil to spread evenly across the pan's surface. The kitchen background with blue cabinets and the backsplash remains visible. [0:02:46]: As the person sets the pan back down on the stovetop, they reach their hand towards the cutting board where the meatballs are placed, indicating the next steps in their cooking process. [0:02:47]: The person uses their right hand to pick up one of the raw meatballs from the cutting board, with the frying pans still in view. [0:02:48]: The meatball is placed into the right frying pan, and the person's hand presses it down gently into the heated pan with the palm, ensuring it makes good contact with the oil. [0:02:49]: The person immediately picks up another meatball, preparing to place it into the frying pan. The partially cooked meatball is visible in the foreground. [0:02:50 - 0:02:51]: The camera angle shifts to the person leaning slightly over the stovetop while adding another meatball to the pan. They are focused on the frying process, with two pieces already in the pan starting to cook. The modern kitchen setting, with plants visible through a window, remains in the background. [0:02:52 - 0:02:53]: The person's right hand moves another meatball from the cutting board to the frying pan. They are carefully placing the meatballs while making sure they are evenly spaced out in the pan. [0:02:54]: Viewing from above, the scenery captures both frying pans. Three meatballs sizzle in the right pan, and the person’s hand is about to place a fourth one. The left pan remains empty, with oil spread evenly across its surface. [0:02:55]: The fourth meatball is placed into the frying pan, and the person adjusts its position slightly to ensure it touches the pan evenly, allowing it to cook uniformly alongside the others. [0:02:56]: The person’s hand presses down gently on the fourth meatball, flattening it slightly to help it cook more evenly in the hot oil. The other three meatballs continue to sizzle. [0:02:57]: With all four meatballs now in the pan, the person’s hand begins to withdraw, and the meatballs are spaced out and starting to brown around the edges.  [0:02:58 - 0:02:59]: The scene captures a close-up of the meatballs cooking in the pan, with the person’s fingers adjusting one of the meatballs to ensure it cooks properly. The sizzling sounds of the meat cooking are implied through the close-up visuals of the meat in the hot oil.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What item is the man holding in his left hand right now?",
        "time_stamp": "00:02:40",
        "answer": "D",
        "options": [
          "A. A spatula.",
          "B. A red pepper mill.",
          "C. A glass bowl.",
          "D. A bottle of olive oil."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_15_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:02]: The video begins with a close-up of a person grasping a small green bottle over a white ceramic bowl placed on a wooden cutting board. They pour a brown liquid into the bowl while holding the bottle with both hands. In the background, there is a stovetop with a frying pan containing meat patties on the left and a dark blue cabinet on the right. [0:04:03]: The person continues to pour the brown liquid into the bowl, which now has a spoon inside it. Their left hand holds the bottle, while their right hand adjusts the bottle cap. [0:04:04 - 0:04:07]: The camera angle shifts to show a broader view of the kitchen. The person stands in front of a white-tiled wall with various kitchen utensils, dishes, and spice jars on shelves mounted above the counter. They hold a small jar in their right hand and begin to twist the cap off while looking down at the jar. A white plate with a small gray dish on top is on the counter next to the cutting board. Fresh greens are on the counter in front of the person. [0:04:08 - 0:04:11]: The scene transitions to a top-down view of the cutting board, where the person pours the contents of a small brown bottle into the bowl with the spoon. Several small bowls with ingredients are placed on the counter around the cutting board. The person's left hand holds the cap of the small brown bottle, while their right hand adjusts the bottle to control the pouring. [0:04:12 - 0:04:14]: Switching back to an eye-level perspective, the person continues to pour the brown liquid into the bowl, carefully holding the bottle with their right hand and the cap in their left. Shelves with various kitchen items and a bright red sign that reads \"COOK\" are prominently displayed against the white brick wall in the background. [0:04:15 - 0:04:16]: The shot again changes to an overhead view of the kitchen counter. The frying pan with meat patties sizzles on the stovetop, while a pot of rice with garnishes is on the adjacent burner. The person reaches for a different container on the counter, preparing to add more ingredients to the bowl. [0:04:17 - 0:04:19]: The final frames show the person sprinkling spices from a container into the bowl on the cutting board. The surrounding area features an array of fresh vegetables and kitchen utensils, highlighting a busy and well-equipped kitchen environment. The scene ends with the person beginning to mix the ingredients in the bowl.\n[0:04:20 - 0:04:40] [0:04:20 - 0:04:22]: In the first scene, an overhead shot shows a kitchen countertop. On the left side, there is a wooden cutting board with an assortment of leafy greens and colorful vegetables placed next to it. A white bowl with a spoon is positioned at the bottom left of the cutting board. A large oval dish with a black insert sits above the cutting board. A stainless steel stovetop occupies the middle of the frame, featuring a frying pan with five meat patties cooking inside. To the right of this pan is another pan containing a colorful mixture of rice and red vegetables. [0:04:23 - 0:04:24]: A person in a black shirt, standing in front of shelves filled with plates, glasses, green and transparent bottles, and various kitchen utensils. The person appears to be looking toward the right side of the frame. The word \"COOK\" in large red letters is prominent on the shelf behind them. [0:04:25 - 0:04:27]: The person moves slightly to the right and then centers themselves in front of the stovetop. They use a spoon and a fork to handle the patties in the frying pan. Additional details of the background include brick walls decorated with shelves holding an assortment of cooking equipment, such as plates, bowls, and rolling pins, along with multiple knives mounted on a magnetic strip. [0:04:28 - 0:04:30]: A close-up shot captures the person's hands using a spoon to gently press down and then flip one of the meat patties in the frying pan. The rice pan in the background remains in its position, with its contents undisturbed. The seasoning and small bottles are visible near the stovetop. [0:04:31 - 0:04:33]: The close-up angle continues, focusing on the frying pan as the person adjusts the position of the meat patties. The meat patties have developed a browned crust. The person’s hand is seen adjusting the heat knobs on the stovetop. [0:04:34 - 0:04:36]: The camera angle shifts back to a wider shot of the person working at the stovetop. The person is leaning slightly over the stovetop, seeming to concentrate on adjusting the heat and position of the pan. The background with kitchen tools and utensils provides an organized and equipped kitchen setting. [0:04:37 - 0:04:39]: The overhead view returns, showing the frying pans from a top perspective. The person’s hands use utensils to flip and move the meat patties, which are now evenly browned, in the frying pan. The rice in the adjacent pan remains colorful with visible red vegetable chunks and greens. The stovetop’s knobs and burners are clearly visible.\n[0:04:40 - 0:05:00] [0:04:40 - 0:04:44]: The cooking scene begins with a view of a kitchen stove with two frying pans on it. The left frying pan contains partially cooked meatballs, arranged in a single layer and slightly browned on one side. The right frying pan contains a dish consisting of white rice topped with sliced red chilies and green herbs. A hand holding the handle of the left pan is about to adjust its position. [0:04:44 - 0:04:44]: A man is shown in a kitchen setting, preparing ingredients. He is wearing a black t-shirt and is holding a black pepper grinder above a white bowl placed on a wooden chopping board. Various ingredients, including a bunch of fresh green herbs, are laid out on the counter. [0:04:45 - 0:04:47]: The man continues to season the contents of the white bowl using a pepper grinder. He then pours a dark liquid from a bottle into the bowl. The countertop is cluttered with culinary tools and fresh ingredients, indicating ongoing food preparation. [0:04:47 - 0:04:49]: He moves his body slightly to the left while placing the bottle back on the counter. Various kitchen objects, including jars and bottles, are visible on the shelves in the background, signifying a well-stocked kitchen. [0:04:49 - 0:04:52]: The perspective changes to a top-down view focusing on the bowl on the cutting board. The man's hands are adding the dark liquid from the bottle into the bowl, which contains a mixture of chopped ingredients.  [0:04:52 - 0:04:53]: A closer view shows the hand holding the utensil, continuing to add the liquid ingredient while a spoon rests inside the bowl. [0:04:54 - 0:04:57]: The scene switches back to a medium shot, with the man now standing beside the stove. He has slightly turned his body and is holding what appears to be a lime in his left hand, preparing to add it to the bowl. One meatball remains in the left pan, and the right pan with the rice dish remains unchanged. [0:04:58 - 0:04:59]: The man places the lime on the cutting board and presses it slightly, preparing to release its juice. He then focuses intently on the task at hand, still standing beside the stove in a well-lit, organized kitchen.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What colors are visible in the dish of rice on the stovetop?",
        "time_stamp": "0:04:39",
        "answer": "A",
        "options": [
          "A. White, red, and green.",
          "B. White and green.",
          "C. Brown and green.",
          "D. Brown, red, and green."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_15_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:05]: The video begins with a person in a black T-shirt standing at a kitchen counter. The kitchen has white subway tile walls and open wooden shelves filled with various kitchen items, including jars, plates, and bowls. The word 'COOK' in large red letters is prominently displayed on the top shelf. The person is handling a lime on a wooden cutting board, preparing to cut it with a knife. A white bowl with some ingredients is placed in the center of the cutting board. [0:05:05 - 0:05:07]: The person slices the lime in half and moves one half closer to the white bowl. A stainless steel pan is on the stove to the left, with some food cooking. The person's focus is on squeezing the lime juice into the bowl. [0:05:07 - 0:05:09]: The person continues to squeeze the lime juice into the bowl, using both hands to press the lime halves. The juice drips into the bowl, which already contains some ingredients. There are bottles and a glass jar with liquid near the cutting board, indicating that a sauce or dressing might be being prepared. [0:05:09 - 0:05:11]: The person finishes squeezing the lime and places the squeezed lime halves on the cutting board. They reach for the white bowl, combining the ingredients with a spoon. The action is deliberate, ensuring that the mixture is well-combined. [0:05:11 - 0:05:13]: The person starts moving towards the stove with the bowl in hand, preparing to use the mixture in the cooking process. The kitchen environment is clean and organized, with green vegetables and various cooking tools visible on the countertop. [0:05:13 - 0:05:15]: The person attentively mixes the ingredients in the white bowl, ensuring everything is well-blended. The bright kitchen is well-lit, with natural light coming from a window to the left. The greenery from plants is noticeable, adding a fresh ambiance to the kitchen setting. [0:05:15 - 0:05:17]: The person pours the contents of the white bowl into a smaller, darker bowl. The mixture has a rich, textured appearance, indicating it might be a sauce or dressing with multiple ingredients combined. [0:05:17]: The final frame shows the smaller bowl filled with the mixture, placed on the wooden cutting board. The camera captures the details of the kitchen in the background. The countertop has various kitchen tools, plates, and a bunch of fresh herbs, highlighting an environment of active cooking.\n[0:05:20 - 0:05:40] [0:05:20 - 0:05:21]: The video opens in a modern kitchen with a white brick backsplash and open wooden shelves displaying various kitchen items like plates, bottles, and small decorative objects. On the countertop, there are two frying pans centered on a stove. A man wearing a black shirt is seen near the stove, holding a small white plate in his right hand, and he extends his left hand towards the stove. [0:05:21 - 0:05:22]: The man shifts position slightly to his left, moving away from the stove. His hands are not visible in this frame, probably because they are lower than the frame capture. The kitchen setup remains consistent, with the \"COOK\" sign still prominently displayed at the back. [0:05:22 - 0:05:24]: Positioned in front of the stove, the man turns his head slightly towards his right and holds a frying pan with his right hand. His left hand is not visible, and the focus is on him cooking, with the frying pan positioned on the stove. [0:05:24 - 0:05:25]: Still holding the pan, the man looks intently at its contents, showing concentration. The pan is positioned slightly to the left, and he raises it slightly upwards. [0:05:25]: The camera zooms in on the contents of the frying pan from a different angle, revealing finely chopped vegetables and rice being cooked. The background becomes blurred, focusing on the cooking action. [0:05:26]: The close-up of the frying pan continues, showing the same contents being stirred. The hand holding the pan is visible with a slight grip, and the stirrer uses another utensil, probably a spatula, which is not shown in this frame. [0:05:27 - 0:05:30]: The camera switches to an overhead view of the two frying pans on the stove. The left pan contains several meatballs, while the right pan shows the rice mixture with vegetables. The man's right hand holds a spatula. [0:05:30 - 0:05:33]: The overhead view persists while the man stirs the contents of the right pan with the spatula, mixing the ingredients evenly. The cooking process looks active, and the man makes sure the ingredients are well incorporated. [0:05:33 - 0:05:34]: The camera angle returns to a side view of the stove, showing the man actively stirring the rice mixture. The pan with the cooked meatballs remains untouched. [0:05:34 - 0:05:36]: The man continues cooking, leaning slightly forward towards the stove. His concentration is evident as he continually stirs the rice mixture. [0:05:36]: He briefly steps away from the stove, indicating a pause in his cooking. He may be turning his attention to another task or ingredient outside the frame. [0:05:37 - 0:05:38]: The overhead camera frame returns, showing the man grabbing a small container from the countertop while still holding the spatula. Various cooking ingredients and utensils are visible around the stove, indicating an ongoing cooking activity. [0:05:38 - 0:05:39]: The man checks the contents of a small bottle before making any further adjustments to the food. He holds it in his right hand while focusing on the bottle label with a concerned expression.\n[0:05:40 - 0:06:00] [0:05:40 - 0:05:41]: The video begins in a modern kitchen with an individual standing behind a kitchen counter that contains three frying pans, a chopping board with green vegetables, and a white plate. The person, in a black t-shirt, is seen bending over a frying pan, sprinkling something into it. [0:05:41 - 0:05:42]: The person continues to sprinkle ingredients into the frying pan, tilting their head slightly as they focus on the task. Shelves with jars, plates, and containers, along with a large \"Cook\" sign, are visible in the background. [0:05:42 - 0:05:45]: The person looks up, facing the camera, and begins to speak. They hold a hand grater over the frying pan and grate an ingredient into the pan. Shelves stacked with dishes and kitchen items are placed against a white tiled wall. [0:05:46 - 0:05:51]: The person grates more of the ingredient into the frying pan, focusing intently on the task. Behind them, a collection of kitchen utensils and appliances are neatly organized on the shelves and countertop. The person’s posture is bent forward, steadily grating. [0:05:51 - 0:05:52]: From an overhead view, the person's hands are seen grating the ingredient into a pan filled with rice and vegetables while another pan beside it contains neatly arranged meatballs. The person continues to work diligently, preparing the food. [0:05:52 - 0:05:54]: The grating of the ingredient continues while the camera maintains the top-down perspective. Both frying pans are on the stove, with one containing bubbling meatballs and the other, a mix of colorful vegetables and rice as the person works. [0:05:54 - 0:05:56]: The overhead view persists, showing a detailed look at the two sizzling pans on the stove. The person continues grating over the pan with rice and vegetables, making sure the ingredients are evenly distributed. [0:05:56 - 0:05:57]: The camera shifts back to a frontal view. The person lifts the frying pan with meatballs, tilting it to check the food inside. The backdrop contains neatly arranged kitchenware against the white brick wall. [0:05:57 - 0:05:59]: The person sets the pan with meatballs back on the stove and uses a tool to reposition the meatballs within the pan. They handle the pan with expertise, ensuring the meatballs cook evenly.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color are the walls in the kitchen?",
        "time_stamp": "00:05:05",
        "answer": "A",
        "options": [
          "A. White.",
          "B. Black.",
          "C. Green.",
          "D. Blue."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_15_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:02]: The first-person view shows a person cooking on a stovetop. A stainless steel pan is on the left burner with cooked brown meatballs inside, and the person is using a spoon to handle the meatballs. Next to the burner is a countertop with a white plate of vegetables, including lettuce, cucumber, green onions, and a small bowl of dipping sauce. A towel, presumably for handling hot items, is tucked at the person's waist. [0:08:03 - 0:08:06]: The camera angle shifts to a top-down view, capturing a broader scene. The stovetop has two pans; the left one contains meatballs, and the right one has rice. Both pans are positioned on different burners. The countertop still has the white plate of vegetables, and another tray with more green herbs is visible nearby. The person is focused on cooking, rotating the meatballs in the pan on the left burner. [0:08:07 - 0:08:08]: The same top-down view continues, showing the person using cooking utensils to handle the food. They are working with the meatballs, bringing them from the pan to the plate of vegetables on the countertop. Their attention remains on the cooking process. [0:08:09 - 0:08:12]: The perspective returns to a closer view of the white plate with vegetables. The person is placing the last meatball onto the plate. The arrangement includes green leafy lettuce, cucumber slices, green onions, and fresh herbs, along with a bowl of dipping sauce. The cooking area in the background is slightly blurred but still visible. [0:08:13 - 0:08:16]: The scene transitions to a lateral view of the cooking area. The person, wearing a black shirt, is now focused on stirring the contents of a pan with a spatula. Behind them is a kitchen with a blue and white color scheme, shelves with various kitchenware, and a brick-patterned backsplash. The countertop in the foreground still holds the plate of prepared food. [0:08:17 - 0:08:19]: The person continues to cook, demonstrating flipping techniques with the pan. They appear concentrated on ensuring everything cooks evenly. The kitchen ambiance remains consistent with its modern design and organized layout. The camera captures them gesturing, likely explaining or narrating the cooking process, adding a dynamic element to the scene.\n[0:08:20 - 0:08:40] [0:08:20 - 0:08:21]: A person, dressed in a black shirt, is in a kitchen that features white brick walls and wooden shelves holding plates, jars, and other kitchen utensils. A large red sign reading \"COOK\" is visible on the wall. The person is reaching towards a frying pan on a gas stove. To the right, there is a plate of green vegetables. [0:08:21 - 0:08:22]: The person is holding the frying pan in the left hand and a spoon in the right hand, appearing to be in the process of stirring or preparing its contents. The focus is on the steam rising from the pan and the concentration on the person's face as they work.  [0:08:22 - 0:08:23]: The person begins to tilt the pan with their left hand. The contents of the pan, which appears to be some cooked food, are being maneuvered with the spoon towards another container or dish. [0:08:23 - 0:08:24]: The person continues tilting the pan, pouring the food gently into a white bowl positioned on a countertop. The steam continues to rise, indicating the food is hot. The bowl starts to collect the food, which seems to include rice and vegetables. [0:08:24 - 0:08:27]: The person steadily pours the remaining contents of the pan into the bowl, which is now almost full with the food. The rice and vegetables can be seen clearly. The person's movements are careful and deliberate, ensuring all the food goes into the bowl. [0:08:28 - 0:08:32]: With the food fully in the bowl, the person sets the pan aside. The bowl is filled with steaming food, primarily rice and some visible vegetables like peppers. The countertop holds other ingredients and utensils used in the cooking process, suggesting a well-prepared cooking session.  [0:08:32 - 0:08:35]: The person makes a final check on the bowl, possibly giving a final touch to the food arrangement. Then, the pan is lifted away, and attention is given to wiping down the counter or managing minor spills around the stove area. [0:08:36 - 0:08:38]: The person sets the pan down on the stove. The stovetop has some scattered bits of rice around it, indicating that some food may have fallen during the transfer. The bowl of food is now in the foreground, with steam slightly dissipating. Surrounding objects like jars, other cooking ingredients, and utensils become more focused. [0:08:39]: The person uses a cloth to wipe down the countertop and stove area, tidying up any residual mess. The movements are quick yet efficient, reflecting a final effort to leave the workspace clean after cooking. The fresh vegetables and utensils remain partially in the view, boosting the sense of a completed and organized cooking task.\n[0:08:40 - 0:09:00] [0:08:40 - 0:08:45]: The scene is set in a modern and organized kitchen with a white-tiled backsplash and wooden shelves filled with various kitchen items, such as plates, glasses, and bottles. A person wearing a black shirt stands behind a kitchen counter with several items, including a pan on the stove, a bowl, a cutting board, and a plate of vegetables. The person is seen pouring the contents of a bowl into a pan on the stove, holding the bowl with a piece of cloth. [0:08:46 - 0:08:47]: The person adjusts the bowl on the counter, ensuring the contents are properly in the pan.  [0:08:48 - 0:08:49]: The person appears to stir or mix the contents in the pan with a utensil, concentrating on the task.  [0:08:50 - 0:08:51]: The person brings a bowl close to their face, observing or smelling the dish.  [0:08:52]: The person makes an expressive gesture with their hands, possibly to indicate the taste of the dish. [0:08:53]: The person turns and moves towards the plate of vegetables on the right side of the counter, picking up some ingredients. [0:08:54]: The camera angle shifts, providing a closer view of the person holding a large white plate with various vegetables and some meatballs. The stove and other items on the counter are partially visible.  [0:08:55 - 0:08:58]: The person presents the plate, showing different angles and possibly describing the dish. The background reveals more details of the organized kitchen, with items on the shelves and counters. In the foreground, a bowl of rice appears alongside the plate of vegetables and meatballs. [0:08:59]: The last frame provides a top-down view of the plate of food and the bowl of rice on a dark wooden table. The dish consists of fresh greens, sliced cucumbers, radishes, chives, a small bowl of sauce, and four meatballs, with the bowl of rice featuring chopped vegetables.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What sequence of actions was the person follow after tilting the pan?",
        "time_stamp": "0:08:41",
        "answer": "A",
        "options": [
          "A. Pouring food into a bowl, setting the pan aside, and wiping the countertop.",
          "B. Chopping vegetables, stirring the pan, and explaining the recipe.",
          "C. Pouring food into a bowl, adding more ingredients, and tasting the dish.",
          "D. Stirring the food, plating the dish, and washing utensils."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_15_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:21]: The video begins in a well-lit kitchen with a clean, white brick backsplash and wooden shelves holding various kitchen items such as bowls, bottles, and a teapot. A man in a grey t-shirt is looking downwards, possibly at a preparation surface. His posture suggests he is focused on a task at hand. [0:02:22 - 0:02:23]: The view shifts to a top-down perspective of a wooden cutting board on a kitchen counter. Several kitchen tools and ingredients are neatly arranged. On the left, there is a pile of red sticks, resembling rhubarb, which are perfectly cut and lined up. Next to them are two rectangular pieces of pastry resting on a baking sheet lined with parchment paper. Various small bowls containing different ingredients are positioned nearby, including an orange, butter, and a beaten egg in a bowl. [0:02:24]: The man's hands come into view as he reaches for a couple of cut pieces of rhubarb. The focus seems to be on organizing or maneuvering the pieces. [0:02:25]: The next frame shows the man adjusting the position of the baking tray on the counter. [0:02:26]: His hands now hold a brown bowl and he is stirring a liquid mixture inside it, possibly preparing a glaze or an egg wash. [0:02:27 - 0:02:28]: The view returns to the overhead shot of the working surface as the man continues to stir the mixture in the brown bowl. His focus remains on ensuring the mixture is well-blended. [0:02:29 - 0:02:30]: He picks up a brush and dips it into the bowl, indicating he may be ready to apply the mixture to the pastries. There is clear intent and precision in his movements. [0:02:31 - 0:02:32]: The man then looks more closely at what appears to be the pastries on the baking tray, holding the brown bowl in one hand and the brush dipped in the mixture in the other. [0:02:33 - 0:02:34]: He starts brushing the edges of one of the pastry rectangles with the mixture from the bowl. His brush strokes are deliberate and even. [0:02:35 - 0:02:37]: The process of brushing continues, with the man ensuring every part of the edge of the pastry is well-coated. The thoroughness of his actions suggests an attention to detail and a desire for an even application. [0:02:38 - 0:02:39]: He transitions to the other side of the same pastry, continuing to apply the mixture in smooth, consistent strokes. The task flows seamlessly as part of the baking preparation process.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What item is resting on a baking sheet lined with parchment paper?",
        "time_stamp": "0:02:23",
        "answer": "B",
        "options": [
          "A. Several red sticks of rhubarb.",
          "B. Two rectangular pieces of dough.",
          "C. A bowl of beaten eggs.",
          "D. A pile of orange slices."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Action Recognition",
        "question": "What does the man do after picking up the brush?",
        "time_stamp": "0:02:30",
        "answer": "B",
        "options": [
          "A. Starts stirring a liquid mixture.",
          "B. Apply egg wash to the dough.",
          "C. Adjusts the position of the baking tray.",
          "D. Reaches for a couple of cut pieces of rhubarb."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_33_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:20 - 0:09:40] [0:09:20 - 0:09:21]: A person is holding an orange in their left hand and grating it with a grater using their right hand over a glass mixing bowl. The mixing bowl, containing what appears to be a white creamy substance, is located on a wooden cutting board. The scene is in a kitchen with a blue and beige color palette, visible countertops, and cooking utensils. [0:09:22 - 0:09:24]: The person continues grating the orange. They look at the task at hand, seemingly concentrating. The background shelves are lined with jars, bowls, and various kitchen items. There is a red sign that says \"COOK\" on a shelf.  [0:09:25 - 0:09:27]: The viewpoint changes to an overhead view, showing the person vigorously grating the orange into the bowl. Surrounding the bowl on the countertop are various ingredients and kitchen utensils, including butter, sugar, and a cutting board with chopped ingredients. [0:09:28 - 0:09:31]: The person's facial expression changes as they continue to grate the orange. They appear focused and determined. Various kitchen implements and containers can be seen in the background, along with a portion of white brick wall tiles. [0:09:32 - 0:09:33]: The person resumes grating the orange over the glass bowl. Near the bowl are other cooking ingredients and utensils, like a frying pan on the stovetop, which hints at ongoing cooking preparations. [0:09:34 - 0:09:36]: The person steps aside to place the orange down on the counter and picks up what appears to be a knife. The setup of the kitchen includes neatly arranged dishes, containers, and utensils displayed on the shelves and countertops.  [0:09:37 - 0:09:39]: A close-up view of the person scraping the zest off the grater into the bowl using a knife. They meticulously ensure that the zest collects in the bowl, which still contains the previous white creamy mixture. The focus is on the careful and detailed process of zesting.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person using to grate the orange?",
        "time_stamp": "0:09:21",
        "answer": "C",
        "options": [
          "A. A spoon.",
          "B. A knife.",
          "C. A grater.",
          "D. A fork."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the bowls' mixture into which the orange zest is grated?",
        "time_stamp": "0:09:27",
        "answer": "B",
        "options": [
          "A. Yellow.",
          "B. White.",
          "C. Green.",
          "D. Brown."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the person use a knife after grating the orange?",
        "time_stamp": "0:09:43",
        "answer": "C",
        "options": [
          "A. To clean the countertop.",
          "B. To chop more ingredients.",
          "C. To scrape the zest off the grater into the bowl.",
          "D. To cut the orange into slices."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_33_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video starts with a close-up of a computer component with a futuristic design. It consists of a black housing with some metal parts and an RGB light scheme that transitions between blue, purple, and red colors. There's a dark background with no other distinguishable objects. [0:00:03 - 0:00:05]: The camera angle shifts to a closer view of another part of the computer that reveals a black and blue section with an illuminated 'X'. This close-up highlights the detailed design and partially hidden components. [0:00:06 - 0:00:07]: The focus changes to show a logo on the side of the computer housing. The background is still dark, and the computer cables are visible. The housing is sleek, with metal and plastic components reflecting subtle lighting. [0:00:08]: A pair of hands assemble a computer graphics card, holding the device delicately. The graphics card includes a circuit board with cooling hardware attached. The background suggests an indoor setting organized for assembly work. [0:00:09 - 0:00:15]: The scene cuts to a person sitting in front of a table, explaining something while handling the computer component. The setting is an indoor environment with good lighting. The person speaks while pointing to different parts of the hardware, demonstrating an in-depth discussion or tutorial. [0:00:16 - 0:00:19]: The final scene shifts to a different angle with a person handling the same computer component on a table with a plain, dark backdrop featuring red streaks. The person carefully adjusts the hardware, giving attention to its placement and connection. The lighting highlights the design and build quality of the device.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What element is illuminated in the close-up view of the computer part right now?",
        "time_stamp": "00:00:06",
        "answer": "B",
        "options": [
          "A. A red circuit board.",
          "B. A ROG logo.",
          "C. A MSI logo.",
          "D. Computer cables."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_119_real.mp4"
  },
  {
    "time": "[0:01:40 - 0:02:00]",
    "captions": "[0:01:40 - 0:02:00] [0:01:40 - 0:01:45]: In the beginning, a pair of hands holds a GPU, examining it closely. The dark GPU is connected by black cables to a device positioned in the top-right. One hand points at various elements on the GPU using a finger, while the other hand grasps it firmly. On the white table below, multiple electronic device parts and tools are scattered. [0:01:46 - 0:01:47]: The perspective shifts to an overhead view. The same hands continue to manipulate the GPU, now working on a specific section of the hardware while still attached to the device on the right. [0:01:48 - 0:01:52]: The hands detach a copper heat sink from the GPU. They carefully lift and remove this component, revealing more detailed circuitry underneath. The camera angle shows the exposed GPU, focusing on its intricate parts. [0:01:53 - 0:01:58]: The GPU is held up by both hands, which rotate it slightly, displaying the circuitry and components. The heat sink removed earlier can be seen in the background. [0:01:59]: The last frame focuses on the GPU lying flat on the white table with the heat sink positioned in the background. The camera angle is low, showing the GPU level near the surface of the table.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What component is being detached from the GPU just now?",
        "time_stamp": "0:01:52",
        "answer": "B",
        "options": [
          "A. A fan module.",
          "B. A copper heat sink.",
          "C. A plastic cover.",
          "D. A backplate."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_119_real.mp4"
  },
  {
    "time": "[0:03:20 - 0:03:40]",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:23]: The initial frames of the video depict a computer’s internal hardware, prominently featuring a rectangular, black component labeled \"Matrix\" with RGB lighting. This component is mounted inside a computer rig. Surrounding the component are various circuit boards, cables, and cooling units. The RGB lighting alternates colors between red, blue, and purple, highlighting parts of the internal hardware setup. The background is dark, ensuring the lighting effects and hardware details are clearly visible. [0:03:24 - 0:03:27]: A person's hand appears, lifting a large GPU labeled \"RTX 4090\" off a table. The GPU has a fan on one side and a recognizable design associated with high-performance graphic cards. The table beneath is light-colored, contrasting the black computer components and the dark background. [0:03:28 - 0:03:29]: The video transitions to depicting a monitor displaying the game \"Cyberpunk 2077\". The game's menu is visible, showing options like 'Continue', 'New Game', 'Load Game', etc. In one frame, the focus shifts to an in-game setting and the interface showing various performance statistics like GPU temperature, clock, fan speed, and power consumption. [0:03:30 - 0:03:31]: The display changes to show the computer setup stationed on a desk. The setup is equipped with multiple cooling hoses, hardware components, and a monitor showcasing gameplay from \"Cyberpunk 2077\". The room’s dark ambiance is interrupted by a vertical light beam from the left, casting shadows and illuminating part of the setup. [0:03:32 - 0:03:40]: The final segment of the video shows a dark screen with bright white and purple bars representing GPU thermal performance stats. The statistics compare different GPUs (4090 Matrix, 4090 Suprim Liquid, and 4090 FE) over conditions of \"Cyberpunk 4K RT Ultra\" for 35 minutes. The displayed parameters include GPU temperature, hotspot, memory temperature, and GPU clock speed, illustrating a comparative performance analysis among the GPUs.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "Which colors are the RGB lighting alternating between on the \"Matrix\" component right now?",
        "time_stamp": "0:03:23",
        "answer": "C",
        "options": [
          "A. Red, green, and blue.",
          "B. Blue, purple, and green.",
          "C. Red, blue, and purple.",
          "D. Red, purple, and green."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_119_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:05:20]",
    "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:09]: The first-person perspective video begins showcasing three separate benchmarking results of graphics cards, displayed side by side in a vertical format. Each card's statistics, including FPS, temperature, clock speed, fan speed, and power consumption, are shown. Card models include 4090 FE, 4090 Suprim X, and 4090 Matrix. The video's background is a futuristic cityscape, bright with digital billboards, buildings, and street-level activities. [0:05:10 - 0:05:14]: The scene shifts to a close-up of a single GPU unit in an open position, displaying its internal components. The GPU is predominantly black with an angular, sleek design, and contains a prominent logo on its front. The setting appears to be in a well-lit, clean environment, possibly a workstation. [0:05:15 - 0:05:19]: The video now shows two GPU units placed on a flat, white surface against a matte black background. One GPU is connected to a cooling mechanism with black tubes extending from it, while the other GPU remains unconnected, lying adjacent to it. Both GPUs are of modern design, with large fan structures and geometric casing. The lighting emphasizes their matte and glossy textures.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What graphics card models are shown on the benchmarking results right now?",
        "time_stamp": "00:05:09",
        "answer": "D",
        "options": [
          "A. 4080 FE, 4090 Suprim X, 4090 Matrix.",
          "B. 4090 FE, 4080 Suprim X, 4090 Matrix.",
          "C. 4090 FE, 4090 Suprim X, 4080 Matrix.",
          "D. 4090 FE, 4090 Suprim X, 4090 Matrix."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_119_real.mp4"
  },
  {
    "time": "[0:06:20 - 0:06:28]",
    "captions": "[0:06:20 - 0:06:28] [0:06:20 - 0:06:21]: A person with short brown hair, wearing a gray t-shirt, is sitting at a desk with a piece of computer hardware in front of them. They look engaged and are gesturing with their right hand. [0:06:21 - 0:06:22]: The person continues gesturing, raising their right index finger as if making a point. [0:06:22 - 0:06:23]: The person seems to be explaining something, with their hand likely resting on the desk. [0:06:23 - 0:06:24]: The person begins a broader gesture with their left hand, possibly elaborating on their previous point. [0:06:24 - 0:06:25]: The person faces the camera directly, placing their hand on the desk. [0:06:25 - 0:06:26]: Another gesture with their right hand is visible, this time moving towards their left. [0:06:26 - 0:06:27]: The person raises their right index finger again, emphasizing another point. [0:06:27 - 0:06:28]: The person rests their right hand on their chest and looks directly at the camera.",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "What is the likely purpose of the person's gestures right now?",
        "time_stamp": "00:06:27",
        "answer": "B",
        "options": [
          "A. Asking a question.",
          "B. Explaining a concept.",
          "C. Writing something down.",
          "D. Watching a demonstration."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_119_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why is this rabbit dressed in red and white knocking on the door now?",
        "time_stamp": "0:00:36",
        "answer": "A",
        "options": [
          "A. Because it smells the fish and feels it doesn't taste good.",
          "B. Because it wants to join the party inside the house.",
          "C. Because it saw something interesting through the window.",
          "D. Because it is lost and looking for its way back home."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_231_real.mp4"
  },
  {
    "time": "[0:01:25 - 0:01:55]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why is this iron gate cartoon character angry now?",
        "time_stamp": "0:00:58",
        "answer": "D",
        "options": [
          "A. Because someone tried to paint him a different color.",
          "B. Because he was just splashed with water by a mischievous cat.",
          "C. Because the wind blew his hinges, making him squeak loudly.",
          "D. Because he was just forcibly stuffed a fish by the red rabbit in red and white clothes."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_231_real.mp4"
  },
  {
    "time": "[0:02:50 - 0:03:20]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why is this rabbit, dressed in green and white, surprised now?",
        "time_stamp": "00:02:24",
        "answer": "C",
        "options": [
          "A. Because he found an unexpected egg in the chicken's nest.",
          "B. Because the chicken suddenly started talking.",
          "C. Because he can't distinguish the gender of the chicken he is holding now.",
          "D. Because the chicken he is holding suddenly grew larger."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_231_real.mp4"
  },
  {
    "time": "[0:04:15 - 0:04:45]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does this rabbit dressed in green and white want to hide in the toilet?",
        "time_stamp": "00:03:19",
        "answer": "D",
        "options": [
          "A. Because he accidentally broke a valuable vase and wants to avoid getting caught.",
          "B. Because he saw a scary shadow moving in the hallway.",
          "C. Because he is trying to escape from a loud and annoying party.",
          "D. Because after he opened the door, he found a noose and a skull."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_231_real.mp4"
  },
  {
    "time": "[0:05:40 - 0:06:10]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why did the rabbit dressed in red and white just have to use a pool cue to strike the iron gate lying on the ground?",
        "time_stamp": "00:06:42",
        "answer": "A",
        "options": [
          "A. Because he wants to get all the balls into the mouth of the iron gate at once.",
          "B. Because the iron gate blocked his path, and he needed to move it out of the way.",
          "C. Because he thought the flattened gate would make a perfect ramp for his skateboard.",
          "D. Because he was trying to impress his friends with a strong trick shot."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_231_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video begins with a black screen. [0:00:01 - 0:00:02]: Two images of plastic building block structures, each transforming into different letters of the alphabet, \"A\" and \"B,\" are shown. The background is dark, and \"ALPHABET TRANSFORM\" is displayed in large, white text at the top. [0:00:03 - 0:00:06]: The screen transitions to three horizontal purple lines on a white background, on which text starts to appear. The text reads \"Alphabet A transform to Aeroplane\" across three lines in white text. [0:00:06 - 0:00:10]: A hand appears on a white marble surface, assembling green plastic building blocks. The hand places multiple blocks on the surface in preparation for building. [0:00:11 - 0:00:13]: Continuation of the build. Another yellow plastic block is added on top of the green blocks. [0:00:14 - 0:00:16]: The hand adjusts and places more blocks, stacking them carefully. [0:00:17 - 0:00:19]: The structure is nearly complete. The hand adds a red block, and the build starts forming the shape of an airplane.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the plastic block added on top of the green blocks?",
        "time_stamp": "0:00:16",
        "answer": "C",
        "options": [
          "A. Red.",
          "B. Blue.",
          "C. Yellow.",
          "D. Purple."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_204_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:02]: The scene starts with a flat, light-colored surface where a cluster of large, multicolored plastic building blocks are arranged. The blocks are primarily red, green, and yellow. Two hands are hovering above the arrangement. [0:01:02 - 0:01:04]: The hands begin rearranging the blocks, moving them around on the surface. A yellow block is picked up from the cluster. [0:01:04 - 0:01:06]: One of the yellow blocks is set aside, and the hands continue to reconfigure the remaining blocks. [0:01:07 - 0:01:09]: An orange-red rectangular building block is picked up and moved to the center of the scene. [0:01:10 - 0:01:11]: A green block is picked up and attached to one end of the red rectangular block. [0:01:12 - 0:01:14]: The green block is firmly adjusted to ensure it is securely connected to the red block. [0:01:15 - 0:01:18]: The hands pick up another block, a yellow piece, and begin to place it on top of the red block. [0:01:19 - 0:01:20]: The yellow block is now securely attached to the red block, making the structure taller. Other blocks remain scattered nearby on the surface.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What are the primary colors of the plastic building blocks shown right now?",
        "time_stamp": "0:01:02",
        "answer": "B",
        "options": [
          "A. Blue, green, and yellow.",
          "B. Red, green, and yellow.",
          "C. Red, blue, and yellow.",
          "D. Green, blue, and orange."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Event Understanding",
        "question": "Which block was picked up and attached to the top of red block first just now?",
        "time_stamp": "0:01:25",
        "answer": "C",
        "options": [
          "A. A white block.",
          "B. A green block.",
          "C. An yellow block.",
          "D. A blue block."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_204_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:05]: A series of colorful building blocks are arranged on a flat surface. There are mostly red, green, and orange blocks configured in a cross-like shape. A hand interacts with the blocks, adding pieces to create a structure. The hand first moves a green block into position, followed by an orange one. [0:02:06 - 0:02:08]: The hand adds more blocks to the structure, including green and orange blocks, stacking them precariously on the sides. The block structure gains height as the pieces are added. [0:02:09 - 0:02:15]: The background remains a slightly marbled light gray surface. The hand continues to move the blocks, adjusting and securing the new pieces to the structure. The block structure reaches its final form with an additional orange block placed on top. [0:02:16]: The block structure stands tall with a combination of the red, green, and orange blocks in a stable formation. [0:02:17 - 0:02:19]: A purple animated overlay appears with three purple bars, on which white text reads, \"Alphabet B transform to Box.\" The text is centered within the bars and transitions smoothly between frames.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What was the white text on the purple animated overlay read just now?",
        "time_stamp": "0:02:21",
        "answer": "B",
        "options": [
          "A. \"Alphabet A transform to Box\".",
          "B. \"Alphabet B transform to Box\".",
          "C. \"Alphabet C transform to Box\".",
          "D. \"Alphabet D transform to Box\"."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_204_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:06]: The video depicts a pair of hands disassembling a colorful structure made of plastic building blocks on a light-colored surface. The structure comprises three layers: the top and bottom layers are red blocks, while the middle layer has orange and green blocks; [0:03:07 - 0:03:11]: The hands continue to dismantle the block structure, gradually lowering the blocks to the surface and laying them flat; [0:03:12 - 0:03:13]: The hands disassemble the top layer and mix the blocks around before completely dismantling all layers; [0:03:14 - 0:03:15]: Various colored blocks (red, orange, and green) are seen scattered around, with some partially stacked; [0:03:16 - 0:03:18]: The hands converge to gather the scattered blocks into a pile; [0:03:19 - 0:03:20]: The video concludes with most of the blocks being collected in a pile while one hand remains on the surface beside the pile.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What happens to the blocks after they are disassembled and mixed?",
        "time_stamp": "00:03:18",
        "answer": "B",
        "options": [
          "A. The blocks are arranged in a new pattern.",
          "B. The blocks are scattered and then gathered into a pile.",
          "C. The blocks are thrown away.",
          "D. The blocks are sorted by color."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_204_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video begins with an image of a clock, the hands consisting of a knife and fork against a plain, white background. It is set at an angle, with the clock positioned centrally and a bold '10' partially visible, predominantly white in color, with a faint grey bordering. [0:00:01 - 0:00:04]: The 'RAMSAY in 10' logo appears, superimposed over the clock image. 'RAMSAY' is written in large, bold blue letters, occupying the top half of the frame. 'in' is in smaller blue letters positioned slightly lower and to the left. The '10' retains the same shape and color as the previous frame. A person is standing on the right side of the frame, wearing a dark blue shirt, visible from the chest upwards. [0:00:05]: The scene transitions to a kitchen setting with the person standing in front of a backdrop of white subway tiles and wooden shelves holding various kitchen items such as glass jars, bottles, utensils, and plates. The word 'COOK' appears prominently on the top left shelf in red, large letters. [0:00:06 - 0:00:08]: The person continues speaking, gesturing occasionally, and slightly adjusting their position. The shelves behind them are filled with various kitchen items including plates, bowls, jars, knives, and other utensils. The word 'HOT' is displayed in white letters on the lower shelf to the left. [0:00:09 - 0:00:15]: The focus shifts to a closer view of the person, with more emphasis on their facial expressions. The background remains the same, featuring the well-organized kitchen setup with a consistent arrangement of jars, knives, and utensils. The individual appears to be explaining something, moving as they talk. [0:00:16]: The view zooms out to a wider shot of the kitchen again, showing the person using both hands to emphasize their point. The arrangement of kitchen items remains unchanged, and the overall bright and organized setting is maintained. [0:00:17 - 0:00:19]: The person stands still, facing the camera directly, with the kitchen backdrop clearly visible. They appear to be wrapping up their explanation, maintaining a neutral posture, and occasionally using hand gestures for emphasis.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What objects make up the hands of the clock shown at the beginning of the video?",
        "time_stamp": "00:00:20",
        "answer": "C",
        "options": [
          "A. Fork and spoon.",
          "B. Knife and spoon.",
          "C. Knife and fork.",
          "D. Spoon and chopstick."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_44_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:00:00 - 0:00:06]: The video begins with an overhead view of a stove with two pans positioned on it. The left pan is empty, while the right pan contains a mixture of food, including chunks that appear to be vegetables and meat. The person wearing the recording device is holding a striped towel in their right hand, adjusting the pan on the right with food in it. The background shows a countertop surface made of a speckled material. [0:00:06 - 0:00:12]: The scene transitions to a wider view of a kitchen, displaying a man facing away from the camera, appearing to be preparing something at the counter. The kitchen setup includes shelves holding various plates, cups, and kitchen tools. The man swiftly moves from the left side of the counter to the right, still holding the striped towel, occasionally glancing back at the stove. [0:00:12 - 0:00:14]: The man's right hand reaches into a small bowl, taking a pinch of seasoning. He then walks back towards the stove, where the food in the right pan is sizzling. He sprinkles the seasoning into the pan, using a steady hand to ensure even distribution.  [0:00:14 - 0:00:17]: The scene shifts to a close-up of the right pan on the stove, where the food inside is visibly cooking with steam rising. The person's hand is sprinkling seasoning into the pan with a yellow kitchen tool visible in the background along with various other items. The food is beginning to caramelize, indicating it is being sautéed. [0:00:17 - 0:00:19]: The perspective moves back to the overhead view. The person's left hand places two pieces of fried bread on a wooden cutting board beside the stove. The shot captures additional kitchen items like butter, green apples, and a bowl of ingredients, contrasting with the stainless steel countertop and stove setup. [0:00:19 - 0:00:21]: The hands are spreading butter onto one piece of the fried bread, using a knife to cover the surface evenly. Various kitchen items and the empty left pan are still visible, providing context to the cooking process taking place. [0:00:21 - 0:00:23]: The video returns to a close-up of the pan with the cooked food. The person's hand is seen adding another ingredient to the mixture, this time focusing on enhancing the food's flavor and possibly adjusting the heat, ensuring the food cooks perfectly. [0:00:23 - 0:00:25]: The camera captures an overhead view once more, where the identical action of adding ingredients to the sizzling food in the pan repeats. Furthermore, adjusting the heat settings on the stove is also indicated as part of the ongoing cooking process. [0:00:25 - 0:00:27]: A close-up view of the pan follows again. Here, the detailed cooking process is highlighted, showing the mixture of vegetables and meat caramelizing more deeply as they cook. The person's hand continues to adjust seasoning and heat, ensuring the food is perfect. [0:00:27 - 0:00:29]: The scene continues in the kitchen, showing the man standing at the stove from a side view. He holds the pan’s handle with a cloth and appears to be shaking or stirring the contents to ensure even cooking. The kitchen's background displays shelves with neatly arranged dishes and kitchen tools. [0:00:29 - 0:00:31]: The man continues to move the pan, his gaze fixed on the cooking food, demonstrating his focus on achieving thorough and even cooking. Various kitchen elements, such as the stove controls, spice jars, and utensils, are visible, creating a well-organized cooking environment. [0:00:31 - 0:00:33]: The man tilts the pan slightly while continuing to cook, indicating a smooth transition and focused cooking technique. The stove and other kitchen equipment provide a consistent backdrop, emphasizing a structured cooking process.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is in the left pan?",
        "time_stamp": "0:04:03",
        "answer": "B",
        "options": [
          "A. Pasta and sauce.",
          "B. Pear slices sprinkled with cinnamon powder.",
          "C. Only vegetables.",
          "D. Only meat."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Object Recognition",
        "question": "What is the person holding in their right hand at the beginning?",
        "time_stamp": "0:04:21",
        "answer": "B",
        "options": [
          "A. A spatula.",
          "B. A striped towel.",
          "C. A fork.",
          "D. A knife."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_44_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:25]: The video begins with an overhead shot of a kitchen counter and stove. On the counter to the left, there are two croissants on a white plate, a bowl containing butter, an empty bowl, and a green apple. On the stove, there are two frying pans, the first pan contains a piece of fried food, while the second pan contains a mixture with visible chunks of vegetables or fruit. The video appears to be filmed from a first-person perspective. A pair of hands is seen in the foreground, dipping what appears to be sliced bread into a mixing bowl containing a yellow liquid, likely eggs for a French toast recipe.  [0:05:26 - 0:05:29]: The camera angle shifts slightly, providing a closer view of the frying pans and the person’s hands continuing to dip the sliced bread into the yellow liquid. The bread is then lifted from the bowl, dripping slightly, and moved toward the frying pans on the stove. [0:05:30 - 0:05:36]: The scene cuts to a side view of the kitchen, framing the stove and the counter. The person is shown placing the soaked bread into one of the frying pans. The kitchen in the background features white brick walls and wooden shelves lined with colorful dishes and utensils. The person then uses a towel to wipe their hands while monitoring the cooking process. [0:05:37 - 0:05:39]: The camera angle returns to a close-up of the frying pans. The contents of the second frying pan continue to simmer while the bread in the first frying pan starts to brown. The person then adds an ingredient, possibly honey or syrup, to the second frying pan with the mixture of chunks. The action in the video is methodical and focused on the cooking process, with the diligent preparation of food items and careful attention to the stove.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the liquid in the mixing bowl that the bread is dipped into?",
        "time_stamp": "00:05:25",
        "answer": "B",
        "options": [
          "A. Red.",
          "B. Yellow.",
          "C. Blue.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the person use a towel?",
        "time_stamp": "00:05:36",
        "answer": "B",
        "options": [
          "A. To clean the stove.",
          "B. To avoid burning his hand.",
          "C. To cover the bread.",
          "D. To wipe the counter."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_44_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:18",
        "answer": "C",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 0.",
          "D. 4."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_78_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:05",
        "answer": "C",
        "options": [
          "A. 2.",
          "B. 4.",
          "C. 3.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_78_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:02",
        "answer": "C",
        "options": [
          "A. 5.",
          "B. 6.",
          "C. 4.",
          "D. 3."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_78_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:01",
        "answer": "C",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 5."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      },
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:33",
        "answer": "C",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 5."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_78_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins showing a group of remote-controlled cars racing on an indoor track. The cars are various colors, including green, red, black, and others with distinctive patterns. The cars are positioned closely together, some headlights turned on, and are moving along a curved section of the track. The track is surrounded by a low wall and has sections of green turf. [0:00:03 - 0:00:06]: The cars continue to race, with the leading cars beginning to spread out. The track has gentle curves, and the cars follow the bends smoothly. A few cars take the lead while others lag slightly behind. The background shows barriers and various plants as decor. [0:00:07 - 0:00:15]: The view shifts to a wider perspective, showing more of the racecourse. Some cars are taking a turn around a tighter curve. In the background, a large indoor venue filled with spectators watching the race. The onlookers are behind metal barriers, indicating an organized event. The racing cars continuously maneuver around corners with some cars breaking away from the main pack. [0:00:16 - 0:00:19]: As the race proceeds, a person stands within the track holding a remote control, likely the operator of one of the cars. The cars continue speeding along the course with intricate designs and decals visible on them. The track’s detailing includes small plants and barriers indicative of a competitive racing environment. [0:00:20]: The final frame shows another close-up of a person kneeling down, fixing or adjusting something on the track while the cars continue to race in the background. The spectators remain focused on the ongoing action, adding to the lively atmosphere of the event.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What feature do some of the cars have turned on?",
        "time_stamp": "00:00:03",
        "answer": "B",
        "options": [
          "A. Hazard lights.",
          "B. Headlights.",
          "C. Brake lights.",
          "D. Turn signals."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_491_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:40 - 0:02:00] [0:01:40 - 0:01:44]: The video begins with a view of a polished indoor track featuring two small, remote-controlled cars in action. The track features diverse terrains, including a smooth metallic surface and patches of green turf bordered by decorative shrubs. A crowd behind the fence in the background is watching the race attentively, indicating the scene is likely at a competitive event or exhibit.  [0:01:42 - 0:01:44]: The perspective shifts to a group of people standing behind a barricade, looking down at the track. Each of them is holding a remote control, and they are focused on operating their respective cars. The group is a mixture of individuals, suggesting a diverse but dedicated fan base of the event. [0:01:46 - 0:01:48]: The action returns to the track, where a larger number of remote-controlled cars are racing at high speed on the meticulously designed track. The track curves and turns are well-marked, and several cars are negotiating these turns, showcasing the drivers' skills. [0:01:50 - 0:01:52]: The cars navigate the track, showing efficient handling. The cars' headlights and taillights are visible, enhancing their realistic appearance. The scene captures the competitive nature of the race with cars close to one another, maneuvering swiftly. [0:01:53 - 0:01:54]: The camera continues to follow the cars as they move down the track. The green and red cars are prominently leading the pack. Other cars are in pursuit, highlighting the constant motion and excitement of the race.  [0:01:55 - 0:01:56]: Another perspective offers a broader view of the track, capturing more cars and providing context to the scene. Decorative plants and mini-structures add to the realistic appearance of the miniature racing environment. [0:01:57 - 0:01:58]: The cars, including the prominent green and red models, continue to maneuver around the track's curves, displaying impressive control. The green car is ahead, while the red car follows closely. [0:01:59 - 0:02:00]: The view remains dynamic, emphasizing the graceful movements of the remote-controlled cars as they navigate the track. The backdrop features more spectators and mini-construction structures, adding to the atmosphere of a well-organized and engaging event.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the event taking place in the video?",
        "time_stamp": "0:02:00",
        "answer": "D",
        "options": [
          "A. A car exhibition.",
          "B. A miniature track construction demonstration.",
          "C. A remote-controlled car sales event.",
          "D. A remote control car drifting competition."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      },
      {
        "task_type": "Object Recognition",
        "question": "What are the people behind the barricade holding right now?",
        "time_stamp": "0:01:42",
        "answer": "C",
        "options": [
          "A. Cameras.",
          "B. Drinks.",
          "C. Remote controls.",
          "D. Cell phones."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_491_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:21]: On a smooth, polished racing track, two remote-controlled (RC) cars are seen. They are highly detailed, with patterned decals on their bodies. One car is black and white, while the other is black with orange accents, both moving closely together. There is a yellow and black barrier in the background. [0:03:21 - 0:03:22]: Both RC cars continue to move alongside one another on the track, closely navigating a curve. A small green grassy area is present on the left side, adorned with shrubs and small plants. [0:03:22 - 0:03:23]: The two RC cars are still maneuvering close to each other, with their rear lights illuminated. The surrounding area has green grassy patches and more detailing of shrubbery and barricades. [0:03:23 - 0:03:24]: Spectators are visible behind a barrier fence as the RC cars continue their movement. The background includes structural features of the track and more grassy patches with shrubs. [0:03:24 - 0:03:25]: The RC cars start to steer away from the barrier fence, continuing to race against each other. The path continues to curve, maintaining the same polished surface. [0:03:25 - 0:03:26]: As the cars proceed, they begin to head towards a group of other RC cars. There is more greenery visible both on the left and the right sides of the track. [0:03:26 - 0:03:27]: Several other RC cars appear on the track, colorful and diverse in design, all moving in the same direction. The initial two cars are now part of this larger group. [0:03:27 - 0:03:28]: The group of RC cars, including the two original ones, are seen spread across the width of the track, creating a dynamic race scenario. The background continues to have green, grassy areas and barriers. [0:03:28 - 0:03:29]: As the race progresses, the RC cars navigate through a wide area on the track, showing an organized layout with distinct lanes and colored barriers. [0:03:29 - 0:03:30]: The black and white RC car is seen making a sharp turn, exhibiting a precise maneuver. The red RC car is close behind, maintaining speed and direction on the same path. [0:03:30 - 0:03:31]: Both cars are driving around a corner of the racing track. The track is marked with red and white barriers. A yellow and green RC car is visible in the background, along with additional track layout elements and barriers. [0:03:31 - 0:03:32]: The two cars continue to race around the corner, closely following each other. More RC cars can be seen in the distance, indicating a competitive race environment. [0:03:32 - 0:03:33]: The detailed layout of the track becomes visible, with more RC cars spread across the racing area. The background contains various small-scale models, track markings, and barriers. [0:03:33 - 0:03:34]: The black and white RC car and the red RC car continue racing, with the black and white car slightly ahead. They are driving around another bend in the track. [0:03:34 - 0:03:35]: The cars are now moving past more decorative elements on the track, which include small faux plants and grassy patches strategically placed. [0:03:35 - 0:03:36]: Both cars navigate through another curve in the track. There are other RC cars visible in the track area, adding to the complexity of the race. [0:03:36 - 0:03:37]: The cars are driving around a section of the track bordered with yellow and black striped barriers. Tiny model trees and bushes also line the sides of the track. [0:03:37 - 0:03:38]: The race continues as the cars move through a more congested area, with other RC cars visible to the right. There are track features such as pylons and small outdoor equipment. [0:03:38 - 0:03:39]: The black and white RC car and the red car continue to race on the track, while several other RC cars follow behind. The background includes various miniature props and track elements. [0:03:39 - 0:03:40]: The two main cars approach a section bordered by red and white barriers. There is a green section on the left, featuring small ornamental plants and racing decor.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What do the two main RC cars do after making a sharp turn?",
        "time_stamp": "0:03:38",
        "answer": "D",
        "options": [
          "A. Stop racing.",
          "B. Start moving in reverse.",
          "C. Swap lanes.",
          "D. Continue racing around a corner."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_491_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:02]: The video starts with a view of a small-scale racetrack where several remote-controlled cars are moving. The background is filled with spectators standing behind a metal fence, watching the race. Various cars are on the track, with a significant number of them in a group on the left side, including a vibrant green car in the front. [0:05:02 - 0:05:04]: The cars begin to move down the track, forming a line. The green car leads the way, followed closely by a red car with white stripes. The track is smooth and has designated lanes with mini traffic signs and decorative bushes. [0:05:04 - 0:05:06]: The leading group of cars continues down the track, maintaining their positions. The cars have different designs and colors, with some having bright lights illuminated. The background includes green patches of artificial grass and more spectators. [0:05:06 - 0:05:08]: The green and red cars take a turn on the track, drifting slightly to maintain speed. The track here has more decorative elements, such as small potted plants and mini road barriers. The green car has bright headlights and a rear spoiler. [0:05:08 - 0:05:10]: Both cars continue to take another turn, with the red car closely trailing the green car. The driving area expands, providing more space for the cars to maneuver. The environment features miniature trees and a more open view of the mini racetrack setup. [0:05:10 - 0:05:12]: The cars enter a straight path on the track, with the green car still leading. They are now passing through a part of the track that has tight turns and divided lanes, marked by cones and guardrails. [0:05:12 - 0:05:14]: The cars approach another series of turns, navigating smoothly around them. The green car’s rear lights are clearly visible, and the red car is closely following. Spectators can be seen in the background, observing the race intently. [0:05:14 - 0:05:16]: Both cars continue through the curved part of the track, demonstrating skillful maneuvering. The background shows a more detailed view of the racetrack, including various miniature buildings, traffic signs, and another car in motion. [0:05:16 - 0:05:18]: The green and red cars drive past a section of the track with more decorative greenery and road dividers. They appear to pick up speed as they move into a straight segment of the track. [0:05:18 - 0:05:20]: As they continue forward, the video captures a broader view of the track, with additional remote-controlled cars appearing in the frame. The scene ends with the green car maintaining its lead position, closely followed by the red car and other participants.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Which car is leading the race right now?",
        "time_stamp": "00:05:24",
        "answer": "D",
        "options": [
          "A. Blue car.",
          "B. Red car.",
          "C. Yellow car.",
          "D. Green car."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_491_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken by the individual just now?",
        "time_stamp": "00:00:10",
        "answer": "A",
        "options": [
          "A. The individual collected fried shrimp, walked to a counter, and placed them beside the mat.",
          "B. The individual chopped vegetables, moved to a stove, and started frying them.",
          "C. The individual took bread slices from a box, toasted them, and spread butter.",
          "D. The individual handled raw fish, sliced it into pieces, and arranged it on a plate."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_353_real.mp4"
  },
  {
    "time": "[0:03:02 - 0:03:12]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken by the individual just now?",
        "time_stamp": "00:03:12",
        "answer": "B",
        "options": [
          "A. The individual arranged slices of raw fish on a sushi mat and added some wasabi.",
          "B. The individual collected fried shrimp, walked to a counter, and placed them on a sushi mat with vegetables.",
          "C. The individual assembled a sandwich with various condiments and grilled it.",
          "D. The individual prepared a salad by chopping vegetables and tossing them in a bowl."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_353_real.mp4"
  },
  {
    "time": "[0:06:04 - 0:06:14]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken by the individual just now?",
        "time_stamp": "00:06:14",
        "answer": "A",
        "options": [
          "A. The individual prepared a sushi roll by adding avocado slices to a seaweed sheet.",
          "B. The individual assembled a sandwich with various condiments and grilled it.",
          "C. The individual prepared a salad by chopping vegetables and tossing them in a bowl.",
          "D. The individual cooked a steak, added seasoning, and plated it with a side of vegetables."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_353_real.mp4"
  },
  {
    "time": "[0:09:06 - 0:09:16]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken by the individual just now?",
        "time_stamp": "00:09:16",
        "answer": "A",
        "options": [
          "A. The individual selected various types of sushi rolls, placed them into a takeout box.",
          "B. The individual picked up assorted pastries, placed them in a box, and packed it for an event.",
          "C. The individual collected different kinds of fruits, placed them into a basket, and wrapped it with a decorative lining.",
          "D. The individual arranged a selection of cookies in a box and added a ribbon for presentation."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_353_real.mp4"
  },
  {
    "time": "[0:12:08 - 0:12:18]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken by the individual just now?",
        "time_stamp": "00:12:18",
        "answer": "A",
        "options": [
          "A. The individual assembled a sushi roll by adding greens.",
          "B. The individual toasted bread, added avocado slices, and prepared a sandwich.",
          "C. The individual chopped vegetables and added them to a cooking pot with water.",
          "D. The individual prepared dessert by adding cream to a mix of fruits."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_353_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:08]: A first-person perspective captures the scene of someone preparing to paint. On the left side of the frame, there is a watercolor palette with various colors arranged vertically, including a visible row of small circular wells containing pigments. The wells contain yellow, orange, red, purple, and dark blue watercolor paints that are arranged from top to bottom. The palette is placed on a wooden surface. To the right of the palette lies a rectangular glass dish partially filled with water, with a paintbrush visible inside. The person's hand is seen holding and manipulating the paintbrush in the water, preparing it for painting. The surface of the water in the dish appears slightly disturbed by the brush's movement. The right half of the frame is occupied by a blank sheet of white paper positioned onto the wooden surface;  [0:00:09]: The hand and paintbrush are temporarily absent from view, highlighting the blank white paper and the water dish with a slight splash of paint on the edge;  [0:00:10 - 0:00:12]: The person's hand returns, continuing to work with the paintbrush within the water dish;  [0:00:13 - 0:00:14]: The person's left arm extends across the frame, reaching towards the water dish. More of the person's action involving the paintbrush and palette is visible;  [0:00:15 - 0:00:17]: The person dips the paintbrush into the uppermost blue paint on the palette. The process of getting more paint onto the brush is shown, with slight splashes of dark blue paint in the mixing area within the dish;  [0:00:18]: The hand and paintbrush are again absent, with the view showing the white paper and slight paint residue within the water dish;  [0:00:19]: The person prepares to continue painting, with the hand holding the brush close to the palette, getting additional paint.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the arrangement of the items on the wooden surface now?",
        "time_stamp": "00:00:08",
        "answer": "A",
        "options": [
          "A. The watercolor palette is on the left, and the white canvas is on the right.",
          "B. The watercolor palette is on the right, and the glass dish is on the left.",
          "C. The watercolor palette is in the center, and the paper is on the right.",
          "D. The glass dish is in the center, and the paper is on the left."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_141_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:46]: A hand holding a paintbrush is seen applying a mix of green paint on the left side of a white paper, which has a previously drawn brown and green abstract form. To the left of the paper, a watercolor palette with six circular wells containing different colors is visible. The palette has smudges of mixed colors. The surface beneath the paper and palette is dark brown, likely a wooden table. The brush repeatedly touches the paper between the green and brown areas. [0:02:47 - 0:02:51]: The hand dips the paintbrush into the paint palette's blue section, adding more color to the paper. The previously described painting remains the same as the movements of the paintbrush continue to blend and modify the green-brown area. [0:02:52 - 0:02:57]: The hand continues to paint, blending different shades of green on the paper. The same paint palette with smeared colors and the dark brown table surface remains visible. [0:02:58 - 0:03:00]: The hand continues painting over the green area, refining the transitions between colors while using the same palette. The background and details remain consistent.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the painting activity just shown in the video?",
        "time_stamp": "00:03:00",
        "answer": "A",
        "options": [
          "A. The hand is painting a deep green and light green abstract form on paper.",
          "B. The hand is drawing a detailed landscape.",
          "C. The hand is sketching with a pencil before painting.",
          "D. The hand is creating a watercolor portrait."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_141_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:21]: A painting is present on the right side, featuring a landscape with a large tree branch crossing horizontally. The artist uses watercolors, and green, brown, blue, and yellow tones are visible. On the left side, there is a palette of watercolor paints with multiple colors. The hand of a person holds a paintbrush and dips it into the water in the palette. [0:05:22 - 0:05:24]: The hand with the paintbrush moves from the palette to the painting, starting to apply paint to a specific area on the right side, presumably adding details or filling in colors on the landscape. [0:05:25 - 0:05:32]: The artist continues working on the painting, with the hand and paintbrush moving steadily across the canvas, focusing on the area around the tree branch. The shadow of the hand and brush, caused by the lighting, is visible on the painting. [0:05:33 - 0:05:34]: The hand momentarily moves back to the palette, possibly to mix new colors or rinse the brush in the water, then returns to the painting to continue working on it. [0:05:35 - 0:05:37]: The brush is again employed on the painting, refining details and adjusting the color tones. The movements are deliberate, indicating careful attention to detail and technique. [0:05:38 - 0:05:39]: The painting shows more detail and refinement in the area where the artist has been working, particularly around the tree branch. The hand with the paintbrush continues to be active, emphasizing the enhancement of the landscape.",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What is the artist likely to do next?",
        "time_stamp": "00:05:34",
        "answer": "B",
        "options": [
          "A. Sign the painting.",
          "B. Add more details to the painting.",
          "C. Clean up the workspace.",
          "D. Start a new painting."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_141_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:19]: The video captures a first-person perspective of someone painting a watercolor scene. The person uses a brush in their right hand and is working on a landscape that features a blend of colors suggesting green foliage, a brownish pathway or tree branch, and patches of yellow and blue, possibly representing grass and sky or water. The artist's hand is positioned to the right of a paint palette containing several colors, which is placed on the left side of the frame. The person frequently dips the brush into the paint palette to add more color to the painting, moving the brush across the paper in different directions to blend and apply the paint, creating texture and shape within the artwork. The action is happening on a brown tabletop, which serves as the background. The entire scene is shot from above, providing a clear view of both the palette and the painting process.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the paint palette positioned relative to the artist's hand?",
        "time_stamp": "00:08:19",
        "answer": "A",
        "options": [
          "A. To the left of the hand.",
          "B. To the right of the hand.",
          "C. Directly below the hand.",
          "D. Above the hand."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Event Understanding",
        "question": "What is the artist creating in the video?",
        "time_stamp": "00:08:19",
        "answer": "B",
        "options": [
          "A. A portrait of a person.",
          "B. A watercolor landscape.",
          "C. An abstract painting.",
          "D. A charcoal sketch."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_141_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What brand is the black car?",
        "time_stamp": "00:00:05",
        "answer": "D",
        "options": [
          "A. BMW.",
          "B. Mercedes.",
          "C. Audi.",
          "D. Porsche."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_264_real.mp4"
  },
  {
    "time": "[0:01:53 - 0:01:58]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the logo on the driver's steering wheel right now?",
        "time_stamp": "00:01:53",
        "answer": "D",
        "options": [
          "A. Ferrari.",
          "B. Lamborghini.",
          "C. Mercedes.",
          "D. Porsche."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_264_real.mp4"
  },
  {
    "time": "[0:03:46 - 0:03:51]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the RPM gauge's maximum range right now?",
        "time_stamp": "00:03:49",
        "answer": "A",
        "options": [
          "A. Red.",
          "B. pink.",
          "C. Blue.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_264_real.mp4"
  },
  {
    "time": "[0:05:39 - 0:05:44]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the current speed shown on the phone's screen right now?",
        "time_stamp": "00:05:41",
        "answer": "D",
        "options": [
          "A. approx 150 km/h.",
          "B. approx 210 km/h.",
          "C. approx 170 km/h.",
          "D. approx 190 km/h."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_264_real.mp4"
  },
  {
    "time": "[0:07:32 - 0:07:37]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the current speed shown on the phone's screen right now?",
        "time_stamp": "00:07:32",
        "answer": "D",
        "options": [
          "A. approx 212 km/h.",
          "B. approx 222 km/h.",
          "C. approx 242 km/h.",
          "D. approx 232 km/h."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_264_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is trees located right now?",
        "time_stamp": "00:00:20",
        "answer": "D",
        "options": [
          "A. Directly beside the cyclist.",
          "B. Behind the cyclist.",
          "C. On the right side of the road.",
          "D. On the both sides of the road."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_182_real.mp4"
  },
  {
    "time": "[0:01:51 - 0:02:11]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Which direction is the road curving right now?",
        "time_stamp": "00:02:11",
        "answer": "D",
        "options": [
          "A. To the left.",
          "B. It is straight.",
          "C. It is a U-turn.",
          "D. To the right."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_182_real.mp4"
  },
  {
    "time": "[0:03:42 - 0:04:02]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the white post with a reflector located right now?",
        "time_stamp": "00:03:56",
        "answer": "D",
        "options": [
          "A. In the middle of the road.",
          "B. On the left side of the road.",
          "C. At the end of the road.",
          "D. Along the both sides of the road."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_182_real.mp4"
  },
  {
    "time": "[0:05:33 - 0:05:53]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the white road marker located right now?",
        "time_stamp": "00:05:34",
        "answer": "D",
        "options": [
          "A. On the left side of the road.",
          "B. In the middle of the road.",
          "C. Directly ahead on the road.",
          "D. On the both sides of the road."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_182_real.mp4"
  },
  {
    "time": "[0:07:24 - 0:07:44]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is on the right side of the cyclist right now?",
        "time_stamp": "00:07:45",
        "answer": "D",
        "options": [
          "A. A pedestrian path.",
          "B. A signpost.",
          "C. A ditch.",
          "D. A metal guardrail."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_182_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the sequence of actions that just happened?",
        "time_stamp": "00:00:10",
        "answer": "D",
        "options": [
          "A. The person cleaned a cup, filled it with water, and placed it on a tray.",
          "B. The person prepared utensils, boiled water, and served a customer.",
          "C. The person washed a cup, brewed espresso, and poured it into a mug.",
          "D. The person cleaned a pitcher, filled a glass with iced coffee."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_359_real.mp4"
  },
  {
    "time": "[0:02:26 - 0:02:36]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions performed just now?",
        "time_stamp": "00:02:36",
        "answer": "D",
        "options": [
          "A. The person grabbed almond milk, prepared a latte, and handed it to a customer.",
          "B. The person cleaned cups, filled a jug with almond milk, and adjusted the espresso machine settings.",
          "C. The person made a cappuccino, added whipped cream, and served it on a tray.",
          "D. The person picked up a box of almond milk, and poured milk into a pitche."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_359_real.mp4"
  },
  {
    "time": "[0:04:52 - 0:05:02]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the sequence of actions that just happened?",
        "time_stamp": "00:05:02",
        "answer": "D",
        "options": [
          "A. The person poured coffee into a to-go cup, cleaned the counter, and organized utensils.",
          "B. The person poured milk into a cup, placed it in the sink, and threw away a lid.",
          "C. The person grabbed a to-go cup, dumped its content into the trash, and cleaned the brewer.",
          "D. The person picked up an iced coffee and drank it, disposed of a cup into the trash,."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_359_real.mp4"
  },
  {
    "time": "[0:07:18 - 0:07:28]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the sequence of actions that just happened?",
        "time_stamp": "00:07:28",
        "answer": "D",
        "options": [
          "A. The person cleaned a coffee machine, prepared a cup, and served an espresso.",
          "B. The person prepared a coffee pot, filled it with water, and brewed a pot of coffee.",
          "C. The person picked up a milk pitcher, steamed milk, and poured it into a cup.",
          "D. The person grabbed a portafilter, filled it with coffee grounds, weighed it, and adjusted the amount with a spoon."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_359_real.mp4"
  },
  {
    "time": "[0:09:44 - 0:09:54]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the sequence of actions that just happened?",
        "time_stamp": "00:09:54",
        "answer": "D",
        "options": [
          "A. The person grabbed a glass, picked a lid, and filled the glass with water.",
          "B. The person picked up a glass, inspected it, and placed it on the counter.",
          "C. The person picked up glasses, stacked them, and put them aside.",
          "D. The person grabbed a glass, examined it, and walked towards another area."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_359_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: In a dimly lit room, a person in a white coat stands in the foreground. They appear to be facing the camera. A screen in the background shows another person seated and talking, with various icons and text displayed near the screen's right edge. [0:00:03 - 0:00:08]: The scene transitions to a black screen featuring different inventory sections labeled \"Player,\" \"Trucking Cargo,\" \"Backpack,\" and \"Ground.\" Items like books, a cup, and other objects are distributed among these sections. On the left side of the screen is an avatar with personal information displayed below. [0:00:09 - 0:00:10]: Back in the real world, a person with short blond hair and a striped shirt stands outside near a large black vehicle. The environment appears industrial with some greenery in the background. [0:00:11 - 0:00:12]: The same person is seen carefully placing a large cardboard box with a black crosshatch pattern on it near the vehicle. They are standing closer to the vehicle's open side. [0:00:13 - 0:00:14]: The person is now in the process of closing or interacting with the back door of the vehicle. The screen displays an icon and text indicating actions, such as options to open or close the door. [0:00:15 - 0:00:20]: The view returns to the black inventory screen from before, showing updated item placements. More items appear to be organized under the \"Player\" and \"Trucking Cargo\" sections, including consumables like food cans. The person moves between the vehicle and another nearby person, likely coordinating the task at hand. [0:00:21]: The person is seen walking away from the vehicle and towards a building entrance, smartphone in hand, possibly checking or sending a message.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:00:06",
        "answer": "B",
        "options": [
          "A. Opening the back door of the vehicle.",
          "B. Interacting with the back door of the vehicle.",
          "C. Loading items into the vehicle.",
          "D. Checking the inventory screen."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_280_real.mp4"
  },
  {
    "time": "0:02:40 - 0:03:00",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:43]: The video begins with a first-person perspective inside a game environment. The character being controlled is standing in front of two other characters. The scene is set outside a store with an ATM on the left. It's nighttime, the sidewalk and street are visible, and there's a sign \"More Trash for Less Cash.\" The controlled character is wearing a striped shirt. [0:02:44 - 0:02:50]: The controlled character begins to walk towards the two characters standing by the entrance of the store. One of the characters, who is closer, starts moving towards the store entrance as well. [0:02:51 - 0:02:55]: The perspective shifts to the left side, showing an ATM and part of the store façade with a large \"binco\" sign above. The controlled character pauses and looks back towards the two characters near the entrance. [0:02:56 - 0:03:02]: The character continues walking past the ATM and towards a large truck parked nearby. The truck has \"g\" visible, possibly part of a logo. [0:03:03 - 0:03:06]: The controlled character walks around the truck, now looking at it from the rear near the sidewalk area. Several buildings are visible in the background, and the scene remains dark, consistent with nighttime. [0:03:07 - 0:03:10]: The character walks to the side of the truck, and it appears they are attempting to open or interact with it. [0:03:11 - 0:03:14]: The controlled character successfully opens the door of the truck and climbs inside. [0:03:15]: The perspective shifts to inside the truck. The character is now inside the truck, preparing to drive it. The surroundings outside are partially visible, including the buildings and some streetlights. [0:03:16 - 0:03:18]: The truck starts moving forward, away from its initial parked position. The \"binco\" sign from the store remains visible on the left side of the frame. [0:03:19 - 0:03:21]: The truck moves further, and the perspective shows more of the surrounding buildings and streets as it gains speed. [0:03:22 - 0:03:30]: The truck continues driving through the urban environment, with various buildings, streetlights, and nighttime city elements visible in the background. The character drives further into the city, and the area appears well-lit with light reflections on the street.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the controlled character doing right now?",
        "time_stamp": "0:02:53",
        "answer": "B",
        "options": [
          "A. Walking past the ATM.",
          "B. Opening the door of the truck.",
          "C. Walking towards two characters.",
          "D. Standing in front of the store."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_280_real.mp4"
  },
  {
    "time": "0:05:20 - 0:05:40",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:22]: The video is viewed from a first-person perspective. A person with short blond hair, wearing a dark jacket with some worn-out fabrics at the shoulders, stands in a dimly lit room. On the left-hand side of the screen, there's a smaller video overlay showing another individual, possibly the streamer, seated in a room with lit decorative lights. This person is engaged with a game or video on their monitor. [0:05:23 - 0:05:25]: The individual on the main screen, who is likely an avatar, keeps changing clothing options in a game menu visible on the right side. The interface shows different avatar clothing pieces, which are being scrolled through and selected.  [0:05:26 - 0:05:29]: The avatar continues to change outfits while the person in the smaller overlay reacts to these changes. The background of the room remains largely unchanged, featuring a patterned wall and windows with blinds. [0:05:30 - 0:05:32]: As different outfits are selected, the avatar starts to move slightly, indicating actions like rotating or adjusting clothing. The person in the overlay remains focused on the screen, likely making selections or reacting to the avatar's appearance. [0:05:33 - 0:05:35]: The clothing options continue to be cycled through. The avatar is now seen making a hand gesture as if interacting with something. The individual in the smaller overlay leans closer to the screen, indicating concentration or interest. [0:05:36 - 0:05:38]: The avatar's outfit is changed again to a long, dark coat with tattered shoulders, revealing a black shirt underneath. The individual in the overlay appears to be inspecting the avatar's new look closely. [0:05:39]: As the video progresses, the same pattern continues - the avatar's outfits are changed through the game interface, and the person in the smaller screen remains engaged, showing various reactions and interactions with the ongoing clothing changes. The background remains consistent with minor variations in lighting.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the avatar doing right now?",
        "time_stamp": "00:05:35",
        "answer": "B",
        "options": [
          "A. Changing clothing options.",
          "B. Picking out clothes.",
          "C. Rotating in place.",
          "D. Walking across the room."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_280_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:04]: A character with medium blonde hair, dressed in a gray polo shirt and light shorts, stands in a room with various objects and furniture. The character is being viewed from a first-person perspective.  [0:08:05 - 0:08:07]: The view shifts to a mannequin wearing a black skirt and a purple top, placed next to tables with folded clothes. The background includes a glass window and cases with items. [0:08:08 - 0:08:11]: The camera refocuses on the same blonde character from before, standing in the center of the room, near the right side, with shelves and tables behind them filled with various items. [0:08:12 - 0:08:15]: A woman stands closer to the camera while the blonde character is slightly behind her, looking around the room. The space features medium-height shelves filled with clothes. [0:08:16 - 0:08:18]: The camera remains focused on the same characters. The man and woman appear to be in a clothing store, indicated by the racks and displays of clothes around them. [0:08:19 - 0:08:20]: The individual in the black skirt and purple top remains in the frame, suggesting the focus is on selecting or observing clothing, with multiple interactions and observations taking place in this short span.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the blonde character doing right now?",
        "time_stamp": "00:08:20",
        "answer": "D",
        "options": [
          "A. Walking towards a window.",
          "B. Observing items around the room.",
          "C. Talking to the woman nearby.",
          "D. Picking up clothes."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_280_real.mp4"
  },
  {
    "time": "0:09:40 - 0:09:51",
    "captions": "[0:09:40 - 0:09:51] [0:09:40 - 0:09:43]: Inside a room with dim lighting, shelves with colorful items are on the right side of the background. The camera is positioned from the perspective of a first-person view, showing a character creation or customization menu on the right side of the screen. The menu includes categories like \"Jacket,\" \"Glasses,\" \"Hair,\" \"Upper,\" \"Legs,\" and \"Shoes.\" In front of the camera, a male avatar stands, wearing a grey polo shirt, black pants, and white shoes. The male avatar has light-colored hair and stands in a relaxed posture. Another character, whose figure is partially cut off and shadowed, is visible on the right. [0:09:44 - 0:09:47]: The male avatar remains in the same spot, while the menu on the right side highlights \"Jacket.\" The item changes slightly as different jackets are selected, including jacket colors and styles. [0:09:48 - 0:09:51]: The selection process continues with a high-visibility vest appearing on the male avatar in one of the frames. Several choices are made rapidly, showing different versions of safety vests and jackets. The background and figures remain consistent throughout these frames.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the outermost piece of clothing currently being worn on the upper body?",
        "time_stamp": "00:09:49",
        "answer": "B",
        "options": [
          "A. A pair of glasses.",
          "B. A high-visibility vest.",
          "C. A grey polo shirt.",
          "D. A new hairstyle."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_280_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: At the beginning, the person is barely visible, sitting in a dimly lit room. Only the outline of the face and a black shirt are noticeable against the dark background.  [0:00:01 - 0:00:03]: As the video progresses, the lighting improves, and the individual holds a large, rectangular object with \"RTX 4090\" written on it in white letters. They present the object directly in front of them, showing its size and shape. The object appears to be a computer graphics card with one large fan visible. [0:00:04 - 0:00:06]: The person examines the graphics card from different angles, tilting it slightly to the left and right while maintaining a neutral expression. The background remains dark, with a faint green light illuminating the scene from the left side. [0:00:07 - 0:00:09]: The individual continues to present the graphics card, holding it steadily with both hands. The fan and the label \"RTX 4090\" are clearly visible. The person occasionally glances down at the card, presumably checking its condition or features. [0:00:10 - 0:00:12]: They maintain eye contact with the camera, perhaps explaining or discussing the graphics card's features. Their facial expressions remain calm and engaged while they hold the card at chest level. [0:00:13 - 0:00:15]: The person makes slight hand gestures, alternating between pointing at the graphics card and holding it. The camera angle and lighting remain consistent, ensuring clear visibility of both the card and the presenter's expressions. [0:00:16 - 0:00:19]: Towards the end, the person slightly tilts the card forward, showing more details of its structure. They conclude their presentation while looking directly at the camera, with a thoughtful expression, as they pause briefly before the video ends.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is written on the rectangular object that the individual is presenting?",
        "time_stamp": "00:00:15",
        "answer": "B",
        "options": [
          "A. GTX 1080.",
          "B. RTX 4090.",
          "C. RTX 3070.",
          "D. RX 6900."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_104_real.mp4"
  },
  {
    "time": "[0:03:40 - 0:04:00]",
    "captions": "[0:03:40 - 0:04:00] [0:03:40 - 0:03:46]: The video begins in a dimly lit corridor divided into three vertical segments. Each segment displays different graphics card models (RTX 2080 Ti, RTX 3090, and RTX 4090) along with their respective frame rates. A silhouette of a person stands in each segment, walking forward. The background is a tiled floor with a bright light at the end of the corridor. [0:03:47 - 0:03:49]: The scene transitions to a side-by-side comparison of two segments, showing different graphics card models (RTX 2080 Ti on the left and RTX 4090 on the right). Both segments feature a person running forward along a corridor with reflective tiled floors and bright light from an open space ahead. [0:03:50 - 0:04:00]: The video then shifts to an outdoor aerial scene, showcasing a plane flying over snowy mountains under a clear blue sky. This segment features a split-screen comparison of the RTX 4090, highlighting \"DLSS 3 Perf\" and \"Frame Generation\" settings. The left side of the screen shows a frame rate of 113 FPS, while the right side shows a frame rate exceeding 200 FPS. The landscape is detailed with snow-covered peaks and scattered clouds in the blue sky.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Just now, which graphics card model has its frame rate exceeding 200 FPS?",
        "time_stamp": "00:04:00",
        "answer": "C",
        "options": [
          "A. RTX 2080 Ti.",
          "B. RTX 3090.",
          "C. RTX 4090.",
          "D. GTX 1080."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_104_real.mp4"
  },
  {
    "time": "[0:07:20 - 0:07:40]",
    "captions": "[0:07:20 - 0:07:40] [0:07:20 - 0:07:37]: The video showcases a split-screen view showing a first-person perspective from two different races in what appears to be a Formula 1 video game. The left screen focuses on a driver in a \"WILLIAMS\" car, while the right screen shows another driver in a \"MERCEDES\" car. The detailed dashboards with various buttons and indicators are visible, with both drivers primarily moving forward and navigating slight turns. Both dashboards display information about gear, speed, and lap time. On the left side, the names \"VERSTAPPEN\" and \"BOTTAS\" appear, indicating rival drivers that the player is racing against. On the right side, the names \"RUSSELL\" and \"ALONSO\" are visible as competitors. The settings at the top indicate that both perspectives are running on RTX 4090 with high graphics settings, utilizing DLSS and Frame Generation technology. [0:07:38 - 0:07:39]: The scene transitions to an individual holding an NVIDIA RTX 4090 graphics card. The card is prominently displayed with a focus on its design, featuring a large cooling fan and sleek black and silver finish. The person's hands carefully present the card against a dark, blurred background, drawing attention to the hardware.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which rival drivers are shown in front of the first-person perspective of the \"WILLIAMS\" car right now?",
        "time_stamp": "00:07:37",
        "answer": "B",
        "options": [
          "A. RUSSELL and ALONSO.",
          "B. VERSTAPPEN, BOTTAS, RUSSELL, MAGNUSSEN and ALBON.",
          "C. HAMILTON and LECLERC.",
          "D. VETTEL and GASLY."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_104_real.mp4"
  },
  {
    "time": "[0:11:00 - 0:11:20]",
    "captions": "[0:11:00 - 0:11:20] [0:11:00 - 0:11:12]: The video shows a graphical stress test running, likely on a computer screen. The background is filled with a dynamic, fur-like, colorful pattern with hues of orange, yellow, and brown, indicative of a GPU stress test. In the center of this pattern is a floating window displaying detailed system information, likely for monitoring purposes. This window lists various metrics and values such as CPU, GPU usage, and system temperature. Above this window, in the top center section, are key statistics about the GPU: \"RTX 4090,\" temperature readings, clock speed, and power consumption. The temperatures and power consumption values change slightly throughout these frames; [0:11:12 - 0:11:20]: The scene shifts, revealing an open computer case setup on a well-lit desk with a cool blue backdrop. Prominently featured is a high-end graphics card with multiple cables connected to it, including thick power cables and data cables. There are RGB-lit fans glowing in pink and orange on the right side of the GPU. In the background, a monitor displays the previously seen graphical stress test pattern. Additionally, a person's hand appears to be adjusting something on top of the PC case, indicating some interaction with the computer's hardware.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What key statistics about the GPU were shown just now?",
        "time_stamp": "0:11:12",
        "answer": "B",
        "options": [
          "A. CPU usage and system temperature.",
          "B. GPU model, temperature readings, clock speed and power.",
          "C. Memory usage and network speed.",
          "D. Hard drive space and cooling fan speed."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_104_real.mp4"
  },
  {
    "time": "[0:13:40 - 0:13:53]",
    "captions": "[0:13:40 - 0:13:53] [0:13:40 - 0:13:46]: In a brightly lit, minimalistic setting, two graphics cards are placed side by side on a white surface. The graphics card on the left, labeled \"RTX 4090,\" is slightly elevated and being positioned next to the other card with a hand visible. The card on the right is labeled \"RTX 3090\" and is already stationary. Both cards are primarily black with metallic accents and have large cooling fans. The background is a simple black, enhancing the focus on the graphics cards. [0:13:47 - 0:13:52]: The scene shifts to a person sitting indoors against a dark background. The individual has short dark hair and wears a plain black t-shirt. They are speaking directly to the camera. The setting includes some green ambient lighting, and various pieces of equipment are visible in the background, including shelves and possibly a computer setup. The person appears relaxed and engaged while communicating.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the visual focus of the scene right now?",
        "time_stamp": "00:13:46",
        "answer": "B",
        "options": [
          "A. The setup of a gaming console.",
          "B. Two graphics cards on a white surface.",
          "C. A close-up of a computer screen.",
          "D. A group of people discussing."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_104_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins with a first-person perspective of a miniature race track set. The track, made of black materials, curves to the right and is bordered by green, grass-like terrain and trees. A fence and a billboard with some text are visible on the left side. A small orange vehicle is in the background on the left; [0:00:03 - 0:00:06]: The track and the surroundings remain similar, but a blur of a small blue toy car passes through the scene swiftly;  [0:00:04 - 0:00:06]: The video transitions to a black screen with “NEXT GEN DIECAST RACING” displayed in stylized, colorful text with a glitching effect;  [0:00:06 - 0:00:07]: The screen shows the same text but in solid yellow letters;  [0:00:07 - 0:00:09]: It continues to show “NEXT GEN DIECAST RACING,” emphasizing that it's about the next generation of diecast racing; [0:00:07 - 0:00:10]: A detailed miniature race setup comes into view. Positioned on the track are several colorful toy race cars, including green, white, purple, blue, and red, lined up ready for racing. There is a backdrop of a loading dock scene with a Walmart truck and other toy vehicles near a fenced-off area. The area includes multiple garages and animated toy figures; [0:00:11 - 0:00:15]: Text appears over this image, announcing “NEXT GEN PISTON CUP, Race 2 - Round 1.” The race cars are still in the same lined-up positions on the racetrack; [0:00:16]: The text changes to “Group 3 and 4.” The race cars are still aligned on the track. The background setup remains consistent with the previous scenes.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is announced over the image of the detailed miniature race setup right now?",
        "time_stamp": "0:00:14",
        "answer": "B",
        "options": [
          "A. \"NEXT GEN PISTON CUP, Race 1 - Round 2\".",
          "B. \"NEXT GEN PISTON CUP, Race 2 - Round 1\".",
          "C. \"NEXT GEN TOY RACE, Round 2 - Race 1\".",
          "D. \"NEXT GEN TOY RACE, Race 1 - Round 2\"."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_486_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:01]: A scoreboard is displayed listing standings for Group 3 - Heat 2, showing the participants and their scores: #36 Rich Mixon with 8 points, #20 Jackson Storm with 7 points, #48 Aaron Clocker with 5 points, and #11 Chris Roamin’ with 2 points. The backdrop of the scoreboard includes a green landscape with trees, parts of a black racetrack, and a red helicopter in the sky. [0:03:02 - 0:03:12]: Four race cars are positioned on the \"Thunder Mountain Speedway\" starting line. The cars, placed in two rows, consist of different colors (pink, orange, blue, and green). The backdrop shows a painted sky with clouds and trees. The cars remain stationary, preparing for the start of the race. Cars at the front row include #36 Rich Mixon (pink) and #11 Chris Roamin’ (orange), while the second row has #48 Aaron Clocker (blue) and #20 Jackson Storm (green). [0:03:13 - 0:03:17]: The race cars begin to move down the track, accelerating as they proceed. The background includes a grassy landscape, trees, painted sky with clouds, and a racing-themed setup with signs displaying \"TBR\" and \"JH\". The cars quickly gain speed as they navigate the racetrack, maintaining their positions in a close grouping. [0:03:18 - 0:03:19]: The pink race car, presumably #36 Rich Mixon, leads the race, significantly ahead of the other cars. The background features a green landscape with hills, parts of the track, and a blue tent-like structure. The other race cars are trailing behind in a significantly more spread-out manner. [0:03:20]: The race continues with the pink car in the lead navigating a curve. The surrounding backdrop includes the racetrack, grassy terrain, hills, and other racing infrastructure.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the car driven by Rich Mixon?",
        "time_stamp": "00:03:02",
        "answer": "A",
        "options": [
          "A. Pink.",
          "B. Blue.",
          "C. Green.",
          "D. Orange."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Event Understanding",
        "question": "Which car is leading the race?",
        "time_stamp": "00:03:20",
        "answer": "D",
        "options": [
          "A. #20 Jackson Storm.",
          "B. #48 Aaron Clocker.",
          "C. #11 Chris Roamin'.",
          "D. #36 Rich Mixon."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_486_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:05]: Four race cars are lined up in two rows on a race track at Thunder Mountain Speedway. The cars are colorful, with various designs, and are positioned at the starting line, ready to race. The backdrop shows a painted sky with clouds and trees around the track. In the top left corner, there is a label \"Group 4 - Heat 1\". The names \"#68 H.J. Hollis\" and \"#90 Paul Conrev\" are displayed at the bottom, indicating the occupants of Row 1;  [0:06:05 - 0:06:11]: The camera angle remains the same, but now the display shows the occupants of Row 2: \"#92 Sheldon Shifter\" and \"#15 Harvey Rodcap\". The names and labels suggest an organized race event with multiple heats;  [0:06:11 - 0:06:14]: The race starts, and the cars speed forward. The green car in the foreground leads, followed closely by the other vehicles. The motion blur indicates rapid movement, and the surrounding scenery remains constant, emphasizing the cars' acceleration;  [0:06:14 - 0:06:15]: The perspective shifts to a wider view of the track, showing the cars as they begin to navigate a turn. The background includes signs and markers, showing a well-organized racetrack environment;  [0:06:15 - 0:06:16]: The cars race through the turn, maintaining close positions. The track design features sharp curves and hills, adding complexity to the race;  [0:06:16 - 0:06:18]: The view zooms out further to reveal more of the track, showing the leading car, a green vehicle, navigating the turns ahead of the others. The track is surrounded by grassy areas and hills, enhancing the outdoor racing vibe;  [0:06:18 - 0:06:19]: The scene shifts to an overhead view of the track's curved section. The cars are in the middle of a tight turn, highlighting the racing skill required. Infield features emergency vehicles, barriers, and decorations, emphasizing the race's seriousness;  [0:06:19 - 0:06:20]: The green vehicle rounds the curve, demonstrating its handling and speed. The other cars follow closely, showcasing competitive racing. The track's rubber surface and boundary lines are clearly visible, ensuring the race's structure.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "How are the race cars positioned right now?",
        "time_stamp": "00:06:00",
        "answer": "C",
        "options": [
          "A. In a single line.",
          "B. Randomly scattered.",
          "C. In two rows.",
          "D. In a circle."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_486_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:08]: The video shows a scene from a model car racetrack. The track is black and winds through a hilly, grassy landscape. There are several model race cars in different colors on the track, moving quickly. In the infield area, there are toy models of emergency vehicles, construction equipment, and other cars. There are small signs and advertisements along the inside of the track’s curve. Trees and hills form the background. The word \"Replay\" is displayed prominently on a black background in the upper right corner;   [0:09:09 - 0:09:19]: The scene shifts to a close-up of a standings board for a race, titled \"Group 4 - Heat 3.\" The board shows the rankings: 1. #15 Harvey Rodcap (12 points) 2. #90 Paul Conrev (11 points) 3. #92 Sheldon Shifter (5 points) 4. #68 H.J. Hollis (3 points). The background reveals more of the model racetrack, with detailed scenery including a red helicopter model and various miniature structures.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Who is ranked first for Group 4 - Heat 3?",
        "time_stamp": "00:09:48",
        "answer": "D",
        "options": [
          "A. #90 Paul Conrev.",
          "B. #92 Sheldon Shifter.",
          "C. #68 H.J. Hollis.",
          "D. #15 Harvey Rodcap."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_486_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:40 - 0:02:00] [0:01:40 - 0:01:45]: A frying pan containing several clams is placed on a gas stove burner, with the flame on. A person’s hand holds the lid's handle, raising it slightly while steam escapes from the sides. Various kitchen items, including a spice rack and utensils, are visible in the background, against blue cabinets.   [0:01:46 - 0:01:49]: The person appears in view, standing behind the stove. They wear a blue shirt and are engaged in explaining something while making gestures with their hands. The kitchen setting includes a white brick backsplash, an organized arrangement of dishes, and cooking equipment hanging on the wall.  [0:01:50 - 0:01:51]: They focus on the pan again, gripping the lid, as if about to lift it. The broader kitchen environment with neatly stacked dishes, bottles, and utensils remains visible in the background.  [0:01:52 - 0:01:54]: The person continues to explain, making hand gestures and emphasizing points animatedly. They stand near a blue cabinet and a white brick wall, giving a detailed step-by-step of what they are doing.  [0:01:55 - 0:01:56]: The person is seen turning toward the countertop, gesturing as they talk. The countertops have several cooking items, including a chopping board. [0:01:57 - 0:01:58]: The camera shifts to an overhead view of a cutting board, with multiple small bowls containing spices, a large knife on the left, and a piece of seasoned raw meat on the board. A hand reaches out, placing the meat on the cutting board.  [0:01:59]: The person’s hand firmly holds the meat, positioning it for cutting while the knife is poised on the board, ready to slice through the meat.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is placed on the gas stove burner right now?",
        "time_stamp": "0:01:19",
        "answer": "D",
        "options": [
          "A. A saucepan with boiling water.",
          "B. A kettle with tea.",
          "C. A wok with vegetables.",
          "D. A frying pan containing several garlic."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Event Understanding",
        "question": "What is the person doing with the bread on the cutting board?",
        "time_stamp": "0:01:59",
        "answer": "C",
        "options": [
          "A. Marinating it.",
          "B. Grilling it.",
          "C. Preparing to cut it.",
          "D. Weighing it."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_46_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:22]: A person in a blue shirt stands in a kitchen with blue and white cabinets. To the right, there is a white brick wall. The person gestures with their hands, and two built-in ovens are visible behind them. A countertop with various kitchen items like jars and a cutting board is present. [0:03:22 - 0:03:23]: The person leans slightly to the right, focusing on something out of the frame. A close-up reveals the countertop below their elbows.  [0:03:24 - 0:03:26]: A bird’s-eye view of a stovetop shows a covered pan on the left burner and an open grill pan on the right burner. The person’s hands are placing two sandwich slices on the grill pan. [0:03:27 - 0:03:30]: The camera zooms out a bit, capturing more of the stovetop and the grill pan. The person adds more sandwiches to the pan. They hold a small bowl, possibly containing oil or butter, and drizzle it over the sandwiches. [0:03:31 - 0:03:32]: A close-up of the three sandwiches grilling on the pan. In the background, there are utensils, a spice jar, and a glass bowl. [0:03:32 - 0:03:33]: The person now stands back, pointing something out to someone, possibly giving cooking instructions. The camera captures more of the kitchen area, including a shelf filled with plates, jars, and a wine bottle. [0:03:34 - 0:03:35]: The person leans forward to adjust the sandwiches on the grill pan. The close-up shows the action of flipping or pressing the sandwiches down.  [0:03:36 - 0:03:37]: The person continues to adjust and press the sandwiches with their fingers on the grill pan. The view provides a glimpse of a large bowl on the left and various countertop items. [0:03:38 - 0:03:39]: The person, still focused on the grill pan, slightly lifts the sandwich slices with a spatula, checking for doneness. The kitchen setup in the background remains the same.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What does the person do after placing sandwich slices on the grill pan?",
        "time_stamp": "00:03:30",
        "answer": "A",
        "options": [
          "A. Pours oil  over the sandwiches.",
          "B. Adds seasoning to the sandwiches.",
          "C. Puts a lid on the grill pan.",
          "D. Turns off the burner."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_46_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:03]: The scene takes place in a modern, well-equipped kitchen. The brick walls are painted white, and there is a wooden shelf holding various kitchen items, such as green and clear glass bottles, plates, and bowls. The kitchen has a large countertop that appears to be made of concrete. Large red letters spelling \"COOK\" are displayed prominently on the upper left wall. Below the shelf is a counter with several appliances and utensils neatly arranged. The depicted individual, wearing a blue T-shirt, is standing behind the counter. He is preparing food, initially on the left side of the counter, and then moving to the right. [0:05:03 - 0:05:05]: The individual continues working on the meal. He reaches towards the far right side of the counter and grabs something from the cutting board. The kitchen remains organized with various utensils and ingredients in view. [0:05:05 - 0:05:10]: Switching angles to an overhead view, the camera shows a closer look at the cooking process. On the counter, there are spices in small bowls, half a lemon, and some sliced bread on a wooden cutting board. A gas stove with a covered pot containing a simmering mix of ingredients occupies the central part of the frame. The individual takes pieces of bread and begins placing them into the pan positioned on the stovetop. [0:05:10 - 0:05:12]: Returning to a frontal perspective, the individual continues to meticulously place the bread slices onto the pan. The kitchen background, with its neatly arranged shelves and utensils, is still visible. [0:05:12 - 0:05:14]: The individual begins to use a mortar and pestle to grind some ingredients. This new movement happens while maintaining focus on the mixture being prepared. The counter is seen with various kitchen tools and ingredients, including bowls with ground spices and fresh herbs. [0:05:14 - 0:05:19]: There is a close-up of the individual continuing to work on the mixture. He adds what appears to be ground black pepper from a small black bowl. After mixing the contents, he checks the overall progress of the meal. He places the prepared mixture down and reaches for the bread slices, still carefully attending to his cooking.\"",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "0:05:14",
        "answer": "C",
        "options": [
          "A. Spread the butter onto the piece of toast.",
          "B. Spread the jam onto the biscuit.",
          "C. Spread the condiments onto the slice of bread.",
          "D. Spread the mayonnaise onto the sandwich bun."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_46_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:41]: In a kitchen with wood and blue cabinetry, gray stone countertops, and white tile backsplash, someone is working at a counter. On the counter, there is a bowl, a wooden cutting board, and some slices of bread. The person is wearing a blue t-shirt and is focused on preparing the food. They appear to be garnishing a dish with herbs.  [0:06:42 - 0:06:45]: The camera captures the person continuing to garnish the food. A pan is visible on the stovetop to the left. Several small bowls with various ingredients are spread out on the counter. The person reaches into the bowl and carefully places what looks like herbs onto the bread slices. [0:06:46 - 0:06:49]: The view shifts to an overhead shot, providing a clearer view of the ingredients and preparations. There are three pieces of bread spread with a dark paste, a knife, a cut lemon half, and several small bowls containing shallots, sauces, and seasonings. The person continues to garnish the bread slices, transferring herbs from the bowl. [0:06:50 - 0:06:51]: The focus returns to a side view, showing the person using a spoon to place onion slices and herbs onto the bread slices methodically, creating an organized and neat presentation. [0:06:52 - 0:06:53]: The person continues to place more herbs onto the bread slices, ensuring even distribution. The area is kept organized, with different ingredients neatly placed on the counter. [0:06:54 - 0:06:56]: The spoon is used again to add the final touches of herbs onto the bread, making sure every slice gets garnished equally. The person works with precision and care. [0:06:57 - 0:06:58]: The person uses their hands to assist in adding the herbs, ensuring everything is in place. The ingredients in the bowls are now mostly used, indicating the near completion of the garnish process. [0:06:59]: The final garnishing is being added with a spoon, securing the herbs and onions on top of the slices of bread, completing the preparation process. The surroundings are tidy, and the food looks ready to be served.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What object is the person using to garnish the food?",
        "time_stamp": "0:06:54",
        "answer": "D",
        "options": [
          "A. Fork.",
          "B. Knife.",
          "C. Tongs.",
          "D. Spoon."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_46_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: On the left on a white gallery wall, there is an abstract floral painting with vibrant colors such as yellow, green, and red. Yellow and green swirls dominate the composition, with red and pink accents. On the right side of the frame, there are two tall, slender wooden sculptures on white pedestals, placed against a white curtain.  [0:00:01 - 0:00:05]: The view shifts slightly to the left, showing more of the white wall and the floral painting. The painting remains constant, with its lively colors and shapes. The sculptures are still visible to the right.  [0:00:05 - 0:00:09]: The camera continues panning left, revealing more of the gallery wall. A large, predominantly blue abstract painting with scattered purple and white elements is now completely in view next to the floral painting. Both paintings are prominently displayed on the wall with their respective artist labels below them. [0:00:09 - 0:00:14]: The camera moves further left. Both the blue and the green paintings become fully visible, revealing their contrasting yet complementary abstract styles. The vibrant blue painting with hints of white and purple continues to make a visually striking statement, while the green one displays a dark background with bright, colorful splashes. The sculptures are still partially visible on the right-hand side. [0:00:14 - 0:00:16]: The camera now shows two full paintings: one with a dark background and bright assorted colors on the left, and the previously seen blue painting. The left-hand painting adds a diverse palette with leafy shapes and other organic forms, contrasting with the chaotic yet harmonious blue hues of its neighboring painting.  [0:00:16 - 0:00:20]: The view continues leftward, showing additional abstract compositions. The new painting on the far left is dark with vibrant splashes of yellow, green, blue, and red, creating a lively color contrast. The rest of the gallery setup, including smaller objects like a decorative brown cube and the slender sculptures initially in view, blends cohesively with these colorful artworks.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the artist labels located in relation to the paintings?",
        "time_stamp": "00:00:09",
        "answer": "A",
        "options": [
          "A. Above the paintings.",
          "B. Next to the paintings.",
          "C. Below the paintings.",
          "D. On the paintings."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best describes these artworks?",
        "time_stamp": "0:00:20",
        "answer": "A",
        "options": [
          "A. A mix of abstract paintings with vibrant colors and sculptures on pedestals.",
          "B. A series of black and white photographs.",
          "C. A collection of modern furniture designs.",
          "D. A display of classical paintings with gold frames."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_465_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:02]: The video starts by showing a corridor with art pieces hanging on white walls. One of the paintings is colorful, with characters in vibrant outfits on the left. This corridor is part of an art gallery. The viewpoint is moving forward, capturing a closer look at the paintings.  [0:08:03 - 0:08:06]: The focus is now on a tall, vertical painting with a dark blue center and lighter edges. Several spotlights illuminate the painting, creating reflections on its surface. In the background, more artworks on white walls and visitors walking around are visible. The camera continues to move closer.  [0:08:07 - 0:08:09]: The viewpoint shifts downward, revealing additional framed artworks leaning against the wall below the tall painting. One frame shows an image of a person, possibly a photograph with a light brown background. Another artwork with abstract shapes and vibrant colors is also visible.  [0:08:10 - 0:08:12]: As the viewpoint continues to move along the wall, it focuses on a few more framed artworks stacked on the floor. There is a variety of colors and shapes, including a piece featuring a figure with a yellow background. The camera now captures part of another painting hanging on the wall, characterized by a mix of vibrant abstract patterns. [0:08:13 - 0:08:16]: The camera tilts up to show a large framed textile piece hanging on the wall. This artwork features an intricate, woven pattern with a white circular shape at the top, bordered in red. Nearby, people are seen exploring the gallery, and the details of the exhibition space come into view. [0:08:17 - 0:08:19]: The focus shifts to a colorful, large painting suspended on the adjacent wall. The painting showcases a visually striking combination of geometric and floral patterns with two figures depicted in it. The vibrant colors and detailed patterns dominate the artwork, providing a vivid visual experience. Visitors continue to walk through the gallery, observing the art.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is prominently featured in this tall, vertical painting?",
        "time_stamp": "00:08:07",
        "answer": "B",
        "options": [
          "A. Characters in vibrant outfits.",
          "B. A dark blue center with white edges.",
          "C. Abstract shapes and vibrant colors.",
          "D. Geometric and floral patterns."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Causal Reasoning",
        "question": "Why are there reflections on the surface of the tall painting?",
        "time_stamp": "00:08:04",
        "answer": "C",
        "options": [
          "A. Because it has a glossy finish.",
          "B. Because it is covered in glass.",
          "C. Because it is illuminated by several spotlights.",
          "D. Because the camera is using a flash."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Object Recognition",
        "question": "What is being displayed on the video now?",
        "time_stamp": "00:08:52",
        "answer": "B",
        "options": [
          "A. A bookshelf filled with books.",
          "B. A painting depicting a bookshelf.",
          "C. A painting with many trees.",
          "D. A painting with many people in it."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_465_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00] The video starts with an animation on a blue background showing a large number '10' in white. The inside of the '0' features a clock-like display with a knife and fork crossing each other. [0:00:01 - 0:00:04] The scene transitions to a person standing against a blue backdrop with the words \"RAMSAY in 10\" prominently displayed behind them. The knife and fork clock is now integrated into the zero of the \"10\". [0:00:05 - 0:00:19] The person is now in a kitchen environment, standing in front of a white brick wall with shelves containing various kitchen items such as cups, plates, bottles, and jars. A large red \"COOK\" sign is visible on the top left corner of the wall. Throughout this segment, the person is seen speaking, gesturing with their hands, and occasionally looking to the side. The kitchen counter has utensils and dishes organized neatly on it.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is prominently displayed behind the person standing against a blue backdrop?",
        "time_stamp": "00:00:20",
        "answer": "D",
        "options": [
          "A. \"CHEF in 10\".",
          "B. \"COOK in 10\".",
          "C. \"BAKE in 10\".",
          "D. \"RAMSAY in 10\"."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_40_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:01]: The video begins with a close-up view of hands working over a small blender cup placed on a wooden cutting board. The background includes a gas stove to the left, blue cabinets, and a wooden drawer. The person is adding fresh herbs to the blender cup filled halfway with a creamy liquid. [0:04:01 - 0:04:02]: The hands are seen squeezing the herbs between fingers, ensuring they are finely broken up before dropping them into the blender cup. [0:04:02 - 0:04:03]: The camera shifts to reveal more details about the person. A man in a teal polo shirt stands at the counter, continuing to handle the herbs. Behind him, there are white brick walls, shelves with dishes, and kitchen utensils. [0:04:03 - 0:04:04]: The man concentrates on adding the herbs to the blender cup, pulling apart the green leaves. Various ingredients, like bottles and spices, are organized on the counter around him. [0:04:04 - 0:04:05]: Continuing from the previous segment, the man adds the last bits of herbs into the blender cup. His expression indicates a focus on ensuring the correct quantity is used. [0:04:05 - 0:04:06]: The man is breaking apart more herbs over the blender cup. The countertop displays ingredients, including an avocado, oil, and spices ready for use. Cookware and dishes are neatly arranged behind him. [0:04:06 - 0:04:07]: The scene transitions to an overhead view, showing the person's hands intermittently adding herbs to the cup. The cutting board is clean, with a few small herb leaves scattered around. A few additional ingredients, such as small bowls of salt and pepper, are present around the workspace. [0:04:07 - 0:04:08]: The person continues to add herbs to the blender cup. Above the cutting board view, the kitchen setup includes a gas stove and various small kitchen tools like a blender lid and seasoning bowls. [0:04:08 - 0:04:09]: Details from the previous segment continue. The person's fingers are breaking apart the herbs before placing them into the blender cup. The camera angle allows for a clear view of the workspace and positioned herbs. [0:04:09 - 0:04:10]: The process of adding herbs progresses, with hands clearly seen handling the fresh ingredients. The creamy liquid in the cup shows slight signs of infusion from the herbs. [0:04:10 - 0:04:11]: The overhead shot reveals the herb preparation steps. The person's hands place the final amount of herbs into the blender cup. The countertop displays several small containers and kitchen utensils. [0:04:11 - 0:04:12]: The scene focuses on the person's hand positioning the blender cup. Preparation appears complete, and the person is about to finish arranging the ingredients. [0:04:12 - 0:04:13]: The person's hand gently moves towards the left, placing a seasoning container down. The workspace is tidy with all necessary ingredients near the wooden cutting board. [0:04:13 - 0:04:14]: The video shows the person reaching towards something on the left, likely completing the final steps of preparation. [0:04:14 - 0:04:15]: The hand appears to be adjusting or placing additional seasoning or herb containers while ensuring the area remains organized for subsequent steps. [0:04:15 - 0:04:16]: The man finishes adding the herbs, and he begins to position the blender cup centrally on the cutting board. He looks down, ensuring everything is in place. [0:04:16 - 0:04:17]: The man adjusts the blender cup on the cutting board. He reaches out to grab a small jar from the counter. Other ingredients like bottles and seasoning containers are within easy reach. [0:04:17 - 0:04:18]: The man picks up an avocado with a knife in hand, ready to prepare it. The blender cup with the creamy liquid and herbs sits in the background. His focus shifts from the herbs to the avocado. [0:04:18 - 0:04:19]: He starts to cut the avocado, carefully slicing it with the knife. The workspace remains organized, with the blender cup in the middle containing the mixtures of ingredients. His attention remains on precisely cutting the avocado.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person adding to the blender cup?",
        "time_stamp": "0:04:00",
        "answer": "D",
        "options": [
          "A. Fresh fruits.",
          "B. Spices.",
          "C. Oil.",
          "D. Fresh herbs."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_40_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: A pair of hands is slicing bread into small cubes on a wooden cutting board. The bread is light brown with a rough texture, and the knife has a straight, shiny blade. On the left side, a dark rectangular stove burner is visible.   [0:08:01 - 0:08:02]: One hand is moving towards a pile of bread cubes scattered on the wooden cutting board, while the other hand holds the knife. A portion of the dark stove remains visible in the background. [0:08:02 - 0:08:03]: The video shifts to an overhead view of the kitchen setup. On the left, there are various ingredients placed on the countertop, including a small bowl of a green substance, a round citrus fruit, and a small dish of chopped ingredients. In the center, a grill pan with two halves of a lemon and a piece of grilled meat is on the stove. [0:08:03 - 0:08:04]: The view remains overhead as the hands rearrange bread cubes on the cutting board to the left. The grill pan continues to emit steam, indicating it is hot. [0:08:04 - 0:08:05]: A hand holding a black pan moves it over the burner on the right, with the bread cubes still on the cutting board to the left. The grilled meat and lemons remain in the center grill pan. [0:08:05 - 0:08:06]: The hand continues to position the pan over the burner, while the other hand begins to move towards the left. The bread cubes on the cutting board and the grill pan in the center remain unchanged. [0:08:06 - 0:08:07]: The hand on the left arranges the bread cubes on the cutting board. The other hand adjusts the pan on the right. The counter remains cluttered with various kitchen items and ingredients. [0:08:07 - 0:08:08]: The hands of a person with short gray hair are visible, pivoting from the cutting board towards the pan on the right. The bread cubes and grill pan with the grilled meat and lemon halves are still in view. [0:08:08 - 0:08:09]: One hand picks up a black pan filled with bread cubes, moving it over towards the burner on the right. The cutting board is now clearer with fewer bread cubes, with the grill pan and its contents visible in the center. [0:08:09 - 0:08:10]: The bread cubes are added to the pan, which sits atop the burner. Cooking utensils and containers with ingredients are placed around the stove. In the foreground, the bread cubes start to brown in the pan. [0:08:10 - 0:08:11]: The pan with bread cubes is on the burner, with steam rising, indicating the bread is being heated. The background showcases a variety of cooking supplies and a large blue pot on the counter. [0:08:11 - 0:08:12]: Bread cubes continue to brown in the pan on the burner. The adjacent grill pan, with grilled meat and lemon halves, remains in the center. Surrounding cooking tools and ingredients retain their positions. [0:08:12 - 0:08:13]: The pan with bread cubes is still on the burner, the cubes starting to toast. Steam rises steadily, with the background kitchen setup unchanged, incorporating various utensils, a large blue pot, and a dark stove. [0:08:13 - 0:08:14]: The pan with bread cubes is shifted slightly. The person holding the pan stirs the cubes, ensuring uniform toasting. Surrounding kitchen objects, including the plates and utensils, remain as is. [0:08:14 - 0:08:15]: Hands adjust the pan on the burner, as bread cubes are toasted. To the right, grilled meat and lemon halves on the grill pan are seen. A person steps back from the stove, revealing the blue pot in the background. [0:08:15 - 0:08:16]: A middle-aged person in a green shirt leans forward towards the grill pan, checking the food. Shelves with various utensils and a white brick backsplash are in the background. [0:08:16 - 0:08:17]: The person continues checking the food on the central grill pan, adding ingredients. The background showcases a neatly organized kitchen with shelves holding crockery and cooking supplies. [0:08:17 - 0:08:18]: The person in the green shirt reaches towards the cutting board, picking up a small round container. A well-organized kitchen, with labeled containers and hanging utensils, forms the backdrop. [0:08:18 -",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person slicing with the knife?",
        "time_stamp": "0:08:01",
        "answer": "D",
        "options": [
          "A. Lemon.",
          "B. Meat.",
          "C. Apple.",
          "D. Bread."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What food is on the baking tray now?",
        "time_stamp": "0:08:26",
        "answer": "A",
        "options": [
          "A. Two slices of lemon and a roasted chicken.",
          "B. Three slices of lime and a grilled fish.",
          "C. Four slices of orange and a baked ham.",
          "D. Five slices of cucumber and a roasted turkey."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Causal Reasoning",
        "question": "What indicates that the grill pan is hot?",
        "time_stamp": "0:08:12",
        "answer": "B",
        "options": [
          "A. The grill pan has lemons on it.",
          "B. The grill pan is emitting steam.",
          "C. The grill pan is being repositioned.",
          "D. The grill pan is making noise."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_40_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: A hand holding a rectangular pink box with a beaded keychain and a pen inside, the keychain has colorful beads and a small flower, while the pen is golden with white and pink accents. The background is plain grey.   [0:00:03 - 0:00:07]: The box is opened, and the beaded keychain with various beads including a wooden one and a flower charm along with the pen are placed side by side on a grey surface. Below the keychain, part of a ribbon is visible.  [0:00:08 - 0:00:11]: Text displaying \"Beaded Keychain + Pen Box\" on a black background. [0:00:12 - 0:00:16]: The view shifts to a close-up of a cutting machine, labeled with \"Beaded Keychain + Pen Box.\" A sheet of light pink paper is being handled and positioned on a cutting mat labeled \"LightGrip.\" [0:00:17 - 0:00:19]: The hand places the cutting mat with the pink paper into the cutting machine. The scoring wheel in the machine is adjusted, and text indicates \"Scoring Wheel\" while the hand secures it in place.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is being done with the cutting mat right now?",
        "time_stamp": "00:00:21",
        "answer": "D",
        "options": [
          "A. Removing the cutting mat from the cutting machine.",
          "B. Cutting the pink paper on the mat manually.",
          "C. Decorating the cutting mat with stickers.",
          "D. Placing the cutting mat into the cutting machine."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_53_real.mp4"
  },
  {
    "time": "0:00:40 - 0:01:00",
    "captions": "[0:00:40 - 0:01:00] [0:00:40 - 0:00:41]: A close-up view of a pink sheet with cutout outlines placed on a cutting mat marked with measurements along the edges. A white container with compartments is positioned on the left side, partially visible. [0:00:42]: The pink sheet remains in the same position on the cutting mat. There is a packaging of clear acetate sheets visible in the foreground on a white surface. The packaging displays the branding \"Cricut\" and indicates the product type as \"Clear Acetate Sheets\".  [0:00:43 - 0:00:44]: The packaging of the clear acetate sheets is held in the left hand, showing a photograph of a finished craft project inside the transparent acetate box. The instruction \"Peel the protective sheet off 1 side of the Acetate\" is displayed at the bottom of the frame. [0:00:45 - 0:00:49]: The left hand proceeds to peel off the protective sheet from the surface of the clear acetate, revealing the transparent material beneath. The hand carefully lifts the edge of the acetate sheet, while the background displays the same craft project image shown on the packaging. [0:00:50]: The acetate sheet is almost completely peeled off, and the photograph of the finished craft inside the transparent box is prominently displayed, with a delicate pink flower and a decorative item inside the box. [0:00:51 - 0:00:53]: The packaging of the clear acetate sheets is now held up, prominently showing the branding and the product type. In the background, a storage unit with compartments is visible, indicating a crafting workspace. [0:00:54 - 0:00:57]: The view shifts to a Cricut Maker 3 cutting machine, which is in the process of cutting the insert and window pieces. The machine's components are visible, including the cutting blade and the mat, which now has parts of the acetate sheet secured on it. [0:00:58]: The Cricut Maker 3 cutting machine continues its operation, further cutting the insert and window pieces with precise movements.  [0:00:59]: The scene transitions to a view of the cut insert and window pieces laid out on a gray surface. Beside the pieces are crafting tools, including a pen with decorative beads and several small multicolored beads arranged in a pattern.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action is being performed right now?",
        "time_stamp": "00:01:00",
        "answer": "D",
        "options": [
          "A. Cutting the insert and window pieces with scissors.",
          "B. Peeling off the protective sheet from the acetate.",
          "C. Operating the Cricut Maker 3 cutting machine.",
          "D. Arranging the cut pieces and beads on a gray surface."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_53_real.mp4"
  },
  {
    "time": "0:01:20 - 0:01:40",
    "captions": "[0:01:20 - 0:01:40] [0:01:20 - 0:01:22]: A pair of hands, with trimmed nails, is holding a keychain in the left hand and a clear plastic sheet in the right hand. Near the two objects on the top left of the video is a stylized pen, which has decorative beads on the top end. Underneath these hands is a pink sheet of paper with a rectangular cutout in the middle. The person's sleeves are purple. [0:01:23 - 0:01:24]: The hands start to angle the keychain towards the plastic sheet. The pen remains stationary on the left side of the frame, while the pink sheet of paper lies flat on the surface. [0:01:25 - 0:01:26]: The left hand is holding the keychain closer to the plastic sheet, with fingers clasped around the keychain's clasp. The pen is still positioned to the left, but now the right hand is holding it. The pink paper with the rectangular cutout is consistently placed below the hands. [0:01:27]: Holding the keychain in the left hand, the pair of hands angle the pen towards the plastic sheet while the right hand adjusts its grip. [0:01:28 - 0:01:30]: The hands manipulate the objects delicately, positioning the pen and the keychain onto the clear plastic sheet and adjusting their orientations. Throughout this, the background setup remains the same, featuring the pink cutout sheet. [0:01:31 - 0:01:39]: Gradually, the left hand places both the keychain and the pen against the plastic sheet. The decorative beads on the pen and the keychain are visible, and the faint reflections on the plastic add depth to the scene. The consistency in the pink paper’s position and the pen’s appearance in the frame is maintained.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are the hands holding right now?",
        "time_stamp": "00:01:26",
        "answer": "D",
        "options": [
          "A. The pink sheet of paper.",
          "B. The clear plastic sheet.",
          "C. The decorative beads.",
          "D. The pen and keychain."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_53_real.mp4"
  },
  {
    "time": "0:02:00 - 0:02:20",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:02]: A pair of hands is seen handling a pink paper craft, which appears to be cut and folded into a rectangular frame with an open window. To the left of the hands, there is a piece of plastic packaging containing a decorative pen with beads and a small flower;   [0:02:02]: As the hands continue to handle the paper craft, small tabs are folded inward on the lower part of the frame;   [0:02:02 - 0:02:04]: The tabs are pressed down along the edges, making clean folds. The decorative pen remains in its position to the left of the frame;   [0:02:04 - 0:02:05]: The paper frame is lifted and extended, revealing its flexibility and the dimensions of the open window. The edge work on the paper frame appears clean and precise;   [0:02:05 - 0:02:07]: The hands are adjusting the sides of the paper frame, still ensuring that the folds and tabs are correctly positioned. The left hand holds the paper as the adjustments are made with the right hand;   [0:02:07 - 0:02:09]: A bottle of \"450 Quick Dry Adhesive\" is held in the left hand, presented to the camera. The other paper pieces and the decorative pen in plastic packaging remain in the same position;   [0:02:09]: The adhesive bottle is now used; it is angled downward to apply glue along the folds and edges of the paper frame. The adhesive is applied with precise control;   [0:02:09 - 0:02:12]: The hands methodically continue to apply glue around the frame. Text appears on the screen instructing to \"Glue down the acetate window,\" indicating the next step in the crafting process;   [0:02:13]: A clear acetate sheet is picked up with both hands and is positioned above the window in the paper frame;   [0:02:13 - 0:02:16]: The acetate window is carefully aligned and secured into place, with adhesive ensuring it remains where desired. Each step is deliberate to avoid misalignment or air bubbles;   [0:02:16]: With gentle pressure, the acetate window is firmly pressed to adhere fully to the glued areas;   [0:02:17]: The both hands continue to press onto specific spots, ensuring the acetate and frame are securely bonded together;   [0:02:18 - 0:02:19]: The hands complete the attachment process with final touches, smoothing out any imperfections along the sides and edges. The acetate window is now fully integrated into the pink paper frame.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the hands do just now?",
        "time_stamp": "00:02:23",
        "answer": "D",
        "options": [
          "A. Cut a new piece of paper.",
          "B. Fold another frame without glue.",
          "C. Decorate the paper frame with beads.",
          "D. Smooth out the acetate window on the paper frame."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_53_real.mp4"
  },
  {
    "time": "0:02:40 - 0:03:00",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:45]: Hands are seen working on a pink paper object, applying glue along the edge with precision. The object being worked on appears to be a folded piece of pink paper with several flaps. On the left side, there is a pen with a decorative beaded chain lying on a transparent packaging.  [0:02:46 - 0:02:49]: The position of the hands remains consistent, pressing down on the glued flap of the pink paper object to secure it. The hands gently run along the fold to ensure that the glue is properly adhered. The beaded chain pen remains stationary on the left. [0:02:50 - 0:02:54]: The partially constructed pink paper box is being carefully lifted and turned around in the hands. The hands are making sure all edges are aligned and opening flaps are visible. The decorative beaded pen in its packaging continues to be in the same spot to the left of the workspace. [0:02:55 - 0:02:59]: The hands continue to manipulate and inspect the pink paper box, pressing down on its sides to reinforce the glued edges. The top flaps of the box are being gently worked on and are being adjusted for proper fitment. The hands then hold up the open box to a clearer view, revealing its hollow interior as the beaded pen remains at rest in its initial position.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the hands do just now?",
        "time_stamp": "0:03:04",
        "answer": "D",
        "options": [
          "A. Folded the pink paper object and pressed the glue.",
          "B. Applied glue along the edges of the pink paper object.",
          "C. Decorated the pink paper object with the beaded pen.",
          "D. Put the pen and keychain into the pink box."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_53_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:05]: The video showcases a cobblestone street curving slightly to the right. On the left side of the street, there are parked cars and a small, two-story beige building with a balcony. The right side has a green lawn on a raised area with small trees and street lamps. The street is lined with stone and some modern buildings further ahead;  [0:00:06 - 0:00:12]: As the camera moves forward, more of the street and the distant skyline come into view. There is a row of green trash bins on the left side of the street. The background reveals a cityscape with numerous red-roofed buildings and distant structures under a bright blue sky with scattered clouds;  [0:00:13 - 0:00:19]: The camera continues to move along the street, which now descends downhill. The green bins remain visible on the left, and an olive-green garage or storage door appears on the left side along with a white wall. The road ahead descends toward more white buildings with orange-tiled roofs alongside parked cars and more visible cityscape in the background.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the green trash bins located?",
        "time_stamp": "0:00:03",
        "answer": "C",
        "options": [
          "A. On the right side of the street.",
          "B. In the background near the cityscape.",
          "C. On the left side of the street.",
          "D. Next to the olive-green garage door."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_328_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:43]: A narrow, cobblestone street stretches forward, bordered by yellow and beige buildings on the right and a grey concrete structure on the left. A parked red scooter and rundown painted wall are visible on the left. In the background, an extensive metal bridge dominates the skyline, with a yellow crane in view against a clear blue sky. [0:02:44 - 0:02:57]: Further down the cobblestone street, two vans appear on opposite sides, facing away. The white van has 'Faixa Vertical' written on it. Overhead, the towering metal bridge remains prominent, with intricate intersecting metal beams. On the right, stone walls and the edges of the yellow building descend towards smaller structures. [0:02:58 - 0:02:59]: The view closes in on the metal bridge's base, highlighting its robust metallic framework. Below, a white car and a white van are parked along the curb, while the small buildings ahead become clearer. The bright blue sky remains unclouded overhead.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What vehicle is parked on the left side of the narrow street right now",
        "time_stamp": "00:02:42",
        "answer": "D",
        "options": [
          "A. A blue bicycle.",
          "B. A white van.",
          "C. A yellow motorcycle.",
          "D. A red scooter."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_328_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:24]: The video opens with a broad view of a riverside promenade on a sunny day, with the river on the right and a tree-lined pedestrian area on the left. In the background, numerous multi-colored buildings line the riverside, stretching up a hill. Two people walk along the sidewalk near the river. [0:05:25 - 0:05:28]: As the camera moves forward, more details of the waterfront and the array of tightly packed buildings in the background become visible. Several boats are anchored in the river, with a sailboat noticeable in the foreground. On the right, a large metal bridge comes into view, spanning the river. [0:05:29 - 0:05:33]: The perspective shifts closer to the bridge, revealing a tree and lampposts lining the walkway on the left. A stone staircase leads down towards the river's edge. The waterfront promenade is made up of stone pavement with railings along the edge, offering a clear view of the water and the cityscape across the river. [0:05:34 - 0:05:37]: The camera moves past an antique green and gold vehicle parked by the curb, while a group of people can be seen sitting on benches and walking on the promenade near the bridge's base. The bridge's towering metal structure arches prominently over the scene, with sunlight casting shadows on the stone walkways. [0:05:38 - 0:05:39]: The video concludes with a closer look at the green and gold antique vehicle, emphasizing its vintage design and the detail on its exterior. In the background, people are seen continuing along the walkway, and cars are parked along the street on the right.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What are the colors of the antique vehicle parked by the curb right now?",
        "time_stamp": "0:05:39",
        "answer": "D",
        "options": [
          "A. Red and white.",
          "B. Blue ,Green and silver.",
          "C. Black and yellow.",
          "D. Green ,Brown and gold."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_328_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:02]: A metal bridge with intricate trusses stretches horizontally, showcasing a pedestrian walkway to the right and a road to the left. In the immediate foreground, two people walk side by side on the pedestrian pathway. One is wearing a black t-shirt and white shorts with a black backpack; the other wears blue jeans and a green backpack. Further ahead, several more pedestrians are visible. Mountainous terrain and buildings, painted in various light hues, are visible in the background. [0:08:03 - 0:08:07]: The two individuals continue walking along the bridge. The person wearing a black t-shirt and white shorts turns his head slightly to the left, suggesting some interaction or observation. A gray car approaches from behind, passing close to them on the road to the left. In the background, a variety of structures, including multi-story buildings and small houses, contrast against the lush greenery of the hillside. [0:08:08 - 0:08:12]: The car moves ahead on the road, leaving more open space behind the walking individuals. The pedestrians on the bridge remain steady in their path. The bridge itself displays patches of graffiti on its metal structures, adding an urban touch to the scene. The location blends architectural elements from different periods, with the bridge’s heavy iron trusses contrasting against the backdrop of historic buildings. [0:08:13 - 0:08:17]: The two individuals remain central to the frame, continuing their walk along the pedestrian pathway. Additional graffiti becomes visible on the bridge’s structure. The architecture on the hillside becomes clearer, revealing more residential and possibly commercial buildings nestled amongst the green vegetation. The sky overhead is bright and clear, creating a picturesque ambiance. [0:08:18 - 0:08:19]: A white van enters the frame from the left, driving along the road portion of the bridge, approaching the area where the two central figures are walking. The design and features of the van are modern, with license plates and side mirrors clearly visible. The pedestrians maintain their course, while the van's presence introduces a sense of motion and activity to the scene.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is visible in the foreground on the pedestrian pathway?",
        "time_stamp": "0:08:02",
        "answer": "D",
        "options": [
          "A. A cyclist and a runner.",
          "B. A person with a red backpack.",
          "C. A group of people standing still.",
          "D. Two people walking side by side."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_328_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:58]",
    "captions": "[0:09:40 - 0:09:58] [0:09:40 - 0:09:55]: The video captures a first-person perspective walking down a stone-paved street along a riverbank. The viewer walks on the left side of the path, which is lined with tall street lamps and a row of parked bicycles. To the right, there are several buildings of varying colors and architectural styles, featuring ornate balconies and large windows. Many of the buildings have outdoor seating areas with white umbrellas and tables, suggesting they are cafes or restaurants. Some pedestrians walk along the sidewalk, while boats are moored along the river to the left of the path. The sky is clear with some scattered clouds, depicting a sunny day; [0:09:56 - 0:09:58]: The movement continues along the path, gradually approaching a set of steps leading down by the river. A couple walks ahead, and the activity around the cafes remains consistent, with patrons seated under the umbrellas. The river, populated with wooden boats, is visible on the left side, with a cityscape featuring various buildings and a distant bridge across the river.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action does the viewer take in the video?",
        "time_stamp": "0:09:58",
        "answer": "A",
        "options": [
          "A. Walking down a stone-paved street.",
          "B. Sitting at a cafe.",
          "C. Riding a bicycle.",
          "D. Boarding a boat."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_328_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:31]: A person wearing a blue shirt stands at a kitchen counter made of wood, mixing ingredients in a transparent bowl using a wooden spoon. The kitchen wall is tiled with white bricks, and there are two shelves above the counter. The top shelf contains various decorative items, including a large red \"COOK\" sign, glass jars, and other kitchenware. On the lower shelf, there are more kitchen utensils and containers. A stainless steel pot is placed on a stove to the left of the person, and a wooden cutting board lies on the counter directly in front of them. Nearby, there are several small containers, one of which is blue. The person concentrates on mixing the ingredients while tilting the bowl slightly.  [0:02:32 - 0:02:37]: The camera angle shifts to show the entire counter from a distance, capturing a wider view of the kitchen. The person continues mixing in the glass bowl. The stove with the stainless steel pot is seen on the left side of the counter. A white bowl and a small container with some creamy substance are also visible on the counter. A voice, likely the producer’s, informs the person, \"you have 8 minutes 30 seconds.\" [0:02:38 - 0:02:39]: The camera transitions to an overhead shot, depicting the person continuing to mix the ingredients in the transparent bowl using a wooden spoon. The glass bowl is placed on the wooden cutting board, surrounded by other ingredient containers, including a small dish with butter and a bowl with a powdery substance. The camera captures the person's hands and the motion of mixing, emphasizing the ingredients inside the bowl. The stove with a griddle pan is clearly seen on the countertop beside the cutting board.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the shirt worn by the person mixing ingredients at the kitchen counter?",
        "time_stamp": "0:02:31",
        "answer": "B",
        "options": [
          "A. Red.",
          "B. Blue.",
          "C. Green.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Text-Rich Understanding",
        "question": "How much time does the person have left to complete their task according to the producer?",
        "time_stamp": "0:02:50",
        "answer": "C",
        "options": [
          "A. 5 minutes 30 seconds.",
          "B. 7 minutes 30 seconds.",
          "C. 8 minutes 30 seconds.",
          "D. 9 minutes 30 seconds."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_34_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:42]: A person in a blue shirt is in a kitchen with white brick walls and wooden shelves lined with dishes and utensils. The person is vigorously mixing a brown mixture in a glass bowl with a whisk. A measuring jug and a saucepan are placed on a countertop. [0:04:43 - 0:04:46]: The perspective shifts to a top-down view of the bowl with the brown mixture being whisked. Butter and brown sugar in small bowls are placed on the wooden cutting board, as well as a wooden spoon and a small frying pan. [0:04:47 - 0:04:50]: The scene changes back to the original angle, showing the person still whisking the mixture. The kitchen's background includes a large red sign that spells \"COOK\", a microwave, and various kitchen utensils on shelves. [0:04:48 - 0:04:51]: The person moves quickly towards the microwave, holding a small bowl, and begins to interact with the microwave. [0:04:52 - 0:04:54]: After adjusting the microwave, the person returns to the kitchen counter and starts to mix the contents of the small bowl with a spoon. [0:04:55 - 0:04:59]: The view switches back to the top-down perspective as the person continues spooning ingredients from the small bowl into the larger glass bowl, blending the mixture evenly. Butter and seasoning remain in their respective bowls on the cutting board.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person vigorously mixing in a glass bowl?",
        "time_stamp": "0:04:42",
        "answer": "B",
        "options": [
          "A. A white creamy mixture.",
          "B. A brown mixture.",
          "C. A yellow batter.",
          "D. A green sauce."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_34_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:02]: A person in a blue shirt stands in a bright kitchen with white brick walls and blue cabinets. They reach down with both hands, seemingly to pick up or adjust objects on the wooden countertop. [0:07:02 - 0:07:03]: The person begins to stir or mix a thick batter-like substance in a large clear mixing bowl using a wooden spoon. The bowl is held in their left hand, while their right hand stirs the mixture. [0:07:03 - 0:07:04]: The person continues to mix the substance, now holding the bowl at a higher level and closer to their torso. The wooden spoon is deeply immersed in the thick mixture. [0:07:04 - 0:07:08]: The person tilts the clear mixing bowl to the side, pouring the thick, brown batter into a small white bowl on the wooden countertop. They scrape the sides of the mixing bowl with the spoon to ensure all the contents are transferred. [0:07:09 - 0:07:12]: An overhead view shows the person using a red spatula to transfer more of the thick mixture into the white bowl. Various ingredients, including butter, chocolate powder, and what appears to be sugar, are arranged on the counter next to a stove with a frying pan. [0:07:13 - 0:07:14]: The overhead view shows the emptied mixing bowl placed to the side, and the thick mixture now settling in the white bowl in the center of the wooden countertop. The surrounding kitchen equipment remains in place. [0:07:15 - 0:07:17]: The person uses a white cloth to clean the edge of the white bowl containing the mixture. Their movements are careful and deliberate. Some of the ingredients and kitchen utensils are still visible on the wooden countertop. [0:07:18 - 0:07:19]: The person places the cleaned white bowl back onto the wooden countertop. The camera focuses closely on the bowl, showing the thick mixture inside and the smooth surface on top. The person's hands are adjusting the bowl's final position.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What does the person use to mix the batter-like substance?",
        "time_stamp": "0:07:03",
        "answer": "B",
        "options": [
          "A. A metal whisk.",
          "B. A wooden spoon.",
          "C. A red spatula.",
          "D. A plastic fork."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_34_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:20 - 0:09:40] [0:09:20 - 0:09:22]: At the beginning of the video, a middle-aged man is seen in the kitchen wearing a blue t-shirt. He is walking toward a black microwave positioned on the left side of the kitchen counter. The kitchen has a white brick wall with shelves that hold glasses, plates, and various colorful bottles. Kitchen utensils and a wooden cutting board are placed on the counter along with a pot on the stove. [0:09:22 - 0:09:25]: The man continues his task by opening the microwave door with his left hand. He then places a small white bowl inside the microwave. On the counter, there are small kitchen tools and a stainless steel pot on the stove, which appears to be turned on. [0:09:25 - 0:09:27]: The man closes the microwave door and presses the buttons to set the time or temperature for cooking or heating. He focuses intently on the task, ensuring everything is set correctly. [0:09:27 - 0:09:30]: The scene transitions to the man stepping away from the microwave toward the stove. He is seen holding a white cloth in his right hand, wiping some utensils or the countertop as he moves. [0:09:30 - 0:09:32]: The man starts stirring the contents of the small stainless steel pot on the stove. He uses a wooden spoon to mix the ingredients in the pot. Various cooking utensils are placed on the countertop adjacent to him. [0:09:32]: The camera provides a top-down view of the stove and pot, where the man continues to stir the contents. The mixture inside the pot appears to be brownish as he carefully mixes it. [0:09:33 - 0:09:36]: The man uses his right hand to pour a liquid from a small dark container into the pot, continuing to stir the mixture with the wooden spoon. [0:09:36 - 0:09:38]: The camera angle changes back to a side view of the man, who is still carefully pouring the liquid into the pot while simultaneously stirring it. He makes sure no ingredients spill outside the pot while maintaining a steady hand to control the flow of the liquid.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What does the man place inside the microwave?",
        "time_stamp": "0:09:25",
        "answer": "A",
        "options": [
          "A. A small white bowl.",
          "B. A silver pot.",
          "C. A glass jar.",
          "D. A blue plate."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_34_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a completely black screen. [0:00:01 - 0:00:04]: The scene transitions to a large screen displaying the text \"One more thing...\" in white letters against a black background. A person is walking on stage from the left side beneath the text. [0:00:04 - 0:00:05]: The screen fades to black, and an outline resembling a stylized pair of glasses appears in white against the black background. [0:00:05 - 0:00:07]: The outline solidifies into a sleek, black, visor-like device. The background remains black, making the device stand out prominently. [0:00:07 - 0:00:09]: The perspective zooms out slightly, revealing more detail of the visor-like device. The background remains mostly dark, with subtle hints of a setting or objects starting to come into focus in the background. [0:00:09 - 0:00:10]: The scene changes to a bright, white background filled with multiple Apple products placed in a row. This includes an iMac, several MacBooks, a Mac mini, and a Mac Pro. All the devices have a similar orange and yellow screen background. [0:00:10 - 0:00:11]: The white background and the array of Apple products are maintained, showcasing the variety of devices. [0:00:11 - 0:00:13]: The scene switches to a person sitting at a desk with a laptop open in front of them. The background features shelves with various tech gadgets. The person appears to be speaking directly to the camera, gesturing with one hand while the other rests on the desk near the laptop. [0:00:13 - 0:00:16]: The person continues speaking, with occasional gestures. The background and setting remain the same, with the focus on the person and the laptop. [0:00:16 - 0:00:17]: The laptop screen transitions to display a dynamic graphic pattern in shades of blue and white. The design features curves and reflections, giving a sleek and modern feel. [0:00:17 - 0:00:18]: The focus remains on the laptop screen with the dynamic graphic pattern, now fully illuminated and taking up most of the frame. [0:00:18 - 0:00:19]: The background turns white, and text appears on the left side of the screen indicating \"MacBook Air 13 (2022)\" along with a showing of the laptop on the right with the same blue and white graphic pattern on the screen. [0:00:19 - 0:00:20]: The scene compares two MacBook models side by side. On the left is the \"MacBook Air 11 (2010-2015)\" with an older design and background, while on the right is the \"MacBook Air 13 (2022)\" with the sleek, modern design and the blue graphic pattern on the screen.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What text is prominently displayed on the screen right now before the scene transitions to an outline resembling glasses?",
        "time_stamp": "00:00:05",
        "answer": "D",
        "options": [
          "A. Introducing the future.",
          "B. Just one more thing...",
          "C. One last item...",
          "D. One more thing..."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_107_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:03:20]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:05]: The video begins with a dark background featuring text “DaVinci Resolve 导出测试” at the top center. Below the text, two bar charts are displayed. The left chart represents MacBook Air 13, while the right chart shows MacBook Air 15. Each bar chart is divided into H.264 and H.265 sections. The MacBook Air 13 scores are 2:52 for H.264 and 2:53 for H.265. The MacBook Air 15 scores are 2:51 for H.264 and 2:53 for H.265. [0:03:06 - 0:03:12]: The scene transitions to a brightly lit space with a white tabletop. A person sits at the table, wearing a light gray shirt and glasses, speaking towards the camera. A silver laptop is open on the table. The background is a blue wall with shelves holding various items including cameras and decorations. [0:03:13 - 0:03:15]: The onscreen person continues speaking, occasionally using hand gestures to emphasize points. The position of the person, laptop, and background remains consistent. [0:03:16]: The video cuts to another data screen, this time showing results for \"Adobe(23.4) 视频制作.\" Two bar charts represent scores for \"Premiere Pro\" and \"After Effect.\" MacBook Air 13 scores are 322 for Premiere Pro and 835 for After Effect. MacBook Air 15 scores are 327 for Premiere Pro and 1042 for After Effect.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What scores are shown for the MacBook Air 13 just now for \"Premiere Pro\" in the \"Adobe(23.4) 视频制作\" results?",
        "time_stamp": "00:03:16",
        "answer": "A",
        "options": [
          "A. 322.",
          "B. 835.",
          "C. 327.",
          "D. 1042."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_107_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:06:20]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00]: Two laptops are positioned side by side on a white surface. The laptop on the left is a light color with a logo in the center of its back panel, while the laptop on the right is a darker color with a similar logo in the center of its back panel. Both laptops are positioned facing away from the viewer. The background is dark, making the laptops stand out clearly. [0:06:01]: A single laptop, with the screen facing the viewer, is shown. The screen displays a desktop background that is a blend of orange and yellow hues. There are application icons lined up at the bottom of the screen, suggesting it is a macOS interface. The keyboard is visible, and the laptop is centered on the same white surface as before. There are some Chinese characters overlaid on the screen. [0:06:02]: A dark-colored laptop is being placed inside a yellow and black backpack that has the word \"CYBERPUNK\" printed on the sides. Two hands are holding the laptop as it is being positioned into the main compartment of the backpack, which is lying flat on the white surface. [0:06:03]: The laptop has now been fully placed inside the backpack. A red tag labeled \"EP PORTED/ED\" is visible within the backpack's compartment. The hands that were holding the laptop have moved away, and the top of the yellow backpack is clearly visible. [0:06:04]: The dark-colored laptop is partially visible again as it is being removed from the backpack. The red tag remains visible inside the bag, emphasizing the same yellow and black design. Only the lower portion of the laptop is shown, and the rest of the yellow backpack is prominently displayed. [0:06:05]: The laptop, now being held with a different orientation, is fully removed from the backpack. It is held with two hands and oriented such that the logo on its cover is visible and facing the viewer. The yellow backpack remains in the background on the same white surface. [0:06:06 - 0:06:07]: The laptop continues to be held up by two hands, maintaining the same position and orientation as the previous frame, with the logo in the center clearly visible.  [0:06:08 - 0:06:09]: Two laptops are displayed side by side, with their screens facing the viewer. Both screens show a web browser displaying the same webpage about a detailed image of a circuit board. The lighter-colored laptop is on the left, and the darker-colored one is on the right, providing a direct comparison of their screen sizes and designs. [0:06:10 - 0:06:12]: The scene remains the same, with both laptops side by side displaying the same webpage. Minimal changes occur as both screens still show the webpage about a circuit board, showcasing the consistency in their display features. [0:06:13]: The screen content on both laptops changes to display a software interface with multiple panels, sliders, and graphs, indicating the operation of some sort of audio or video editing software. The dark background emphasizes the detailed interface on both screens. [0:06:14 - 0:06:19]: A hand is seen moving from the left side of the frame toward the keyboard of the lighter-colored laptop. The hand appears to be navigating or interacting with the controls displayed on the screen. Both laptops still exhibit the audio or video editing software interface, maintaining their positions side by side.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What word is printed on the sides of the yellow and black backpack right now?",
        "time_stamp": "00:06:02",
        "answer": "C",
        "options": [
          "A. TECHNO.",
          "B. FUTURE.",
          "C. CYBERPUNK.",
          "D. HACKER."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_107_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:20]",
    "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:06]: A front view of a MacBook Air laptop is shown with colorful patterns on the screen. The text \"MacBook Air 15\" is prominently displayed in the center of the screen. The laptop is against a dark background with the timestamp \"0:09:00\" and a handle @Chilichill in the lower left corner. [0:09:06 - 0:09:10]: The video transitions to showcasing another MacBook, this time labeled \"MacBook Pro 14.\" The laptop is shown from a front angle with the screen partially displaying a geometric design. The background remains dark, with the timestamp \"0:09:06.\" [0:09:10 - 0:09:15]: The scene changes again, this time to display the \"MacBook Pro 16.\" The laptop is presented from the front with the screen showing a curved design with a mix of dark and light colors. The handle @Chilichill is still visible in the lower left corner. [0:09:15 - 0:09:20]: Finally, the video transitions to a different laptop model labeled \"MacBook Air 13.\" The laptop is shown from the front with a wavy blue and black pattern on the screen. The setting remains dark, and the handle is still present in the lower left corner.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which MacBook model is shown with a geometric design on the screen right now?",
        "time_stamp": "00:09:07",
        "answer": "B",
        "options": [
          "A. MacBook Air 15.",
          "B. MacBook Pro 14.",
          "C. MacBook Pro 16.",
          "D. MacBook Air 13."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_107_real.mp4"
  },
  {
    "time": "[0:12:00 - 0:12:19]",
    "captions": "[0:12:00 - 0:12:19] [0:12:00 - 0:12:14]: A man is seated at a white table with a silver laptop to his left. He is wearing a light grey t-shirt and glasses. He has short black hair, and he is speaking to the camera in a brightly lit studio with a blue background. Behind him, there are shelves containing various items such as cameras, lights, and decor. He uses his hands expressively while speaking, at times moving them close to his body and at other times gesturing outwardly; [0:12:14 - 0:12:19]: The scene transitions to an animated graphic on a black background that features a blue and white stylized logo resembling a gear and text next to it. The logo and the text slightly change their luminosity and positioning over the course of these frames.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What color is the background right now while the man is speaking?",
        "time_stamp": "0:12:14",
        "answer": "C",
        "options": [
          "A. Red.",
          "B. Green.",
          "C. Blue.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_107_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What brand logo is displayed on the steering wheel?",
        "time_stamp": "00:00:23",
        "answer": "D",
        "options": [
          "A. Scania.",
          "B. Mercedes.",
          "C. MAN.",
          "D. Volvo."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_456_real.mp4"
  },
  {
    "time": "[0:01:52 - 0:01:57]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is directly in front of the vehicle?",
        "time_stamp": "00:01:55",
        "answer": "C",
        "options": [
          "A. A park.",
          "B. A construction site.",
          "C. A crosswalk.",
          "D. A gas station."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_456_real.mp4"
  },
  {
    "time": "[0:03:44 - 0:03:49]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What color is the car driving next to the bus?",
        "time_stamp": "00:03:45",
        "answer": "C",
        "options": [
          "A. Blue.",
          "B. Green.",
          "C. Red.",
          "D. White."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_456_real.mp4"
  },
  {
    "time": "[0:05:36 - 0:05:41]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the building with the slanted roof visible right now at the driver's left side?",
        "time_stamp": "00:05:35",
        "answer": "A",
        "options": [
          "A. Red.",
          "B. White.",
          "C. Blue.",
          "D. Black."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_456_real.mp4"
  },
  {
    "time": "[0:07:28 - 0:07:33]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the digit displayed on the rear panel of the yellow bus right now?",
        "time_stamp": "00:07:31",
        "answer": "D",
        "options": [
          "A. 60.",
          "B. 40.",
          "C. 30.",
          "D. 50."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_456_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:08]: The video starts with a close-up of a colorful construct made of interlocking plastic blocks shaped like a rectangular flatbed vehicle on red wheels. The flatbed consists of green and yellow blocks, while the cabin section is comprised of alternating blue, green, orange, and yellow blocks. Gradually, a white toy car with black windows and blue details moves into the frame from the right side and is positioned onto the flatbed of the block vehicle. [0:00:09]: The scene cuts to a blank white surface, and the previous setup with the block vehicle and the toy car is no longer visible. [0:00:10 - 0:00:12]: A hand appears in the frame from the left side holding a yellow interlocking block consisting of two rows of four studs. The hand places the block on the surface and starts to release it. [0:00:13 - 0:00:15]: The hand reappears holding another yellow interlocking block, identical to the first one. The hand places the block on the white surface next to the first block. [0:00:16 - 0:00:19]: The hand continues to add more interlocking blocks of the same type and yellow color in a straight line formation on the white surface, positioning them one after the other. The lineup of blocks grows as an additional smaller block with one row of four studs is placed at the end. The hand carefully positions it to ensure that it fits well with the other blocks.",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many studs does each yellow interlocking block placed by the hand have?",
        "time_stamp": "00:00:15",
        "answer": "A",
        "options": [
          "A. Eight.",
          "B. Sixteen.",
          "C. Four.",
          "D. Twelve."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Counting",
        "question": "How many interlocking blocks are positioned in a straight line by the hand?",
        "time_stamp": "00:00:19",
        "answer": "D",
        "options": [
          "A. Three.",
          "B. Five.",
          "C. Seven.",
          "D. Four."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_203_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:01]: A hand is reaching for a green rectangular block attached to a row of yellow and orange LEGO-like bricks placed on a flat surface. [0:01:02 - 0:01:03]: The hand starts to move the green rectangular block upwards while the yellow and orange bricks remain stationary. [0:01:04 - 0:01:08]: The hand grasps the green block, lifts it slightly, and maneuvers it; the positioning of the blocks on the surface is consistent. [0:01:09 - 0:01:11]: The hand moves back slightly and grasps the green block again, ready to make another adjustment. [0:01:12 - 0:01:14]: The hand grabs the green block and begins to pull it towards the viewer. [0:01:15 - 0:01:16]: The hand continues to pull the green block upwards, slightly displacing it from the original row of bricks. [0:01:17 - 0:01:18]: The hand lifts the green block completely out of alignment with the yellow and orange blocks. [0:01:19]: The hand places the green block back into a different position on the surface, rearranging the formation.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the block that the hand is reaching for right now?",
        "time_stamp": "00:01:16",
        "answer": "C",
        "options": [
          "A. Yellow.",
          "B. Orange.",
          "C. Green.",
          "D. Blue."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_203_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:04]: An arm extends from the left side of the frame, holding a blue toy brick. The brick is placed down on a flat, light-colored surface. There are more blue bricks placed neatly in a rectangular shape on the surface.   [0:02:05 - 0:02:06]: Another blue brick is picked up and placed on top of the previously placed blue bricks, forming a two-layered structure. [0:02:07 - 0:02:08]: The hand continues to adjust the blue bricks, ensuring they are firmly attached to each other. [0:02:09 - 0:02:10]: A green toy brick is picked up and placed on the upper left side of the blue bricks, adding a new color to the structure. [0:02:11 - 0:02:14]: A yellow toy brick is picked up and added to the right side of the green brick, increasing the height of the structure and adding more color variation. [0:02:15 - 0:02:17]: The yellow brick is pressed down to ensure it is secured onto the blue and green bricks. [0:02:18 - 0:02:19]: An orange toy brick is picked up and added to the top of the structure, on the upper right side, which completes the multi-colored layered structure.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What color is the brick first added after the initial blue bricks form a two-layered structure?",
        "time_stamp": "0:02:14",
        "answer": "A",
        "options": [
          "A. Green.",
          "B. Yellow.",
          "C. Orange.",
          "D. Red."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      },
      {
        "task_type": "Event Understanding",
        "question": "What is the sequence of bricks added to the structure after the blue bricks?",
        "time_stamp": "0:02:21",
        "answer": "A",
        "options": [
          "A. Green, yellow, orange.",
          "B. Yellow, green, orange.",
          "C. Orange, yellow, green.",
          "D. Green, orange, yellow."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_203_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions just taken?",
        "time_stamp": "00:00:10",
        "answer": "C",
        "options": [
          "A. The individual prepared a sandwich, applied condiments, and wrapped it for serving.",
          "B. The individual selected toppings, spread sauce, and baked a pizza.",
          "C. The individual prepared a pizza peel, dusted it with flour, and placed a dough bag on the counter.",
          "D. The individual sliced vegetables, assembled a salad, and served it in a bowl."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_415_real.mp4"
  },
  {
    "time": "[0:02:58 - 0:03:08]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken by the individual just now?",
        "time_stamp": "0:03:08",
        "answer": "D",
        "options": [
          "A. The individual crafted a sandwich, adding lettuce and tomatoes, then cutting it in half before serving.",
          "B. The individual prepared a salad by mixing several ingredients and garnishing it with dressing.",
          "C. The individual baked a pizza, adding cheese and pepperoni, before placing it into the oven.",
          "D. The individual composed a pizza, topping it with spinach, mushrooms, and some type of reddish ingredient."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_415_real.mp4"
  },
  {
    "time": "[0:05:56 - 0:06:06]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the individual's actions just now?",
        "time_stamp": "00:06:06",
        "answer": "A",
        "options": [
          "A. The individual prepares a pizza dough by rolling it out and forming a circular shape.",
          "B. The individual decorates a cake by applying frosting and adding toppings.",
          "C. The individual prepares a sandwich by layering ingredients and cutting it in half.",
          "D. The individual makes a sushi roll by spreading rice and adding fish."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_415_real.mp4"
  },
  {
    "time": "[0:08:54 - 0:09:04]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taking place just now?",
        "time_stamp": "00:08:59",
        "answer": "B",
        "options": [
          "A. The individual was spreading sauce on a pizza base and putting toppings on it.",
          "B. The individual was applying flour to a pizza board and preparing the workstation for making pizza.",
          "C. The individual was washing kitchen utensils and cleaning the floor.",
          "D. The individual was grating cheese and chopping vegetables for a salad."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_415_real.mp4"
  },
  {
    "time": "[0:11:52 - 0:12:02]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the recent actions shown?",
        "time_stamp": "00:12:02",
        "answer": "B",
        "options": [
          "A. The worker picked up dough, kneaded it, and placed it into a proofing oven.",
          "B. The worker stretched a pizza dough, then added marinara sauce to the center, spreading it out.",
          "C. The worker prepared a sandwich, added condiments, and wrapped it in paper.",
          "D. The worker chopped vegetables, placed them in a bowl, and added dressing."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_415_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:07]: A person wearing a navy blue t-shirt with a pocket on the left side is seated at a white table against a dark blue background. They hold a smartphone in their right hand, which they examine closely, sometimes gesturing with their left hand or pointing at the phone. The background features shelves with gadgets and decorations, including some small framed items on the left and black devices on the right.  [0:00:08 - 0:00:09]: The person shifts to holding the phone with their left hand, bringing it closer to the camera to display the back side of the device. They continue to conversate, often gesturing with their right hand.  [0:00:10 - 0:00:13]: The screen transitions to a dark background displaying various icons of Snapdragon chip numbers arranged in a grid. The icons include Snapdragon 845, 855, 855+, 865, 865+, 870, 888, and 888+, along with the prominent Snapdragon 8 Gen 1 and 8+ Gen 1 icons. [0:00:14 - 0:00:20]: The focus narrows down to the Snapdragon 8+ Gen 1 icon, showing it prominently against the dark background. The icon is golden with the Snapdragon logo in red and white.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What item is the person holding close to the camera right now?",
        "time_stamp": "00:00:07",
        "answer": "D",
        "options": [
          "A. A framed picture.",
          "B. A black device.",
          "C. A navy blue t-shirt.",
          "D. A smartphone."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_113_real.mp4"
  },
  {
    "time": "[0:02:40 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40]: The first image shows a screen displaying technical specifications for a device with an 8+ Gen 1 processor. The text indicates a 15% reduction in SoC power consumption, 30% GPU power reduction, and 30% CPU improved power efficiency, with these values also presented in another language beneath the English text. The screen has a dark background with the information highlighted in various colors. [0:02:41 - 0:02:44]: In these frames, a person is sitting at a white table in front of a blue gradient wall. Behind the person, there are shelves with various objects, including small figurines and electronic devices. The person is holding a mobile device in their left hand while making gestures with their right hand, sometimes appearing to press or tap the device. The person is wearing glasses and a navy-blue T-shirt with a design on the front pocket. There are some small electronic components on the table to the right. [0:02:45 - 0:02:53]: The screen in these frames displays the text \"Geekbench 5 CPU测试\" with various bar graphs showing CPU performance metrics. The benchmarks for different processors, such as A15, A14, 三国8+ Gen 1, and 骁龙旗舰, are represented by horizontal bars in different colors. The scores for both single-core (单核) and multi-core (多核) tests are shown, with the bars varying in length based on the performance scores. [0:02:54 - 0:03:00]: Another set of frames show the same bar chart with slight variations in the highlighted areas, indicating specific data points within the results. The processors listed include A15 and A14, as well as others written in a different language. These comparisons highlight the relative performance strengths and weaknesses of each processor based on the Geekbench 5 CPU benchmark tests.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the CPU multi-core score shown in the video for the A15?",
        "time_stamp": "00:03:00",
        "answer": "D",
        "options": [
          "A. 4317.",
          "B. 1741.",
          "C. 4211.",
          "D. 4908."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_113_real.mp4"
  },
  {
    "time": "[0:05:20 - 0:05:40]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:22]: The video starts with a black background displaying the text \"3DMark Wild Extreme Stress 測試\" in white at the top center and left-aligned multilingual text in faint grey.  [0:05:23 - 0:05:27]: A bar chart appears against the same black background. The chart title \"3DMark Wild Extreme Stress 測試\" is still visible at the top and is accompanied by six horizontal colored bars in red, orange, and yellow tones representing different processors and their respective performance percentages and scores. Each bar has labels indicating percentages and figures: A15 (71.2%, 3024), A16 (66.7%, 2131), and four others. [0:05:28]: The screen continues to show the bar chart without changes. [0:05:29 - 0:05:33]: The camera then captures an individual sitting at a white table in a well-lit room with a blue background, holding a smartphone. They appear to be explaining something, looking slightly left down the lens. There are shelves behind the person holding different objects, including small figures and electronic devices. [0:05:34 - 0:05:36]: The person maintains a similar posture, holding the smartphone, looking at it, and then straight ahead while continuing to speak. The background and surroundings remain consistent. [0:05:37 - 0:05:39]: The camera now focuses on the individual's face who is slightly smiling, and gesturing with their right hand. The person appears to be demonstrating or reviewing something on the smartphone. The setting remains unchanged, with the same background and objects visible.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What title is displayed at the top center of the black background right now?",
        "time_stamp": "00:05:27",
        "answer": "D",
        "options": [
          "A. 3DMark Benchmark Results.",
          "B. Processor Performance Test.",
          "C. 3DMark Extreme Test.",
          "D. 3DMark Wild Extreme Stress 測試."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_113_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:08:20]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:08]: A person is seated at a white table in a room with a dark blue background. There are shelves on either side of the person, with various objects and electronics displayed. The person is wearing glasses and a dark blue shirt with a logo on the front pocket, which also contains a pen. They are holding a silver device with their left hand and gesturing with their right hand while looking into the camera.  [0:08:08 - 0:08:19]: The video transitions to a black screen with white text on it, followed by a bar chart with the title \"《原神》游戏实时帧率\" along with the temperature, WiFi, and duration details. The chart depicts frame rates in different colors, representing various devices over time. Average frame rate values (fps) and some contextual information are displayed. [0:08:19 - 0:08:20]: A detailed comparison is shown with a highlighted section, indicating the power consumption (in Watts) of several devices while maintaining a stable 60fps.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the title of the bar chart being displayed right now?",
        "time_stamp": "00:08:19",
        "answer": "D",
        "options": [
          "A. 游戏设备对比.",
          "B. 帧率和温度关系.",
          "C. 不同设备的功耗比较.",
          "D. 《原神》游戏实时帧率."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_113_real.mp4"
  },
  {
    "time": "[0:10:20 - 0:10:38]",
    "captions": "[0:10:20 - 0:10:38] [0:10:20 - 0:10:24]: A person is sitting at a white table holding a black smartphone. They are wearing glasses and a dark blue t-shirt with a small design on the pocket. The background is blue with some shelves holding various items, like speakers and decorative objects. On the screen, there are several icons and words in Chinese displayed. [0:10:25 - 0:10:26]: The person continues talking while holding the smartphone. The icons and words on the screen change. [0:10:27 - 0:10:31]: The smartphone's screen is now visible to the camera and shows a shopping application with various images of products, mostly electronics, including graphics cards. The person is still talking. [0:10:32]: The person abruptly stops talking and appears to be moving the smartphone slightly away from the camera. There is also some visual noise or glitching appearing on the video. [0:10:33 - 0:10:34]: An animation transitions in, showing a logo with a gear and lines and Chinese text on a black background with tiny white specks resembling stars. [0:10:35 - 0:10:38]: The screen turns completely black, with nothing visible.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What type of products are predominantly displayed on the shopping application right now?",
        "time_stamp": "00:10:31",
        "answer": "D",
        "options": [
          "A. Clothes.",
          "B. Books.",
          "C. Furniture.",
          "D. Electronics."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_113_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins with a vibrant night view of a city skyline from a waterfront perspective. The water in the foreground reflects the colorful lights of the skyscrapers. The tallest building is centrally located, illuminated with white and green lights. To the right side, the frame includes the edge of an orange-lit structure where a crowd of people can be seen enjoying the view. [0:00:03 - 0:00:04]: As the video progresses, the skyline remains consistent, with various lights from the buildings creating a dynamic mixture of colors. The tall central building has a shining top, and other buildings are lit with multiple colors such as blue, green, and pink. The reflection on the water prominently shows these colors, making the scene even more picturesque. [0:00:05 - 0:00:10]: The scene continues to focus on the skyline, capturing the glowing and pulsating movements of the lights on the buildings. The surrounding skyscrapers have an array of lights, with some transitioning in brightness. There are occasional light beams shooting upwards, adding more vibrance to the night scene. [0:00:11 - 0:00:15]: The spectators on the right remain visible, indicating that the viewpoint hasn't changed. Various light patterns can be seen clearly on the water surface, with city lights reflecting brightly. The tallest building still dominates the center, maintaining its bright white illumination, while other buildings continue to display different colors. [0:00:16 - 0:00:20]: In the final segment, the skyline view remains the focal point. The water continues to reflect the colorful lights of the buildings. Changes in the lighting patterns and slight movements in the reflections can be observed. The overall ambiance stays vibrant, showcasing the lively nightscape of the city. The crowd on the right remains engaged with the scenic view.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the crowd of people located in relation to the city skyline?",
        "time_stamp": "00:00:02",
        "answer": "C",
        "options": [
          "A. On the left side of the frame.",
          "B. In the center of the frame.",
          "C. On the right side of the frame.",
          "D. At the top of the frame."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_337_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:19]: The video reveals a breathtaking nighttime cityscape scene viewed from a distance across a body of water. The skyline is dominated by numerous illuminated high-rise buildings, showcasing an array of vibrant lights in various colors such as blue, pink, green, and yellow. The lighting on the buildings creates beautiful reflections on the surface of the water, adding to the visual spectacle. In the background, a few clouds can be seen in the dark sky. Central to the skyline is a distinctive building with a spire, brightly lit and standing out against the other structures. The video remains relatively steady, maintaining focus on the cityscape throughout. The camera seems to be positioned on the opposite side of the water, capturing the entirety of the scene without any visible obstructions.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the main focus of the video?",
        "time_stamp": "00:03:20",
        "answer": "C",
        "options": [
          "A. A parade in the city.",
          "B. A busy street market.",
          "C. A breathtaking nighttime cityscape.",
          "D. A sports event."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_337_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:02]: In a bustling terminal or covered walkway, people are seen walking in various directions. The ceiling has a structural framework with beams, and the lighting consists of bright fluorescent lights. Some people are standing around, while others are in motion. In the background, there's a lit advertisement or shopfront. [0:06:03 - 0:06:04]: More people continue to walk under the covered area. A man in a white striped shirt is prominently in the center, moving toward the camera, and another person in a red shirt with a purple backpack walks by.  [0:06:05 - 0:06:06]: The crowd thins slightly as people keep moving past various pillars supporting the structure. In the background, there are lights from vehicles or shops outside the covered area. [0:06:07 - 0:06:09]: Many people, including someone wearing a red shirt and a purple backpack, navigate the terminal. They walk past green and white vending machines and pillars. There are more brightly lit shops or stalls outside the covered area. [0:06:10]: A few more people are walking down the terminal, with the person in the red shirt and purple backpack still in view. The setting shows vending machines and additional structural features such as beams and pillars. [0:06:11 - 0:06:13]: The same individual with the red shirt and purple backpack continues down the pathway, joined by others. They pass more vending machines and another blue pillar. The background remains busy with lights and people. [0:06:14 - 0:06:16]: As the camera moves further down the terminal, the individual with the purple backpack and other people progress. Some remain near the vending machines, and others head towards the curved road revealed in the frame.  [0:06:17 - 0:06:18]: The terminal shows a crowded scene with people walking in both directions. A green kiosk is visible on the left, and a red sign next to a large pillar in the center appears to give directions or information. [0:06:19]: Near the end of the sequence, the video captures people continuing in various directions, maintaining the scene's busy and active atmosphere. The man with the red shirt and purple backpack is now further down the terminal.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the backpack worn by the person in the red shirt?",
        "time_stamp": "00:06:06",
        "answer": "C",
        "options": [
          "A. Green.",
          "B. Black.",
          "C. Purple.",
          "D. Blue."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_337_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:19]: The video depicts a vibrant nighttime street scene in a busy, well-lit urban area. People are walking along a wide pedestrian pathway, flanked by tall buildings. The video is shot from a first-person perspective, as if the viewer is walking among the crowd. The area is illuminated with various lights and colorful signs, including a large digital billboard overhead that changes its display intermittently. To the right of the pathway, bright advertisements for brands like \"Tom Ford\" are prominently visible on the building facades. On the left, there is a row of slender vertical poles supporting an overhead structure, with the illuminated interior of a building visible in the background. The ground is made of large stone tiles arranged in a regular pattern. The crowd consists of people of different ages and attire, casually walking or standing, and some are engaged in conversation. The atmosphere is lively and bustling, characteristic of an urban nightlife scene.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What kind of atmosphere is depicted in the video?",
        "time_stamp": "00:09:19",
        "answer": "B",
        "options": [
          "A. Calm and quiet.",
          "B. Lively and bustling.",
          "C. Tense and stressful.",
          "D. Serene and peaceful."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "What is located to the left of the pedestrian pathway?",
        "time_stamp": "00:09:19",
        "answer": "C",
        "options": [
          "A. A large digital billboard.",
          "B. Bright advertisements for brands.",
          "C. A row of slender vertical poles supporting an overhead structure.",
          "D. A water fountain."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_337_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:16",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_96_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:01:00",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 1."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_96_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:15",
        "answer": "A",
        "options": [
          "A. 3.",
          "B. 4.",
          "C. 6.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_96_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:28",
        "answer": "B",
        "options": [
          "A. 2.",
          "B. 5.",
          "C. 3.",
          "D. 7."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_96_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:40",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 4.",
          "C. 9.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_96_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the character cuddle the teddy bear and smile in bed?",
        "time_stamp": "00:00:30",
        "answer": "C",
        "options": [
          "A. Because the character is preparing for bedtime.",
          "B. Because the character is trying to send a message.",
          "C. Because the character feels comforted by the teddy bear.",
          "D. Because the character is participating in a game."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_246_real.mp4"
  },
  {
    "time": "[0:02:12 - 0:02:42]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does Mr. Bean become disturbed?",
        "time_stamp": "00:02:25",
        "answer": "B",
        "options": [
          "A. Because the worker is whistling loudly.",
          "B. Because the worker is using a jackhammer.",
          "C. Because the worker is throwing tools around.",
          "D. Because the worker is shouting instructions."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_246_real.mp4"
  },
  {
    "time": "[0:04:24 - 0:04:54]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the person adjust a lever and press a red button on the machine?",
        "time_stamp": "00:04:35",
        "answer": "A",
        "options": [
          "A. Because the person wants to power up the machine.",
          "B. Because the person needs to deflate the tent.",
          "C. Because the person wants to shut down the machine.",
          "D. Because the person is testing the lever's mechanism."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_246_real.mp4"
  },
  {
    "time": "[0:06:36 - 0:07:06]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why is there an abundance of egg cartons and eggs causing clutter in the house?",
        "time_stamp": "00:07:03",
        "answer": "A",
        "options": [
          "A. Because the man in the red hat operates heavy machinery.",
          "B. Because someone is baking a large number of cakes.",
          "C. Because a delivery of eggs made a mistake.",
          "D. Because the family is preparing for a party."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_246_real.mp4"
  },
  {
    "time": "[0:08:48 - 0:09:18]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why do the workers outside notice an issue with the pipeline?",
        "time_stamp": "0:09:10",
        "answer": "C",
        "options": [
          "A. Because there was a scheduled maintenance check.",
          "B. Because the pipeline was leaking due to wear and tear.",
          "C. Because overheating cooking supplies produced smoke.",
          "D. Because the workers were alerted by a loud noise."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_246_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:08]: The video begins with a close-up of a set of personalized agate coasters arranged on a gray grid-patterned surface. Each coaster has a smooth, rounded shape with a translucent center and a pinkish edge. The edges are gilded with a metallic gold color, giving each coaster a refined and elegant look. Names are handwritten in gold lettering on each coaster. The names \"Melissa,\" \"Emily,\" and \"Brad\" are shown first. Subsequently, \"Thomas\" appears on another coaster. The coasters are neatly arranged, with \"Melissa\" set on top of a stack of similar coasters, while \"Emily,\" \"Brad,\" and \"Thomas\" are positioned separately below, in a slightly clustered manner. [0:00:09 - 0:00:13]: The text \"Personalised Agate Coasters\" appears on a black screen, displayed in a simple, elegant font. [0:00:14 - 0:00:20]: The scene transitions to a digital workspace on a computer screen labeled \"Welcome, Vanessa\" at the top. The interface shows various project thumbnails, including \"Personalized Ornaments,\" \"Personalized Christmas Stockings,\" \"Tree Gift Wrap,\" and \"1,2,3 Storage Buckets.\" Each project has a clear thumbnail image and a title below it. The workspace is clean and well-organized, reflecting a user interface designed for managing personal crafting projects.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is being shown right now?",
        "time_stamp": "00:00:05",
        "answer": "B",
        "options": [
          "A. A collection of colorful notebooks.",
          "B. Personalized agate coasters with names on them.",
          "C. A stack of books with gilded edges.",
          "D. A digital workspace interface."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_61_real.mp4"
  },
  {
    "time": "0:01:40 - 0:02:00",
    "captions": "[0:01:40 - 0:02:00] [0:01:41 - 0:01:44]: The video starts with a screenshot displaying a digital canvas open in a design software application. On the canvas, there is a pink square positioned on the left side and three typed names: \"Emily\" at the bottom left, \"Thomas\" in the middle, and \"Brad\" on the right.  [0:01:44 - 0:01:46]: The cursor appears, right-clicking on the pink square, bringing up a context menu with several options, including 'Cut,' 'Copy,' and 'Paste.' [0:01:46 - 0:01:49]: The context menu is closed, returning focus to the canvas with the pink square and the names \"Emily,\" \"Thomas,\" and \"Brad.\" [0:01:49 - 0:01:51]: The cursor moves and clicks on the name \"Emily,\" highlighting it and bringing up blue bounding boxes around the text. [0:01:51 - 0:01:53]: A right-click on the name \"Emily\" brings up the same context menu seen before, with options like 'Cut,' 'Copy,' and 'Paste.' [0:01:53 - 0:01:55]: The screen returns to the initial state with no context menus visible. The design on the canvas shows the pink square and the three names. [0:01:55 - 0:01:57]: The right-click is applied to the pink square again, and the context menu reappears. [0:01:57 - 0:02:00]: The name \"Emily\" is copied and pasted just below the original one, with the cursor selecting and highlighting the new copied name \"Emily.\" [0:02:00 - 0:02:03]: The cursor double-clicks on the new \"Emily,\" allowing text editing, and changes \"Emily\" to \"Emil\" mistakenly. [0:02:03 - 0:02:04]: The cursor deletes the extra \"y,\" leaving just \"Emil\" highlighted on the canvas. [0:02:04 - 0:02:07]: Text editing continues as the cursor types in \"yliss,\" completing the name as \"Melissa.\" [0:02:07 - 0:02:09]: The cursor moves away, and now the canvas shows the names \"Emily,\" \"Melissa,\" \"Thomas,\" and \"Brad.\" [0:02:09 - 0:02:12]: With \"Melissa\" selected, the cursor drags it slightly to the right, aligning it more neatly with the other names. [0:02:12 - 0:02:15]: The cursor moves \"Melissa\" by dragging it upward, positioning it above \"Thomas,\" shifting its position on the canvas.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the cursor doing right now?",
        "time_stamp": "0:01:54",
        "answer": "C",
        "options": [
          "A. Pasting the name \"Brad\".",
          "B. Highlighting the name \"Emily\".",
          "C. Dragging \"Melissa\" upward.",
          "D. Right-clicking on the pink square."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_61_real.mp4"
  },
  {
    "time": "0:03:20 - 0:03:40",
    "captions": "[0:03:20 - 0:03:40] [0:03:24 - 0:03:25]: The video begins with an open software interface, displaying a grid background with text written in a cursive font. In the center, there is a red rectangle overlay, positioned to the left side of the screen. Below it, the text \"Thomas\" is written in black script. [0:03:25 - 0:03:26]: The view shifts slightly leftward, and another word, \"Melissa,\" in the same black cursive font, is now visible above the red rectangle. The words \"Emily\" and \"Brad\" are also in black cursive font, with \"Emily\" situated to the right of \"Thomas\" and \"Brad\" below them both. [0:03:26 - 0:03:27]: The software interface is still displayed, with a closer focus on the red rectangle and the word \"Melissa,\" which overlays the rectangle.  [0:03:27 - 0:03:28]: Little movement continues to be seen in the view, with the red rectangle remaining under the word \"Melissa.\" The text is clear and legible, outlining the interface's use. [0:03:28 - 0:03:29]: A small icon or cursor appears near the text \"Melissa,\" suggesting interaction or editing within the software. The other names remain static in the same positions. [0:03:29 - 0:03:30]: The view zooms out slightly, including all four names in the frame again. The interaction seems focused around the text \"Melissa.\" [0:03:30 - 0:03:31]: The cursor or icon now moves, indicating a possible adjustment or placement happening within the red rectangle area. [0:03:31 - 0:03:32]: The word \"Thomas\" and the red rectangle remain in their positions. The layout of the text appears to be arranged systematically on the grid background. [0:03:32 - 0:03:33]: The interface shows the word \"Thomas\" being outlined by interaction points, suggesting manipulation within the software. The text \"Emily\" and \"Brad\" remain untouched. [0:03:33 - 0:03:34]: The red rectangle remains static, overlapping with \"Melissa,\" while \"Thomas\" is highlighted, indicating ongoing adjustments or edits. [0:03:34 - 0:03:35]: The text \"Emily\" becomes the central focus. The cursor moves to interact with \"Emily,\" which is positioned adjacent to the other names. [0:03:35 - 0:03:36]: Slight adjustments are made to the word \"Emily.\" The red rectangle still covers part of \"Melissa\" calmly. [0:03:36 - 0:03:37]: The words \"Melissa,\" \"Thomas,\" and \"Brad\" are visible, with the cursor making movements to refine the placement of \"Emily.\" [0:03:37 - 0:03:38]: No major changes occur. \"Brad\" is at the lower left corner of the screen, still untouched. [0:03:38 - 0:03:39]: The cursor approaches the text \"Brad\" now. The red rectangle overlaps a portion of \"Melissa,\" maintaining a consistent interface style. [0:03:39 - 0:03:40]: Detailed adjustments or placements appear near the text \"Brad.\" Other elements, including \"Thomas\" and \"Emily,\" remain unchanged. [0:03:40 - 0:03:41]: The view includes all four names consistently spaced out on the grid background. The red rectangle still overlays \"Melissa.\" [0:03:41 - 0:03:42]: The focus remains steady on the text \"Brad\" along with the consistent interface appearing to be part of graphic design software. [0:03:42 - 0:03:43]: The remaining view retains the same layout with \"Brad\" being adjusted slightly. The interface and other elements like the red rectangle and the names \"Melissa,\" \"Thomas,\" and \"Emily\" stay static.  [0:03:43 - 0:03:44]: The interaction seems to finalize adjustments, with the view capturing the arranged names \"Melissa,\" \"Thomas,\" \"Emily,\" and \"Brad\" along with the overlaid red rectangle on the grid background. The scene ends steadily showing the composed graphical layout.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the cursor doing just now?",
        "time_stamp": "00:03:30",
        "answer": "B",
        "options": [
          "A. Highlighting the word \"Thomas\".",
          "B. Adjusting the placement of \"Brad\".",
          "C. Interacting with the word \"Melissa\".",
          "D. Making changes to the word \"Emily\"."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_61_real.mp4"
  },
  {
    "time": "0:05:00 - 0:05:20",
    "captions": "[0:05:00 - 0:05:20] [0:05:01 - 0:05:02]: A hand is holding a sheet of gold foil, cutting it with a pair of black-handled scissors. The scissors are oriented horizontally, and the gold foil is slightly crumpled and held in place by the thumb and fingers. The background is a gray grid pattern mat. [0:05:02]: The hand places two pieces of the cut gold foil on the gray grid pattern mat. The pieces are positioned side by side, overlapping slightly. [0:05:03 - 0:05:04]: A larger sheet of paper labeled \"Cricut\" is introduced, held by the same left hand, and partially covering the gold foil pieces. The background remains the gray grid pattern mat. [0:05:04 - 0:05:06]: The hand is holding the \"Cricut\" sheet in place while beginning to cut it with the same pair of black-handled scissors. The cutting motion goes through the previous frame. The hands are positioned to the left, near the corner of the sheet. [0:05:06 - 0:05:07]: The hand folds the \"Cricut\" sheet with the gold foil inside. The scissors are used to make another cut, the hands holding the sheet steady. The grid background is consistent. [0:05:07 - 0:05:09]: A separated piece of the \"Cricut\" sheet is seen in the hand. The gold foil piece from earlier is visible on the grid mat. [0:05:09 - 0:05:11]: The first cut gold foil piece is placed directly in the center of the frame on the grid mat background. The \"Cricut\" sheet is brought back into view, seemingly being prepared to be adhered. [0:05:11 - 0:05:12]: The hand positions the \"Cricut\" sheet over one of the gold foil pieces, partially covering it. Another pair of hands prepares to trim it. [0:05:12 - 0:05:13]: The next action involves cutting a piece from the \"Cricut\" sheet to match the size of the gold foil. Two final pieces of gold foil rest on the grey grid mat's background. [0:05:13 - 0:05:13]: The two gold foil pieces, now with \"Hello\" engraved on them, lie horizontally on the mat. One hand begins to apply another piece of the \"Cricut\" sheet over one of the gold foil pieces. [0:05:13 - 0:05:14]: The hands carefully adjust the \"Cricut\" sheet over the gold foil to ensure proper alignment before pressing it down. [0:05:14]: The left hand aligns another piece of the \"Cricut\" sheet with the gold foil, ensuring the sheet's writing is visible. The right hand holds it steady from the bottom. [0:05:14 - 0:05:15]: With the \"Cricut\" sheet correctly positioned, the hands press down on it to make sure it adheres firmly to the gold foil, ensuring a complete and secure attachment. [0:05:15 - 0:05:16]: The focus shifts to the hands holding and working on another piece of gold foil, pressing down on it to ensure it is secure and flat, with some surface details visible. [0:05:16 - 0:05:17]: The hands prepare to peel back the \"Cricut\" sheet, showing the adherence of the gold foil beneath. The previous adhesive work appears to be successful. [0:05:17]: The hand starts to peel the \"Cricut\" sheet back, revealing the intended design on the gold foil. The foil retains some reflections and details against the grid pattern's backdrop.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the hand doing right now?",
        "time_stamp": "0:05:17",
        "answer": "D",
        "options": [
          "A. Cutting another piece of gold foil.",
          "B. Pressing down on the gold foil.",
          "C. Folding the \"Cricut\" sheet.",
          "D. Peeling back the \"Cricut\" sheet."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_61_real.mp4"
  },
  {
    "time": "0:06:40 - 0:07:00",
    "captions": "[0:06:40 - 0:07:00] [0:06:41 - 0:06:42]: A pair of hands is holding a small piece of transparent material with the name \"Brad\" written in elegant golden script. Two other pieces, one with the name \"Emily\" and another partially showing another name, are placed above on a grid-patterned work surface. [0:06:42 - 0:06:43]: The hands place the \"Brad\" name piece onto the grid-patterned surface, using both hands to smooth it down. The \"Emily\" and another name piece remain in their original positions above. [0:06:44]: The camera focuses on three name pieces placed separately on the grid surface, labeled \"Brad,\" \"Emily,\" and \"Meliss,\" with the hands momentarily out of view. [0:06:45 - 0:06:46]: One hand picks up a new name piece, starting to peel a protective layer off the back while the other hand steadies the white name piece. The \"Brad,\" \"Emily,\" and \"Meliss\" pieces remain on the grid mat. [0:06:47]: The peeling continues as the hands begin to remove more of the protective layer from the name piece. The previously placed \"Brad,\" \"Emily,\" and \"Meliss\" name pieces remain on the work surface. [0:06:48 - 0:06:49]: The hands continue to work on removing the protective layer, while the camera angle allows \"Melissa\" to be fully seen. \"Brad\" and \"Emily\" name pieces remain clearly positioned on the grid-patterned surface. [0:06:50]: After successfully removing the layer, the hands position a new \"Emily\" name piece with clear backing, preparing to stick it onto the surface. The previously placed name pieces remain visible on the grid mat. [0:06:51 - 0:06:52]: The hands align and place the new \"Emily\" name piece onto the surface. The other name pieces are visible in the same positions as before. [0:06:53]: The motion involves picking up a piece labeled \"Thomas,\" while \"Emily\" and \"Melissa\" names stay in their spots on the surface. [0:06:54 - 0:06:55]: One hand processes another name piece, peeling off the protective layer, while the other holds \"Melissa\" in place. \"Thomas\" and \"Emily\" pieces remain in place on the grid-patterned surface. [0:06:56 - 0:06:57]: The peeling of the protective layer from the back of \"Melissa\" continues as the hands work precisely. The names \"Emily\" and \"Thomas\" stay in their respective positions on the grid mat. [0:06:58 - 0:06:59]: The hands lift and position a layer from a \"Melissa\" name piece, beginning to set it on the grid-patterned surface. \"Thomas\" and \"Emily\" pieces remain untouched in the background. [0:07:00]: The \"Melissa\" name piece is placed accurately onto the surface, with \"Emily\" and \"Thomas\" in the background. [0:07:01 - 0:07:02]: One hand picks up what appears to be two stacked pieces, holding them up from the surface. The \"Thomas\" name piece is visible in its original spot. [0:07:03]: Both hands work on a new layer set with all pieces stacked together, while the \"Thomas\" name ensures constant placement visibility. [0:07:04 - 0:07:05]: Working on removing the backing from another name piece, hands carefully handle the next steps. \"Brad,\" \"Emily,\" and \"Thomas\" pieces are evident. [0:07:06 - 0:07:07]: One hand arranges the stacked pieces, smoothing any creases formed in the transparent layers. [0:07:08 - 0:07:09]: The name pieces remain visible on a grid with another piece in hand, working on the detailing. [0:07:10]: Aligning the focused name piece, hands ensure its clarity, maintaining precision on each name applied.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the hands do just now with the \"Brad\" name piece?",
        "time_stamp": "0:06:43",
        "answer": "B",
        "options": [
          "A. Held it up to the light.",
          "B. Placed and smoothed it down onto the surface.",
          "C. Removed a protective layer.",
          "D. Dropped it onto the floor."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_61_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located to the right side of the road right now?",
        "time_stamp": "00:00:20",
        "answer": "D",
        "options": [
          "A. A gas station.",
          "B. A bank.",
          "C. A park.",
          "D. A cafe."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_169_real.mp4"
  },
  {
    "time": "[0:02:13 - 0:02:33]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What are the main colors of the flowers in the hanging baskets right now?",
        "time_stamp": "00:02:14",
        "answer": "D",
        "options": [
          "A. Green and black.",
          "B. Red and pink.",
          "C. Red and blue.",
          "D. White, yellow and purple."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_169_real.mp4"
  },
  {
    "time": "[0:04:26 - 0:04:46]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the kayaks located right now?",
        "time_stamp": "00:04:40",
        "answer": "B",
        "options": [
          "A. On the right side of the road.",
          "B. On the left side of the road.",
          "C. On the grass beside the path.",
          "D. Docked at the shore."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_169_real.mp4"
  },
  {
    "time": "[0:06:39 - 0:06:59]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located on the left side of the cyclist right now?",
        "time_stamp": "00:06:49",
        "answer": "B",
        "options": [
          "A. A field.",
          "B. A canal.",
          "C. A dense forest.",
          "D. A building."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_169_real.mp4"
  },
  {
    "time": "[0:08:52 - 0:09:12]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located on the left side of the path right now?",
        "time_stamp": "00:09:11",
        "answer": "B",
        "options": [
          "A. A river.",
          "B. A line of trees.",
          "C. A building.",
          "D. A field."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_169_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins with a close-up of someone's hands picking up a decorative hair comb, which has jewels or beads, from a wooden surface. There is attention to the detail in the comb and the hands that hold it. [0:00:03 - 0:00:07]: The scene shifts to a wider view of a room with two women. One woman, dressed in a dark, long-sleeved dress, stands beside another woman, who is seated and wearing a white dress. The room has a prominently patterned pink wallpaper, and there is a wooden chest of drawers in the background, topped with a mirror, a lamp, and small decorative items. The woman in the white dress holds a hand mirror and touches her face or hair. [0:00:08 - 0:00:10]: A close-up shot focuses on the seated woman looking into her hand mirror while the standing woman tends to her hair, perhaps adjusting it or styling it. [0:00:11 - 0:00:12]: The standing woman continues to adjust the seated woman's hair, ensuring it is well arranged. [0:00:13 - 0:00:14]: A close-up shot shows hands carefully placing the decorative hair comb into the seated woman's hair, securing it properly. [0:00:15 - 0:00:16]: The camera shifts back to the wider view of the room with both women. The seated woman continues to look into her hand mirror while the standing woman completes the hair arrangement. [0:00:17 - 0:00:18]: The woman in the dark dress begins to step back, possibly to examine her work or to give the other woman space. [0:00:19 - 0:00:20]: The standing woman approaches again to further adjust or ensure the seated woman's hair and appearance are perfect, while the seated woman looks up, engaging in a brief conversation or moment of interaction.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What item does the video start with someone picking up?",
        "time_stamp": "0:00:02",
        "answer": "A",
        "options": [
          "A. A decorative hair comb.",
          "B. A wooden comb.",
          "C. A hand mirror.",
          "D. A jeweled necklace."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_152_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:40 - 0:02:00] [0:01:40 - 0:01:44]: The video begins with a close-up view of a person's back. They are wearing a light brown corset with black stripes and white lace trim at the top. The person has their arms slightly raised, and you can see part of their white dress beneath the corset. In the background, there is a wooden dresser with some objects on it, including what looks like a small decorative object and other miscellaneous items. [0:01:45 - 0:01:49]: The person starts to tighten the corset laces. Their arms are now moving, and their hands are pulling on the black laces. The video continues to focus on the back view, showing the laces being adjusted. The background remains the same with the dresser and decorative items visible. [0:01:50]: The person's arms are now closer to their body, pulling the laces tighter. [0:01:51 - 0:01:53]: A transition occurs, and the person has finished tightening the corset. The back of the corset now appears neatly laced, and the person’s arms are lowered to their sides. The corset now appears perfectly fitted. [0:01:54 - 0:01:59]: The camera angle shifts to the front view, revealing the entire outfit. The corset is fastened with metal clasps down the center. The person is adjusting their outfit slightly, and in the background, a bed with white sheets is visible against a pink patterned wall. The camera remains steady as the person makes final adjustments to their clothing, ensuring everything is in place.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What are the colors of the corset and its trim worn by the person?",
        "time_stamp": "0:01:44",
        "answer": "B",
        "options": [
          "A. Light brown with white stripes and black lace trim.",
          "B. Light brown with black stripes and white lace trim.",
          "C. White with black stripes and light brown lace trim.",
          "D. Black with white stripes and brown lace trim."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Action Recognition",
        "question": "What action did the person just perform?",
        "time_stamp": "0:01:49",
        "answer": "C",
        "options": [
          "A. Fastening the metal clasps.",
          "B. Adjusting the white dress.",
          "C. Tightening the corset laces.",
          "D. Lowering their arms."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_152_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:22]: A young woman with light skin and curly dark blonde hair is shown in profile, facing left. She is wearing a high-neck black garment with white lace trim. The background features pink and white patterned wallpaper. [0:03:23 - 0:03:25]: The focus shifts to include a second person, whose hands are visible, holding a white garment. The second person is dressed in a dark-colored outfit. This scene is set in front of a wooden dresser with decorative items and a lamp on top. [0:03:26 - 0:03:29]: The young woman in the white dress is beginning to put on the white garment handed to her by the other person. The woman is seen adjusting the garment while the other person assists. [0:03:30 - 0:03:32]: The focus shifts to another young woman with light skin and curly light brown hair. She is looking down while buttoning the front of her intricate, white, laced dress. She stands in front of the same pink and white patterned wallpaper. [0:03:33 - 0:03:39]: The second young woman continues to button and adjust the dress, focusing on intricate details such as buttons and lace. The dresser and its decorative items remain in the background.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the left side person holding in his hands?",
        "time_stamp": "00:03:25",
        "answer": "A",
        "options": [
          "A. A white garment.",
          "B. A black hat.",
          "C. A book.",
          "D. A pair of shoes."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_152_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:10]: A woman with long, curly blonde hair styled in an intricate updo is seen from a side angle in front of a vanity mirror. She is wearing a green and black striped jacket with lace details. The background features a pink and white patterned wallpaper. The mirror reflects the back of her head, and there are white gloves hanging on the mirror frame. The woman places her hands on her neck, adjusts a piece of jewelry or clothing near her collar, and appears to be preparing herself; [0:05:11 - 0:05:19]: The woman continues her preparations, now focusing on adjusting her earrings. Her eyes are fixed on the mirror as she carefully touches her ear. The reflection in the mirror shows her concentration. After finishing with her earrings, she looks down at something in her hand, possibly reassessing her appearance or preparing for the next step in her routine.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is hanging on the mirror right now?",
        "time_stamp": "00:05:10",
        "answer": "A",
        "options": [
          "A. White gloves.",
          "B. A hat.",
          "C. A scarf.",
          "D. A necklace."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_152_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video starts with a black screen. [0:00:01]: A black screen is visible with four icons in Chinese on the left side: a like button, a comment button, a favorite button, and a share button. [0:00:02 - 0:00:04]: A black screen displays white text in Chinese, stating that the video was shot entirely on an iPhone. [0:00:05 - 0:00:06]: A close-up of someone adjusting their checkered blazer. The background appears slightly blurred. [0:00:07 - 0:00:09]: The person is seen from a larger perspective walking down a staircase. The background shows a brightly colored window, casting light on the surroundings. [0:00:10]: A European-style building with intricate architectural details is shown. Tree branches are partially visible on the right side. [0:00:11]: A clear view of the Eiffel Tower against a blue sky dotted with light clouds. Buildings are seen in the foreground. [0:00:12]: Someone holding a smartphone horizontally, capturing the Eiffel Tower with buildings visible on the sides of the screen. [0:00:13 - 0:00:14]: A transition scene where the image appears blurred, showing the Eiffel Tower partially out of focus. [0:00:15]: A close-up of a smartphone screen showing a street view with a person walking towards the camera. The phone is placed on dry ground with some foliage around. [0:00:16 - 0:00:17]: A wide shot of the smartphone on the ground, surrounded by dry grass and plants. [0:00:18 - 0:00:19]: A person wearing black sneakers steps into the frame. The person's leg is partially bent as if about to pick up or interact with the phone.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What information is displayed as white text in Chinese on the black screen right now?",
        "time_stamp": "00:00:03",
        "answer": "B",
        "options": [
          "A. The video was edited using advanced software.",
          "B. The video was shot entirely on an iPhone.",
          "C. The video highlights scenic city landscapes.",
          "D. The video was produced by a famous filmmaker."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_110_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:04:20]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:05]: A person is seated on a foldable stool outdoors near a calm river. They are wearing a white t-shirt, blue shorts, and dark sneakers. A small, round black table stands to their right, topped with a red thermos flask. Lush green vegetation surrounds the area. Trees and shrubs line the opposite riverbank, while modern buildings with flat roofs are visible in the background. The sky above is mostly clear and blue, hinting at pleasant weather. [0:04:06 - 0:04:15]: The camera angle shifts to a close-up view of a smartphone being held in both hands. The phone screen shows an intense action game with vibrant colors and fast-paced movements, prominently featuring characters, health bars, and various game controls. Bright effects and rapid attacks dominate the screen. [0:04:16 - 0:04:19]: The video transitions to data presented on a graph. The graph compares the performance of different devices, with numerous bars in shades of blue and green. Each bar is labeled with device names and performance metrics. The labels are in Chinese, and numerical values indicate performance scores, comparing devices like iPads, iPhones, and other smartphones.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What language are the graph labels showing performance metrics written in right now?",
        "time_stamp": "00:04:19",
        "answer": "D",
        "options": [
          "A. English.",
          "B. Japanese.",
          "C. Korean.",
          "D. Chinese."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_110_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:08:20]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:02]: The video shows the close-up view of the back of a smartphone, which is golden in color and placed on a black, honeycomb-patterned surface. The phone has three camera lenses and a flash arranged in a square module at the top left corner. [0:08:03 - 0:08:11]: The perspective changes to a first-person view where someone is using a smartphone to look at different images on a social media application. A young man wearing a white t-shirt and standing in a room with professional lighting equipment is visible in the background. The focus in the frame is on swiping through various images on the phone screen. [0:08:12 - 0:08:13]: Another scene shows a room with two musicians, one holding a double bass and another holding a saxophone, standing near a person adjusting a camera on a tripod. This room has large windows allowing natural light to pour inside. [0:08:14 - 0:08:15]: A close-up shot shows the back of a smartphone with a grey finish and the Apple logo. The phone’s camera module with three lens cutouts is visible, and part of a person can be seen holding the phone. [0:08:16 - 0:08:18]: The perspective shifts to a display showing a video recording interface on a smartphone. The screen is divided into four sections showing different camera angles labeled \"ultra wide,\" \"wide,\" \"tele,\" and \"selfie.\" As the interface is interacted with, the sections show different camera feeds, including close-ups and wideshots of two people. [0:08:19]: The interface screen now shows labels for different camera modes (\"ultra wide,\" \"wide,\" \"tele,\" and \"selfie\") in dimmed lighting with a \"CONFIRM\" button at the center.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What labels are shown on the display screen right now?",
        "time_stamp": "00:08:15",
        "answer": "D",
        "options": [
          "A. \"HDR,\" \"Portrait,\" \"Landscape,\" and \"Night\".",
          "B. \"Pro,\" \"Normal,\" \"Custom,\" and \"Basic\".",
          "C. \"Ultra wide,\" \"Wide,\" \"Depth,\" and \"Zoom\".",
          "D. \"Ultra wide,\" \"Wide,\" \"Tele,\" and \"Selfie\"."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_110_real.mp4"
  },
  {
    "time": "[0:12:00 - 0:12:20]",
    "captions": "[0:12:00 - 0:12:20] [0:12:00 - 0:12:03]: The video begins with a serene outdoor scene showing a bridge crossing over a calm body of water. The bridge is constructed with concrete and wooden rails, and green vegetation surrounds it, including trees and bushes. In the distance, a person in a blue shirt is seen riding a tricycle slowly across the bridge from left to right. The background shows more greenery and a structure that appears to be a house with a visible roof. [0:12:03 - 0:12:05]: The camera zooms in on the person riding the tricycle on the bridge. The tricycle has a small cart attached to it. The person appears focused on riding as they move towards the right side of the frame. Dense green shrubs are visible along the bridge's sides. [0:12:06 - 0:12:11]: The scene transitions to an indoor environment resembling an airport or a train station. The floor is reflective and polished. The scene is primarily focused on a hand holding a smartphone with a 'SPEEDTEST' app running on the screen. The hand and phone are centered while people move around in the background, some carrying luggage. The lighting is bright, and various signage can be seen, including an advertisement display screen. [0:12:11 - 0:12:15]: The SPEEDTEST app on the smartphone begins the testing process. There is a circular progress bar on the screen, indicating the test's progress. Behind the phone, various people are seen walking, some moving towards the camera. Signage and lights are bright and colorful, creating an active atmosphere. [0:12:15 - 0:12:20]: The focus shifts to a closer view of the smartphone screen. The SPEEDTEST results start appearing, showing metrics like download and upload speeds. The background remains busy with people moving around, and the environment continues to be well-lit with visible signs and advertisements. The phone is held steadily in the frame, and the results on the screen become the primary focus.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What app is being used on the smartphone right now?",
        "time_stamp": "00:12:09",
        "answer": "B",
        "options": [
          "A. FITNESS TRACKER.",
          "B. SPEEDTEST.",
          "C. WEATHER FORECAST.",
          "D. MAPS NAVIGATION."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_110_real.mp4"
  },
  {
    "time": "[0:15:20 - 0:15:30]",
    "captions": "[0:15:20 - 0:15:30] [0:15:20 - 0:15:21]: The video starts with a first-person perspective of a narrow concrete path through lush greenery. A lone person walks slightly in the distance, carrying what appears to be a large black item on their back and another bag. The weather is overcast, and there are power lines visible in the background.  [0:15:22]: As the person continues walking on the path, an overlay text appears on the screen, with a humorous design possibly meant to resemble pixelated retro game text, suggesting someone is gossiping about them from behind, providing a lighthearted tone to the scene. The surrounding green vegetation remains the same. [0:15:23]: The camera angle then changes to show a small rustic building or shelter. Another person is seen sitting in front of the building, busy with some agricultural task, possibly sorting dried vegetation. A brown dog is moving towards the camera, adding a sense of an active rural environment. [0:15:24 - 0:15:25]: The camera perspective switches back to the narrow path and the same person walking away with their belongings. The background features the same lush greenery and an overall serene rural landscape, with fields and distant trees. [0:15:26 - 0:15:28]: The video continues with the person still walking on the path, now accompanied by the brown dog. The greenery and fields remain consistent, with the person and dog now moving further down the path. The scene retains its rural charm, created by the green plants, overcast sky, and empty path stretching forward. [0:15:29 - 0:15:30]: As the person and dog continue walking, they start to venture further down the path, nearing the edge of the visible scene. The consistent vegetation and background of fields indicate a peaceful countryside setting. The dog stays close to the person, highlighting a sense of companionship on their journey.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What does the overlay text on the screen right now humorously suggest?",
        "time_stamp": "00:15:22",
        "answer": "B",
        "options": [
          "A. Someone is praising the person.",
          "B. Someone is gossiping about the person.",
          "C. Someone is giving directions to the person.",
          "D. Someone is warning the person."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_110_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:01]: The video begins with a view of a kitchen from a first-person perspective. Two pans are on the stove, positioned side by side. A person's hand reaches out, taking hold of a spatula. White kitchen cabinets and countertops are visible in the background, along with various kitchen utensils and appliances. [0:02:01 - 0:02:02]: The scene changes slightly as the hand continues to move over the kitchen counter, close to the pans on the stove. The person begins to prepare food, reaching toward the nearby items. The focus remains on the hands and the cooking area. [0:02:02 - 0:02:03]: The camera shifts slightly for a clearer view of the person working in the kitchen. The focus is on the upper body of the person, who is wearing a dark-colored shirt. The background reveals more details of the kitchen, such as utensils and white cabinets with glass doors. [0:02:03 - 0:02:04]: The person continues to concentrate on the task, looking down at the counter. A closer view of the person's face and hands is seen, emphasizing the attention to the cooking process.  [0:02:04 - 0:02:05]: The camera angle widens, showing the person standing in front of the stove with two pans cooking. The kitchen has a modern design, with stainless steel appliances, white cabinets, and a tile backsplash. Sunlight streams in through the window, illuminating the room. [0:02:05]: The person maintains focus on the food in the pans, ensuring it is cooked properly. The kitchen counter is organized, with a few plates and a cooking board visible on the left side. [0:02:06]: The person reaches for another kitchen utensil with the right hand while keeping an eye on the pans. The scene shows the detailed layout of the kitchen, with drawers and cabinets arranged neatly. [0:02:07]: After adding an ingredient, the person momentarily glances away from the stove. The background remains constant, showing the orderly arrangement of the kitchen and the bright, sunny view through the window. [0:02:08]: The person starts to walk toward the left, perhaps to gather more ingredients or utensils. The left side of the counter comes into view, revealing more kitchen tools and dishes. [0:02:09]: The person picks up a container from the counter and moves back toward the stove, still focused on the task at hand. The kitchen environment remains consistent, with clean and organized surfaces. [0:02:10]: The individual returns to the stove, holding the container. The focus shifts back to the cooking area, showing the person’s dedication to preparing the food. [0:02:11]: The person adds the content from the container to the pans, carefully stirring the ingredients. The kitchen setup is designed for efficient cooking, with all necessary items within reach. [0:02:12]: Continuing to cook, the person adjusts the heat of the pans while making sure the food in each pan is evenly cooked. The camera captures the person’s attentive movements and the organized kitchen space. [0:02:13]: The person uses the spatula to flip the items in the pan, ensuring even cooking. The scene highlights the precise movements and the neat kitchen environment. [0:02:14]: A close-up view of the pan reveals slices of vegetables sizzling in oil. The person's hands are busy adjusting food in the pan to make sure it cooks evenly. [0:02:15]: The person continues to work on the pan, adjusting the vegetables carefully. Steam rises from the pan, indicating that the food is cooking well. [0:02:16]: The person briefly lifts the pan to prevent the ingredients from sticking, shaking it gently to drizzle the oil evenly over the vegetables. [0:02:17]: The individual places the pan back down and uses a spatula to rearrange the food, ensuring everything is cooked properly. [0:02:18]: The camera shifts back to a wider angle, showing the person gesturing with one hand while explaining or thinking about the next step. The organized kitchen provides a tranquil and efficient cooking environment. [0:02:19]: The person continues to gesture, possibly explaining a part of the recipe. The focus is on the individual's expression and hand movements, with the kitchen serving as a clean and well-organized backdrop.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "0:02:07",
        "answer": "A",
        "options": [
          "A. Sprinkle seasonings into the pot.",
          "B. Pours oil into the pans.",
          "C. Slices vegetables on the counter.",
          "D. Washes hands at the sink."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Event Understanding",
        "question": "What does the person do after adding an ingredient to the pans?",
        "time_stamp": "0:02:07",
        "answer": "A",
        "options": [
          "A. Adjust the food in the pot to evenly distribute the seasonings on top.",
          "B. Adjusts the heat of the pans.",
          "C. Cleans the countertop.",
          "D. Moves to the dining table."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_26_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:01]: A person's hand is seen holding a black frying pan containing mixed vegetables and meat pieces over a stove. The pan is tilted, causing the ingredients to rise into the air. Another black frying pan is on the adjacent burner to the right. Both pans are over a stainless steel stovetop with horizontal grooves. [0:06:01 - 0:06:02]: A closer view shows the person's hand gripping the pan handle, which is still over the stove. The vegetables and meat pieces are now settled in the pan. [0:06:02 - 0:06:03]: The scene transitions to a kitchen view. A person, with short hair, wearing a dark T-shirt, is standing in front of an open refrigerator. Vegetables and sliced items are on the kitchen counter to the left.  [0:06:03]: The person is now reaching towards the refrigerator's interior with both hands placed on the knobs. [0:06:04]: The person opens the refrigerator door, revealing a variety of items including bottles, containers, and food items on the shelves inside. [0:06:05]: The person takes out a bottle and turns away from the refrigerator towards the stove. The kitchen counter in front is lined with various ingredients, including sliced vegetables. [0:06:06 - 0:06:07]: Returning to the stove, the person holds the bottle in one hand and attempts to pour its contents into the frying pan, causing the steam to rise.  [0:06:08 - 0:06:09]: The bottle is closer, and more liquid is being poured into the frying pan, which is positioned centrally on the stove. [0:06:10 - 0:06:11]: The liquid continues pouring, creating steam around the cooking food in the pan.  [0:06:12 - 0:06:13]: The person, seen mid-motion from the front, moves slightly away from the stove, wiping their hands with a cloth while the steam rises from the cooking food. [0:06:13 - 0:06:14]: Steam envelops the person, who holds the frying pan and shakes it slightly to mix the ingredients. [0:06:14 - 0:06:15]: The person is back at the stove, still holding and shaking the frying pan amidst the steam. [0:06:15 - 0:06:16]: With continued shaking, the person stands at the stove, producing more steam. The background shows kitchen cabinets and a window allowing natural light to stream in. [0:06:17 - 0:06:18]: An overhead view captures the person holding the frying pan above the stove, which now has multiple pans. The countertop shows various ingredients like pickles and a white bowl. [0:06:18 - 0:06:19]: The camera continues to provide an overhead perspective, showing the person stirring the contents inside the frying pan. The kitchen counter surface is visible, consisting of multiple cooking utensils and ingredients.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What item is holding by his right hand right now?",
        "time_stamp": "00:06:01",
        "answer": "A",
        "options": [
          "A. A bottle.",
          "B. Only vegetables.",
          "C. Only meat pieces.",
          "D. Sliced fruits."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Action Recognition",
        "question": "What action is the person performing right now?",
        "time_stamp": "00:06:11",
        "answer": "A",
        "options": [
          "A. Pours the contents into the frying pan.",
          "B. Puts the bottle back in the refrigerator.",
          "C. Places the bottle on the counter.",
          "D. Drinks from the bottle."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the person shake the frying pan after pouring the liquid into it?",
        "time_stamp": "00:06:16",
        "answer": "A",
        "options": [
          "A. To mix the ingredients.",
          "B. To cool down the pan.",
          "C. To stop the steam.",
          "D. To clean the pan."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_26_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: In a bustling outdoor scene near the waterside, numerous people are walking along a cobblestone-paved area. The sky is transitioning from dusk to night, creating a striking contrast between the brightly lit buildings and the dim surroundings. On the left, a modern city skyline illuminated with vibrant lights reflects beautifully on the water. People of various ages move around, some in groups and some individually, while others appear to be dining at outdoor tables on the right side near a building with large glass windows and warm interior lights. [0:00:04 - 0:00:08]: The video continues to capture the lively atmosphere. The building on the right displays illuminated signage, and diners sit at tables under umbrellas, enjoying their meals. The walkway remains busy with passersby, some holding hands or taking leisurely strolls. Palm trees are visible in the background, adding a tropical touch to the area. The horizon still exhibits the remnants of a colorful sunset blending into the approaching night. [0:00:09 - 0:00:12]: As people walk around, the camera angle captures a wider view of the promenade. The cityscape across the water becomes more prominent, showcasing tall buildings with multi-colored lights that reflect on the water’s surface. The crowd thins slightly, but a steady flow of pedestrians remains. Lantern-like street lights dot the sidewalk, offering a warm glow to the public space. [0:00:13 - 0:00:16]: The promenade becomes busier as more people enter the area. Groups of visitors stop to enjoy the view or engage in conversation. On the left, more of the city lights become visible, highlighting iconic skyscrapers and their neon signs. The overall ambiance is vibrant, with a mix of locals and tourists soaking in the evening atmosphere. [0:00:17 - 0:00:20]: The video captures the thriving nightlife, with crowds continuing to stroll along the waterside. The mixture of natural and artificial light creates a picturesque scene. Various elements like palm trees, waterfront views, and the illuminated cityscape contribute to the overall lively and inviting setting. Street performers and entertainment elements might be out of view but could be inferred from the crowd's interaction and engagement.",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "Based on the crowd's interaction and engagement, what might be inferred to be present but out of view?",
        "time_stamp": "0:00:20",
        "answer": "B",
        "options": [
          "A. A quiet library.",
          "B. Street performers and entertainment elements.",
          "C. A calm and deserted area.",
          "D. An indoor concert."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_335_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:59]: The video begins in a first-person perspective of a modern architectural space that appears to be an outdoor or semi-outdoor area, possibly part of a large building complex. The environment is well-lit with individual ceiling lights, and the ground is paved with patterned tiles. The space features large beige columns made of small square tiles and horizontal beige-tiled panels that cover parts of the ceiling and walls. There are railings on the right-hand side, indicating a drop or stairs leading somewhere below. The background shows an urban setting with high-rise buildings illuminated against the night sky. As the footage progresses, movement suggests that the camera is slowly advancing forward. To the left, there is some open space where multiple light sources such as ceiling lights and wall-mounted lights illuminate the area. Occasionally, a person can be seen walking or standing in the distance, enhancing the sense of scale and depth of the scene. The ceiling lights create a rhythmic pattern along the walkway, and the overall atmosphere feels serene and quiet despite being in a potentially busy urban area; [0:03:00]: The viewer’s perspective continues to move forward, aligning with the row of columns and passing by more windows and open spaces that show glimpses of the outside cityscape.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the camera doing right now?",
        "time_stamp": "00:03:14",
        "answer": "A",
        "options": [
          "A. Moving forward slowly.",
          "B. Staying still.",
          "C. Moving backward.",
          "D. Panning left and right."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_335_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:30]: The video begins with an evening cityscape under a deep blue sky. The scene showcases several tall buildings, predominantly in beige and white tones, with numerous lit windows. One building on the left displays geometric shapes outlined by lights. Trees can be seen in the foreground, their dark silhouettes framing the scene, with a yellow bus and people visible at the base of the buildings. Moving forward, a person dressed in dark clothing runs toward the camera on a broad, tile-patterned pavement. Various pedestrians are visible in the background, walking or standing. [0:05:31 - 0:05:39]: The person running moves closer to the camera through the frame. The area is well-lit with street lamps casting light on the paved ground and nearby shrubbery. Vehicles, including buses and cars, continue to move along the street bordering the paved area. More pedestrians walk across the frame, including individuals dressed in light-colored clothing. The scene continues to feature the tall buildings and cityscape illuminated by various lights. [0:05:40]: In this frame, the scene shows people and vehicles moving along the street and pavement. A bus is prominent in the foreground, with passengers visible through the windows. Streetlights illuminate the area, with shrubs lining the path. The tall buildings in the background remain a constant, lit by numerous lights as the evening advances.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the trees located in relation to the buildings in the cityscape?",
        "time_stamp": "00:05:30",
        "answer": "C",
        "options": [
          "A. Behind the buildings.",
          "B. To the right of the buildings.",
          "C. In the foreground, framing the scene.",
          "D. On the rooftops of the buildings."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_335_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:06]: The video showcases a vibrant city environment at nighttime with a first-person view. Numerous tall buildings, some illuminated with lights, create an urban skyline. Notable among them is a prominently lit skyscraper with a large advertisement displayed on its facade, featuring white and green colors. The road in the foreground consists of multiple lanes and cars, with a mix of buses, vans, and motorcycles occupying them. Streetlights line the road, casting a warm glow. On the sidewalk to the left, pedestrians are walking along, some near the camera and others further away. Trees with green leaves are visible on either side, their branches extending over the road. The sky above is clear with a deep blue hue, contributing to the evening atmosphere. [0:08:07 - 0:08:14]: As the video progresses, the first-person perspective moves slightly forward, providing a closer view of the buildings and people on the sidewalk. The prominent skyscraper with an advertisement remains central in the frame, and more details of the surrounding architecture become apparent. A double-decker bus enters the scene from the left, moving in the direction of traffic. Pedestrians continue to walk on the sidewalk, one individual wearing a red jacket and another holding a bag, indicating a typical urban evening scene. [0:08:15 - 0:08:19]: The perspective further advances, showing additional elements like parked vehicles along the road and a clearer view of the buildings on the right side, which have ornate designs and are also illuminated. More cars pass by, including taxis identifiable by their roof lights. Pedestrians, including some children, walk along the sidewalk, enjoying the evening. The overall scene maintains its vibrant urban feel with the combination of moving traffic, illuminated buildings, and active pedestrian movement.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "Which type of bus entered the scene just now?",
        "time_stamp": "00:08:14",
        "answer": "D",
        "options": [
          "A. Single-decker bus.",
          "B. Mini-bus.",
          "C. Coach bus.",
          "D. Double-decker bus."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_335_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:57]",
    "captions": "[0:09:40 - 0:09:57] [0:09:40 - 0:09:42]: The scene shows a bustling city sidewalk at night. Several pedestrians walk near a large, triangular-structured building on the left side of the frame. The building is constructed from light-colored tiles. Some people are standing near poles with street signs and advertisements. The background features a busy road with various types of vehicles, including cars and buses, and additional high-rise buildings with illuminated windows. The sky is dark blue, indicating early night. [0:09:43 - 0:09:46]: Some pedestrians continue to walk along the sidewalk while others are stationary. There is movement in the background as vehicles pass by. Street signs and advertisements are visible, and the activity level indicates a lively urban area. A tall building with many lit windows and the word “PRINCE” prominently displayed at the top stands across the street. [0:09:47 - 0:09:50]: The view shows a clearer angle of the busy street with parked and moving vehicles. Pedestrians continue along the sidewalk, interacting and moving around the various street signs and lampposts. The illuminated building with the “PRINCE” sign and other lighted shops and storefronts can be seen more distinctly. [0:09:51 - 0:09:55]: The camera angle shifts slightly, focusing more on the street lined with vehicles. Numerous cars are parked along the roadside and in motion. The high-rise building and storefronts remain visible in the background. Pedestrian activity continues on the sidewalk, and the area appears well-lit with streetlights and illuminated signs from nearby shops. [0:09:56 - 0:09:57]: The video concludes with a broad view of the street and sidewalk. More pedestrians and vehicles are visible, with the latter either driving by or parked along the road. The surroundings remain brightly lit by streetlights and various illuminated signs, contributing to the lively nighttime atmosphere of the urban setting.",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "What indicates that the area is lively and urban?",
        "time_stamp": "00:09:55",
        "answer": "B",
        "options": [
          "A. Presence of street performers.",
          "B. Numerous pedestrians and vehicles and tall buildings.",
          "C. Absence of advertisements.",
          "D. Quiet surroundings."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_335_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:00:11",
        "answer": "B",
        "options": [
          "A. The properties of polygon.",
          "B. The concept of triangle.",
          "C. How to measure the sides of a rectangle.",
          "D. The definition of a polygon."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_224_real.mp4"
  },
  {
    "time": "[0:01:34 - 0:02:04]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:01:54",
        "answer": "B",
        "options": [
          "A. The properties of acute angles.",
          "B. The difference between acute angle and obtuse angle.",
          "C. How to identify right triangles from angles.",
          "D. The relationship between different types of triangles."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_224_real.mp4"
  },
  {
    "time": "[0:03:08 - 0:03:38]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:03:39",
        "answer": "D",
        "options": [
          "A. The angle sum of triangles.",
          "B. The concept of right triangles.",
          "C. Important theorems about scalene triangles.",
          "D. The equal sides' number of Isosceles triangles."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_224_real.mp4"
  },
  {
    "time": "[0:04:42 - 0:05:12]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:04:53",
        "answer": "B",
        "options": [
          "A. Examples of right triangles.",
          "B. Types of angles of equilateral triangles.",
          "C. The concept of similar triangles.",
          "D. Differences between quadrilaterals and triangles."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_224_real.mp4"
  },
  {
    "time": "[0:06:16 - 0:06:46]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker do next?",
        "time_stamp": "00:06:35",
        "answer": "D",
        "options": [
          "A. Discuss the properties of isosceles triangles.",
          "B. Introduce the concept of congruent triangles.",
          "C. Explain the Pythagorean theorem.",
          "D. Solve for the unknown angle."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_224_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:05]: A person is holding tongs and stirring a pot of noodles that are boiling in water. The pot is stainless steel and is placed on a stovetop. The camera angle is close-up, focusing on the pot and the hand holding the tongs. [0:01:06 - 0:01:10]: The camera angle changes to a wider shot, where the person is seen standing in a kitchen. She continues to stir the noodles in the pot with tongs. She is in a modern kitchen with white cabinets and a large stainless steel refrigerator to her left. Various items such as a pepper grinder, a bottle of olive oil, and a block of butter are on the counter. [0:01:11 - 0:01:13]: The person continues to stir the noodles in the pot. She then stops and turns to face a frying pan that is placed on a stovetop to her right. She picks up a bottle of olive oil with her right hand and prepares to pour it into the frying pan. [0:01:14]: The camera zooms in to a close-up view of the frying pan. The person begins to pour olive oil into the pan. [0:01:15 - 0:01:16]: After pouring the olive oil, she stops and stands near the stovetop, speaking while holding the bottle. She gestures with her left hand, explaining something. [0:01:17 - 0:01:18]: The person lowers the frying pan onto the stovetop with one hand while still holding the bottle of olive oil with the other. [0:01:19]: The camera shifts to a closer view of the stovetop area where the frying pan is being placed down. Steam is rising from the pot of noodles noticeable in the scene.\n[0:01:20 - 0:01:40] \n[0:01:40 - 0:02:00] ",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person holding right now?",
        "time_stamp": "0:01:06",
        "answer": "C",
        "options": [
          "A. A spatula.",
          "B. A spoon.",
          "C. An olive oil bottle.",
          "D. A whisk."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_21_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:05]: A woman stands in a modern kitchen with white cabinetry and a stainless steel stove, facing the camera. She's wearing a red tank top and is holding a large pepper grinder over a black frying pan on the stovetop. There's a stainless steel pot with a ladle beside the frying pan, and steam is rising from it. Various items, including a green ceramic teapot and a bottle of oil, are positioned on the countertop; the kitchen has a clean and organized look, and sunlight streams in through a window in the background.  [0:02:06 - 0:02:09]: The camera zooms in on the frying pan, where food is sizzling and steaming. Steam fills the frame, clouding some details of the pan's contents. [0:02:10 - 0:02:14]: The camera zooms out, revealing the woman continuing to use the pepper grinder over the frying pan. She then places the pepper grinder down and appears to reach for something off-camera to her left while holding the ladle with her right hand. An oven mitt, bottle of oil, and a green teapot are visible on the countertop.  [0:02:15 - 0:02:17]: The camera focuses closer on the woman, showing her face more clearly as she stirs the contents of the pot with the ladle. [0:02:18 - 0:02:19]: The camera zooms back out, showing the woman continuing to stir the pot contents while speaking to the camera. The caption \"RECIPE IS IN THE DESCRIPTION BELOW\" appears at the bottom of the screen.\n[0:02:20 - 0:02:40] \n[0:02:40 - 0:03:00] ",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the woman holding over the black frying pan?",
        "time_stamp": "0:02:45",
        "answer": "C",
        "options": [
          "A. A wooden spoon.",
          "B. A salt shaker.",
          "C. noodles.",
          "D. A spatula."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_21_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:02]: A close-up view of a frying pan on a stove. Inside the pan, there are partially cooked noodles that appear to be in the process of being stirred. A pot, which is oriented upside down, has just poured additional liquid, likely water, into the pan, as evidenced by the stream of liquid flowing out. The countertops around the stove are clean and modern in appearance, and the stove itself is positioned against a tiled backsplash. [0:03:03]: The scene shifts to show a wide-angle view of a woman standing in a modern kitchen. She is positioned to the left of the camera frame, stirring the contents of a pot on the stovetop. Her hair is long and brown, and she is dressed in a reddish tank top. The kitchen features white subway tiles on the wall and a large window with an ample view of greenery outside. A pot with a thriving green plant occupies the corner space below the window. [0:03:04 - 0:03:05]: The woman stops stirring and reaches for a different pot. The kitchen is well-lit with natural light coming through the window. To the right of the stove is a bottle whose label is partially visible, suggesting cooking oil or another liquid ingredient. Various other kitchen elements such as utensils, a pepper grinder, and possibly a salt shaker are seen on the countertop. [0:03:06 - 0:03:11]: The woman begins adding a white substance, possibly grated cheese or another ingredient, into the frying pan with the noodles. As she does this, steam rises from the pot she was stirring earlier, indicating it contains a hot liquid or food. Her hand movement is deliberate as she sprinkles the ingredient evenly over the entire surface of the noodles. [0:03:12 - 0:03:17]: The camera focuses back on the frying pan, zooming in to show a detailed view of the woman’s hand sprinkling the white substance into the pan. The noodles are starting to incorporate the added ingredient, and steam is gently rising from the dish, showing it is being actively cooked. [0:03:18 - 0:03:19]: The scene shifts back to a wider angle where the woman is seen again stirring the pot on the stove. She occasionally glances at the frying pan, ensuring the cooking process is proceeding correctly. The atmosphere remains bright and clean, characteristic of a modern, well-maintained kitchen.\n[0:03:20 - 0:03:40] \n[0:03:40 - 0:04:00] ",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the woman holding while she is stirring the pot on the stove?",
        "time_stamp": "0:03:05",
        "answer": "C",
        "options": [
          "A. A ladle.",
          "B. A whisk.",
          "C. A tongs.",
          "D. A spatula."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_21_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:01]: A person in a sleeveless red top is holding a silver pot and pouring noodles into a black frying pan positioned on a stovetop. The stovetop has a burner and a metal grid. [0:04:01]: The person has long brown hair and is focusing on the task. The background features white kitchen cabinets and a countertop. [0:04:02 - 0:04:03]: The person is looking downward, possibly at the pot or the frying pan, with their face slightly downward and to the left. [0:04:04]: The person is still engaged with the cooking activity, holding a utensil near the frying pan, which contains noodles. [0:04:05 - 0:04:07]: The person continues stirring the noodles in the frying pan, making sure they are evenly cooked. The frying pan starts to release steam, indicating the heating of the contents. [0:04:08 - 0:04:09]: The camera captures a wider view of the kitchen with the person standing over the stove, stirring the noodles in the pan. There is a window in the background allowing natural light to fill the room and a countertop with a bowl of colorful fruits, a bottle of oil, and an assortment of utensils. [0:04:10 - 0:04:11]: The person looks up momentarily, probably talking or interacting with someone off-camera, while continuing to stir the food. [0:04:12 - 0:04:13]: The person resumes looking at the frying pan, attentively cooking the noodles. The room remains well-lit with clear visibility of kitchen details, including a refrigerator and cabinets. [0:04:14 - 0:04:16]: The person uses a pair of tongs to stir the contents of the frying pan thoroughly. The kitchen environment maintains its neat and bright appearance. [0:04:17 - 0:04:19]: A close-up shot shows the noodles being stirred in the frying pan. The noodles are yellow and appear to be softening as they mix with a liquid in the pan. Steam continues to rise, indicating the food's hot temperature.\n[0:04:20 - 0:04:40] \n[0:04:40 - 0:05:00] ",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the pot being used by the person in the kitchen?",
        "time_stamp": "00:04:01",
        "answer": "B",
        "options": [
          "A. Black.",
          "B. Silver.",
          "C. Red.",
          "D. White."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_21_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:05:40]",
    "captions": "[0:05:00 - 0:05:20] \n[0:05:20 - 0:05:40] ",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What item is the woman holding in her hand right now?",
        "time_stamp": "0:05:15",
        "answer": "B",
        "options": [
          "A. A red book.",
          "B. A salt grinder.",
          "C. A cup of coffee.",
          "D. A newspaper."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_21_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What kind of lighting condition is present right now in the video?",
        "time_stamp": "00:00:03",
        "answer": "C",
        "options": [
          "A. Daylight.",
          "B. Dusk.",
          "C. Nighttime.",
          "D. Dawn."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_455_real.mp4"
  },
  {
    "time": "[0:02:13 - 0:02:18]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What type of traffic signal is present right now?",
        "time_stamp": "00:02:16",
        "answer": "D",
        "options": [
          "A. A green light.",
          "B. A yellow light.",
          "C. A flashing light.",
          "D. A red light."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_455_real.mp4"
  },
  {
    "time": "[0:04:26 - 0:04:31]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type of vehicle is visible directly ahead the camera right now?",
        "time_stamp": "00:04:27",
        "answer": "C",
        "options": [
          "A. A sedan.",
          "B. A motorcycle.",
          "C. A truck.",
          "D. A bicycle."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_455_real.mp4"
  },
  {
    "time": "[0:06:39 - 0:06:44]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the other bus visible right now?",
        "time_stamp": "00:06:40",
        "answer": "C",
        "options": [
          "A. Red.",
          "B. Green.",
          "C. Yellow.",
          "D. Blue."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_455_real.mp4"
  },
  {
    "time": "[0:08:52 - 0:08:57]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is mounted on the dashboard right now?",
        "time_stamp": "00:08:56",
        "answer": "A",
        "options": [
          "A. A phone with GPS application.",
          "B. A radio receiver.",
          "C. An air freshener.",
          "D. A digital clock."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_455_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a dark scene where two small windows are visible in the top left and right corners of the screen. The window on the left shows a man with long hair wearing headphones, while the window on the right displays various chat messages. [0:00:02 - 0:00:03]: The scene transitions to a room with dark walls and a wooden ceiling. A window is visible on one of the walls, and a character is crouching on the floor with text indicating a \"Character Selector.\" [0:00:04 - 0:00:05]: The camera view changes to an elaborate room with a desk, bookshelves, a chandelier, and a patterned rug. The text \"Character Selector\" is still present at the bottom of the screen. [0:00:06 - 0:00:07]: The scene switches to a map titled \"Spawn Selector\" showing various locations and details. The window showing the man and the chat window remain in their respective positions. [0:00:08 - 0:00:10]: The view returns to the elaborate room with the desk, bookshelves, and chandelier. Various items are arranged on the desk. [0:00:11]: The screen is black, with only the small windows on the left and right showing the man and the chat messages. [0:00:12 - 0:00:13]: The scene changes to an outdoor area at a dockyard with an industrial background. A man is facing away from the camera, wearing a dark shirt and khaki shorts, standing near a blue shipping container labeled \"THRIFTEX.\" [0:00:14 - 0:00:15]: The camera angle shifts slightly, showing more of the dockyard environment. Another character’s body is visible on the ground near the man. [0:00:16 - 0:00:17]: The camera provides a closer view of the character lying on the ground, revealing details such as their green vest, white goggles, and ear protection. [0:00:18 - 0:00:19]: The man standing in the dark shirt and khaki shorts continues to stand, with the character on the ground now struggling to get up, revealing more of the surrounding industrial area. [0:00:20]: The character on the ground moves, beginning to stand up, revealing another character in khaki uniform observing the scene. Various containers and industrial equipment are visible in the background.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the character on the ground doing right now?",
        "time_stamp": "00:00:16",
        "answer": "B",
        "options": [
          "A. Lying motionless.",
          "B. Beginning to stand up.",
          "C. Running away.",
          "D. Adjusting their goggles."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_289_real.mp4"
  },
  {
    "time": "0:02:40 - 0:03:00",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:45]: The video shows a vehicle from a first-person perspective driving through an industrial area with large container cranes and storage units. The road is wide and marked with lanes, and the sky is clear. The vehicle is heading straight on a concrete road lined with yellow and white markings and surrounded by parked trailers on the sides. [0:02:46 - 0:02:50]: As the vehicle drives forward, it passes more stationary trailers on both sides while approaching some yellow barriers ahead. The road's surroundings remain largely consistent with the industrial setting, including large cranes, parked trailers, and shipping containers. [0:02:51 - 0:02:53]: The vehicle continues driving forward and then makes a slight left turn. The area ahead shows more parked trailers and the road slightly winds around industrial buildings. There are palm trees and a small parking area on the left side of the road. [0:02:54 - 0:02:55]: The vehicle continues its left turn, passing more industrial buildings and parked trailers. The road layout shows a curved path with minimal traffic and various directional signage. More palm trees and parking zones are visible. [0:02:56 - 0:03:00]: Finally, the vehicle heads towards an exit marked with overhead signs. The exit road leads towards a barrier and checkpoint area, indicating a transition point out of the industrial zone. The vehicle aligns itself with the exit road, approaching the checkpoint barriers where entry and exit are regulated.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the vehicle doing right now?",
        "time_stamp": "00:03:00",
        "answer": "B",
        "options": [
          "A. Making a left turn.",
          "B. Approaching the exit road.",
          "C. Passing yellow barriers.",
          "D. Driving through parking zones."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_289_real.mp4"
  },
  {
    "time": "0:05:20 - 0:05:40",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:22]: The video is from a first-person perspective, traveling along a multi-lane road in an urban environment. The road is lined with palm trees and medium-height buildings. To the left, there are tall apartment buildings and a billboard in the background. The vehicle is in the right lane of the road.  [0:05:23 - 0:05:24]: The vehicle approaches an intersection with a pedestrian crossing and a sign indicating to \"Not Block Intersections.\" There are no other vehicles immediately adjacent to the vehicle at this point.  [0:05:25 - 0:05:26]: The vehicle proceeds through the intersection. The traffic light turns green, allowing the vehicle to continue moving forward. Buildings and palm trees are visible on both sides of the road. [0:05:27 - 0:05:30]: The vehicle continues straight on the road with clear traffic signals visible ahead. The road is flanked by several buildings, including a colorful one on the left and a water tower on the right. Other vehicles are visible ahead and to the sides. [0:05:31 - 0:05:33]: As the vehicle progresses, the traffic signal ahead changes to green. The vehicle maintains a steady speed. Several cars are parked along the sides of the street. [0:05:34 - 0:05:36]: Larger buildings and high-rise structures become visible. The road has multiple lanes, and some cars are seen moving along with the vehicle.  [0:05:37 - 0:05:38]: The vehicle passes through another intersection without stopping. More vehicles, including a blue car, are seen parked on the right side next to a beige wall. [0:05:39 - 0:05:41]: The video continues with the vehicle moving past parked cars on the right, including a yellow and a white car. In the background, tall buildings and palm trees are visible. [0:05:42 - 0:05:44]: As the vehicle proceeds further down the road, it approaches another traffic signal which is red. Nearby are multiple lanes of traffic moving in both directions, along with parked vehicles on the right. [0:05:45 - 0:05:46]: The vehicle waits at the red light before continuing through the intersection. The road ahead is clear with more palm trees and tall buildings lining the sides. [0:05:47]: The traffic lights turn green, allowing the vehicle to continue forward. The surroundings remain urban with some greenery. [0:05:48 - 0:05:49]: The vehicle continues to move at a steady speed, passing by buildings and additional intersections. Different lanes and traffic signs are visible. [0:05:50 - 0:05:52]: As the vehicle moves forward, it navigates through the large road with well-marked lanes. More parked vehicles and urban infrastructure become visible. [0:05:53 - 0:05:55]: The vehicle approaches another intersection with a red traffic light. Multiple buildings, including a tall tower with palm trees, can be seen to the right. [0:05:56 - 0:05:57]: At the intersection, the vehicle turns right onto another street. Sidewalks and various bushes with autumn-colored leaves are visible. [0:05:58 - 0:05:59]: The vehicle continues along the new street. It passes a series of buildings and some parked cars, with additional vehicles visible ahead. [0:06:00]: The video perspective shifts slightly, displaying more of the sidewalk and nearby greenery. Palms trees remain a prominent feature of the urban landscape. [0:06:01 - 0:06:02]: The vehicle drives carefully along the urban streets, navigating through buildings and intersections, maintaining its path forward without any major interruptions.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the vehicle do just now?",
        "time_stamp": "00:05:25",
        "answer": "C",
        "options": [
          "A. Stopped at a red light.",
          "B. Turned left onto another street.",
          "C. Continued along the new street.",
          "D. Reversed into a parking space."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_289_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:03]: The video begins with a first-person perspective of driving a box truck on a multi-lane highway. In the distance, there are various signs indicating road directions. To the left, a red car is traveling at a similar pace, and the lanes are separated by yellow and white lines. Palm trees and green hills are visible in the background, along with a couple of overpasses. [0:08:03 - 0:08:05]: The box truck approaches the exit sign for Murrieta Heights, and the truck steers slightly towards the right. The red car continues to travel straight ahead, becoming less visible as the truck starts to take the exit lane. [0:08:06 - 0:08:08]: The truck moves completely into the exit lane, leaving behind the main highway. Road signs indicate the lanes leading to Murrieta Heights and La Mesa, and an arrow on the ground directs the truck's path. [0:08:09 - 0:08:11]: The truck begins to drive up an exit ramp, approaching a bend to the right. A green sign ahead shows directional information for further exits. The ramp is bordered by concrete barriers and lined with streetlights. [0:08:12 - 0:08:14]: The truck continues on the exit ramp, following the curve and rising slightly in elevation. A black car enters the frame from the left, joining the exit lane. [0:08:15 - 0:08:17]: The truck makes a tight right turn, passing beneath another green directional sign. The road straightens out as the truck approaches the end of the ramp. [0:08:18 - 0:08:20]: The truck exits the ramp onto a quieter road flanked by greenery and lower-speed vehicles. The road ahead is clear with occasional traffic signs and small structures visible along the sides.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the truck do just now?",
        "time_stamp": "00:08:13",
        "answer": "C",
        "options": [
          "A. Approached a bend to the right on a highway.",
          "B. Exited the main highway and steered onto an exit lane.",
          "C. Entered a less busy road bordered by greenery.",
          "D. Passed a red car traveling at a similar pace."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_289_real.mp4"
  },
  {
    "time": "0:10:40 - 0:10:46",
    "captions": "[0:10:40 - 0:10:46] [0:10:40 - 0:10:41]: The video shows a first-person perspective with the camera facing forward. A van is reversing into a parking area, and the scene includes a red boundary box indicating the parking zone. The background features a concrete wall with a garage door. [0:10:42]: The van continues reversing into the designated parking area, aligning with the red parking boundary. There is visible smoke or dust around the van as it moves. [0:10:43]: The parking boundary box turns green, indicating that the van is correctly positioned within the parking area. The concrete wall and garage door remain in the background. [0:10:44]: The camera view changes to another angle showing the van fully parked. A person stands next to a control panel on the right side, with another small building or structure nearby. [0:10:45]: The van parked in front of a building labelled \"grime\" is visible. Other structures and greenery are in the background, and there are people nearby. [0:10:46]: The camera now shows a person wearing a safety vest and headphones standing outside, next to the rear of the parked van, within what looks like an industrial area with closed garage doors.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person wearing a safety vest and headphones doing right now?",
        "time_stamp": "00:10:46",
        "answer": "A",
        "options": [
          "A. Walking to the rear of the parked van.",
          "B. Reversing the van into the parking area.",
          "C. Operating the control panel.",
          "D. Standing quietly."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_289_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: Four glittering 3D paper stars lie on a grid-patterned cutting mat. The stars are positioned in a slightly irregular formation with each star facing outward. The stars vary in colors, including silver, gold, and pink.  [0:00:01 - 0:00:15]: Two hands, wearing purple sleeves, gently lift the gold star in the center. The hands position the star directly in front of the camera, showcasing its intricate design. The star is rotated slowly, displaying its symmetry and glittering surface. The hands then carefully place the star back down among the other stars. [0:00:16 - 0:00:19]: The frame transitions to a black screen with the text \"3D Paper Star\" centered, indicating the focus of the video.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the hands do just now?",
        "time_stamp": "00:00:15",
        "answer": "A",
        "options": [
          "A. Picked up the gold star.",
          "B. Folded a pink star.",
          "C. Arranged the stars in a line.",
          "D. Removed the silver star."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_55_real.mp4"
  },
  {
    "time": "0:02:00 - 0:02:20",
    "captions": "[0:02:00 - 0:02:20] [00:00:00 - 00:00:03]: The video starts with a close-up view of a pair of hands working on a craft project. The background is a gray checkered surface, likely a cutting mat. The hands hold a piece of white paper with gold glittery triangles protruding from the sides. The person is folding the paper along pre-defined creases. The person uses their left hand to hold the base of the paper and their right hand to pinch and fold a corner piece. The glimmering gold accents contrast with the plain white paper, creating an intriguing visual. [00:00:04 - 00:00:06]: The person continues to crease the edges of the paper, creating sharper folds and aligning the corners precisely. The hands move in a synchronized manner, with the left-hand maintaining pressure on the paper while the right-hand ensures the gold-trimmed fold aligns perfectly. [00:00:07 - 00:00:09]: The hands then move to the opposite side, repeating the same folding process. The focus remains on creating sharp and precise creases. The gray checkered cutting mat background provides a clear, unobstructed view of the hands and the paper. [00:00:10 - 00:00:11]: The person continues to manipulate the paper, with the hands making small adjustments to ensure all edges and folds are aligned correctly. The person’s attention to detail is evident in their careful movements. [00:00:12 - 00:00:17]: The individual begins folding another side of the paper, focusing on making sure each crease lines up with the previous folds. The glittery gold triangle sections of the paper catch the light, adding a sparkly detail to the otherwise plain white paper. The hands work diligently and methodically, indicating that this is a delicate and precise craft project. [00:00:18 - 00:00:19]: With the final adjustment, the person ensures that all the creases are firm and the paper maintains its folded form. The video captures the intricate details of the folding process and the contrasting colors and textures of the materials used in the craft.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:02:19",
        "answer": "A",
        "options": [
          "A. Folding the paper along the creases.",
          "B. Cutting the paper.",
          "C. Painting the paper.",
          "D. Decorating the paper with colors."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_55_real.mp4"
  },
  {
    "time": "0:04:00 - 0:04:20",
    "captions": "[0:04:00 - 0:04:20] [0:00:00 - 0:00:02]: A person's hands, wearing a purple sweater, hold a golden, glittery star ornament. The star is four-pointed with a hollow center. The background consists of a grey grid mat. [0:00:03 - 0:00:04]: The person rotates the star ornament to show its side profile, revealing more of its three-dimensional structure.  [0:00:05]: The person places the ornament flat on the grid mat, with the star's points oriented horizontally and vertically.  [0:00:06]: They bring another identical golden, glittery star ornament into view, setting it beside the first one.  [0:00:07]: Both stars are now aligned horizontally side by side on the mat.  [0:00:08]: The person picks up the first star slightly, showing its edge once again, and holds it closer to the second star.  [0:00:09]: The person holds the first star in front of the second star, giving a clearer view of its hollow center.  [0:00:10 - 0:00:14]: The person begins to apply glue to the first star's hollow center edges. The glue bottle is white with a blue label and orange cap. They apply the glue carefully to connect the two stars. [0:00:15 - 0:00:19]: The gluing process continues steadily. The person rotates the star slightly, ensuring the glue is evenly applied around the hollow center edges where the two stars will be connected, thus completing the ornament’s assembly.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "0:04:07",
        "answer": "A",
        "options": [
          "A. Applying glue to the hollow center edges of the star.",
          "B. Rotating the star ornament to show its side profile.",
          "C. Holding one star in front of another.",
          "D. Placing the star ornament flat on the grid mat."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_55_real.mp4"
  },
  {
    "time": "0:06:00 - 0:06:20",
    "captions": "[0:06:00 - 0:06:20] [0:06:01 - 0:06:02]: A pair of hands are working on a craft project on a gray, grid-patterned mat. To the left, there is a square piece of light blue glittery paper, positioned horizontally. On the right, there is a white, folded paper structure with multiple layers. The hands are folding another piece of blue glittery paper along its diagonal axis, forming a smaller triangle. [0:06:02 - 0:06:03]: The hands are in the process of creasing the blue triangle, ensuring the fold is sharp. The white folded structure remains on the right, and the flat blue square paper is now slightly above. [0:06:03 - 0:06:04]: The hands lift the folded blue triangle, holding it up with both hands for closer inspection. The white paper structure is in the same position on the right. [0:06:04]: Holding the blue triangle in their left hand, one of the hands reaches for a pair of orange-handled scissors placed to the right. [0:06:04 - 0:06:05]: The right hand picks up the scissors and prepares to cut through the blue triangle. [0:06:05 - 0:06:06]: The scissors make the first incision near the pointed end of the triangle. [0:06:06 - 0:06:07]: The cutting continues with the scissors making a second cut towards the base of the blue triangle. [0:06:07 - 0:06:08]: Another incision is made further down along the folded edge of the triangle. [0:06:08 - 0:06:09]: A final cut is made, resulting in a series of slits along one side of the blue triangle. [0:06:09 - 0:06:10]: The blue triangle with the slits is set aside as the hands put down the scissors. [0:06:10]: The hands pick up a new piece of white paper from the top of the stack on the right. [0:06:10 - 0:06:11]: The new white paper is unfolded and examined before it is refolded along its diagonal. [0:06:11 - 0:06:12]: The hands secure the fold, creating a white triangle similar to the blue one. [0:06:12]: The white triangle is manipulated, creating a series of folds that look similar to the pre-folded white structure already present. [0:06:12 - 0:06:13]: The white structure is set aside on top of the existing white pieces to the right, while the hands reach for another piece of blue paper. [0:06:13 - 0:06:14]: A new piece of blue glittery paper is taken in hand and folded in half along its diagonal. [0:06:14 - 0:06:15]: The hands crease the new blue triangle efficiently, preparing it for the next cuts. [0:06:15 - 0:06:16]: The edges of the folded triangle are smoothed out to ensure precise folds. [0:06:16 - 0:06:17]: The hands create another series of slits, similar to the previous blue triangle. [0:06:17]: The slit blue triangle is added to the growing pile, and the hands reach for another piece. [0:06:17 - 0:06:18]: Another blue paper is folded in half, reaffirming the diagonal fold. [0:06:18 - 0:06:19]: The blue triangle is expertly creased, similar to the previous ones. [0:06:19 - 0:06:20]: The hands complete additional folds, adding complexity to the triangle structure.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are the hands doing right now?",
        "time_stamp": "0:06:18",
        "answer": "A",
        "options": [
          "A. Making creases on a triangular shape.",
          "B. Unfolding a piece of paper.",
          "C. Cutting out shapes from a paper.",
          "D. Painting on the paper."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_55_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:08:01 - 0:08:11]: A pair of hands is seen working on a paper craft project on a gray grid-patterned work surface. The person is manipulating pieces of paper. The main piece is a white paper strip that forms a diamond shape, and there's a smaller, light blue strip being woven through the diamond shape. In the background, several similar diamond-shaped paper structures are visible, stacked to the upper left side. The person carefully adjusts the positioning of the blue strip, ensuring it interlocks properly within the white diamond shape. The individual's left hand holds the white diamond shape steady while the right hand weaves the blue strip. [0:08:12 - 0:08:13]: The person places a purple hot glue gun towards the connection point between the blue and white paper pieces, applying the glue. [0:08:14 - 0:08:15]: The glue is applied, and the right hand still firmly holds the glue gun while the left hand maintains the position of the paper pieces to ensure they stick together securely. [0:08:16 - 0:08:20]: After applying the glue, both hands come back into view, adjusting the blue paper strip once more to ensure a tight fit within the white diamond shape.  [0:08:21 - 0:08:22]: The person releases the paper craft project briefly, allowing it to rest momentarily before handling it again. [0:08:23 - 0:08:24]: The hands then reach out to adjust another part of the craft project, bringing another, similar diamond-shaped unit resembling the first, into manipulation. The person begins the same process of weaving a new strip through it.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:08:16",
        "answer": "A",
        "options": [
          "A. Adjusting the blue paper strip for a tight fit.",
          "B. Applying glue to the paper pieces.",
          "C. Cutting a new strip of paper.",
          "D. Stacking finished paper structures to the side."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_55_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video begins with a black screen. It then transitions to a scene showing a person seated behind a white desk. The individual is wearing glasses, a green jacket over a white T-shirt, and is positioned centrally. The background consists of a blue wall with shelves on either side. Items on the shelves include gadgets and decorations, all evenly spaced and neatly arranged. [0:00:04 - 0:00:06]: The person makes hand gestures, initially appearing to be explaining something passionately. A close-up view shows the person clenching fists and then making a gesture with open hands near their face, indicating excitement or emphasis. [0:00:07 - 0:00:09]: As the person continues talking, a graphic of a MediaTek chip appears. The chip is labeled \"MediaTek 天玑 9000\" with sections identifying different components such as CPU and Connectivity. The background of the graphic is white. [0:00:10 - 0:00:12]: The graphic remains on screen for a few more moments, allowing observation of the detailed layout of the chip's components. Subsequently, the view shifts to display a larger image of the MediaTek chip with the text \"5G MediaTek 天玑 9000\" superimposed, emphasizing the main subject of the video. [0:00:13 - 0:00:15]: The angle shows a comparison between the MediaTek 天玑 9000 chip and another chip labeled \"Snapdragon 8 Gen 1.\" Both chips are positioned next to each other on a white background with a \"VS\" symbol in between, highlighting a comparison being made. [0:00:16 - 0:00:19]: The person in the green jacket reappears and continues the explanation. Movements include hand gestures and facial expressions, indicating the explanation's importance. The view captures the individual focusing toward the viewer, maintaining engagement. [0:00:20]: The video concludes with the individual seated, hands placed on the desk, and a concluding statement being made. The well-organized background remains visible, contributing to the setting's professional appearance.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the main subject of the comparison shown right now?",
        "time_stamp": "00:00:10",
        "answer": "B",
        "options": [
          "A. The gadgets on the shelves.",
          "B. The MediaTek 天玑 9000 chip and another chip labeled Snapdragon 8 Gen 1.",
          "C. The person's hand gestures and facial expressions.",
          "D. The blue wall with shelves in the background."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_111_real.mp4"
  },
  {
    "time": "[0:03:40 - 0:04:00]",
    "captions": "[0:03:40 - 0:04:00] [0:03:40 - 0:03:48]: The video presents a detailed technical diagram describing the architecture of a MediaTek 9000 chipset. On the left side of the frame, there is a depiction of a 5G MediaTek chip with \"天玑 9000\" written below it. Adjacent to this chip, a detailed breakdown of the CPU and GPU layout is shown, with different rectangular blocks labeled with specifications such as \"X2 3.05GHz,\" \"A710 2.85GHz,\" and \"A510 1.8GHz.\" These blocks are color-coded and arranged in rows, indicating the design and structure of the CPU and GPU. The CPU section at the top includes several blocks with varied levels of cache (8MB L3, 256KB L2, and 512KB L2). Below the CPU blocks, the GPU section labeled \"G710MC10\" is highlighted in purple and includes several smaller blocks within. [0:03:49 - 0:03:54]: As the video progresses, the focus shifts more towards the GPU section, emphasizing the \"G710MC10\" label. The CPU and its description remain static in the background, but the arrangement of the purple blocks for the GPU becomes more central. The overall visual maintains a dark background to contrast the vibrant and distinct colors of the chipset components. [0:03:55 - 0:04:00]: The screen completely transitions to highlight only the GPU section. The purple blocks are more prominently displayed, with the central block labeled \"G710MC10.\" Additionally, Chinese text appears below these blocks, providing further details. The text, written in white, is clear against the dark background, explaining that each Shader Core comprises two execution units. This final representation underscores the GPU architecture and functional specifics.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the label of the highlighted central block in the GPU section right now?",
        "time_stamp": "00:04:00",
        "answer": "B",
        "options": [
          "A. A510 1.8GHz.",
          "B. G710MC10.",
          "C. X2 3.05GHz.",
          "D. A710 2.85GHz."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_111_real.mp4"
  },
  {
    "time": "[0:07:20 - 0:07:40]",
    "captions": "[0:07:20 - 0:07:40] [0:07:20 - 0:07:22]: The video shows a performance comparison chart titled \"GFXBench Aztec Ruin测试.\" The chart depicts performance in 1440p resolution with Metal and OpenGL ES 3.1 (fps). Several horizontal bars represent different devices' performance metrics. In this initial segment, devices with labels such as A15 and A14 are displayed with blue and pink bars next to numerical values denoting their performance. [0:07:23 - 0:07:30]: Another device in the chart, labeled as 天玑9000, is highlighted with a surrounding red box. The specific values shown next to this device are 42 and 100. Additionally, the blue and pink bars of different lengths indicate comparative performance metrics. [0:07:31 - 0:07:35]: The highlighted device label shifts to 骁龙8 Gen1 with 43 and 115 as its performance values. The chart's horizontal bars continue to depict the relative performance of these devices, highlighting how they stand compared to others in the list using different shades and lengths. [0:07:36 - 0:07:40]: The chart progresses to show further comparative data, where the labels and values may alter slightly as information is being highlighted. Additionally, performance metrics for (Vulkan) and other devices such as the Apple A13 and Kirin 9000 appear with their respective colored bars and values indicating their performance differences.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What are the performance values shown for the device labeled 天玑9000 right now?",
        "time_stamp": "00:07:30",
        "answer": "D",
        "options": [
          "A. 42 and 115.",
          "B. 43 and 100.",
          "C. 100 and 42.",
          "D. 42 and 100."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_111_real.mp4"
  },
  {
    "time": "[0:11:00 - 0:11:20]",
    "captions": "[0:11:00 - 0:11:20] [0:11:00 - 0:11:20]: The video displays a graph illustrating the frame rate performance of the game \"Genshin Impact\" under various settings and conditions. The frame rate is measured in frames per second (fps) and is plotted on the y-axis against time on the x-axis, where multiple colored lines represent different devices or settings.  Prominently, four device labels—A15, 天玑9000, 骁龙8 Gen1, and 骁龙888 (Dimensity 9000, Snapdragon 8 Gen1, and Snapdragon 888, respectively)—are marked with different colored lines. The graph shows consistent frame rates for most devices, hovering around 50 to 60 fps. Meanwhile, the top indicates the test conditions, which include a temperature of 25°C at 200 lux under a 30-minute test in China. A legend on the left side reports the average frame rate and power consumption for each device: A15 at 59 fps/3.9W, 天玑9000 at 59 fps/4.8W, 骁龙8 Gen1 at 56 fps/6.3W, and 骁龙888 at 48 fps/5.1W. The lines exhibit slight fluctuations, indicating minor variations in frame rate over time. Towards the end of the video, a noticeable increase in variability is seen in the lines, suggesting possible changes in performance or conditions.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the average frame rate and power consumption for the 骁龙8 Gen1 device right now?",
        "time_stamp": "00:11:20",
        "answer": "C",
        "options": [
          "A. 59 fps/3.9W.",
          "B. 59 fps/4.8W.",
          "C. 58 fps/6.3W.",
          "D. 48 fps/5.1W."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_111_real.mp4"
  },
  {
    "time": "[0:14:00 - 0:14:05]",
    "captions": "[0:14:00 - 0:14:05] [0:14:00 - 0:14:02]: A person with short hair and glasses sits at a table, facing the camera. They wear a green jacket over a white t-shirt with a design. The background is a blue gradient wall with shelves on either side holding various objects like books and gadgets. Four large icons hover across the screen: a thumbs-up icon, a no symbol, a star, and a share symbol. The person gives a thumbs-up gesture. [0:14:02 - 0:14:03]: The person raises their left hand and points with their thumb towards their left shoulder while still smiling at the camera. The same icons are displayed across the screen. [0:14:03 - 0:14:04]: The person puts their left hand down and gives a thumbs-up with their right hand, maintaining a smile and leaning slightly forward. The icons remain on the screen. [0:14:04 - 0:14:05]: The person crosses their arms on the table, with their right hand visible, smiling gently at the camera. The screen then transitions to a black background with a blue and white logo and text in Chinese, resembling animated graphics against a starry sky backdrop.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which symbol is NOT displayed across the screen right now?",
        "time_stamp": "00:14:02",
        "answer": "D",
        "options": [
          "A. Thumbs-up icon.",
          "B. forward icon.",
          "C. Star.",
          "D. Heart."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_111_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the white sign with the number 50 right now?",
        "time_stamp": "00:00:10",
        "answer": "D",
        "options": [
          "A. At the front of the road.",
          "B. At the back of the road.",
          "C. On the left side of the road.",
          "D. On the right side of the road."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_183_real.mp4"
  },
  {
    "time": "[0:02:03 - 0:02:23]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located on the left side of the road while the cyclists are passing right now?",
        "time_stamp": "00:02:08",
        "answer": "D",
        "options": [
          "A. A coffee shop.",
          "B. A National Championships banner.",
          "C. A grocery store.",
          "D. A bank."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_183_real.mp4"
  },
  {
    "time": "[0:04:06 - 0:04:26]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located on the right side of the road right now",
        "time_stamp": "00:04:25",
        "answer": "D",
        "options": [
          "A. A coffee shop.",
          "B. A bank.",
          "C. A cafe.",
          "D. A brown building."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_183_real.mp4"
  },
  {
    "time": "[0:06:09 - 0:06:29]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the boxley building located right now?",
        "time_stamp": "00:06:10",
        "answer": "D",
        "options": [
          "A. On the left side.",
          "B. Directly ahead.",
          "C. Behind the cyclist.",
          "D. On the right side."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_183_real.mp4"
  },
  {
    "time": "[0:08:12 - 0:08:32]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the positioning of the cyclist with number '901' right now?",
        "time_stamp": "00:08:20",
        "answer": "D",
        "options": [
          "A. At the front of the group.",
          "B. In the middle of the group.",
          "C. On the left side of the group.",
          "D. At the back of the group."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_183_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the content of the video just shown?",
        "time_stamp": "00:00:10",
        "answer": "C",
        "options": [
          "A. An individual selects a burger bun, prepares the toppings, and hands it to a customer.",
          "B. An individual uses a deep fryer to cook a basket of fries, then serves them with extra condiments.",
          "C. An individual picks up a hamburger bun, assembles it with cheese.",
          "D. An individual sets up a drink machine and serves multiple beverages to waiting customers."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_347_real.mp4"
  },
  {
    "time": "[0:03:04 - 0:03:14]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the content of the video just shown?",
        "time_stamp": "00:03:18",
        "answer": "B",
        "options": [
          "A. An individual cleans the workspace, organizes ingredients, and prepares a station for making desserts.",
          "B. An individual assembles multiple burgers by adding lettuce, cheese, and pickles, then places the patties on the buns.",
          "C. An individual prepares sandwiches by slicing bread, spreading mayonnaise, adding lettuce and tomato, and topping with cheese.",
          "D. An individual prepares a pasta dish by boiling noodles, adding sauce, and arranging it on plates."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_347_real.mp4"
  },
  {
    "time": "[0:06:08 - 0:06:18]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the content of the video just shown?",
        "time_stamp": "00:06:18",
        "answer": "A",
        "options": [
          "A. An individual prepares a burger by toasting buns, adding sauce and pickles, and placing cheese slices on each bun.",
          "B. An individual fries patties, adds ketchup and lettuce, and places them in a to-go box.",
          "C. An individual wraps sandwiches, adds mustard and onions, and organizes them on a tray.",
          "D. An individual makes an ice cream sundae by scooping ice cream, adding toppings, and serving it."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_347_real.mp4"
  },
  {
    "time": "[0:09:12 - 0:09:22]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:09:21",
        "answer": "B",
        "options": [
          "A. An individual cleans the workspace, organizes ingredients, and sets up a drink machine.",
          "B. An individual prepares a burger by assembling the bun, adding cheese, onions, and organizing the station with necessary toppings.",
          "C. An individual slices vegetables for a salad, arranges them on a plate, and prepares the dressing.",
          "D. An individual checks kitchen equipment, prepares a pasta dish, and serves it to customers."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_347_real.mp4"
  },
  {
    "time": "[0:12:16 - 0:12:26]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the content of the video just shown?",
        "time_stamp": "00:12:26",
        "answer": "A",
        "options": [
          "A. An individual prepares a burger by selecting buns, adding ketchup, mustard, pickles, and a slice of cheese.",
          "B. An individual assembles a taco by choosing toppings, adding salsa, and wrapping it tightly.",
          "C. An individual makes a hot dog by grilling the sausage, placing it in a bun, and adding ketchup and mustard.",
          "D. An individual prepares a salad by chopping vegetables, adding dressing, and tossing the ingredients."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_347_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the actions taken by the individual just now?",
        "time_stamp": "00:00:13",
        "answer": "A",
        "options": [
          "A. The individual prepared a cocktail, straining it into a glass and adding a garnish.",
          "B. The individual prepared a cold brew coffee, pouring it into a glass and adding ice cubes.",
          "C. The individual set the table with cutlery and plates, then folded napkins.",
          "D. The individual poured a hot coffee, added milk and sugar, and stirred it before serving."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_408_real.mp4"
  },
  {
    "time": "[0:02:53 - 0:03:03]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the actions taken by the individual just now?",
        "time_stamp": "00:03:03",
        "answer": "B",
        "options": [
          "A. The individual cleaned the glasses, added garnishes, and served them to the customers.",
          "B. The individual scooped ice into glasses.",
          "C. The individual wiped down the counter, refilled condiment containers, and organized the bar area.",
          "D. The individual prepared different cocktails, added straws and stirrers, and placed them on the counter."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_408_real.mp4"
  },
  {
    "time": "[0:05:46 - 0:05:56]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the actions taken by the individual just now?",
        "time_stamp": "00:05:47",
        "answer": "B",
        "options": [
          "A. The individual mixed and served a drink, cleaned up the glasses, and rearranged the bottles.",
          "B. The individual poured champagne for customers.",
          "C. The individual prepared multiple cocktails, added garnishes, and served them to customers.",
          "D. The individual restocked the bar, washed and dried glasses, and set the table."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_408_real.mp4"
  },
  {
    "time": "[0:08:39 - 0:08:49]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the actions taken by the individuals just now?",
        "time_stamp": "00:08:49",
        "answer": "D",
        "options": [
          "A. The individuals prepared and cleaned glasses, arranged spirits, and set up the bar for service.",
          "B. The individuals served drinks to customers, rearranged the menu items, and cleaned the bar.",
          "C. The individuals stocked the refrigerator with new items, prepared snacks, and wiped down shelves.",
          "D. The individuals mixed drinks while managing the bar area."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_408_real.mp4"
  },
  {
    "time": "[0:11:32 - 0:11:42]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the actions taken by the individual just now?",
        "time_stamp": "00:11:42",
        "answer": "A",
        "options": [
          "A. The individual prepared different cocktails, added ice and garnishes, and served them to customers at the bar.",
          "B. The individual cooked food, plated the dishes, and served them to customers at the bar.",
          "C. The individual prepared different desserts, added toppings, and served them to customers at the bar.",
          "D. The individual set up bar decorations, cleaned the glasses, and served water to customers at the bar."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_408_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:00:11",
        "answer": "C",
        "options": [
          "A. How the distributive property helps in simplifying equations.",
          "B. The practical applications of the probability theory.",
          "C. The definition of the distributive property.",
          "D. The definition of stochastic process."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_225_real.mp4"
  },
  {
    "time": "[0:02:05 - 0:02:35]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:02:00",
        "answer": "D",
        "options": [
          "A. Combining like terms.",
          "B. Factoring out a common factor.",
          "C. Simplifying the expression further.",
          "D. How to use distributive property to the example."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_225_real.mp4"
  },
  {
    "time": "[0:04:10 - 0:04:40]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:04:30",
        "answer": "B",
        "options": [
          "A. The formula for compound interest.",
          "B. Why the two way get the same answer.",
          "C. Solving quadratic equations.",
          "D. Graphing linear equations."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_225_real.mp4"
  },
  {
    "time": "[0:06:15 - 0:06:45]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker calculate next?",
        "time_stamp": "00:06:43",
        "answer": "A",
        "options": [
          "A. 5 times 7.",
          "B. 47 times 47.",
          "C. 5 times 5.",
          "D. 5 plus 40."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_225_real.mp4"
  },
  {
    "time": "[0:08:20 - 0:08:50]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:08:47",
        "answer": "A",
        "options": [
          "A. The example can not use distributive property.",
          "B. The commutative property of addition.",
          "C. The identity property of multiplication.",
          "D. The distributive property of subtraction."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_225_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: A hand holds a paintbrush, applying dark and brown paint to a partially sketched canvas. The background is slightly blurred but shows a workspace with some warm lighting.  [0:00:02 - 0:00:03]: The painting process continues with more dark paint applied to the canvas, which features what looks like a shiny, metallic structure. Blue paint is also visible, adding depth to the artwork. [0:00:04 - 0:00:06]: The scene shows continued painting with additional layers being added. The metallic object being painted begins to take shape, suggesting it could be armor or a helmet. The background remains consistent, with a warm light source illuminating the scene. [0:00:07 - 0:00:09]: The view shifts to a side perspective of an easel and the artist adjusting it. Surrounding the workspace is a laptop, a coffee mug, and various art supplies on a table. The artist's clothing is visible, including a checked shirt. [0:00:10 - 0:00:11]: The artist continues to arrange the easel and gather materials. The background includes more items like flowers, a wooden circular frame, and a black laptop screen. [0:00:12]: The artist grabs a canvas and begins positioning it on the easel. The workspace shows a mix of art supplies and tools. [0:00:13]: The artist leans over, fully securing the canvas onto the easel. The background reveals more details of the surrounding items and work atmosphere. [0:00:14]: The artist steps back, revealing a sketched outline of a helmeted figure on the canvas. Various tools and a warm light source are on the table.  [0:00:15 - 0:00:16]: The artist looks towards the camera, wearing a cap, while standing in front of the work-in-progress sketch.  [0:00:17]: The artist begins drawing on the canvas, sketching finer details of the helmet.  [0:00:18 - 0:00:19]: The drawing continues, focusing on adding more intricate lines and shading to the sketch. The workspace, illuminated by warm lighting, remains consistent in the background.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What is the main activity depicted in the video so far?",
        "time_stamp": "00:00:19",
        "answer": "A",
        "options": [
          "A. Sketching a helmeted figure.",
          "B. Painting a landscape.",
          "C. Arranging art supplies.",
          "D. Painting a portrait."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_130_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:46]: The video starts with a close-up of a hand holding a paintbrush. The brush is actively painting on a canvas, which has a figure of a character wearing a helmet. The helmet is silver with blue and black accents and appears to cover the entire head. The background is primarily white, with blue brushstrokes surrounding the helmet. The hand is mainly focusing on painting the top portion of the helmet. [0:02:47 - 0:02:51]: The hand continues to work on the helmet's details, refining the shaded areas and adding highlights. The brush moves carefully around the contours of the helmet, especially on the left side, towards the top. The rest of the painting remains mostly unchanged, with the background still partially completed and the figure's armor colors visible. [0:02:52 - 0:02:57]: As the hand paints, more details and depth are added to the helmet. The brush appears to focus on both the reflective parts and the shadowed areas, enhancing the metallic look of the helmet. The colors blend, creating a more polished and realistic appearance. The surroundings of the helmet remain filled with blue and white strokes, maintaining the initial rough layout. [0:02:58 - 0:03:00]: The final seconds show continued work on the helmet, emphasizing the lower part. The brush strokes become finer and more deliberate, ensuring the helmet's surface has a smooth and reflective finish. The blue background around the helmet continues to contrast sharply with the metallic look of the figure.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the painting process just shown in the video?",
        "time_stamp": "00:03:00",
        "answer": "A",
        "options": [
          "A. A hand paints a helmet with silver and blue accents, focusing on refining details.",
          "B. A hand paints a landscape and then switches to a portrait.",
          "C. A hand starts with abstract strokes and then adds a figure wearing armor.",
          "D. A hand repaints a completed painting with new colors."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_130_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:04]: An artist is painting a large, detailed image of a helmeted figure, using a paintbrush to add pink paint to the upper right portion of the helmet. The background is white with some blue and brown accents. [0:08:05]: The artist continues to apply the pink paint in a smooth motion. [0:08:06 - 0:08:08]: The brush moves slightly away, showing a clearer view of the figure, which appears to be a metallic helmet with reflective surfaces, capturing various light reflections. [0:08:09 - 0:08:11]: As the artist works, the details of the painting become more evident with precise strokes filling the canvas. [0:08:12 - 0:08:14]: The paintbrush continues to apply pink paint to the top of the helmet, blending with the existing colors. [0:08:15 - 0:08:17]: The artist's hand and brush are focused on adding finer details and touches to enhance the texture and depth of the helmet. [0:08:18 - 0:08:19]: The video captures the final strokes of pink paint being applied, with the artist making subtle adjustments to blend the colors seamlessly into the overall composition. The light reflects off the metallic surface of the helmet, adding a realistic touch to the artwork.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the artist applying pink paint on the helmet right now?",
        "time_stamp": "00:08:19",
        "answer": "C",
        "options": [
          "A. The upper left portion.",
          "B. The lower right portion.",
          "C. The upper right portion.",
          "D. The lower left portion."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the artist's activity shown in the video just now?",
        "time_stamp": "00:08:20",
        "answer": "A",
        "options": [
          "A. The artist is painting a background with pink paint.",
          "B. The artist is drawing a landscape with various colors.",
          "C. The artist is sketching an abstract figure with fine details.",
          "D. The artist is applying blue and brown accents to a white background."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_130_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:10:49]",
    "captions": "[0:10:40 - 0:10:49] [0:10:40 - 0:10:42]: The video shows a close-up of a painting on an easel, depicting part of a figure in various shades of blue, black, and purple. The figure appears to be wearing a suit with a visible chest piece and straps. The background is a gradient of pink and other warm colors. Several paint tubes and brushes are scattered on the surface in front of the easel.  [0:10:43 - 0:10:49]: The camera starts to dim and goes dark gradually, leaving the screen completely black by 0:10:47.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What objects are scattered behind the easel?",
        "time_stamp": "0:10:42",
        "answer": "B",
        "options": [
          "A. Pencils and papers.",
          "B. Paint tubes and brushes.",
          "C. Books and magazines.",
          "D. Sculpting tools and clay."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_130_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:07]: Three spherical Christmas ornaments are positioned in the center of the frame, each adorned with a large, neatly tied ribbon at the top. The ornament on the left is purple, adorned with the name \"Willow\" in white cursive letters overlaying a large white \"W\". Its ribbon is also purple. The middle ornament is blue, featuring \"Nathan\" in similar white cursive script overlaying a large white \"N\". The ribbon on this ornament is blue. The ornament on the right is pink, with the name \"Honor\" in white cursive lettering overlaying a large white \"H\". This one is tied with a pink ribbon. All the ornaments are glittery, and behind them is a green garland with small, visible branches. The background is a neutral, dark color, and the ornaments are resting on a white surface.  [0:00:08 - 0:00:11]: The screen turns black, and the words \"DIY Glitter Baubles\" appear in the center in white letters. [0:00:12 - 0:00:19]: The scene shifts to a white, textured surface with three clear plastic baubles positioned in a row at the top. To their right, there is a small blue funnel and three tubes of glitter arranged side by side. Two of the glitter tubes are pink and the third is silver. The text \"DIY Glitter Baubles\" is overlaid faintly on this background, indicating the start of a crafting tutorial.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:00:26",
        "answer": "B",
        "options": [
          "A. Filling an ornament with a blue liquid.",
          "B. Rotating a spherical plastic bottle.",
          "C. Wiping an ornament with a paper towel.",
          "D. Holding a pink ornament with a funnel."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_57_real.mp4"
  },
  {
    "time": "0:02:40 - 0:03:00",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:43]: A pair of gloved hands hold a small, spherical object, likely an ornament, over a white quilted background. In front of the hands are three clear plastic globes and three glitter glue tubes, two pink and one purple, positioned at the right side of the frame. The hands rotate the ornament slightly, giving a close-up view of the top. [0:02:43 - 0:02:47]: The hands pick up a plastic syringe filled with a transparent liquid from a metallic lid placed on the surface. They align the syringe with the opening of the ornament, preparing to inject the liquid into it. [0:02:47 - 0:02:54]: Steady pressure is applied to the syringe, injecting the liquid into the ornament. The syringe is immersed in the liquid, filling the plastic ornament. [0:02:54 - 0:02:57]: The hands continue to fill the ornament with the liquid, which gradually rises inside the globe. The surroundings with the background remain unchanged, keeping the focus on the filling process. [0:02:57 - 0:02:59]: The liquid level inside the ornament increases, almost reaching the top. The surroundings and arrangement of items remain the same. [0:02:59 - 0:03:00]: The hands complete the process of filling the ornament with the liquid, removing the syringe from the opening. The ornament is now nearly full, and the items remain in their positions on the background.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the person just do?",
        "time_stamp": "00:03:03",
        "answer": "C",
        "options": [
          "A. Open a clear plastic globe.",
          "B. Rotate the ornament slightly.",
          "C. Remove the syringe from the ornament.",
          "D. Pick up a glitter glue tube."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_57_real.mp4"
  },
  {
    "time": "0:05:20 - 0:05:40",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:21]: A person, wearing black gloves, holds a pink glittery ornament and a piece of paper towel in the hands. Two purple glittery ornaments are positioned on a white, textured surface, with one centered at the upper part and the other to its right. To the right of the ornaments, a small, clear plastic cup and a white object are visible in the background.   [0:05:21 - 0:05:24]: The person continues to hold the pink ornament with the left hand while the right hand now holds a blue funnel, positioned just below the ornament's opening. In the background, the two purple ornaments remain in their initial positions. The clear plastic cup is placed just above the center of the textured surface.   [0:05:24 - 0:05:26]: The hands adjust slightly around the ornament as the person looks closely at it. The blue funnel is still held near the hole of the pink ornament. The background remains unchanged, with the two purple ornaments and the clear cup in their respective positions.   [0:05:26 - 0:05:28]: The person starts inserting the blue funnel into the opening of the pink ornament with careful precision. The clear plastic cup remains on the textured surface, directly below the two purple ornaments which are positioned side by side in the upper part now. [0:05:28 - 0:05:29]: With the funnel now fully inserted into the pink ornament, the person’s left hand shifts slightly towards the bottom of the ornament to secure it. Both purple ornaments remain side by side on the textured surface.   [0:05:29 - 0:05:30]: The left hand releases the pink ornament momentarily and then holds a small red object, possibly the top of the funnel or ornament. The two purple ornaments at the top and the clear plastic cup in the background remain unchanged.   [0:05:30 - 0:05:31]: The left hand, holding the small red object, starts to bring it toward the now empty pink ornament. In the center of the scene, a clear ornament is placed alongside the purple ornaments on the white surface.   [0:05:31 - 0:05:33]: Pouring begins, as the person's right hand is seen holding a blue item with liquid inside the clear ornament. The left hand gently positions the cap on the clear ornament. Three ornaments, one purple, one pink, and the clear one being filled, are carefully arranged.   [0:05:33 - 0:05:34]: The scene focuses on the clear ornament being filled with a blue substance, as the person carefully manipulates it with both hands. The purple and pink ornaments are still at the top with the clear cup seen at the left. [0:05:34 - 0:05:36]: The clear ornament, now filled with blue liquid, is wiped clean with a white paper towel held in the person's left hand. The purple and pink ornaments remain relatively undisturbed in the background.   [0:05:36 - 0:05:37]: The freshly cleaned blue ornament is moved back into view, held securely by the right hand. The left hand holds the blue funnel, indicating another ornament might soon be filled. All three ornaments sit in the central area of the white textured background. [0:05:37 - 0:05:39]: The blue ornament is now fully held by both hands, and it appears the person is carefully examining or adjusting it. The blue funnel remains in the left hand, ready for use. The purple and pink ornaments remain positioned in the background, alongside the clear plastic cup on the left.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:05:39",
        "answer": "B",
        "options": [
          "A. Filling an ornament with a blue liquid.",
          "B. Pouring the object inside the blue ball into the funnel.",
          "C. Wiping an ornament with a paper towel.",
          "D. Holding a pink ornament with a funnel."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_57_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [8:08:54 - 08:08:55]: A black letter \"W\" is positioned prominently in the center of a grid background. Below it, the word \"Willow\" displays in a cursive font. The letters of \"Willow\" are outlined with a black stroke. [08:08:56 - 08:08:59]: A color palette appears in the top left section of the screen. Various colors in square blocks are shown, ranging from shades of yellow to purple. [08:09:00 - 08:09:02]: The outline of the word \"Willow\" is transformed from black to a light purple color. The color palette is still visible, reflecting the selection change. [08:09:03 - 08:09:06]: The color palette disappears from the screen, leaving the focus solely on the design featuring the black \"W\" and the light purple \"Willow\". [08:09:07]: The placement of the cursive \"Willow\" is adjusted slightly, keeping it centered below the \"W\".",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the outline of the word \"Willow\" transform to just now?",
        "time_stamp": "00:08:03",
        "answer": "B",
        "options": [
          "A. Dark blue.",
          "B. Light purple.",
          "C. Bright yellow.",
          "D. Deep red."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_57_real.mp4"
  },
  {
    "time": "0:10:40 - 0:11:00",
    "captions": "[0:10:40 - 0:11:00] [0:00:00 - 0:00:02]: A pair of hands is holding a blue ornament with a sparkly texture. The ornament has a shiny metallic cap at the top. A small grid-patterned adhesive sheet is seen on the table in the background, behind the ornament. The person appears to be peeling off a sticker or decal and applying it to the ornament. [0:00:03 - 0:00:07]: The hands continue to work on applying the sticker to the blue ornament, smoothing it out carefully to remove any bubbles or wrinkles. The background remains the same, with a grid-patterned adhesive sheet positioned in the upper area of the frame.  [0:00:08 - 0:00:15]: The sticker reveals a capital letter \"N\" in white color which is being applied to the center of the blue ornament. The hands ensure that the sticker is firmly pressed onto the surface. The grid-patterned sheet in the background stays in its position. [0:00:16 - 0:00:18]: The hands then pick up another decal, which appears to be a piece of white paper with intricate cursive writing. The person starts to peel the backing off this decal, preparing it to be applied to the ornament. [0:00:19 - 0:00:20]: The cursive writing on the white decal spells out a name. The hands position this decal on the blue ornament just below the large \"N\", carefully aligning it for precise placement. The background continues to show the grid-patterned adhesive sheet.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:11:00",
        "answer": "C",
        "options": [
          "A. Holding a blue ornament next to the grid-patterned adhesive sheet.",
          "B. Smoothing out the sticker of a capital letter \"N.\".",
          "C. Applying a decal with intricate cursive writing to the blue ornament.",
          "D. Peeling the metallic cap off the blue ornament."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_57_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:09]: The video showcases a small storage room or inventory space with shelves stocked with various products, predominantly dairy items. To the left, a cart is filled with cardboard boxes and packs of what appears to be milk or a similar product in red and white packaging. To the right, there are organized shelves with different dairy products, labeled with prices ranging from 15.5 to 39. The person in the video is using a cart full of items and is about to shelve the products. [0:00:10 - 0:00:12]: The focus shifts to the same area with a slightly altered angle, emphasizing the content on the cart and the storage shelves filled with dairy products. The movement suggests the person is preparing to place the products from the cart onto the shelves. [0:00:13 - 0:00:13]: The camera shows a clear niche of shelves filled with items like cream cartons and yogurt cups. The person handling the camera moves closer to the center of the shelves, aligning to address the items for stocking. [0:00:14 - 0:00:15]: The camera maintains its focus on a specific shelf stocked with yogurt cups, placing emphasis on the prices and organization of the products. The person's gloved hand is seen close to the shelf, indicating an imminent action of stocking. [0:00:16 - 0:00:19]: The individual's gloved hand reaches for items on the middle shelf, with clear actions of placing or adjusting dairy products in red and white packaging. The scene shows detailed movement, as the person ensures the products are shelved neatly in the designated area.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the primary product seen on the cart?",
        "time_stamp": "00:00:09",
        "answer": "C",
        "options": [
          "A. Yogurt cups.",
          "B. Cream cartons.",
          "C. Milk or a similar product in red and white packaging.",
          "D. Cheese blocks."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the cart filled with cardboard boxes and packs of products located in relation to the shelves?",
        "time_stamp": "00:00:10",
        "answer": "C",
        "options": [
          "A. To the right.",
          "B. In the center.",
          "C. To the left.",
          "D. Behind the shelves."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_439_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:42]: The video begins in a storage area or stockroom, where various items are stored on carts and shelves. There are stacks of cardboard boxes with a mixture of items, including cartons of colorful products and possibly canned goods. A person, wearing gloves, is handling a cardboard box. [0:02:43 - 0:02:44]: The person moves the cart, which is seen filled with flattened cardboard boxes. The background consists of shelves filled with items, densely packed and organized. [0:02:45 - 0:02:46]: The camera briefly surveys the area, showing long aisles with metal shelves on both sides, filled with various products. There are green stacked boxes and carts loaded with more items and stacked on the left side, while the right side displays more packaged goods.        [0:02:47 - 0:02:48]: The cart is wheeled forward, passing by shelves filled with dairy products such as yogurts and cartons. An area with more stacked boxes and tins is seen, including a red stool placed in the aisle. [0:02:49 - 0:02:51]: The view down the aisle shows more organized shelves packed with goods and large green boxes stacked high. The camera then faces back towards the initial position, moving past the carts and shelves. [0:02:52]: The scene focuses again on the cart filled with empty flattened cardboard boxes. Partially visible is the glove of the person pushing the cart. The walls are plain, and the items on the cart remain in focus, with potentially some items being discarded or organized. [0:02:53 - 0:02:54]: The camera angle changes as the scene shows more shelves filled with diverse items, including jars, cans, and cartons. There are wheeled carts loaded with neatly stacked items of different sizes and formats, moving further down the aisle. [0:02:55 - 0:02:56]: As the video progresses, the person seems to be navigating through the shelves, passing by larger green storage boxes. The packed grocery items show various packaging, and the camera captures the detailed arrangement of goods. [0:02:57 - 0:02:59]: The person moves closer to the shelves to reveal detailed stacks of products, including yogurts, boxes labeled with \"mild yogurt,\" and other dairy goods organized meticulously. The camera movement indicates further inspection or placement of items.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What does the camera capture as it navigates through the shelves?",
        "time_stamp": "0:02:56",
        "answer": "A",
        "options": [
          "A. Detailed arrangement of goods.",
          "B. A person stacking boxes.",
          "C. An empty aisle.",
          "D. A cart being loaded."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "How are the yogurt cartons labeled that the camera captures in detail right now?",
        "time_stamp": "0:02:59",
        "answer": "A",
        "options": [
          "A. \"Mild yogurt\".",
          "B. \"Low-fat yogurt\".",
          "C. \"Greek yogurt\".",
          "D. \"Flavored yogurt\"."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_439_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:47]",
    "captions": "[0:09:40 - 0:09:47] [0:09:40 - 0:09:41]: The video shows a first-person perspective of a person reaching towards a shelf filled with cartons of what appears to be milk or a similar beverage. The cartons are white with red fruit images on the sides. The individual is wearing gloves and a dark jacket. Visible in the background are shelves with various products and a large window to the left, revealing more shelves stocked with beverages. [0:09:42 - 0:09:43]: The person appears to be adjusting or picking up a carton from the right side of the shelf. More cartons, similar in appearance, fill the shelf on the lower and upper levels. [0:09:44]: The individual has removed one carton and is holding it close. Their other hand is positioned to either steady the shelf or to grab another item.  [0:09:45]: The viewpoint shifts to show a broader view of the shelves and the aisle. The person’s hands are visible, showing one hand holding a carton while the other is reaching for another one. The shelves are well-organized, with various products neatly arranged. [0:09:46 - 0:09:47]: The camera now faces down the aisle, showing numerous boxed and packaged goods. There is a cart loaded with more products and stacks of boxes on the floor. The walls are plain, and the overall environment appears to be a storage room or walk-in cooler in a store.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What does the person do after reaching towards the shelf?",
        "time_stamp": "0:09:43",
        "answer": "B",
        "options": [
          "A. Knocks over a carton.",
          "B. Picks up a carton.",
          "C. Rearranges the shelf.",
          "D. Walks away."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_439_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:05]: The video begins with a black screen and a timestamp in the top left corner indicating the time. Over the course of the first few seconds, the screen remains black. [0:00:05 - 0:00:12]: The title \"Round Ornament Box Assembly\" appears at the center of the screen on a black background and stays up for a few seconds. [0:00:12 - 0:00:19]: The scene transitions to a top-down view of a sheet of white cardstock paper placed on top of a decorative paper pad, labeled \"Peppermint.\" The decorative paper pad features a red, white, and silver theme with various festive designs. The person's hands appear, holding the cardstock, and a subtitle \"Heavy weight cardstock works best (110lb / 300gsm)\" appears at the bottom of the screen. The hands rotate the cardstock to different angles to showcase its thickness and texture. Eventually, the hands pull away, revealing the \"Peppermint\" paper pad underneath. The visual of the cardstock is replaced with multiple patterned sheets of paper featuring poinsettia designs. The subtitle reads, \"Using patterned paper is optional, but a lovely touch!\"",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:00:19",
        "answer": "B",
        "options": [
          "A. Holding a decorative paper pad.",
          "B. Displaying some patterned paper.",
          "C. Cutting patterned paper.",
          "D. Writing on the cardstock."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_56_real.mp4"
  },
  {
    "time": "0:03:00 - 0:03:20",
    "captions": "[0:03:00 - 0:03:20] [0:03:01 - 0:03:02]: The video begins with an interface screen visible on a computer monitor. On the left, there is a column with a list titled \"1 Mat,\" \"2 Make It,\" and \"3 Make.\" Each item has an accompanying icon. The center of the screen displays a grid with a design layout for cutting, featuring solid and dashed lines. The top of the screen shows a toolbar with various options. [0:03:03 - 0:03:04]: The scene changes to a screen with a message suggesting to \"Connect machine\" and \"Select Cricut device.\" The message \"To continue, please connect your Maker\" is displayed in the center. [0:03:05 - 0:03:07]: A message saying \"Cut out and assembly\" is displayed against a black background in white text. [0:03:08]: The machine's lower tray is shown in a darkly lit setting, providing a closer look at the rollers and cutting mechanism. [0:03:09 - 0:03:10]: The scene brightens, showing the Cricut machine, its rollers, and the cutting housing clearer. The machine is opened, revealing the interior components in silver and white tones. The background has a mat with gridded lines. [0:03:11]: A light blue mat is loaded into the machine. On the mat, there is a white sheet of material centered for cutting. The word \"Cricut\" is visible on the top left of the mat, and there is a grid pattern on the blue surface. [0:03:12]: A hand appears in the frame, pressing a circular button with a \"C\" symbol on the right side of the Cricut machine. [0:03:13]: The white mat is fed into the machine as the rollers start to pull it in. The cutting tool assembly hovers over the sheet. [0:03:14]: The cutting tool assembly moves to the starting position, preparing to begin the operation. The white mat is now fully aligned inside. [0:03:15]: The cutting mechanism starts to work, moving horizontally across the white sheet. The background reveals more of the mat's grid lines. [0:03:16 - 0:03:17]: The cutting process continues with the tool running precisely along the designated path on the material. [0:03:18]: A close-up of a hand holding a scoring stylus tool, labeled \"Cricut,\" is visible. The focus is on the stylus, which has a fine tip designed for scoring. [0:03:19]: The hand is now slightly repositioned, revealing more of the tool as it's about to be inserted into the machine. [0:03:20]: The stylus is now being inserted into a holder slot on the machine. The hand's movement suggests careful placement. [0:03:21]: The stylus is secured into the machine, preparing for the next task. The hand is withdrawing from the view. [0:03:22]: The machine is ready with the scoring stylus in place. The blue mat with the white material is still positioned correctly inside.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the machine doing right now?",
        "time_stamp": "0:03:20",
        "answer": "C",
        "options": [
          "A. Being turned on.",
          "B. Loading the material into the tray.",
          "C. The scoring stylus is being inserted.",
          "D. Performing the cutting operation."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_56_real.mp4"
  },
  {
    "time": "0:06:00 - 0:06:20",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:01]: A person is using both hands to hold a white piece of paper on a dark gray gridded work surface. There are several other white paper pieces and one pink piece with a floral pattern on the table. The person is folding the paper, creating creases along pre-scored lines, and aligning an angular cut section with a round hole in the middle of the paper. [0:06:01 - 0:06:03]: The person's hands are manipulating the paper, creating more pronounced folds along the score lines around the round hole. Additional white paper pieces are still visible on the table, as well as the pink floral-patterned paper. [0:06:03 - 0:06:05]: The person's hands continue to fold the paper carefully, ensuring the creases are firm and neat. The round hole in the center becomes more defined as the folds take shape. [0:06:05 - 0:06:06]: The hands are momentarily removed from the white paper on the table. The workspace remains organized, with other white and pink floral-patterned paper pieces still positioned around the main work area. [0:06:06 - 0:06:07]: The person picks up the white folded piece of paper, examining and adjusting the folds for precision. [0:06:07 - 0:06:09]: The person continues to hold and rotate the folded paper, adjusting its angles and ensuring the creases around the round hole are accurate and neat. [0:06:09 - 0:06:11]: The person presses down on the creases around the round hole, refining the folds further.  [0:06:11 - 0:06:15]: The person rotates and closely examines the entire folded piece of paper and makes additional adjustments, ensuring the edges and creases are aligned correctly. [0:06:15 - 0:06:17]: The person thoroughly inspects the folded paper, refining the structural integrity of the creases. They ensure the angular cuts and the round hole align well with the rest of the paper’s shape. [0:06:17 - 0:06:19]: The person slightly opens and closes the folded paper, testing the flexibility and alignment of all the parts, particularly focusing on how the round hole integrates with the rest of the paper structure. The person then places the paper on the work surface, exhibiting its final folded form.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "0:06:11",
        "answer": "B",
        "options": [
          "A. Folding another piece of white paper from the table.",
          "B. Rotating and closely examining the folded piece of paper.",
          "C. Removing hands from the white paper on the table.",
          "D. Picking up the pink floral-patterned paper."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_56_real.mp4"
  },
  {
    "time": "0:09:00 - 0:09:20",
    "captions": "[0:09:00 - 0:09:20] [0:08:00 - 0:08:01]: The video begins with a top-down view of a crafting workspace. Several pieces of white paper with various cutouts and fold lines are scattered across the table. In the center of the frame, a pair of hands is holding a piece of white cardstock with a circular cutout in the middle. The hands are positioned near the bottom of the screen, while the other paper pieces are spread out above. [0:08:02 - 0:08:03]: The hands adjust the piece of cardstock, folding a flap back and forth. The shape of the cardstock remains consistent, showcasing a rectangular piece with a large circular cutout in the bottom third. The background consists of a cutting mat with a grid pattern. [0:08:04 - 0:08:05]: One of the hands reaches to grab another piece of cardstock from the top right corner. This piece has a small circular cutout and some fold lines. The main piece with the large circle remains centrally positioned. [0:08:06 - 0:08:07]: The hands move the piece with the flower-like pattern, which is now seen near the top right corner. A patterned paper displaying a design of pink flowers on a white background is visible. The main focus remains on the central piece with the circular cutout. [0:08:08 - 0:08:09]: One hand holds the circular cutout piece while the other hand grabs a bottle of glue from the right side of the frame. The glue bottle is labeled \"450 Quick Dry Adhesive.\" The patterned paper lies toward the upper right corner. [0:08:10 - 0:08:14]: The hands start applying glue around the edge of the circular cutout on the central piece of cardstock. The glue bottle is angled downward, and the person carefully traces the inner perimeter of the circle. [0:08:15 - 0:08:19]: The gluing process continues, with the person ensuring an even distribution of adhesive around the entire circular cutout. The hands steadily move with precision, indicating care and attention to detail in the crafting process. The grid-lined cutting mat remains in the background, emphasizing the organized workspace.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are the hands doing right now?",
        "time_stamp": "0:09:19",
        "answer": "A",
        "options": [
          "A. Tracing the inner perimeter of the circular cutout.",
          "B. Folding a rectangular piece of cardstock.",
          "C. Reaching for a piece of patterned paper.",
          "D. Adjusting a bottle of quick-dry adhesive."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_56_real.mp4"
  },
  {
    "time": "0:12:00 - 0:12:20",
    "captions": "[0:12:00 - 0:12:20] [0:12:00 - 0:12:02]: A person holds a bottle of glue in their right hand, positioned above a flat piece of cardstock on a grid-patterned work surface. The cardstock is unfolded and features sections with snowflake patterns in pink and grey; [0:12:02 - 0:12:03]: The person continues to hold the glue bottle, this time slightly closer to the cardstock, still maintaining the same position on the work surface; [0:12:03]: The person’s left hand is closer to the center of the cardstock, and the glue bottle is brought even closer, ready for application; [0:12:04]: The person sets down the glue bottle and uses both hands to start folding the cardstock. The focus is on folding the edges upwards, which have snowflake patterns; [0:12:05]: The person continues to fold the cardstock, focusing on aligning the edges properly. Both hands are used to ensure the folds are crisp and precise; [0:12:06]: The cardstock is flipped over to show the other side, now partially constructed into a box shape. The person’s hands continue to manipulate the cardstock to form the basic structure; [0:12:07]: The box shape starts to take more form, with the person holding the partially completed box from the sides. The interior of the box is now visible; [0:12:08 - 0:12:10]: The person makes further adjustments to the box, ensuring that the sides are straight and the folds are precise. Both hands work in unison to create sharp creases and firm edges; [0:12:11]: Both hands press the sides of the box inward, emphasizing the alignment and structure. The person ensures that the box shape is sturdy; [0:12:12 - 0:12:13]: The person’s hands hold the box upright, making final adjustments to the sides. The bottom flap is positioned outward temporarily; [0:12:14 - 0:12:15]: The box is now almost fully assembled, with the hands aligning the bottom flap. The flap has a circular window cutout at the top, showing a snowflake design around the edges; [0:12:16 - 0:12:18]: The person completes the assembly of the box, with the front flap being folded up and aligned. Both hands hold the sides to ensure the shape is secure; [0:12:19]: The full construction of the box is shown, with the circular window at the front and the pink snowflake pattern across the visible surfaces. The person’s hands are positioned at the sides, making final adjustments.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "0:12:20",
        "answer": "A",
        "options": [
          "A. Assembling the final components of the box.",
          "B. Applying glue to the cardstock.",
          "C. Cutting out snowflake patterns.",
          "D. Placing the completed box aside."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_56_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "Which make of car is being driven right now?",
        "time_stamp": "00:00:35",
        "answer": "D",
        "options": [
          "A. Ferrari.",
          "B. Lamborghini.",
          "C. McLaren.",
          "D. Porsche."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_253_real.mp4"
  },
  {
    "time": "[0:07:46 - 0:07:51]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What brand of car is being driven right now?",
        "time_stamp": "00:07:00",
        "answer": "D",
        "options": [
          "A. Ford.",
          "B. BMW.",
          "C. Audi.",
          "D. Porsche."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the current oil temperature displayed on the dashboard?",
        "time_stamp": "00:07:47",
        "answer": "D",
        "options": [
          "A. 87.5.",
          "B. 85.0.",
          "C. 94.3.",
          "D. 98.0."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_253_real.mp4"
  },
  {
    "time": "[0:11:39 - 0:11:44]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the current brake bias setting displayed on the dashboard?",
        "time_stamp": "00:11:43",
        "answer": "D",
        "options": [
          "A. +1.00.",
          "B. -0.30.",
          "C. +2.00.",
          "D. -3.00."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_253_real.mp4"
  },
  {
    "time": "[0:15:32 - 0:15:37]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which brand's logo is visible at the trackside right now?",
        "time_stamp": "00:15:36",
        "answer": "D",
        "options": [
          "A. Shell.",
          "B. ExxonMobil.",
          "C. BP.",
          "D. TotalEnergies."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_253_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a view of a racing track where two small remote-controlled cars, one blue and one white, are driving next to each other. Both cars are approaching a curve, and there are orange barriers along the track. In the background, there is a banner with the text \"REVOLUTION DESIGN\" in white, alongside a barrier fence. [0:00:01 - 0:00:02]: The blue and white remote-controlled cars continue to move along the race track parallel to each other. The orange barriers remain visible on the left, and there is another blue banner with \"FALK\" and \"High Performance\" in white text on the right. [0:00:02 - 0:00:03]: The cars are navigating the curve, with the blue car slightly ahead, and the white car beginning to drift towards its inside. [0:00:03 - 0:00:04]: The blue car is making its way around the curve successfully. The white car is now behind, also navigating the curve. The track's surroundings include barricades made of red and white blocks. [0:00:04 - 0:00:05]: The scene widens to show a larger section of the track. The blue car is now further along the path, and the white car is still on the curve. Additional cars appear further back on another section of the track. [0:00:05 - 0:00:06]: The white car approaches another curve lined with red and white barriers, while the blue car has moved out of view. The background includes a wider section of the racing area with multiple paths and a few other remote-controlled cars. [0:00:06 - 0:00:07]: The white car gets closer to the barriers and appears to make contact with the red and white blocks as it turns around the curve. [0:00:07 - 0:00:08]: The white car is almost stationary, facing the barriers after having seemingly bumped into them. [0:00:08 - 0:00:09]: The white car attempts to get back on track, maneuvering away from the barriers it initially contacted. [0:00:09 - 0:00:10]: The white car successfully moves away from the barriers and back onto the track’s path, continuing its way around the curve. [0:00:10 - 0:00:11]: The white car accelerates and moves further along the track, now free from the barriers. [0:00:11 - 0:00:12]: The white car continues to move forward on the track. There are other remote-controlled cars ahead on a different section of the track. [0:00:12 - 0:00:13]: A wider view shows multiple remote-controlled cars navigating the race course. The white car is moving towards the middle of the track, while other cars are seen further ahead. [0:00:13 - 0:00:14]: The white car continues to progress on the track, passing a section with debris and other remote-controlled cars that appear stationary or moving slowly. [0:00:14 - 0:00:15]: The camera follows the white car as it moves further along the track, getting closer to the center. Other cars are situated on different parts of the track. [0:00:15 - 0:00:16]: The video shows a silver remote-controlled car moving at speed, further away from the white car. The entire track is visible with defined racing lines and curves. [0:00:16 - 0:00:17]: The white car continues to navigate the track, getting closer to a large blue banner that reads \"High Performance Tyres\" in white text. [0:00:17 - 0:00:18]: The white car dodges a black structure on the track as it maneuvers around other remote-controlled cars. [0:00:18 - 0:00:19]: The white car maintains a steady pace on the track, remaining in view as it speeds along with other cars. [0:00:19 - 0:00:20]: The white car is in motion approaching the end of the visible track section. The background shows other remote-controlled cars and various track elements, completing the scene.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the main color of the two remote-controlled cars shown in the video right now?",
        "time_stamp": "0:00:04",
        "answer": "B",
        "options": [
          "A. One car is blue and black, while the other car is white.",
          "B. one car is blue with black, another car is white.",
          "C. One vehicle is painted blue with black accents, and the other is white.",
          "D. One car features a blue and black color scheme, and the other car is entirely white."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_483_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:20 - 0:01:40] [0:01:20 - 0:01:22]: At the start, a red and white remote-controlled car is visible on a track, moving ahead of a black and silver remote-controlled car which is slightly behind. The settings seem to take place indoors on a smooth, concrete surface, marked with white lines resembling a racetrack. [0:01:22 - 0:01:28]: The red and white remote-controlled car navigates a curve to the left of the viewer's perspective. Nearby, small piles of what looks like red bricks or debris line the edge of the track, forming a barrier. Aside from this, a portion of a yellow and red Shell gas station model is visible in the background. [0:01:28 - 0:01:39]: The setting changes and multiple remote-controlled cars, predominantly black with various designs and LED lights, are seen driving around a larger, more intricately designed track. This track includes green, grass-like areas with artificial plants and trees in raised sections. It is adorned with various road signs, lane markings, and small barriers. The audience observing from behind metal safety railings is also visible. [0:01:39 - 0:01:40]: Various remote-controlled cars continue to race and drift through the winding track, with more attention placed on their rapid movements and the intricate racecourse elements, including tiny road signs, lane markings, and detailed surroundings. The sound of motors and interactions of the cars on the track gives life to the competitive atmosphere.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What change occurs in the scene of the remote-controlled car race?",
        "time_stamp": "00:01:32",
        "answer": "D",
        "options": [
          "A. The track becomes simpler with fewer obstacles.",
          "B. The cars switch to a gravel surface.",
          "C. The cars move to an outdoor setting.",
          "D. The scene shifts from one car to another three cars."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_483_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:46]: A small, purple remote-controlled car is moving on a glossy, black track. The track has bright lights reflecting off its surface. The car is driving along the track, which has several white lines and markings, and is surrounded by various miniature landscapes and objects. Plastic greenery, flowers, and miniature street lamps are visible near the track's edges;  [0:02:47 - 0:02:51]: The car continues to navigate the track, making turns and passing by small model plants. Occasionally, the track is bordered by white stripes, and the surroundings include meticulously crafted miniature items like decorative bushes and small structures; [0:02:52 - 0:03:00]: The purple RC car continues to move smoothly along the track, taking curves and turns with noticeable agility. In some sections, the track is flanked by small model barriers and more complex layouts indicating designated pathways. The car's rear lights are illuminated, contrasting against the dark track.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the remote-controlled car on the track right now?",
        "time_stamp": "0:02:44",
        "answer": "C",
        "options": [
          "A. Red.",
          "B. Green.",
          "C. Purple.",
          "D. Blue."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Event Understanding",
        "question": "What surrounds the track where the purple remote-controlled car is moving?",
        "time_stamp": "0:03:00",
        "answer": "A",
        "options": [
          "A. Various miniature landscapes and objects.",
          "B. Life-size trees and bushes.",
          "C. Tall buildings and bridges.",
          "D. Open water bodies and sand pits."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_483_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:05]: A first-person perspective shows a multi-colored remote-controlled car with various logos and designs on its body moving on a racetrack. The car is heading towards a blue car visible in the background near a turn;  [0:04:06 - 0:04:07]: The remote-controlled car maneuvers around a circular arrangement with rocks and artificial grass in the center, with white dashed lines marking the track boundary.  [0:04:08 - 0:04:09]: The car continues its movement, passing by a green artificial turf featuring scattered boulders and rocks, and another car is seen in the background;  [0:04:10 - 0:04:11]: The car is now on a wider part of the track, approaching a section with white barriers and overhead signs over the track;  [0:04:12 - 0:04:13]: It continues to navigate through the racetrack, entering a complex turn on a wet, shiny surface;  [0:04:14 - 0:04:15]: The car is making its way around more tight curves, passing small artificial trees and plants, while spectators watch from behind barriers;  [0:04:16]: The remote-controlled car proceeds further, following the track markings closely while navigating sharper turns;  [0:04:17]: The car approaches the final bend of the tight complex, showing steady control despite the wet conditions of the track;  [0:04:18 - 0:04:19]: It successfully completes the complex turns, heading towards a straight path on the track;  [0:04:20]: The car moves at a consistent speed towards the next turn, with its headlights illuminating the shiny racetrack.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the condition of the track right now?",
        "time_stamp": "00:04:45",
        "answer": "A",
        "options": [
          "A. Dry and smooth.",
          "B. Wet and shiny.",
          "C. Dusty and rough.",
          "D. Muddy and slippery."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_483_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The scene opens with a winding, black slot car race track running from the foreground to the middle of the frame, curving left and upward. The backdrop features a blue sky with scattered white clouds and a lush green landscape with trees atop small hills. A chain-link fence on the left side of the track separates the track from a viewer's area behind it, complete with a sign bearing an illegible logo. [0:00:02]: Two small, colorful toy cars, one blue and the other red, appear on the track, racing from the background to the foreground. [0:00:03]: The scene transitions to a black screen with glitchy, 3D-styled text in bold red and yellow letters: \"NEXT GEN.\" Below it reads, \"DIECAST RACING,\" with a subtitle in smaller white text: \"The Next Generation of Diecast Racing.\" [0:00:05 - 0:00:06]: The text screen continues, stabilizes, and the glitch effect minimizes, with the same text layout displayed steadily. [0:00:07 - 0:00:12]: The scene shifts to a different part of the slot car race setup, showing an array of stationary toy cars on a black track. From the far left: a blue car, followed by two purple cars, two silver cars with colorful spoilers (one green, one red), a green car, and another blue car. This setup is backed by a detailed model environment including a fence, a parking area with toy forklifts, and a building with a banner that reads \"Walmart - Save money. Live better.\" Small advertisements for real-life brands are visible along the side of the track. [0:00:13]: White glitch-styled text appears overlaying the cars, reading \"NEXT-GEN PISTON CUP.\" Below it reads, \"Race 2 - Round 1.\" [0:00:14 - 0:00:15]: The camera zooms in slightly on the cars as the text remains on screen. [0:00:16 - 0:00:18]: The screen remains focused on the lineup of toy cars. The white text transitions to \"Group 5 and 6.\" [0:00:19 - 0:00:20]: The text \"Group 5 and 6\" stays on screen, hovering over the static toy car lineup and detailed model environment.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is depicted on the container behind the toy car?",
        "time_stamp": "0:00:12",
        "answer": "B",
        "options": [
          "A. \"NEXT GEN\".",
          "B. \"Walmart - Save money. Live better.\".",
          "C. \"DIECAST RACING\".",
          "D. \"NEXT-GEN PISTON CUP\"."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_485_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:13]: The video shows a scoreboard for Group 5 - Heat 2 in what appears to be a model car racing event. The scoreboard lists four racers and their scores. It displays \"#76 Will Rusch (*2) with 10 points, #61 Tom W. with 6 points, #19 Danny Swervez with 4 points, and #31 Cam Spinner with 2 points.\" The background includes a green, grassy hill with some trees scattered throughout. A red helicopter is flying nearby, adding to the scene's visual details. The track is black and sits in the lower portion of the frame while the vivid green grass extends upward. The setting appears to be an outdoor diorama or model race track. [0:03:14 - 0:03:20]: The video then transitions to a close-up of the starting grid for the next race, labeled as \"Group 5 - Heat 3.\" The cars, vividly colored, are positioned side by side on the starting track. The front row shows the #31 Cam Spinner in a bright blue car and #19 Danny Swervez in a darker blue car with purple accents to the left. In the background, additional trees and painted clouds on a sky-blue backdrop are visible, creating a scenic atmosphere. The track segment included appears orderly and well-designed, focusing on the anticipation before the race begins.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What number is displayed on Tom W.'s car on the scoreboard?",
        "time_stamp": "00:03:29",
        "answer": "B",
        "options": [
          "A. 31.",
          "B. 61.",
          "C. 19.",
          "D. 76."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_485_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:05]: A grey toy race car with neon green wheels and a neon green rear spoiler is positioned on a black racetrack. The car has a sticker labeled \"035\" on the side. A fence with a blurred background of colorful toy cars is visible. Text overlays provide information: \"Group 6\", \"#35 Ronald\", \"Team: VCD132 Racing\", \"Position: 26th (9 pts)\", \"Heat Wins: 0\", \"Fastest Time: 8.348\".  [0:06:06]: The grey car is seen without any text overlays or identifying labels, now slightly repositioned on the track. [0:06:07 - 0:06:12]: A transition to a blue toy race car with black wheels, yellow number \"117\" printed on the side, and decals simulating a racing graphic. This car has a series of text overlays: \"Group 6\", \"#117 Spikey Fillups\", \"Team: Wreckage Bluff Raceway\", \"Position: 15th (11 pts)\", \"Heat Wins: 0\", \"Fastest Time: 9.545\".  [0:06:13]: The blue car remains visible without any text overlays, positioned similarly on the racetrack with the same fenced and toy car filled background. [0:06:14 - 0:06:19]: Another transition to a green toy race car with yellow and black accents, featuring the number \"82\" on its side. This car also has text overlays: \"Group 6\", \"#82 Conrad Camber\", \"Team: G4 Diecast Racing\", \"Position: 14th (11 pts)\", \"Heat Wins: 1,\" and \"Fastest Time: 10.317\". The car is positioned on the racetrack with the same fenced and toy car filled background.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the number on the grey toy race car shown right now?",
        "time_stamp": "00:06:05",
        "answer": "C",
        "options": [
          "A. 117.",
          "B. 82.",
          "C. 035.",
          "D. 26."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Counting",
        "question": "How many different toy race cars are shown right now?",
        "time_stamp": "00:06:34",
        "answer": "C",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 5."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_485_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:09]: The video begins with a close-up shot of a miniature racetrack with toy cars lined up and ready to race. There are four toy cars in two rows. The backdrop features a blue sky with fluffy white clouds and a few miniature trees. The left side of the scene has the sign \"Thunder Mountain Speedway.\" Two cars in Row 1 are labeled #35 Ronald and #117 Spikey Fillups, while two cars in Row 2 are labeled #82 Conrad Camber and #80 Dan Carcia;  [0:09:10 - 0:09:14]: The toy cars start moving down the racetrack, gaining speed. The camera follows the cars, capturing a more dynamic angle that showcases their movement and the surrounding miniature scenery, including more trees and painted background sky;  [0:09:15 - 0:09:16]: The perspective changes to show the cars navigating a bend in the track. Additional miniature elements appear, including a field with race-themed decorations and various miniatures such as other toy vehicles and a small shed;  [0:09:17 - 0:09:18]: The leading toy cars speed ahead on the racetrack, passing by another section of the model scene, which includes a fenced area with more detailed miniature items like toolkits and additional toy cars. One particular car, #35 Ronald, can be seen ahead with #117 Spikey Fillups not far behind;  [0:09:19 - 0:09:20]: The race continues with the toy cars accelerating through the track. The perspective focuses on the cars, giving a sense of their speed and competition as they pass another portion of the race setup with fences and various miniature decorations.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which car is labeled #117?",
        "time_stamp": "00:09:20",
        "answer": "C",
        "options": [
          "A. Ronald.",
          "B. Conrad Camber.",
          "C. S.Fillups.",
          "D. Dan Carcia."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_485_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:06]: The video starts with a detailed close-up of several round, decorative milestone discs arranged on a flat surface. The discs feature different texts that indicate various time periods: “Earth side”, “1 week”, “1 month”, “2 months”, “3 months”, “4 months”, “5 months”, “6 months”, “7 months”, “8 months”, “9 months”, “10 months”, “11 months”, and “12 months.” The discs have scalloped edges, and each one displays decorative patterns in white against background colors like brown, green, and beige. They are neatly laid out, overlapping slightly, and filling the frame completely. [0:00:07 - 0:00:12]: As the camera zooms out slightly, two discs are highlighted. One disc is light brown with the text “1 week,” while the other is dark brown with the text “Earth side.” These discs are set against a plain gray background apart from the others. They lie flat on the surface, and every detail, including the white, intricate scalloped pattern, is clear. [0:00:13 - 0:00:14]: The screen momentarily fades to black. [0:00:14 - 0:00:17]: The words “Baby Milestone Disc” appear centered on the black screen in a straightforward, white font. [0:00:18 - 0:00:19]: The visual transitions to a light gray background with the title \"SOME OF THE MATERIALS YOU WILL NEED\" displayed prominently in the center in black lettering. Below this text, there is a note stating, \"*More information in description*\". The bottom of the frame features a subtle floral pattern in white.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the screen displaying right now?",
        "time_stamp": "00:00:15",
        "answer": "B",
        "options": [
          "A. A detailed close-up of milestone discs.",
          "B. The words “Baby Milestone Disc” centered on a black screen.",
          "C. Two highlighted milestone discs.",
          "D. A light gray background with a title and note."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_52_real.mp4"
  },
  {
    "time": "0:01:40 - 0:02:00",
    "captions": "[0:01:40 - 0:02:00] [0:01:40 - 0:01:45]: A person wearing a purple sweater is holding an orange circular object in their left hand while using their right hand to apply orange paint onto the object with a foam brush. The foam brush has a black rectangular sponge and a wooden handle. On the table in front of them, there are five small open paint cans arranged in a row. The paint colors are, from left to right, brown, light orange, white, orange, and green. [0:01:46]: The person begins dipping the foam brush into the orange paint can. [0:01:47 - 0:01:55]: The person then switches to painting a round object using the white paint from the third can. They steadily apply the paint in smooth, even strokes. The open paint cans, from left to right, continue to be visible, showing the following colors: brown, light orange, white, orange, and green. [0:01:56 - 0:01:59]: The person proceeds to use the foam brush to apply green paint from the fifth can onto another round object. As they paint, the texture and evenness of the paint are clearly visible. The paint cans remain in the same order and position on the table. The video ends at [0:01:59] with text that appears in the lower-left corner of the screen advising to \"Let dry for 30 minutes.\" The person places the foam brush aside.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "0:01:47",
        "answer": "A",
        "options": [
          "A. Switch to painting a round object with white paint.",
          "B. Apply orange paint onto an object with a foam brush.",
          "C. Dip the foam brush into the green paint can.",
          "D. Place the foam brush aside."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_52_real.mp4"
  },
  {
    "time": "0:03:20 - 0:03:40",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:21]: On a grey table, there are several items. Starting from the left, there is a white roll of vinyl with intricate patterns faintly visible on its surface. To the right of the roll, there is a stack of circular discs in varying shades of orange and yellow. A person’s hands, wearing a burgundy sweater, hold a small white tool with a hook.  [0:03:21 - 0:03:22]: The person holds a square sheet of transparent material with a grid pattern on it. The circular discs and the white roll of vinyl are still in the same position on the table. [0:03:22 - 0:03:23]: The individual places the square sheet flat on the table, aligning it with the grid lines. The person's fingers press down around the center.  [0:03:23 - 0:03:25]: Using a yellow and white tool, the person rubs over the transparent sheet, likely ensuring proper adhesion. [0:03:25 - 0:03:26]: The individual continues smoothing the sheet with the tool, making sure it is securely pressed onto the surface below.  [0:03:26 - 0:03:28]: The person begins to peel back the transparent sheet at a corner, revealing a white, scalloped, intricate design underneath. The designs on the roll and the stack of circular discs remain unchanged. [0:03:28 - 0:03:29]: The design says \"Earth side\" within the scalloped shape. The person peels back the remaining transparent sheet, fully exposing the design. [0:03:29 - 0:03:30]: The support tool has been set aside, and the focus is on carefully removing the transparent film without disturbing the design. [0:03:30 - 0:03:31]: With the film fully removed, the person picks up one of the slightly larger orange discs.  [0:03:31 - 0:03:34]: They position the disc centrally beneath the grid sheet, aligning it so the \"Earth side\" design is centered on top. The design looks well-centered on the disc. [0:03:34 - 0:03:36]: From the top view, both hands hold the grid sheet, ensuring the design remains pressed on the orange disc. [0:03:36 - 0:03:39]: Returning to the smoothing tool, the person thoroughly presses over the design on the disc. The objective is to ensure the design is fully secured onto its new surface.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "0:03:39",
        "answer": "A",
        "options": [
          "A. Smoothing a design onto an brown disc.",
          "B. Peeling back a transparent sheet from the table.",
          "C. Holding a white roll of vinyl with intricate patterns.",
          "D. Aligning a transparent sheet with grid lines."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_52_real.mp4"
  },
  {
    "time": "0:05:00 - 0:05:20",
    "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:02]: The video starts with a black screen displaying white text that reads, “How To Assembly The Gift Box Template.” The surroundings are dark, and the text is centered on the screen.  [0:05:03 - 0:05:07]: The camera captures a package of “Kraft Brown Cardstock” paper. The package, covered in clear plastic, is held by two hands near the bottom corners. The label indicates it’s 80lb cover, 12” x 12”, with 50 sheets included. The American flag and the “Clear Path Paper” logo are also visible. [0:05:08]: A hand holds a Cricut tool above a machine, positioned horizontally across the center. The machine’s lid is open, revealing the inside components with clear light white and pink accents. [0:05:09]: The hand inserts the Cricut tool into the slot labeled \"A\" in the machine. The other components remain static. [0:05:10]: The machine's inner part is visible, with two slots labeled \"A\" and \"B\". The paper tray area appears empty, with a metallic bar running horizontally. [0:05:11]: The brown cardstock paper is secured to a blue Cricut mat, positioned inside the machine. The mat has the label “LightGrip” and a grid marked with measurements. [0:05:12 - 0:05:13]: The Cricut machine begins to operate, with the tool moving across the paper. The brown cardstock is being prepared for cutting. [0:05:14 - 0:05:15]: Close-up of the machine cutting the cardstock. The blade traces out shapes on the paper. The mat and paper move slightly as the machine works. [0:05:16 - 0:05:17]: The machine continues outlining shapes on the cardstock. Some distinct cut lines are visible, forming parts of the gift box template. [0:05:18]: The finished cut-outs are ready, clearly showing defined shapes on the brown cardstock. The Cricut mat with the cut shapes is inside the machine. [0:05:19]: The Cricut tool is being lifted, and the cut paper is being prepared for removal from the mat. The frame cuts align perfectly with the mat’s grid lines.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the person just do?",
        "time_stamp": "00:05:18",
        "answer": "A",
        "options": [
          "A. Lift the Cricut tool and prepare to deliver the cut paper.",
          "B. Secure the cardstock to the mat.",
          "C. Insert the Cricut tool into the machine.",
          "D. Begin cutting the cardstock with the machine."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_52_real.mp4"
  },
  {
    "time": "0:06:40 - 0:07:00",
    "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:44]: The scene begins with two hands holding an almost-assembled brown cardboard box over a plain, light gray background. Both hands work together to fold the sides of the box inward, securing it into shape. The box is oriented with its open end facing up as it is gently manipulated with careful precision. [0:06:45 - 0:06:47]: The hands, still holding the cardboard box, proceed to fold down the inner flaps, followed by securing the external sides to form a closed base. Another white object appears near the box but is not interacted with during this time frame.  [0:06:48 - 0:06:50]: The view captures both hands ensuring that the cardboard box holds its newly formed shape. The box is now oriented with the seam side down and the open top directed upwards, showcasing a smooth external surface. The white cylindrical object remains to the side, and the hands rotate the box to ensure that the base is fully sealed and secure. [0:06:51 - 0:06:54]: Next, the focus shifts as the hands hold a white paper with a vinyl sticker attached. Using a pointed tool, one hand carefully peels away excess material, revealing distinct text and designs adhered to the paper. The white object remains in the background. [0:06:55 - 0:06:57]: The peeling continues meticulously, with the hands working in tandem to remove small pieces of the excess material, ensuring the main design remains intact. The corner of the decal begins to lift, showing the contrast between the white backing paper and the design on it. [0:06:58 - 0:06:59]: Finally, one of the hands lifts the now-cleared vinyl decal, using the other hand to fully detach the sticker from the backing paper. The box remains at the top of the frame, and the hands prepare to align the decal, ensuring it is ready for the next step in the crafting process.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the hands do just now with the vinyl sticker?",
        "time_stamp": "0:06:59",
        "answer": "A",
        "options": [
          "A. Lifting the cleared vinyl decal from the backing paper.",
          "B. Securing the sides of the cardboard box.",
          "C. Peeling away excess material from the vinyl sticker.",
          "D. Rotating the box to ensure the base is sealed."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_52_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the large vehicle to the right displaying on the side?",
        "time_stamp": "00:00:11",
        "answer": "B",
        "options": [
          "A. A city map.",
          "B. An advertisement for The Lion King musical.",
          "C. A tourist guide logo.",
          "D. A food delivery service logo."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_391_real.mp4"
  },
  {
    "time": "[0:02:02 - 0:02:07]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is written on the blue bus's side near the top?",
        "time_stamp": "00:02:04",
        "answer": "A",
        "options": [
          "A. Starr.",
          "B. City Lights Entertainment.",
          "C. Broadway Tours.",
          "D. Manhattan Transit."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_391_real.mp4"
  },
  {
    "time": "[0:04:04 - 0:04:09]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the predominant color of the vintage car on the right?",
        "time_stamp": "00:04:02",
        "answer": "C",
        "options": [
          "A. Blue.",
          "B. Red.",
          "C. White.",
          "D. Black."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_391_real.mp4"
  },
  {
    "time": "[0:06:06 - 0:06:11]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What does the van on the right promote?",
        "time_stamp": "00:06:09",
        "answer": "C",
        "options": [
          "A. A moving company.",
          "B. A food delivery service.",
          "C. Sightseeing tours.",
          "D. Outdoor adventures."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_391_real.mp4"
  },
  {
    "time": "[0:08:08 - 0:08:13]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the building on the left with a sign?",
        "time_stamp": "00:08:08",
        "answer": "B",
        "options": [
          "A. Bank of America.",
          "B. Hilton Garden Inn.",
          "C. Holiday Inn Express.",
          "D. Wells Fargo."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_391_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video begins with an animated introduction, showing a large white number '10' on a light blue background, with a knife and fork arranged in a clock-like manner on a round white plate. Following this, a man dressed in a white chef's jacket appears on screen, standing in front of a background with the text \"RAMSAY in 10.\"  [0:00:04 - 0:00:06]: The video transitions to a kitchen scene showing sleek black and white cabinetry with glass-fronted doors. Various kitchen appliances and utensils are visible on the countertops. The man from the introduction, dressed in a casual black t-shirt and gray pants, stands in front of the stove, holding a dish towel. [0:00:07 - 0:00:09]: He appears to be explaining something enthusiastically, smiling and gesturing with his hands. He holds a dish towel in one hand and laughs while talking. Text reading \"Previously Recorded Live\" appears at the top right of the frame. [0:00:10 - 0:00:14]: The man continues his explanation, alternating between looking at the camera and looking down at the counter. His expressions and hand movements show active engagement in what he's discussing. The phrase \"Previously Recorded Live\" remains on screen. [0:00:15 - 0:00:19]: The camera angle changes slightly, centering the man more prominently in the frame. He continues to talk, occasionally pausing for emphasis. His demeanor becomes slightly more serious as he progresses. The kitchen background remains consistent throughout, maintaining a clean and organized appearance.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the man from the introduction holding in his hand right now?",
        "time_stamp": "0:00:09",
        "answer": "C",
        "options": [
          "A. A cookbook.",
          "B. A knife.",
          "C. A towel.",
          "D. A spatula."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_16_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:22]: A large piece of raw ground meat is placed on a ridged grill pan. The grill pan is situated on a gas stove with three burners, and the burner directly under the grill pan is lit with a blue flame. In the background, the lower part of a person, dressed in dark clothing, is visible near the stove. [0:05:22 - 0:05:24]: The person, still in the background, bends down towards the stove, holding a checkered cloth and appears to be adjusting the heat or checking the settings while the raw meat continues to cook on the grill pan. [0:05:24 - 0:05:25]: The person, holding the checkered cloth, faces the stove and reaches out with the cloth toward the grill pan. Several items, such as bottles and kitchen tools, are neatly arranged on the kitchen counter nearby. [0:05:25 - 0:05:26]: The person adjusts the position of the grill pan using the cloth for a better cooking orientation. Behind him, kitchen shelves filled with various kitchen utensils, containers, and a toaster are visible. [0:05:26 - 0:05:27]: Walking away from the stove, the person carries the checkered cloth over their shoulder, heading towards a nearby kitchen counter where additional items like plates and a cutting board with various prepared ingredients are visible. [0:05:27 - 0:05:29]: The person stands by a kitchen counter and starts prepping ingredients. A wooden pepper mill, bottles, and bowls containing various ingredients are placed on the counter. A person uses a smartphone or device to record or stream what the first person is doing. [0:05:29 - 0:05:34]: The person sets aside the checkered cloth on their shoulder and starts chopping a white onion on a cutting board. Bowls containing ingredients like sliced cheese, tomatoes, lettuce, and bacon are arranged on the counter while a bottle of olive oil is visible nearby. [0:05:34 - 0:05:36]: The person continues to chop the onion into thin slices carefully. The surrounding kitchen elements remain consistent, with the stove and its tools in the background and the neatly arranged ingredients on the counter. [0:05:36 - 0:05:37]: After chopping the onion, the person reaches for a bottle of olive oil. Another person holds a recording device, capturing the actions closely, while another bowl with additional chopped items is nearby. [0:05:37 - 0:05:39]: The person pours some olive oil onto the sliced onions on the cutting board, with another person continuing to record the process closely. Various tools and ingredients, such as tomatoes and lettuce, remain organized on the countertop.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is placed on the ridged grill pan right now?",
        "time_stamp": "0:05:27",
        "answer": "A",
        "options": [
          "A. A large piece of raw ground meat.",
          "B. A piece of chicken.",
          "C. A slice of bread.",
          "D. A fish fillet."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_16_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: A metal grill pan sits on a gas stove, with a blue flame burning beneath it. On the pan, there are two round slices of grilled onions, a beef patty, and a few strips of bacon. A person in gray pants is visible on the right side of the frame. In the background, dark-colored kitchen cabinets with silver handles can be seen. [0:08:01 - 0:08:02]: The person uses a white and black checkered towel to grab the handle of the grill pan. The onions, patty, and bacon remain positioned on the pan, and the blue flame continues to burn under the pan on the gas stove. [0:08:02 - 0:08:03]: The person places the towel on the countertop with their left hand. The grilled food remains on the hot pan, and the kitchen cabinets and countertop are seen in the background. [0:08:03 - 0:08:04]: The person's left hand rests on the counter next to the stove. The food continues to cook on the grill pan, with steam rising. The kitchen drawer and cabinets in the background stay unchanged. [0:08:04 - 0:08:05]: A white and black checkered towel is now draped over the person's shoulder while they use a spatula to adjust the bacon in the grill pan. The grilled onions and beef patty are still visible while the gas flame is on. [0:08:05 - 0:08:06]: The person continues to adjust the food on the grill pan using the spatula. The bacon and beef patty are being moved, and the onions maintain their position. Steam rises from the hot food. [0:08:06 - 0:08:07]: The person lifts the spatula away from the bacon after adjusting it. The onions, patty, and bacon are still on the grill pan, with steam visible above them. [0:08:07 - 0:08:08]: The hand holding the spatula is now further away from the pan. The person appears to be explaining or showing something to another individual who is holding a smartphone, possibly recording or taking a photo. The food remains cooking on the grill pan. [0:08:08 - 0:08:09]: The person has moved away from the stove, leaving the food to continue cooking on the grill pan. The pan remains on the gas stove, and steam can still be seen rising from the food. [0:08:09 - 0:08:10]: The person grabs the white and black checkered towel from the countertop again. The onions, patty, and bacon continue to cook on the grill pan. [0:08:10 - 0:08:11]: The person in gray pants steps back from the stove, holding the towel. The food on the grill pan cooks steadily with steam rising. [0:08:11 - 0:08:12]: The person places a small pot next to the grill pan, positioning it close to the stove. The onions, patty, and bacon continue to be visible on the grill pan with steam rising. [0:08:12 - 0:08:13]: The person retrieves a bottle or container from the counter, possibly to add an ingredient to the grill pan. The eggs, patty, and bacon remain in place, and another individual appears to be filming or taking a picture with a phone. [0:08:13 - 0:08:14]: The focus shifts slightly to the right. The background includes kitchen drawers, countertop items, and part of a dish towel. The onions, patty, and bacon are still on the hot grill pan with steam rising. [0:08:14 - 0:08:15]: The onions, beef patty, and bacon continue sizzling on the grill pan, with steam rising. The background showcases dark kitchen cabinets with silver handles. [0:08:15 - 0:08:16]: The person lifts up a yellow slice of cheese, ready to place it on the food cooking in the grill pan. The kitchen with various utensils and shelves in the background can still be seen. [0:08:16 - 0:08:17]: The person, with a dish towel over their shoulder, starts placing the yellow cheese slice onto the food in the grill pan. The onions, patty, and bacon cook with steam rising from the pan. [0:08:17]: The person carefully places the yellow cheese slice on the bacon and beef patty in the grill pan. The cooking food continues to emit steam, and the onions remain to the side of the pan. [0:08:18]: The yellow cheese slice begins to melt atop the beef",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What items are present on the grill pan right now?",
        "time_stamp": "00:08:01",
        "answer": "B",
        "options": [
          "A. Chicken breast, tomato slices, and mushrooms.",
          "B. Grilled onions, beef patty, and bacon strips.",
          "C. Fish fillets, asparagus, and lemon slices.",
          "D. Sausage links, green peppers, and onions."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_16_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:11:00]",
    "captions": "[0:10:40 - 0:11:00] [0:10:40 - 0:10:41]: Standing in a modern kitchen, a person wearing a dark t-shirt and a checkered towel over their shoulder is meticulously garnishing a dish on a white plate placed on a wooden cutting board. Various ingredients and kitchen tools are spread out on the marble kitchen counter, including bowls with sauces, a whole tomato, lettuce leaves, cheese slices, and a phone displaying the time. [0:10:42 - 0:10:43]: The individual continues assembling the dish, carefully placing sliced vegetables on top of a layered composition. Nearby, there is a gas stove with cooking utensils and a bowl of crispy fried items on a paper towel. [0:10:44 - 0:10:45]: Another person in a dark sweatshirt holds a smartphone close to the plate, appearing to record or take a photo of the food preparation. The focus remains on the precise movements of the cook’s hands. [0:10:46 - 0:10:49]: A close-up view of the dish reveals detailed layers of lettuce, tomato slices, and what appears to be grilled vegetables, meticulously arranged on a toasted bun. The counter beneath displays a marble pattern, and the edge of the cutting board is visible. [0:10:50 - 0:10:52]: The individual wipes the rim of the plate with a cloth napkin, ensuring presentation perfection. The camera angle shifts slightly to maintain focus on the assembled dish on the white plate. [0:10:53 - 0:10:55]: The person places the final touches on the layered vegetable dish, adjusting the components to sit just right. [0:10:56 - 0:10:57]: The cook continues refining the presentation of the dish, adding some final garnishes to the top of the prepared layers. The dish looks nearly complete and visually appealing. [0:10:58 - 0:10:59]: The scene pulls back slightly, showing the person bending over to make detailed adjustments to the dish. Another person, with long blonde hair, leans in from the side holding a phone, likely capturing the moment. The kitchen is well-lit, with various ingredients and tools neatly arranged on the counter.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What item is displayed on the phone screen right now?",
        "time_stamp": "00:10:42",
        "answer": "C",
        "options": [
          "A. A recipe.",
          "B. A cooking video.",
          "C. The time.",
          "D. A shopping list."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Action Recognition",
        "question": "What does the person with long blonde hair do in the video?",
        "time_stamp": "00:10:59",
        "answer": "C",
        "options": [
          "A. Cooks a meal.",
          "B. Washes dishes.",
          "C. Record a video about this food.",
          "D. Sets the table."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_16_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:16",
        "answer": "C",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 0.",
          "D. 4."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      },
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:56",
        "answer": "C",
        "options": [
          "A. 1.",
          "B. 3.",
          "C. 0.",
          "D. 4."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_101_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:01:26",
        "answer": "C",
        "options": [
          "A. 3.",
          "B. 4.",
          "C. 0.",
          "D. 1."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_101_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:26",
        "answer": "C",
        "options": [
          "A. 3.",
          "B. 4.",
          "C. 0.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_101_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:41",
        "answer": "C",
        "options": [
          "A. 2.",
          "B. 8.",
          "C. 0.",
          "D. 3."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_101_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:04",
        "answer": "C",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 0.",
          "D. 7."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_101_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:07]: The video starts with a clear view of several round, glittery discs arranged on a white surface. Each disc contains a name written in a calligraphic style, encompassing a variety of colors like pink, blue, green, yellow, and purple. The discs have different patterns, ranging from solid colors to intricate designs like butterflies and leopard prints. The names on the discs include “Odette,” “Desy,” “Linda,” “Wendy,” “Kelly,” “Hilda,” “Aria,” “Jade,” “Mamun,” “Jan,” and others, creating a vibrant and colorful display. The discs also have a glossy finish, suggestive of a resin coating. [0:00:08 - 0:00:11]: The scene transitions to a black background with the text \"How To Resin Over Vinyl\" in a simple white font, indicating the purpose of the video. [0:00:12]: The screen remains black for a moment, likely transitioning to the next part of the video. [0:00:13 - 0:00:19]: The video continues with a close-up view of two keychains consisting of circular discs designed to resemble donuts. Each disc has a name in the middle—\"Aria\" on a pink donut and \"Jade\" on a purple one. The donuts have colorful sprinkles and are edged with a glittery gold border. Each keychain is attached to a ring with a decorative tassel, one in pink and the other in purple, complementing the donut designs. The white background provides contrast, making the colors and details of the keychains stand out prominently.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is being displayed right now?",
        "time_stamp": "00:00:19",
        "answer": "D",
        "options": [
          "A. Round, glittery discs with names.",
          "B. Text \"How To Resin Over Vinyl\".",
          "C. A black screen with text.",
          "D. Circular donut keychains with names."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_62_real.mp4"
  },
  {
    "time": "0:02:00 - 0:02:20",
    "captions": "[0:02:00 - 0:02:20] [0:02:01 - 0:02:12]: Two gloved hands hold a respiratory mask labeled \"3M\" at its center. The mask is presented from various angles, showing dual white filters on either side with green labels. Behind the mask, there is a colorful craft material, with various objects such as a plastic cup and black gloves visible on a blue grid mat background. The holder repeatedly rotates the mask to ensure all sides and angles are visible, displaying detailed features, especially the straps and the filter areas. [0:02:13 - 0:02:15]: The scene shifts to a close-up of a white canvas featuring a heart design with an inner teardrop shape. A heat tool is directed at the canvas, emitting hot air onto the colors surrounding the heart, causing them to blend. The colorful paint splatters around the heart include shades of red, blue, purple, yellow, and green, creating a vibrant art piece on a white tablecloth. [0:02:16 - 0:02:19]: The view returns to a workspace showing various tools and materials arranged on a blue cutting mat. There are black gloves, colorful patterned papers, a resin kit labeled \"Color Pour Resin,\" a small plastic cup, a wooden stick, and a bottle of resin. A pair of hands enters the frame holding two round filters, examining them closely before setting them down on the mat.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the person do just now?",
        "time_stamp": "0:02:23",
        "answer": "D",
        "options": [
          "A. Presented a respiratory mask from various angles.",
          "B. Directed a heat tool at a canvas.",
          "C. Disassembled the resin kit.",
          "D. Held and examined two round items."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_62_real.mp4"
  },
  {
    "time": "0:04:00 - 0:04:20",
    "captions": "[0:04:00 - 0:04:20] [0:04:01 - 0:04:02]: A grid-patterned workspace is filled with crafting materials. The person’s left hand is positioned near the top left, holding down a white piece of material with multicolored speckles, while the right hand is towards the bottom right, gripping a piece of transparent film. On the upper side of the workstation, arranged in a row, are four white squares sporting variously colored speckles; [0:04:03 - 0:04:04]: The left hand shifts slightly, still handling the speckled white material, and now a piece of white material with the name \"Jade\" written in black is clearly seen on the upper left side. The right hand remains in the bottom right, still holding the transparent film in the same position; [0:04:05 - 0:04:06]: The left hand compacts a yellow and white crafting tool. The purple circular object in the middle of the workspace stays unchanged with the transparent film and white pieces scattered around; [0:04:07 - 0:04:08]: The right hand moves closer to a small white square, lifting the corner of the transparent film. The left hand holds a piece covered with multicolored speckles, positioned nearer to the camera. The yellow and white crafting tool is visible at the top right of the frame;  [0:04:09 - 0:04:10]: Both hands grip a different white piece with speckles, seen more closely now. The transparent film is partly affixed to this piece, and the right hand maneuvers the film to ensure correct placement;  [0:04:11 - 0:04:12]: The right hand secures a transparent film onto the white speckled piece, going over the purple circular object near the center. The left hand retracts slightly but still hovers near the workspace; [0:04:13 - 0:04:14]: The right hand places the transparent sheet over the white piece, ensuring adhesion. The secar details of each item on the workspace, such as colorful speckles and writing, are visible; [0:04:15 - 0:04:16]: The right hand centers the white square with the transparent film against the purple circular object. The left hand rests on the workspace, ensuring no unintended shifts; [0:04:17 - 0:04:18]: The yellow and white crafting tool is near the top right, while both hands shift the white square with vibrant speckles closer for better alignment; [0:04:19]: Left hand remains stationary while right hand maneuvers a new speckled white piece back into the workspace; [0:04:20]: The transparent sheet, now on the purple circular object, is scrutinized closely by both hands for perfect application.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is both hands securing right now?",
        "time_stamp": "00:04:20",
        "answer": "D",
        "options": [
          "A. A transparent sheet onto a blue object.",
          "B. A white piece onto a purple circular object.",
          "C. A yellow and white crafting tool onto a white speckled piece.",
          "D. A transparent tape printed with many patterns on it."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_62_real.mp4"
  },
  {
    "time": "0:06:00 - 0:06:20",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:01]: The scene begins with a person wearing black gloves holding a small clear plastic cup in both hands above a blue cutting mat. A bottle with a white body and black cap is placed slightly to the right of the cups.  [0:06:02 - 0:06:03]: The person picks up the bottle with their right hand while holding the cup with their left hand. The bottle is uncapped, and the person begins to pour a clear liquid from the bottle into the cup. [0:06:04]: From a close-up view, the liquid in the cup shows a clear surface with some bubbles forming.  [0:06:05]: The gloved hands now hold two cups, one in each hand, above the blue cutting mat.  [0:06:06]: The person starts to mix the contents of the two cups by pouring one cup into the other while holding them with both hands. [0:06:07 - 0:06:08]: The person uses a wooden stir stick to mix the liquid within the combined cup, holding the stick with their right hand and the cup with their left. [0:06:09 - 0:06:12]: The close-up continues showing the swirling motion of the stick inside the cup as the person stirs the already clear liquid. [0:06:13]: The person briefly stops stirring and adjusts the position of their hands on the cup. [0:06:14 - 0:06:17]: The gloved hands continue the stirring motion, seen from a slightly different angle, keeping the cup steady with the left hand and swirling the stick within the cup. [0:06:18 - 0:06:19]: The video frames show the person consistently stirring, with a slight tilting motion, causing the liquid inside to swirl around.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action is the person performing right now?",
        "time_stamp": "0:06:19",
        "answer": "D",
        "options": [
          "A. Measuring ingredients in two cups.",
          "B. Pouring liquid into a third container.",
          "C. Wearing black gloves.",
          "D. Consistently stirring the liquid."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_62_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:08]: The video begins with a close-up view of a small clear plastic beaker or container, graduated with measurement markings from 0 to 40. The container is placed on a blue, grid-patterned mat. Positioned on top of the container is a circular piece covered in gold glitter. The circular piece also has a popsicle stick extending out to one side. The person holding a black lighter brings the flame close to the gold glitter-covered top, making the flame touch it. The flame moves to different spots on the glitter surface as the lighter navigates around.  [0:08:09 - 0:08:14]: The scene transitions to show a pair of gloved hands, dressed in black latex gloves, gripping another small beaker containing a similarly glitter-covered top. An overhead view reveals another container below, filled with a golden substance and containing a popsicle stick that sticks out. The gloved person carefully examines the glitter-covered top of the second container, rotating and holding it up close.  [0:08:15 - 0:08:19]: Returning their attention to the initial beaker on the blue mat, the person holding and rotating it in the fixed frame. Then, the gloved hands gently put down the container, aligning it well on the blue grid-patterned mat, signifying the end of the video segment.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the person do with the lighter just now?",
        "time_stamp": "0:08:08",
        "answer": "D",
        "options": [
          "A. Ignited a match.",
          "B. Removed the popsicle stick from the container.",
          "C. Adjusted the measurement markings on the container.",
          "D. Brought the flame close to a gold glitter-covered top."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_62_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What action sequence was taken just now by the individual working with the coffee machine?",
        "time_stamp": "00:00:10",
        "answer": "D",
        "options": [
          "A. The individual brewed tea, added sugar and milk, and served it with biscuits.",
          "B. The individual prepared an espresso, added whipped cream, and served it in a glass.",
          "C. The individual brewed coffee, added cream, and stirred it before serving.",
          "D. The individual operated an espresso machine, brewed a shot of espresso, and pour the milk into a cup."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_365_real.mp4"
  },
  {
    "time": "[0:02:17 - 0:02:27]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now by the barista?",
        "time_stamp": "00:02:27",
        "answer": "A",
        "options": [
          "A. The barista measured cportafilter, locked the portafilter into the espresso machine, and add coffee grounds into it.",
          "B. The barista cleaned the espresso machine, prepared the milk pitcher, and steamed milk.",
          "C. The barista brewed a cup of tea, added lemon, and served it to a customer.",
          "D. The barista ground coffee beans, prepared a French press, and set it aside to steep."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_365_real.mp4"
  },
  {
    "time": "[0:04:34 - 0:04:44]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What sequence of actions did the barista perform just now with the steamed milk?",
        "time_stamp": "00:04:43",
        "answer": "B",
        "options": [
          "A. The barista poured milk into a cup, cleaned the espresso machine, and prepared a new cup.",
          "B. The barista steamed the milk, wiped the steam wand, and placed the milk pitcher down.",
          "C. The barista brewed coffee, steamed milk, and added sugar to the cup.",
          "D. The barista measured milk, adjusted the machine settings, and poured the milk into a new pot."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_365_real.mp4"
  },
  {
    "time": "[0:06:51 - 0:07:01]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now by the individual with the coffee grinder?",
        "time_stamp": "00:07:00",
        "answer": "B",
        "options": [
          "A. The individual operated the coffee grinder, collected ground coffee in a metal pitcher, and mixed it with water.",
          "B. The individual adjusted the coffee grinder settings, and transferred the grounds to a portafilter.",
          "C. The individual poured whole coffee beans into the grinder, ground them into a container, and stored the container for later use.",
          "D. The individual set the coffee grinder, filled it with milk, and brewed a latte."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_365_real.mp4"
  },
  {
    "time": "[0:09:08 - 0:09:18]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now by the individual with the coffee grounds?",
        "time_stamp": "00:09:18",
        "answer": "C",
        "options": [
          "A. The individual measured coffee grounds, tamped them into a portafilter, and locked it into an espresso machine.",
          "B. The individual adjusted the grinder settings, measured coffee grounds, and transferred them to a filter.",
          "C. The individual weighed portafilter, added coffee grounds into a glass container from a portafilter with a spoon.",
          "D. The individual prepared a batch of coffee grounds, cleaned the countertop, and disposed of extra grounds."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_365_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video starts with a view of an outdoor area featuring a wide walkway surrounded by greenery and trees. There is a stone planter on the left side with some people sitting and standing nearby. The sky above is partly cloudy. A few people are walking along the pathway, some are dressed in casual clothing, and one person in the foreground appears to be dressed in brown, possibly in a uniform. [0:00:03 - 0:00:04]: More people come into view, including a person in the foreground wearing a light blue shirt walking towards the camera. Additional individuals can be observed on the right side, near a small cart which seems to be a concession or information booth. [0:00:05 - 0:00:06]: The scene continues with more people walking along the path. Prominently, a man in a blue tank top and white cap, accompanied by a woman and a child, is seen walking on the left. A few more people are visible in the background, while those near the cart remain engaged. [0:00:07 - 0:00:08]: The person in the blue tank top has moved further forward, and more pedestrians are scattered across the walkway. The small booth on the right side shows some individuals interacting and standing around it. [0:00:09 - 0:00:10]: On the left, a young man dressed in navy blue and wearing a cap approaches the camera. The path also has fewer people further back, with some seated or resting along the sides of the walkway. The right side becomes less crowded with fewer people around the cart. [0:00:11 - 0:00:12]: The view focuses more on the left side, which includes a large rock structure resembling part of a building or attraction. A couple of people can be seen sitting close to the rock structure. [0:00:13 - 0:00:14]: The left side continues to dominate the view, showing a group of people near the base of the rocky structure. A small pathway or entrance is visible within the rock formation. [0:00:15 - 0:00:16]: More people are seen around the entrance in the rock structure. The area looks like an attraction or themed area. One person dressed in dark clothing is walking away from the entrance. [0:00:17 - 0:00:18]: The crowd increases near the rocky entrance, and some people are entering or exiting the area. The visibility within the entrance becomes clearer, showing dark pathways inside the structure. [0:00:19 - 0:00:20]: The final seconds of the video focus on the entrance with several people visible around it. The surrounding greenery and stone planters continue to frame the scene, emphasizing the outdoor theme of the area.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Where is the young man in navy blue and wearing a cap moving towards?",
        "time_stamp": "0:00:10",
        "answer": "A",
        "options": [
          "A. Towards the camera.",
          "B. Towards the stone planter.",
          "C. Towards the rock structure.",
          "D. Towards the cart."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_311_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:21]: The scene begins in a busy theme park with several people walking around. The environment is brightly lit by the sun, casting clear shadows on the ground. Buildings with vibrant colors flank both sides of the walkway. There are several advertising signs, including a prominent green and white sign to the left.   [0:05:21 - 0:05:22]: The camera continues forward, capturing more of the theme park's lively atmosphere. There is a large, colorful balloon decoration on the right side, and more people are seen strolling, many with children. The sky is partly cloudy, adding depth to the background.  [0:05:22 - 0:05:23]: As the view progresses, the structures and the crowd become more distinct. The sign for “Springfield” on the left side becomes more visible, indicating a themed area. The street continues to be bustling with activity. [0:05:23 - 0:05:24]: The scene remains dynamic with various individuals walking in different directions. A food stand with a bold, colorful design is seen on the right, and more details of the surrounding area come into view. [0:05:24 - 0:05:25]: The detailed decor of the theme park continues to unfold. More signs and colorful decorations are visible on the buildings. People are seen indulging, some enjoying snacks as they stroll. [0:05:25]: The woman in the foreground wears a hat and carries a red purse. The structures continue to display bright, engaging themes, embracing the joyful ambiance of the theme park. [0:05:26 - 0:05:27]: The camera captures more of the detailed theme park setting. Another food kiosk appears on the right, with people enjoying the park's offerings. The lively mood is maintained. [0:05:27 - 0:05:28]: A sign that says \"Welcome to Springfield\" is clearly visible on the left, with decorative elements including cartoon characters enhancing it. [0:05:28 - 0:05:29]: The detailed theme park scene unfolds further, emphasizing the vibrant \"Springfield\" area. People move around the park, with various attractions visible in the background. [0:05:29 - 0:05:30]: More of the “Springfield” sign becomes apparent, revealing more cartoon characters and thematic decorations. Visitors walk around leisurely, adding to the busy mood. [0:05:30 - 0:05:31]: The camera continues to capture the animated environment of the theme park. The buildings and signs are filled with colorful decorations, adding to the festive atmosphere. [0:05:31 - 0:05:32]: More attention is given to the “Springfield” area sign, with people continuing to enjoy their time. The architecture and decor maintain a consistent, lively theme. [0:05:32 - 0:05:33]: A closely focused view of the \"Welcome to Springfield\" sign, featuring cartoon characters and badges. The crowd and environment remain lively. [0:05:33 - 0:05:34]: The sign shows further details, including the year \"1796,\" and depicts three cartoon characters. The crowd continues to move in and out of the park sector. [0:05:34 - 0:05:35]: The view stays on the “Springfield” sign with more cartoon characters and decorative badges visible. The scene is filled with animated movements of park-goers. [0:05:35 - 0:05:36]: Another detailed view of the “Springfield” sign is shown, with vibrant decorations. The setting remains busy with visitors and cheerful with bright skies. [0:05:36 - 0:05:37]: People roam around the theme park near the vibrant “Springfield” sign. The setting underlines a festive atmosphere with its artistic theming. [0:05:37 - 0:05:38]: The whimsical “Springfield” sign continues to be the focal point. People are seen engaging with the park activities, contributing to the lively ambiance. [0:05:38 - 0:05:39]: The camera captures one last image of the “Springfield” sign, featuring cartoonish elements and badges. The vibrant scene is bustling with park visitors.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What does the woman in the foreground carry as she walks through the theme park just now?",
        "time_stamp": "0:05:26",
        "answer": "D",
        "options": [
          "A. A blue backpack.",
          "B. A green tote bag.",
          "C. A yellow suitcase.",
          "D. A red purse."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      },
      {
        "task_type": "Text-Rich Understanding",
        "question": "What detail is included on the \"Welcome to Springfield\" sign?",
        "time_stamp": "0:05:34",
        "answer": "A",
        "options": [
          "A. The year \"1796\".",
          "B. The year \"1896\".",
          "C. The year \"1996\".",
          "D. The year \"2006\"."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_311_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:04]: The video shows the entrance to a building labeled \"MOE'S\" in large red letters on a pink and purple facade. The building has a whimsical and colorful design with geometric patterns. The doors are red with circular windows and gold handles. A person wearing a white shirt and light blue jeans is walking towards the entrance. [0:08:05]: The camera moves slightly closer to the entrance, focusing more on the person walking in. Another person, wearing a hat and holding a drink, is exiting the building, while a child in a white and red shirt is also walking out. [0:08:06 - 0:08:07]: The interior of the building is now visible. It has a colorful, retro design with a patterned ceiling, blue tile floor, and red seating. There are several tables spread across the room. A few people are sitting and chatting at the tables, while others are standing around. [0:08:08 - 0:08:11]: The camera continues to move forward, providing a clearer view of the interior. There are more people visible, sitting in various parts of the room and engaging in conversation. The central area has a pool table, and the walls are decorated with framed pictures and memorabilia. [0:08:12 - 0:08:15]: The camera angle shifts slightly to the right, revealing a bar area where people are gathered. There is a clear path marked by stanchions leading towards the bar. A statue of a character holding a large beer mug appears in the corner. [0:08:16 - 0:08:19]: The camera zooms in on the statue, which resembles a character from The Simpsons. This character is holding a glass mug and is dressed in casual clothes. The background includes a colorful wall with posters and a \"Love Tester\" machine. [0:08:20]: The video ends with a closer view of the statue, highlighting the details of the character's facial expression and the drink in hand, emphasizing a playful and detailed cartoonish aesthetic.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person exiting the building holding right now?",
        "time_stamp": "00:08:03",
        "answer": "D",
        "options": [
          "A. A book.",
          "B. A phone.",
          "C. A bag.",
          "D. A drink."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What is the design style of the building's interior?",
        "time_stamp": "00:08:07",
        "answer": "D",
        "options": [
          "A. Modern and sleek.",
          "B. Minimalist and plain.",
          "C. Industrial and raw.",
          "D. Colorful and retro."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_311_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:05]: In a bustling urban street setting on a sunny day, the scene opens with a wide cobblestone street, and a zebra crossing is prominently featured along the right side of the frame. The sky is a clear blue, and a mix of modern and classic architecture lines the streets. On the left, there is a building with several tables and chairs arranged outside, shaded by white umbrellas. Moving right, a series of four to five-story buildings with red-framed windows and beige facades gradually rise along the street slope. The traffic lights are visible at the intersection, and several cars can be seen driving in both directions or parked on the street. [0:00:05 - 0:00:10]: Several vehicles are moving along the street or are parked. An increasing number of pedestrians and cyclists are noticeable on the sidewalk in front of the buildings. Some tourists may be seen taking pictures or exploring the area near a shop, \"Tourist Service,\" on the ground floor of one of the buildings on the right side of the frame, establishing a dynamic and lively atmosphere. The buildings continue to showcase a mix of commercial and residential purposes, with several people seen moving in and out. [0:00:10 - 0:00:15]: More pedestrians are visible near the tourist service center. The scene maintains its lively city vibe with ongoing movement from pedestrians and vehicles. The angle remains consistent, showing the busy crossroads, more of the cobblestone pavement, the steady movement of cars, and active pedestrians. [0:00:15 - 0:00:20]: Two pedestrians, notably wearing casual attire and backpacks, walk briskly across the street from left to right near the zebra crossing, heading towards the buildings on the far side. The traffic slows slightly to allow for safe crossing, indicating a pedestrian-friendly environment. The scene captures the continuous ebb and flow of urban life with people moving, cars navigating the streets, and the sunshine enhancing the vibrancy of this city setting.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where were the tables and chairs located in the scene just now?",
        "time_stamp": "0:00:05",
        "answer": "C",
        "options": [
          "A. On the right side of the street.",
          "B. Inside a building.",
          "C. In front of the building with white umbrellas.",
          "D. Near the zebra crossing."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_326_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:44]: The video starts with a view of a historic stone cathedral located on the right side, showcasing intricate architectural details. A few people, including a figure in white, are walking on the cobblestone street that curves gently to the left. On the left side, there's a two-story building with large windows and green shutters. A car is parked along the street near the building. [0:02:45 - 0:02:48]: As the video progresses, the person filming is walking closer to the cathedral. The camera catches more details of the ornate facades and arched entrances. There is a \"No Entry\" signpost at the center close to the cathedral's entrance. People are continuing to walk up the street which appears to have a gentle uphill slope. [0:02:49 - 0:02:53]: The camera moves even closer to the cathedral, revealing statues and detailed carvings that decorate the building. A small three-wheeled vehicle, with a \"TUK\" sign on the back, is parked on the street in front of the camera. More individuals are walking near the entrance, and a group of people seems to be taking photos. [0:02:54 - 0:02:57]: The view expands to show the larger cathedral complex, including adjoining buildings to the right, which feature modern architecture. The vehicle remains visible on the street near the bottom right corner. The cobblestone pavement spans the width of the scene, providing a historic contrast to the angle of the newer building structure. [0:02:58 - 0:03:00]: The video concludes with a broader view of the surroundings, highlighting the cathedral's beautiful architecture alongside the modern building. The camera captures more people walking around and enjoying the historic site. The scene is brightly lit by natural daylight, emphasizing the textures and colors of the stone buildings and the stone pavement.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the small three-wheeled vehicle parked?",
        "time_stamp": "0:02:55",
        "answer": "B",
        "options": [
          "A. On the cobblestone pavement near the cathedral.",
          "B. In front of the cathedral.",
          "C. Near the modern buildings.",
          "D. At the bottom right corner of the street."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_326_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:26]: A narrow cobblestone street winds through a picturesque European town lined with colorful buildings. To the right, an ornate, historic building made of stone is visible. Two people are seated on a stone ledge, conversing. The sky is bright blue, and a construction crane is faintly visible in the distance. Multiple \"No Parking\" signs are noticeable along the street. [0:05:27 - 0:05:31]: A white, small tour vehicle with red accents is ascending the hill, occupying the right side of the frame, with the two previously mentioned individuals still seated and engaged in conversation. The identical buildings and \"No Parking\" signs are present along the cobblestone road. [0:05:32 - 0:05:33]: The camera follows the tour vehicle up the slope, showing more of its details, including a promotional sign on the back advertising a tour and a \"Bull Tour\" logo. The stone building and steps are still visible on the right side of the frame. [0:05:34]: The tour vehicle moves further up the cobblestone street, and a sign prohibiting parking (with a blue background and red cross) becomes prominent in the foreground. [0:05:35 - 0:05:36]: The view captures the right side of the white, red-accented tour vehicle along the cobblestone path. In the distance, buildings with a mix of modern and traditional architectural styles become more clear.  [0:05:37 - 0:05:39]: The camera continues to move up the hill, following the tour vehicle closely. The modern building with a blue netting (likely under construction or renovation) is centered in the background. Various signs, including a prominent blue \"Parking\" sign, line the narrow cobblestone street as it stretches ahead.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is visible to the right of the stone street in the video right now?",
        "time_stamp": "00:05:26",
        "answer": "A",
        "options": [
          "A. A historic stone building.",
          "B. A large modern skyscraper.",
          "C. A park with trees.",
          "D. A river with boats."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the promotional sign on the back of the tour vehicle?",
        "time_stamp": "00:05:33",
        "answer": "D",
        "options": [
          "A. Blue and Green.",
          "B. Green and Yellow.",
          "C. Yellow amd Red.",
          "D. Red and Blue."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_326_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:50]",
    "captions": "[0:09:40 - 0:09:50] [0:09:40 - 0:09:44]: The video begins with a view from the perspective of walking along a bridge. The bridge has two sections - the left side is wide and looks like it is designed for vehicles or trams, and the right side is a narrower pedestrian walkway. The walkway has a metal railing. The sky is clear with some scattered clouds, and on the left side of the background, there is a large stone building with a small dome and multiple smaller buildings attached to it. There are a few people walking and standing along the bridge. [0:09:45 - 0:09:49]: As the movement progresses, the perspective is focused more towards the right side, where a person in a red shirt and blue shorts is leaning against the railing, looking down at the river below. The cityscape in the background includes a mix of historical and modern buildings, nestled along the hillside and extending to the water's edge. The river is also visible, reflecting the clear blue sky. The tram tracks on the left remain consistent throughout this segment, and a few pedestrians are visible further ahead on the bridge. The overall environment is bright and sunny.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What types of paths does the bridge have?",
        "time_stamp": "0:09:49",
        "answer": "B",
        "options": [
          "A. Two vehicle lanes and a bike lane.",
          "B. two pedestrian walkways and two tram tracks.",
          "C. A bike lane and a tram track.",
          "D. Two pedestrian walkways and a vehicle lane."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_326_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video opens with a visually striking animation featuring red and blue shapes on the screen, forming abstract patterns. The shapes are bold, with sharp angles and curved elements, moving in a synchronized manner. The background smoothly transitions between colors; [0:00:01 - 0:00:02]: As the animation continues, a \"SUBSCRIBE\" button appears in the center of the screen. The button has a white border with bold white text on a red background. For added emphasis, the button is highlighted by a bright yellow glow; [0:00:03 - 0:00:04]: The scene shifts dramatically to a crowded football stadium, filled with spectators. The field is green, and players and officials, wearing red shirts and holding a ball, walk towards the camera. The atmosphere is lively, filled with anticipation. The Aeromexico and La Liga logos are visible in the background; [0:00:05]: As the procession continues, the camera focuses on a man extending his arm, engaged in an animated conversation. He is wearing a dark polo shirt, and the crowd’s energy is palpable behind him; [0:00:06]: This man, still dressed in a dark polo shirt, embraces another individual, indicating a greeting or sign of respect. The red backdrop behind them accentuates the intense colors of the football stadium environment; [0:00:07]: Both men smile as they continue to greet and embrace others, tightening their hug. The surrounding crowd, some in security vests, and another person in sports attire are visible, adding to the sense of community and excitement; [0:00:08 - 0:00:09]: The camera cuts to an on-field interaction between players, identifiable by their uniforms. One player in green and another in white with the name \"Sergio Ramos\" and the number 4 on his jersey are seen. Several officials stand nearby, closely observing the interaction; [0:00:10]: An aerial view of the football field reveals players from both teams taking positions around the midfield circle, preparing for the game to start. The geometric patterns of the field's lines and the players' formations are clearly visible; [0:00:11]: With the stadium now fully visible, a large crowd fills the stands, suggesting a significant event. The game appears to be starting as players spread out on the field, ready to commence; [0:00:12]: The camera zooms out further, showing a broader view of the field and the arrangements of the players. The white team appears to be on one side and the blue team on the other, with the referee centrally positioned; [0:00:13 - 0:00:14]: The scoreboard overlays the lower part of the screen, showing \"0 - 0\" with team logos, underlining the match's commencement and the score at the starting point; [0:00:15 - 0:00:17]: The game advances, with both teams now actively engaging and moving the ball around the field. Players from both sides are strategically positioned, making attempts to move forward or defend; [0:00:18 - 0:00:20]: The game progresses with fluid movements across the pitch. Players in blue and white jerseys are actively passing the ball, looking for openings in each other's defenses. The scoreboard and La Liga branding remain on screen throughout this period.\n[0:00:40 - 0:01:00] [0:00:40 - 0:00:49]: On a crowded football field during a match, a player wearing a white jersey with the number 2 on the back is running towards a fallen player from the opposing team, who is wearing a purple and blue jersey. The white jerseyed player is in mid-step with one leg off the ground and arms slightly spread out for balance. The background is filled with cheering spectators, a banner with red and white sections, and a scoreboard displaying \"LALIGA.\" Moving to the next frame, the ball is in mid-air, and several players from both teams are positioning themselves underneath it. The white jersey player is closer to the camera, leaning slightly forward, while an oncoming player from the same team is attempting to connect with the ball. [0:00:50 - 0:00:56]: The ball is still airborne, with players from both teams intensely watching its descent. One player in a purple and blue jersey is making a high jump on the right side of the field, trying to head the ball. The goalkeeper, dressed in bright yellow, is seen nearer to the goal post, prepared to make a move. The players continue to converge around the goal area, with the yellow-clad goalkeeper making a fully outstretched dive to save the ball, resulting in a collision near the goal line. The ball is then struck past the goalkeeper who cannot make the save, resulting in a goal. [0:00:57]: Following the goal, the scene transitions to a joyful celebration. A player wearing a purple and blue jersey, numbered 9 on the back, is seen celebrating. He engages in a unique arm gesture with a teammate, who mimics the motion. Another teammate, with a partially visible name on his jersey, joins in the celebration. A crowd of spectators can be seen in the background, bringing life to the moment. [0:00:54 - 0:00:59]: Cutting back to an active play in another stage of the match, an attacking player in white is dribbling the ball up the field, facing defensive pressure from three players in purple and blue jerseys. The dribbling player manages to cut inside, evading one defender. Positioned near the top of the penalty area, he takes a stride to set up a shot as other defenders close in. With a quick step, a player in white manages to shoot, sending the ball past the defenders and goalkeeper. [0:01:00]: As the ball speeds towards the goal, the opposing goalie, dressed in green, dives futilely to stop it. The attacking player throws his arms in the air in celebration as the ball hits the back of the net. The crowd in the background is a blur of excitement, showing various flags and colors, with a large section dressed in black twirling scarves in jubilation.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What logos are visible in the background as players and officials walk towards the camera?",
        "time_stamp": "00:00:04",
        "answer": "D",
        "options": [
          "A. FIFA and Premier League.",
          "B. UEFA and Champions League.",
          "C. Bundesliga and MLS.",
          "D. Aeromexico and La Liga."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the player wearing the number nine red and blue jersey celebrate?",
        "time_stamp": "00:00:53",
        "answer": "C",
        "options": [
          "A. He saved a goal.",
          "B. He received a pass.",
          "C. He scored a goal.",
          "D. He won a free kick."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_1_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:01:40]",
    "captions": "[0:01:20 - 0:01:40] [0:01:21]: The camera shows a soccer match in progress, with a goal being guarded by a goalie in a yellow uniform. In the foreground, multiple players from two different teams stand inside the box, preparing for an offensive play;  [0:01:22]: A close-up of a player in a blue and red jersey with \"Spotify\" as the sponsor logo is seen, covering his mouth in apparent disbelief or surprise. Another player with \"No. 2\" on his back is partially visible to the side;  [0:01:23]: The perspective shifts to a broader view of the field where a player in a blue and red jersey is advancing toward the goal with the ball at his feet. He is closely followed by a cluster of white-clad defenders;  [0:01:24]: As the player in the blue and red jersey approaches the penalty area, several defenders continue chasing while another offensive player on the right makes a run towards the goal to support the attack;  [0:01:25]: Inside the penalty box, the offense intensifies. The player holding the ball is preparing for a potential pass or shot on goal, with several defending players closely guarding the area;  [0:01:26]: The ball is nearing the goalkeeper in the yellow uniform, positioned to block any incoming shots;  [0:01:27]: The offensive player in the blue and red jersey attempts to maneuver past two defenders, edging closer to the goal while evaluating his options;  [0:01:28]: A defender makes a crucial tackle attempt, but the offensive movement persists as the ball continues to float near the penalty area, heightening the intensity of the play;  [0:01:29]: Inside the penalty area, the offensive player tries to outmaneuver multiple defenders while the goalie adjusts his position, creating a highly stressful engagement near the goal;  [0:01:30]: As the chaotic scramble for the ball ensues, players from both teams converge around the ball, leading to a high-pressure situation at the goalmouth;  [0:01:31]: The ball is passed around near the edge of the penalty area. Offensive players in blue and red jerseys are searching for an opening to potentially strike at the goal, while defenders hold their positions;  [0:01:32]: The attacking team maintains possession near the penalty box. Multiple players from both sides are concentrated in this area, ready to intercept or shoot the ball;  [0:01:33]: The ball is skillfully moved around just outside the penalty box as the offensive team probes for a weak spot in the defender's line;  [0:01:34]: A sequence of quick passes between offensive players tests the defenders' resolve, as the ball remains dangerously close to the goal area;  [0:01:35]: The offensive pressure is relentless, with a focused attempt to breach the goal, while the defense organizes themselves to block any potential shots;  [0:01:36]: A player in the blue and red jersey winds up for a shot from outside the penalty box, while defenders rush to intercept it;  [0:01:37]: The ball remains in a threatening position as defenders and offensive players collide, escalating the high stakes of this exchange near the goal;  [0:01:38]: The game continues with intense energy as the ball nears the 6-yard box. Players from both teams are entirely engrossed in this contest, fighting for control of the ball;  [0:01:39]: Another offensive play develops near the corner of the penalty box, as a player in a blue and red jersey maneuvers to keep the ball in play and sustain the attacking momentum.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What logo is seen on the jersey of the player covering his mouth?",
        "time_stamp": "00:01:22",
        "answer": "D",
        "options": [
          "A. Apple.",
          "B. Google.",
          "C. Adidas.",
          "D. Spotify."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the goalie's uniform?",
        "time_stamp": "00:01:20",
        "answer": "D",
        "options": [
          "A. Blue.",
          "B. Red.",
          "C. White.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_1_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:02:20]",
    "captions": "[0:02:00 - 0:02:20] [0:02:01 - 0:02:05]: A football match is underway with players predominantly in white and red kits occupying the field. The camera, capturing the action from the stands, focuses on the penalty area where the play is intensifying. In the left part of the image, a group of players is converging near the top corner of the penalty box, including a player in a blue and red kit intensely focused on controlling the ball while being marked by opposing players. The goalkeeper, in bright yellow attire, is poised and ready near the goalmouth, which is currently protected by a few defenders. The crowd in the background is densely packed and highly engaged, showcasing a variety of banners and team colors. [0:02:06 - 0:02:08]: The scene shifts to the sidelines, where two team officials are seen interacting near the team bench. One official dressed in dark casual attire seems to be giving instructions or communicating, standing nearby a red cooler labeled \"PRIME.\" Customary bright red seats behind them imply it’s their designated technical area. [0:02:09 - 0:02:12]: The camera cuts back to the playing field, showing close-up interactions between players. One player in blue and red kit stands near the sidelines looking towards the crowd, seemingly celebrating or catching a breath. Shortly after, another player from the same team approaches, and they share a brief, heartfelt celebration near the edge of the pitch, expressing camaraderie. [0:02:13 - 0:02:13]: A zoomed-in shot focuses on the feet of the player in the white kit and number 22 on his jersey as he aggressively marks an opponent. The ball, prominently visible, is just a few inches away from the player's feet, indicating a close contest for possession. [0:02:14]: The scene transitions quickly to the middle of the field, capturing a solitary player in a white kit in open play. The field is expansive, with green grass and clear white markings visible, aiding field structure visualization. [0:02:15 - 0:02:16]: The action returns to the attack, where a powerful shot is taken toward the goal by an attacker from the edge of the penalty area. The goalkeeper, dressed in bright yellow, dives full stretch to the right in an effort to save the ball, showcasing a fully committed defensive maneuver. [0:02:17 - 0:02:19]: The camera captures a close-up sequence of another player in the blue and red kit breaking past two defenders inside the penalty box, driving forward with the ball at his feet. The final frames capture a tense moment where the player attempts a shot on goal, while defenders and the goalkeeper put in desperate efforts to block the attempt.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the cooler labeled \"PRIME\" near the team bench?",
        "time_stamp": "00:02:08",
        "answer": "D",
        "options": [
          "A. Blue.",
          "B. Yellow.",
          "C. Green.",
          "D. Red."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_1_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a black background displaying the word “brilliant” lit up in white neon, slightly tilted to the right. [0:00:02 - 0:00:03]: The next scene shows another neon sign. This time, it is a blue neon outline of a food truck with “idiot” written in white neon letters. The word “idiot” is written in a cursive font similar to the “brilliant” earlier. The phrase “...ON THE ROAD” appears below in bold white. [0:00:04]: The scene changes to an outdoor setting with a man standing behind a wooden table. The table is laden with various items, including a fruit basket, some containers, and a few bottles. In the background, there are fallen, gnarled trees and a view of the ocean, with waves crashing against black volcanic rocks. The sky is cloudy, casting a diffused light over the area. [0:00:05]: The man appears to be preparing or explaining something, as he looks attentively at his hands. He is wearing a green t-shirt and black pants, standing straight facing the camera. [0:00:06]: The man is now looking directly at the camera, with his hands placed on the table, possibly introducing himself or the show. The lighting is slightly better, highlighting some cooking utensils on the table. [0:00:07]: He continues to talk to the camera while making gestures with his hands. The background remains the same, with the ocean visible. [0:00:08 - 0:00:09]: He turns to his left, pointing towards something off-screen, possibly explaining or drawing attention to some feature or location. [0:00:10]: The man shifts his focus back to the camera, with his arm still extended in the previous pointing gesture. The bright overhead sun casts shadows of the tree branches on the sandy ground. [0:00:11 - 0:00:12]: He places both hands on the table and talks more, with serious expression, perhaps diving into the details of his content. [0:00:13 - 0:00:14]: The man reaches out to pick something up from the table. The camera angle reveals that the object could be a small container or a jar. [0:00:15]: The man is now holding a small brown container in his hand, lifting it towards the camera as he possibly explains its contents or importance. [0:00:16 - 0:00:19]: He continues to hold the container and talks directly to the camera, ensuring to keep the object prominently in the frame. The background scenery with the ocean and fallen trees remains consistent throughout.\n[0:00:20 - 0:00:40] [0:00:20 - 0:00:24]: A man is standing behind a dark wooden table on a rocky seashore with white, gnarled trees and a blue ocean in the background. He is wearing a green t-shirt, black pants, and red sunglasses, holding a piece of food in his right hand and gesturing with his left. The table is set with cooking utensils, bowls, and ingredients like fruits and vegetables. [0:00:25 - 0:00:28]: He looks down at the piece of food in his hand, then looks back at the camera, holding it closer toward the lens. His left hand rests on the table, close to a wooden cutting board. The cooking items on the table are arranged neatly and his stance remains engaged. [0:00:29 - 0:00:31]: He continues to hold the piece of food in his hand while speaking, occasionally looking at it and then back at the camera. The sky is overcast with heavy clouds, and the sea is calm but dotted with small waves. The background landscape remains constant with the rocky shore and dead trees. [0:00:32 - 0:00:36]: The camera angle changes to a wider shot, showing more of the table and the scenery. The man is still talking and gesturing with his right hand, while his left points at something on the piece of food. The same cooking utensils, bowls, and ingredients are visible, and the setup looks ready for a demonstration. [0:00:37 - 0:00:39]: The camera focuses back on the man in close-up as he continues his explanation, raising his right index finger in a gesture of emphasis. His facial expression is animated and engaging, adding a clear intensity to the scene as he describes the food he's holding.\n[0:00:40 - 0:01:00] [0:00:40 - 0:00:47]: A person wearing a green t-shirt and red sunglasses stands behind a well-organized wooden table, which is set up with several kitchen utensils and ingredients. The table is positioned outdoors, showcasing a picturesque coastline in the background with a tree with bare branches. The ocean waves crash against black volcanic rocks. The person holds a small bread roll in one hand and uses gesturing movements with the other hand. [0:00:48 - 0:00:49]: The person is seen continuing to talk animatedly while gesturing with their hand and holding the bread roll. The table and the setup remain consistent, with a variety of knives, a few small containers, and different kitchen utensils laid out neatly on the table. [0:00:50 - 0:00:55]: The person transitions to focusing on the bread roll they are holding. He lower their head and inspect the bread closely, beginning to cut it. The backdrop of the sea and dry tree remains constant, providing a serene yet striking background. [0:00:56 - 0:00:59]: The camera focuses more closely on the person’s hands. The person is now slicing the bread with a knife on a wooden cutting board. The table is well-arranged with small bowls, a jar, a whisk, and other utensils placed on it. The background remains the same, with the sand and the ocean visible beyond the table.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the sky right now?",
        "time_stamp": "00:00:31",
        "answer": "B",
        "options": [
          "A. Clear with sunshine.",
          "B. Overcast with heavy clouds.",
          "C. Dark and stormy.",
          "D. Clear with scattered clouds."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_10_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:00:00 - 0:00:05]: The video starts with a close-up of hands working on a wooden cutting board placed on a table. The hands are holding a fork and mashing a yellow substance in a small wooden bowl. Other items on the table include three small white bowls, a small ramekin, a plate with several pieces of grilled food, a small jar, and a knife. The background shows an outdoor setting with sandy ground and rocks, suggesting a beach location.       [0:00:05 - 0:00:06]: The person continues mashing the substance in the wooden bowl using the fork. The view slightly zooms out, showing more of the table and revealing a wooden spatula and another knife to the right side of the cutting board.       [0:00:06 - 0:00:10]: The camera zooms out further, and the person wearing a green t-shirt and sunglasses becomes more visible. The background reveals more of the scenic outdoor setting, which includes a coastal area with the ocean visible in the distance and driftwood scattered around.       [0:00:10 - 0:00:13]: The person continues mashing the contents in the bowl while standing behind the table. The beach environment with sandy and rocky foreground and lush green trees in the background is in clear view. Some cooking ingredients and utensils are also visible on the table. [0:00:13 - 0:00:14]: The individual continues working on the mixture in the wooden bowl. The scene shows the person in the same position, surrounded by the same spread of ingredients and cooking tools on the table. [0:00:14 - 0:00:20]: The person stops mashing and places the bowl to one side. He then move towards the other ingredients on the table. The background continues to feature the picturesque coastline and the driftwood structures.  [0:00:20 - 0:00:23]: The camera angle shifts to a wider view, displaying a panoramic image of the table with the person standing in the middle. More of the surroundings, including the black volcanic rocks and ocean waves, are clearly visible. [0:00:23 - 0:00:25]: The person picks up another ingredient from the table. The wooden bowl is now lying still on the table, and the mashing action has ceased. The ocean and sky in the background present a tranquil, natural setting. [0:00:25 - 0:00:26]: Another pick-up sequence, showing the person reaching for an ingredient on the table. The setting remains the same, with the individual focused on preparing the dish. [0:00:26 - 0:00:27]: The person quickly performs a new action on the table, possibly setting down an ingredient. The table arrangement and background remain constant. [0:00:27 - 0:00:30]: The person grabs a jar of dark-colored sauce or syrup and pours it into one of the small white bowls. The beach, ocean, and driftwood background continue to add a picturesque setting to the scene. [0:00:30 - 0:00:32]: The person starts mixing the contents of the white bowl with a spoon or another utensil. The scenic background remains unchanged, showcasing the ocean waves and bright sky. [0:00:32 - 0:00:33]: The camera angle changes slightly, keeping focus on the person mixing the contents in the white bowl. The surroundings continue to display the same beachy environment. [0:00:33 - 0:00:36]: The individual moves back to the cutting board and picks up a yellow fruit from the table. The position on the beach remains constant, with the driftwood, sandy ground, and ocean waves providing a natural backdrop. [0:00:36 - 0:00:38]: The person places the yellow fruit on the cutting board and prepares to cut it with a knife. The table setup and scenic background remain consistent. [0:00:38 - 0:00:40]: The individual begins cutting the yellow fruit on the wooden cutting board. The focus remains on the person’s hands and the fruit, while the background features the same driftwood, rocks, and ocean view. [0:00:40 - 0:00:42]: The person continues cutting the yellow fruit on the table. More pieces of the fruit are visible, and the individual remains focused on the task at hand. [0:00:42 - 0:00:45]: The close-up shot remains on the hands cutting the yellow fruit, with the slices becoming more evident. The outdoor beach setting continues to provide a picturesque background. [0:00:45 - 0:00:50]: The individual completes cutting the fruit into smaller pieces and arr\n[0:02:20 - 0:02:40] [0:00:20 - 0:00:24]: The video opens with a person, seen only from the waist down, slicing bright yellow pineapple on a large wooden cutting board resting on a dark wooden table. The table also holds a plate of neatly arranged, rectangular pieces of meat, four small white and beige bowls, a few knives, and a metal container. The background reveals a rocky shore with gently lapping waves and a somewhat cloudy sky. [0:00:25 - 0:00:27]: The person, wearing a green shirt and a watch on their left wrist, continues to slice the pineapple into smaller pieces while standing at the wooden table. Slightly bent over, they focus on their task. Behind them, the ocean stretches out, and a leafless tree spreads its branches wide, adding to the natural seaside ambiance. [0:00:28 - 0:00:29]: After cutting enough pineapple pieces, the person reaches for the plate of marinated meat, picking it up and glancing toward the camera, perhaps checking the setup. [0:00:30 - 0:00:31]: Turning away from the table, the person starts to move towards a grill setup located on a rocky outcrop by the sea. The rugged coastline and distant waves lend a picturesque backdrop to the scene, with mountainous terrain discernible on the horizon. [0:00:32 - 0:00:35]: The person positions the plate of pineapple and meat on the grill. He carefully place the pineapple pieces on the grill first, their precise movements ensuring even spacing for optimal grilling. The sky above is partly cloudy, creating a bright yet subdued lighting environment. [0:00:36 - 0:00:37]: Grills several pineapple pieces, ensuring they are evenly spaced and perfectly situated over the heat. He work meticulously, checking each piece's position before moving on. [0:00:38 - 0:00:39]: With the pineapple slices now grilling, the person starts to arrange the meat pieces on a separate section of the grill. The close-up view shows the sizzling pineapple, already starting to develop a slight char, capturing the essence of outdoor cooking by the sea.\n[0:02:40 - 0:03:00] [0:02:40 - 0:02:48]: The scene begins with a close-up of a rectangular metal grilling rack placed over a bed of white charcoal. On the right side of the rack, there are several slices of pineapple arranged in a line. A hand is seen placing square slices of meat next to the pineapples, starting from the left side of the grill. The hand belongs to an individual wearing a dark-colored shirt and holding a metal tray with additional slices of meat, dipped in sauce, in the other hand. The person continues to place more meat slices onto the grill, forming a line. [0:02:49 - 0:02:50]: The camera shifts to a close-up shot of the individual wearing a green shirt. The person's face is hidden by the angle of the shot, but their focus is on the grilling process. He have short, blonde hair and are wearing orange-tinted sunglasses. The background consists of a coastal scene with green foliage and scattered driftwood. [0:02:51 - 0:02:52]: The camera moves back to the grill. The person is seen pouring sauce from the metal tray over the pineapple slices and the meat. Some sauce drips onto the grill, indicating that the meat and pineapple are being marinated as they cook. The grill continues to display a combination of pineapples on the right and meat slices on the left, now covered in sauce for flavor enhancement. [0:02:53 - 0:02:54]: After saucing the grill, the individual in the green shirt is seen walking away from the focus area, holding the now-empty metal tray. Their posture is slightly bent, possibly to avoid the heat from the grill. [0:02:55 - 0:03:00]: Shifting to a wider view, a table is set up on a rocky beachfront with kitchen utensils and ingredients. The individual, still in their green shirt and dark pants, positions themselves behind the table. On the left, there is a chopping board, while on the right side of the table, there is a collection of vegetables and cooking tools. The background features black rocks, a skeletal tree, and distant hills under a cloudy sky. The person begins preparing ingredients on the table, picking up a knife and focusing on the task they are performing.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What happens after the person stops mashing and places the bowl to one side?",
        "time_stamp": "0:02:08",
        "answer": "A",
        "options": [
          "A. He pick up pineapple from the table.",
          "B. He begin slicing a yellow fruit.",
          "C. He start grilling pineapple and meat.",
          "D. He pour sauce into a small white bowl."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_10_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:00:00 - 0:00:03]: The video begins with a close-up of a wooden table set up outdoors. On the table, there are several halved bread rolls, a few bowls containing different ingredients, and a wooden cutting board. A person wearing a green T-shirt and a watch is seen spreading a condiment on one of the bread rolls using a brown wooden brush. The background consists of black volcanic rocks, driftwood, and a body of water with trees and mountains in the distance. The person's hands and the objects on the table are the main focal points. [0:00:04 - 0:00:07]: The camera then focuses on the person lifting another half of the bread roll and spreading the condiment onto it. The person is holding the bread in their left hand and using the brush with their right hand to apply the condiment. The surrounding scenery with the rocky beach and driftwood remains visible in the background, maintaining the outdoor coastal setting. [0:00:08 - 0:00:12]: The camera angle changes slightly to capture the person's concentration as they continue spreading the condiment on the bread. Their facial expression shows focus, and they consistently use the brush to evenly distribute the spread. The coastal environment with the rough terrain and ocean remains prominent in the background. [0:00:13 - 0:00:16]: The camera pans out to show more of the setup, including additional ingredients on the table such as jars, bottles, and kitchen utensils. The person continues working on the bread roll, ensuring every part is covered with the condiment. The scenery in the background includes twisted branches and sparse vegetation against the backdrop of the water and mountains. [0:00:17 - 0:00:19]: In the final frames, the person puts down the brush and picks up the completed bread roll, now fully prepared with the condiment. The table remains organized with culinary tools and ingredients neatly arranged. The outdoor location, characterized by the unique coastal environment with volcanic rocks, driftwood, and distant mountains, provides a scenic and natural backdrop to the culinary activity.\n[0:04:20 - 0:04:40] [0:00:20 - 0:00:26]: The video begins with a person, seen from a first-person perspective, working at a wooden table on a beach. The background reveals a serene ocean with small waves hitting the shore, and some driftwood scattered on the sand. The person, wearing a green t-shirt and orange sunglasses, leans over, focusing intently on an object in their hands. Upon closer inspection, the object appears to be a bun cut into halves. The person is holding both halves, with the top open to show the inside.  [0:00:26 - 0:00:33]: The video then shows the person placing the two halves of the bun down onto a wooden chopping board on the table. Various items are arranged on the table, including three small bowls filled with different ingredients, a small jar, and a cup of liquid, possibly oil, along with some utensils. The person proceeds to sprinkle something over the bun halves with their right hand, occasionally reaching for more ingredients from one of the bowls. [0:00:33 - 0:00:39]: The scenery shifts briefly, offering a wider view of the beach, populated with lush green vegetation and a table filled with additional culinary items. The person, who is standing next to the table, then holds both halves of the bun, occasionally glancing down at them. The camera perspective remains at the person’s eye level as they walk towards a grill set up near the water's edge. [0:00:39]: Finally, the person reaches the grill, which contains a few assorted grilled items, including vegetables and another piece of bread. The person places the bun halves on the grill, ensuring they are evenly placed among the other items. He continue to adjust the buns and other grilled pieces carefully with their hands, maintaining focus on the cooking process. The outdoor setting and culinary activities suggest a casual, beachside cooking session.\n[0:04:40 - 0:05:00] [0:04:40 - 0:04:41]: A close-up shot of a pair of buns and grilled vegetables on a metal grille over an open flame. There is also sliced grilled meat, red bell peppers, yellow zucchini, and some kind of root vegetable on the grill next to the buns. The left hand, wearing a watch, is holding half of a bun, preparing to place it on the grill. [0:04:41 - 0:04:42]: The left hand places the second half of the bun onto the grill, next to the other food items. The food is neatly arranged with the buns on the left half and vegetables and meat on the right half of the grilling area. [0:04:42]: The three bun halves are now aligned on the grill, with two halves having already been placed in the previous frames and the third being placed. [0:04:43]: Both hands come into view, gently pressing the top surface of the buns, ensuring they are placed evenly on the grill. The food remains neatly arranged. [0:04:44]: The left and right hands are seen pressing down the bun halves, which appear to be getting slightly toasted on the surface. The vegetables and meat remain on the right side of the grille. [0:04:45 - 0:04:46]: Both hands start pulling the bun halves apart, slightly separating them from each other while still pressing them down, increasing the grill marks on the bread. [0:04:46 - 0:04:47]: The hands continue to open the bun halves carefully, ensuring a nice even toast while the vegetables and meat continue to cook on the right side. [0:04:47]: The camera captures the hands still working on the buns. The arrangement on the grill remains consistent with the previous frames. [0:04:48]: The hands are meticulously adjusting the placement of the buns on the grill for even toasting. The red bell peppers and sliced meat sit prominently visible on the right side. [0:04:49]: The left hand presses one of the bun halves carefully, ensuring it is fully making contact with the grill. The food continues to grill alongside. [0:04:50 - 0:04:51]: The camera angle shifts upwards, showing a person with short blonde hair wearing dark-tinted sunglasses and a green shirt. The background is filled with blurred-out greenery and branches, suggesting an outdoor setting. [0:04:51 - 0:04:54]: The person in the green shirt is seen leaning forward, focusing intensely. The light glimmers on their sunglasses, reflecting the surrounding scenery. The left arm and shoulder prominently feature in the frame. [0:04:54 - 0:04:56]: The subject continues to lean forward with concentration, seemingly working on something below the camera’s view. The background displays a mix of blurred trees and branches. [0:04:56 - 0:04:58]: The focus remains on the individual who seems to be involved in an activity. The background remains consistent with the natural outdoor setting featuring trees and branches. [0:04:58 - 0:04:59]: The camera shifts back to a close-up shot of the grill. The left hand, wearing a watch, is pressing down on a bun half again, ensuring consistent contact with the grilling surface. The food remains neatly arranged on the grill, with the buns on the left and the grilled vegetables and meat on the right.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "How are the food items arranged on the grill?",
        "time_stamp": "0:04:42",
        "answer": "C",
        "options": [
          "A. The buns are on the right side, and the vegetables and meat are on the left side.",
          "B. The buns are on the top, and the vegetables and meat are on the bottom.",
          "C. The buns are on the left side, and the vegetables and meat are on the right side.",
          "D. The buns are at the center, and the vegetables and meat are on both sides."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_10_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:01]: Four hamburger buns are placed on the left side of a grilling rack, while slices of red bell pepper, meat, and pineapple are on the right side. [0:05:02 - 0:05:03]: A frying pan is being held with a white cloth towel and is positioned over the hamburger buns on the left. [0:05:04 - 0:05:06]: The person continues to hold the frying pan over the buns, pressing down with the towel. [0:05:07 - 0:05:10]: The camera angle broadens to show an individual wearing a green shirt and sunglasses, pressing down on the pan with the towel, while standing beside the grill on a rocky shore with the ocean in the background. [0:05:11 - 0:05:12]: The person starts lifting the frying pan, with the towel still in hand, revealing the grilled items underneath. [0:05:13 - 0:05:14]: The buns, bell peppers, meat slices, and pineapples are visible and continue to grill. [0:05:15 - 0:05:17]: Tongs are used to flip one of the hamburger buns, showing a toasted side. [0:05:18 - 0:05:19]: The hands with tongs flip another bun, with the rest of the food items remaining in their original positions on the grill.\n[0:05:20 - 0:05:40] [0:05:20 - 0:05:23]: A pair of hands is using metallic tongs to handle food on a grill. The grill contains a variety of items including two buns, some grilled patties, slices of bell pepper, and pineapple. The person uses the tongs to lift a grilled patty and place it on the top bun;  [0:05:23 - 0:05:25]: The hands shift to reposition another bun, exposing the grill mesh and allowing the buns to become evenly toasted. The person's focus is on ensuring the buns are well-grilled;  [0:05:24 - 0:05:26]: The tongs place a piece of pineapple on a white rectangular plate that sits on a side section of the grill. The white plate already has two pieces of grilled meat;  [0:05:25 - 0:05:27]: The tongs continue to pick up more pineapple slices from the grill, positioning them neatly beside the grilled meat on the plate;  [0:05:26 - 0:05:28]: More grilled pineapple slices are picked up and placed on the plate. The grill shows signs of heat and charring beneath the food items;  [0:05:27 - 0:05:30]: The person arranges the pineapple slices on the white plate, making sure they are neatly organized. The tongs now move toward the pieces of grilled meat;  [0:05:28 - 0:05:31]: A grilled piece of meat is lifted from the grill and positioned beside the pineapple on the white plate. The person appears methodical in their arrangement of food;  [0:05:29 - 0:05:32]: The process of transferring the food continues, with more grilled items being neatly arranged on the white plate;  [0:05:30 - 0:05:34]: The camera zooms out slightly revealing a person wearing a green shirt and sunglasses. He are focused on the grill while standing in an outdoor setting with a scenic coastline in the background. The person alternates between using tongs and a fork for grilling;  [0:05:33 - 0:05:36]: While continuing to move food from the grill to the plate, the person adjusts the food items and ensures everything is cooked properly. The backdrop includes trees and clear skies;  [0:05:37 - 0:05:39]: Finally, the person places the last few pieces of grilled food on the plate, taking a moment to inspect and ensure everything looks perfect. The close-up reveals their meticulous approach to grilling.\n[0:05:40 - 0:06:00] [0:05:40 - 0:05:41]: A person with short, light-colored hair, wearing a green shirt and sunglasses, looks down intently in a bright outdoor location with greenery in the background. [0:05:42 - 0:05:43]: He use metal tongs to pick up a piece of food from a grill. The grill is laden with different foods, including slices of meat and roasted vegetables, set against a light sandy or stone surface. [0:05:44 - 0:05:45]: The person places the food onto a white rectangular plate containing sliced meat, yellow roasted vegetables, and red bell peppers, using the tongs. [0:05:46]: He adjust the positioning of the food on the plate, ensuring it is neatly arranged while using the tongs. [0:05:47]: The image shows the grill with a few pieces of toasted bread buns still on it, alongside the last piece of fried pineapple, while the person adjusts their stance. [0:05:48 - 0:05:49]: He continue looking down as they work, seemingly focusing on their task, surrounded by a mix of dry, bare trees and lush foliage in the background. [0:05:50 - 0:05:52]: The person continues to arrange the food, ensuring everything is well-placed and checking the placement of the tongs, under a partly cloudy sky. [0:05:53]: He shift slightly as they continue working, their right arm outstretched toward the task at hand, while still gazing downward. [0:05:54]: He pause for a moment, seemingly reassessing their actions or the food arrangement, their left arm bent slightly at the elbow. [0:05:55]: The person picks up a white cloth with one hand while holding the tongs in the other, preparing to finish their task. [0:05:56 - 0:05:57]: He carefully lift the plate of food from the grill, showing the completed dish that features slices of meat, toasted buns, and roasted vegetables. The waves and shoreline are visible in the background. [0:05:58]: Holding the plate of food steady in one hand, they begin to turn away from the grill, with the ocean and distant landmass providing a scenic backdrop. [0:05:59]: The person walks away along the rocky shore, carrying the plate of food and white cloth, with the waves gently lapping at the beach and a tree with sparse leaves nearby.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "Which item placed on the grill has a yellow color?",
        "time_stamp": "0:05:04",
        "answer": "C",
        "options": [
          "A. Meat slices.",
          "B. Bell pepper.",
          "C. Pineapple.",
          "D. Hamburger buns."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_10_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:08:00 - 0:08:15] [0:08:00 - 0:08:03]: A person holding a white rectangular plate with a sandwich on top is standing outdoors. The plate is held with both hands, and the person is wearing a green t-shirt and red sunglasses. In the background, there is a beach scene with black rocks, water, and a leafless tree. [0:08:03 - 0:08:09]: The camera focuses more closely on the sandwich on the plate, showing the details of the sandwich which includes slices of meat, what appears to be grilled vegetables, and a brown sauce. The background remains the same, showing the beach with black rocks and water. [0:08:11 - 0:08:12]: The screen displays a dark background with the word \"brilliant\" written in white neon light. [0:08:13]: The screen displays a dark background with a blue neon outline in the shape of a sandwich, and the word \"idiot\" in white neon at the center.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What item is the person holding right now?",
        "time_stamp": "0:07:37",
        "answer": "A",
        "options": [
          "A. A white rectangular plate with hamburgers.",
          "B. A white circular plate with a fish.",
          "C. A blue rectangular plate with a salad.",
          "D. A green square plate with a pie."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_10_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:03 - 0:00:04]: The video captures a figure in purple pixelated armor standing in the center of a stone pathway. The armor features shades of purple with black accents, and the figure wears a helmet that obscures their face. They are positioned within a wooden structure with stone walls, flanked by lanterns hanging from the wooden beams. The backdrop includes an archway that reveals a blue sky and some foliage outside. [0:00:04 - 0:00:05]: The armored figure remains central in the frame, facing directly towards the camera and appearing to walk forward. The wooden structure extends on both sides, with visible wooden shelves and hanging lanterns illuminating the area. A small flower pot with white flowers is positioned to the right, near the entrance. [0:00:05 - 0:00:06]: The figure stands closer to the camera, filling more of the frame and detailed with large green and blue eyes visible through the helmet. The background remains the same, with a stone pathway leading outside and the greenery still visible through the archway. [0:00:06 - 0:00:07]: The camera zooms in further on the armored figure's face, emphasizing their eyes. The scene remains static, with the figure standing still and the background showing the wooden beams and stone pathway. [0:00:07 - 0:00:08]: The perspective changes, showing a different angle where the armored figure is now outside. They are in a lush, green area with various plants and trees. Lanterns hang from the trees, and there are stone structures visible in the background. The scene is more vibrant with natural light illuminating the surroundings. [0:00:08 - 0:00:09]: The figure moves slightly to the right, revealing more of the background. There is a stone pathway with flowers and lanterns leading up to wooden and stone structures. The area looks well-maintained with lush greenery. [0:00:09 - 0:00:10]: The view shifts to show a pixelated clock face in the center of the frame. The background comprises various structures, including a large white building with a stone foundation and wooden elements. There are crates, barrels, and other objects scattered around, suggesting a lively environment. [0:00:10 - 0:00:11]: The perspective zooms out, giving a broader view of the area. The armored figure is seen walking along a stone pathway surrounded by grass and flowers. The environment is rich with details, including a mix of natural and constructed elements like trees, stone walls, and buildings. [0:00:11 - 0:00:12]: The camera continues to follow the armored figure as they walk along the path. There are stone steps leading downwards, and the area becomes more enclosed with stone walls on either side. The surroundings include patches of grass, a wooden fence, and various plants. [0:00:12 - 0:00:13]: The figure approaches an area with stone walls and an iron fence. Beyond the fence, there's a glimpse of additional buildings and structures. The pathway continues to curve, and the figure appears to be descending. [0:00:13 - 0:00:14]: The camera zooms out to show more of the outdoor environment. The figure is seen moving through an area with a mix of stone and wooden structures, surrounded by trees and other greenery. The sky is visible with clouds scattered above. [0:00:14 - 0:00:15]: The figure continues walking through the landscape. The scene captures a variety of elements, including wooden buildings with intricate details, stone pathways, and lush trees. The area appears to be a lively combination of nature and architecture.  [0:00:15 - 0:00:16]: Details on the surrounding structures become more visible, revealing a mixture of medieval-style buildings and natural features like trees and grass. The figure is heading towards a more open area with additional buildings and structures. [0:00:16 - 0:00:17]: The perspective shifts to show the exterior of a large white building with stone steps leading to an entrance. There are wooden beams and decorative elements, indicating a well-crafted structure. The surroundings include trees, stone pathways, and other buildings in the background.  [0:00:17 - 0:00:18]: The camera captures more of the landscape, highlighting a blend of architecture and natural beauty. The sky is partly cloudy, and the environment is rich with greenery and detailed structures, creating a harmonious setting.  [0:00:18 - 0:00:20]: The view pans upwards to reveal a tall, detailed structure with a clock tower. The surroundings remain consistent, with an assortment of trees, stone pathways, and buildings, all under a",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the figure in the video wearing?",
        "time_stamp": "0:00:04",
        "answer": "C",
        "options": [
          "A. A red cloak.",
          "B. Blue jeans.",
          "C. Purple pixelated armor.",
          "D. A green suit."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_188_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:40 - 0:09:00] [0:00:00 - 0:00:20]: In the dimly lit environment, indicative of being inside a game world, there is an expansive sandy ground beneath a dark sky. The scene is set in \"The End,\" a dimension from Minecraft. The camera indicates a first-person perspective, as the health bar, hunger bar, and inventory are visible. In the first frame at 0:00:00, a tall, black creature with glowing purple eyes, known as an Enderman, stands slightly to the front-left. Up in the center of the frame is an Ender Dragon health bar, suggesting a battle is underway. There seems to be purple, sparkly particles around. [0:00:01]: The camera moves closer to the Enderman as it begins to charge toward the viewpoint, indicating an attack. The health bar and inventory remain consistent through this sequence. [0:00:02]: Two Endermen are now in view, one prominently closer on the right side of the frame, swaying side to side, while another remains in the background. The sandy terrain continues to span outward, with distinctive pyramid shapes on the ground coming into view. [0:00:03 - 0:00:04]: More Endermen join the scene as the camera continues moving forward. Their glowing purple eyes are a notable feature against their dark bodies. The sandy backdrop appears quite consistent, with the pyramid shapes forming a path. [0:00:05]: The number of Endermen increases, some standing close together. Two come even closer to the camera, occupying more of the frame. Their features, including long limbs and teleportation abilities exhibited by disappearing particles around them, are pronounced. [0:00:06]: Close-up interaction with two Endermen at 0:00:06, both directly in front of the camera, emphasizes their tall stature and purple-eyed gaze. More Endermen are noticeable in the background with similar characteristics. [0:00:07]: With some Endermen closer than before, the player's health starts visibly depleting slightly, indicating damage from the encounter. Some pyramid patterns on the ground continue to be evident alongside more clusters of dark blocks. [0:00:08]: As the camera looks downward towards the sandy terrain, a distinct focus on the \"Baked Potato\" item in hand suggests a need for health regeneration after the attack. A few Endermen are still visible but further away now. [0:00:09]: The player character eats a baked potato for health regeneration purposes. The frame features a sandy ground with a few Endermen in the distance to the right. [0:00:10]: Continuing to consume the baked potato at 0:00:10, the health bar starts to fill up. Still surrounding them are scattered black structures and Endermen, though fewer in number now. [0:00:11]: Subsequently, the viewpoint shifts focus towards the Ender Dragon flying around one of the tall obsidian pillars. The presence of an Enderman nearby and the swirling particles highlight the ongoing dangerous environment. [0:00:12]: The camera captures the Ender Dragon from a distance, indicating its flight path and threatening presence. The sandy terrain and the tall obelisk structures maintain a sense of vast openness and height. [0:00:13]: The frame gets darker as the camera navigates closer to one of the pitch-black pillars. Some hearts from the health bar are missing, showing the player's need for more strategic actions. [0:00:14]: Upon turning, a group of Endermen crowd around an apparent damage area. Swirling particles where damage has occurred draw attention to active danger zones. [0:00:15]: The player character now consumes a golden apple for additional health and some temporary advantages as indicated by the added potion effect icon in the inventory segment. The sandy expanse and some Endermen in the distance continue to define the environment. [0:00:16]: Moving further away from the crowd of Endermen, each step is pivotal for survival indicated by the shifting perspective over the sandy ground. The camera centers on the environment and the position changes frequently. [0:00:17]: Once again, an encounter with an Enderman is imminent. A single Enderman on the left seems closer as the engagement continues. The health level and effects remain visibly crucial. [0:00:18]: During combat, notable hits against an Enderman turning red upon damage highlight the critical interaction points. Surrounded by consistent sandy terrain and multiple pillars forming the background, the level of threat remains high. [0:00:19]: With an Enderman being struck and turning red due to damage received, additional Endermen watch or wait for their turn to attack in the background. The striking of enemies remains a recurrent activity. [0:00:20]: The scene ends with combating Endermen, evident from the particles flying",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the item being held by the player character right now?",
        "time_stamp": "00:08:47",
        "answer": "B",
        "options": [
          "A. Ender Pearl.",
          "B. Baked Potato.",
          "C. Golden Apple.",
          "D. Diamond Sword."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the player character's health start depleting?",
        "time_stamp": "00:08:42",
        "answer": "B",
        "options": [
          "A. Lack of food.",
          "B. Attack by Endermen.",
          "C. Fall from a height.",
          "D. Poison effect."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_188_real.mp4"
  },
  {
    "time": "[0:13:00 - 0:14:00]",
    "captions": "[0:13:00 - 0:13:20] [0:13:00 - 0:13:01]: The camera initially faces a mountain decorated with greenery and a structure at its peak. The sky is gray, indicating rainy weather. Drops of rain are visible, adding to the wet atmosphere. Below, there is a wooden dock with items such as a shield, hearts, experience level, and various inventory items visible at the bottom center of the display. [0:13:01 - 0:13:02]: The view shifts slightly to the right, revealing more details of the dock, including a potted plant with purple flowers on the left. In the distance, there is a multi-colored complex structure built into the rocky hill, featuring buildings with varying roof colors. [0:13:02 - 0:13:03]: Moving forward, the view approaches a staircase leading up to the structures. Two prominent buildings appear near the center, one with a red roof and the other with a brown roof. A wooden boardwalk extends from the dock towards these buildings. [0:13:03 - 0:13:04]: Upon closer approach, the structures become more defined. The staircase leading up to the buildings is flanked by wooden railings. Various buildings and structures are nestled into the rock face, with lush greenery decorating the spaces between them. [0:13:04 - 0:13:05]: The path leads the viewer up towards the wooden stairs that are part of a more extensive build. The stairs look sturdy, with several stone and wooden elements. Two lampposts with lit lanterns hang on either side of the initial stairs. [0:13:05 - 0:13:06]: A more focused view of the stone staircase now dominates the shot. The intricate woodwork of the surrounding structures frames the steps. [0:13:06 - 0:13:07]: Ascending the stairs, the view points straight up to the top, where the buildings continue to rise against the backdrop. [0:13:07 - 0:13:08]: Reaching the top, the perspective shifts to the right, revealing a grassy hillside with wildflowers, including a striking red flower near the center. [0:13:08 - 0:13:09]: The view shifts towards a wooden structure, probably a fence or a part of the building on the right as the grassy hill stretches upwards. [0:13:09 - 0:13:10]: Looking back, several wooden elements of the previous structures are again in view. In the backdrop, a tall, blue-roofed tower or building comes into sight, contrasting against the dull sky. [0:13:10 - 0:13:11]: The scene returns to the stairway with wood structures on either side. A hay bale is visible to the right-hand side. [0:13:11 - 0:13:12]: Ascending the stone steps again, the view emphasizes the path leading towards the higher ground through the wood and stone arrangements. [0:13:12 - 0:13:13]: At the top of the stairs, greenery dominates the foreground with several yellow and black figures resembling bees hovering mid-air. [0:13:13 - 0:13:14]: An attempt to climb the grassy hill is made. The camera focuses on the uneven earth and green foliage. [0:13:14 - 0:13:15]: The ascent is pursued further. The perspective offers a view downwards toward the wooden structures previously encountered at the dock area. [0:13:15 - 0:13:16]: Continuing to climb upwards, the detailed texture of the grass and earth comes into sharp focus, along with distant structures. [0:13:16 - 0:13:17]: The climb is maintained, facing the green pasture blending into the structures behind. [0:13:17 - 0:13:18]: Upon reaching a higher point, the view looks back for a panoramic perspective, showcasing structures, docks, and the sea in the distance. [0:13:18 - 0:13:19]: Orienting back towards the hilltop, the rain persists, maintaining the gloomy and wet scene as the ascent continues. [0:13:19 - 0:13:20]: Near the hilltop, more of the wooden and grassy landscape is observed. The bees are still visible, maintaining the natural, vibrant scene against the dreary weather.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What can be seen to the left-hand side of the stairway?",
        "time_stamp": "00:13:11",
        "answer": "D",
        "options": [
          "A. A potted plant.",
          "B. A blue-roofed tower.",
          "C. A hay bale.",
          "D. A red flower."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_188_real.mp4"
  },
  {
    "time": "[0:16:00 - 0:17:00]",
    "captions": "[0:16:40 - 0:17:00] [0:16:41 - 0:16:44]: The video begins in a daytime outdoor setting within a Minecraft world, showing a complex structure in the background built from wooden materials and stone, with various chests stacked systematically. Smoke or steam rises from multiple points on the structure. In the foreground, there are wooden and stone blocks arranged beside a blue block with a pixelated face, resembling a villager. [0:16:44 - 0:16:46]: The scene focuses on the blue block with a pixelated face, which is positioned beside a wooden and stone structure. The block's detailed features come into sharper view. [0:16:46 - 0:16:47]: The video continues to highlight the pixelated face on the blue block. The background still consists of wooden and stone structures, with some greenery visible on the stone ground. [0:16:47 - 0:16:48]: The focus remains on the blue block with the pixelated face, which includes green eyes and other colorful details. The adjacent structures in the background stay unchanged. [0:16:48 - 0:16:49]: The frame transitions to a Minecraft character with green hair and a black mask, in an indoor environment with wooden walls. The character wears purple and black armor and holds an item in their hand. Lanterns hang in the background. [0:16:49 - 0:16:50]: The same character remains in the frame, prompting the appearance of a \"Subscribe\" button with an exclamation mark icon in front of them. [0:16:50 - 0:16:51]: The \"Subscribe\" button changes to show the channel's name with a checkmark indicating an active subscription.  [0:16:51 - 0:16:52]: The character continues to face the viewer, with the subscription confirmation visible in the frame. The surroundings show more indoor wooden structures and storage chests lining the walls. [0:16:52 - 0:16:54]: The indoor scene persists, showing the character with green hair and a black mask staying in focus. Additional items and plants can be seen around the area, along with neatly arranged chests and blocks. [0:16:54 - 0:16:55]: The screen fades to black, indicating a transition to an end screen. [0:16:55 - 0:16:59]: The end screen displays a list of Diamond Patrons on the left, with names listed in two columns within a blue border. On the right, there is a list of Gold Patrons in a similar two-column format within a yellow border. [0:16:59 - 0:17:05]: The end screen continues to show the names of Diamond and Gold patrons. The bottom of the screen includes URLs and icons for additional social media and websites associated with the content creator.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action is being performed by the character in purple and black armor right now?",
        "time_stamp": "00:16:46",
        "answer": "C",
        "options": [
          "A. Running.",
          "B. Attacking an enemy.",
          "C. Holding an item and prompting a \"Subscribe\" button.",
          "D. Building a structure."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_188_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:08]: The video starts with a first-person perspective showing a storage area filled with various boxes and crates of goods. The person appears to be wearing gloves and is maneuvering a cart loaded with multiple crates of dairy products. The shelves on the left side contain items with noticeable price tags and labels. The person begins moving the cart further into the aisle. [0:00:09 - 0:00:20]: As they move through the aisle, the person stops their cart near more shelves stocked with various items, perhaps yogurts or creams. They begin restocking the products from the cart onto the shelf directly in front of them. The items being restocked are carton packages of some dairy product, and the individual reaches out to place the items neatly on the shelf. The surroundings are organized, with items placed in visible rows.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person maneuvering at the start of the video?",
        "time_stamp": "0:00:08",
        "answer": "A",
        "options": [
          "A. A cart.",
          "B. A trolley.",
          "C. A box.",
          "D. A crate."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_445_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:42]: A person wearing black gloves is seen positioning their hands over a display of dairy products, such as milk cartons and yogurt cups, stacked in various arrangements. The display includes items with blue, green, red, and white packaging. The individual appears to be organizing or inspecting the items on a pallet. [0:02:43]: The individual shifts focus to a cart filled with more dairy products. Some items are in blue packaging, while others are white with red and green elements. The cart is in what seems to be a storage or stockroom area. [0:02:44 - 0:02:46]: The individual moves back to the pallet with the products, continuing to adjust the items, ensuring they are properly aligned and organized. The surrounding area remains consistent with a storage room setting, including shelves and storage carts nearby. [0:02:47 - 0:02:51]: The individual further adjusts the items on the pallet, moving some products to different positions. The space remains consistent, with visible metal shelves containing more boxed products. The person seems methodical, ensuring products are placed correctly. [0:02:52 - 0:02:55]: The perspective shifts slightly to show more of the shelving units in the storage room. The shelves are stocked with a variety of boxed and bottled goods. The individual continues handling the dairy products, taking some items from the pallet to place on the shelves. [0:02:56 - 0:02:58]: The individual focuses on placing the last few items from the pallet onto the shelves. The spacing and positioning of items on the shelf suggest an effort to align and organize for easy access and inventory management. [0:02:59]: The shelves appear fully stocked with various dairy products, and the individual steps back, completing the task of stocking and organizing the items in the storage room. The room is well-lit, and products are neatly arranged, indicating a finished task.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What kind of products is the person handling?",
        "time_stamp": "0:03:00",
        "answer": "A",
        "options": [
          "A. Dairy products.",
          "B. Bakery items.",
          "C. Canned goods.",
          "D. Fresh produce."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_445_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:22]: In a storage room, a person wearing black gloves and a black jacket is holding a cardboard box and standing in front of a shelf filled with cartons of differently-priced goods. The person is facing a section of 35.95 labeled cartons.  [0:05:23 - 0:05:24]: The person walks towards a stack of various goods, including neatly arranged cartons and other items. They begin to reach towards the stack while still holding the box. [0:05:25]: The person stands next to the stack, with a view of items on the floor and a trolley nearby. The box appears to be placed or adjusted. [0:05:26 - 0:05:28]: The person starts returning to the shelf area, passing other stacks of goods. They pick up another carton from the shelf with several price tags ranging from 28.95 to 31.95. [0:05:29 - 0:05:31]: The person reaches up to the shelf and places a carton on it, adjacent to the 35.95 labeled cartons. The box remains on the floor beside the person's feet. [0:05:32 - 0:05:34]: The person continues placing more cartons onto the shelf, filling spaces next to others. They are focused on stocking the cartons of similar products. [0:05:35 - 0:05:37]: The person picks another carton from the box on the floor and places it on the same shelf. They adjust the placement to ensure proper alignment of rows. [0:05:38 - 0:05:40]: Returning to a view of the storage room, the person continues organizing the cartons, with stacks of various goods visible around them. The trolley and other items remain in the background.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the main activity of the person in the storage room?",
        "time_stamp": "00:05:40",
        "answer": "B",
        "options": [
          "A. Checking inventory levels.",
          "B. Stocking and organizing cartons on shelves.",
          "C. Cleaning the storage room.",
          "D. Labeling the cartons."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_445_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: The view is from a first-person perspective. The scene shows a person pushing a wire cart filled with various items, including a stack of blue containers and yellow-labeled items. The cart is in a storage room with cartons of red and white packaged goods stacked against the wall. The floor is concrete, and there are some cardboard pieces and other items scattered near the stacks of cartons. [0:08:02 - 0:08:04]: The person is lifting a group of blue-labeled containers from the pile of red and white packaged goods. They place the blue-labeled containers into the wire cart. The pile of red and white cartons is on a plastic sheet close to the wall, suggesting the area is used for storage. There are various other boxes and items around the room. [0:08:05 - 0:08:07]: Returning to the stack of red and white cartons, the person lifts another set of blue-labeled containers. The cart is positioned next to the stack, making it easy for the person to transfer items into it. The wall and floor continue to frame the background, with scattered packaging materials around. [0:08:08 - 0:08:09]: The person places another set of blue-labeled containers into the cart. The view captures a glimpse of the nearby stacks and the wire cart filled with blue-labeled and other assorted items. The room continues to have scattered cardboard and loose items around. [0:08:10 - 0:08:11]: The person is now lifting white-labeled items from the green rectangular cartons stacked next to the red and white cartons. These green cartons are positioned horizontally and have branding on them. The wire cart is visible as the person transfers the items into it, with red and white cartons and scattered cardboard still in the frame. [0:08:12 - 0:08:14]: Going to the rows of red and white cartons, the person organizes and moves some items around. The background remains consistent with the previous scenes, showing the white wall, concrete floor, and scattered packaging materials. The person's hands are active in adjusting and picking items. [0:08:15 - 0:08:17]: The person carries some red and white cartons towards the back corner, near the metal storage racks. This area has more boxes and storage devices, continuing the theme of an organized storage space. The cartons are stacked and arranged along the wall and storage racks. [0:08:18 - 0:08:20]: Showing a top view of the neatly arranged cartons and the areas surrounding the storage room. The person continues to handle the stacks of red and white cartons, reflecting a systematic process of managing and organizing storage inventory. The room's layout reveals various storage methods, from wire carts to metal racks, reinforcing the efficient usage of space.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type of containers is being placed into the wire cart?",
        "time_stamp": "00:08:07",
        "answer": "A",
        "options": [
          "A. White and blue labeled.",
          "B. Red and white labeled.",
          "C. Green and white labeled.",
          "D. Yellow labeled."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing after lifting the white-labeled items?",
        "time_stamp": "00:08:14",
        "answer": "A",
        "options": [
          "A. Put them on the cart.",
          "B. Sorting and organizing items.",
          "C. Pushing the cart out of the room.",
          "D. Disposing of the items."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_445_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located on the right side of the road right now?",
        "time_stamp": "00:00:22",
        "answer": "A",
        "options": [
          "A. A white house.",
          "B. Several cars.",
          "C. A cafe.",
          "D. A bank."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_296_real.mp4"
  },
  {
    "time": "[0:03:38 - 0:03:58]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the cyclist approaching right now?",
        "time_stamp": "00:03:53",
        "answer": "A",
        "options": [
          "A. A sharp left curve.",
          "B. A steep incline.",
          "C. A sharp right curve.",
          "D. A flat, straight road."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_296_real.mp4"
  },
  {
    "time": "[0:07:16 - 0:07:36]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Which of the following best describes the road's layout right now?",
        "time_stamp": "00:07:22",
        "answer": "A",
        "options": [
          "A. The road is straight with a slight downhill grade.",
          "B. The road is curving sharply to the left.",
          "C. The road is curving gently to the right.",
          "D. The road is straight with a slight uphill grade."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_296_real.mp4"
  },
  {
    "time": "[0:10:54 - 0:11:14]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Which of the following best describes the road's layout right now?",
        "time_stamp": "00:11:13",
        "answer": "A",
        "options": [
          "A. The road is curving sharply to the right.",
          "B. The road is curving gently to the right.",
          "C. The road is straight with a slight downhill grade.",
          "D. The road is straight with a slight uphill grade."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_296_real.mp4"
  },
  {
    "time": "[0:14:32 - 0:14:52]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the car relative to the cyclist right now?",
        "time_stamp": "00:14:21",
        "answer": "A",
        "options": [
          "A. In front of the cyclist.",
          "B. Behind the cyclist.",
          "C. Beside the cyclist on the left.",
          "D. Beside the cyclist on the right."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_296_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the car directly in front right now?",
        "time_stamp": "00:00:04",
        "answer": "A",
        "options": [
          "A. White.",
          "B. Black.",
          "C. Yellow.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_458_real.mp4"
  },
  {
    "time": "[0:01:49 - 0:01:54]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What brand logo is visible on the rear of the car directly in front right now?",
        "time_stamp": "00:01:52",
        "answer": "D",
        "options": [
          "A. Toyota.",
          "B. Honda.",
          "C. Audi.",
          "D. Porsche."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_458_real.mp4"
  },
  {
    "time": "[0:03:38 - 0:03:43]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What colored barriers are visible on the passenger-side sidewalk right now?",
        "time_stamp": "00:03:39",
        "answer": "D",
        "options": [
          "A. Red.",
          "B. Green.",
          "C. Yellow.",
          "D. Blue and white."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_458_real.mp4"
  },
  {
    "time": "[0:05:27 - 0:05:32]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the vehicle approaching the driver's right side right now?",
        "time_stamp": "00:05:27",
        "answer": "A",
        "options": [
          "A. Green.",
          "B. Red.",
          "C. Blue.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_458_real.mp4"
  },
  {
    "time": "[0:07:16 - 0:07:21]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the make of the vehicle directly in front right now?",
        "time_stamp": "00:07:18",
        "answer": "D",
        "options": [
          "A. Toyota.",
          "B. Honda.",
          "C. Nissan.",
          "D. Mitsubishi."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_458_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: A person in a white jersey with red checkered patterns stands on a green field. The number \"13\" and the name \"Sosa\" is visible on the back of the jersey. The person has short cropped hair, and their gaze is focused ahead.;  [0:00:01 - 0:00:04]: The person takes a step, maintaining their focused expression as they move forward. The field remains consistently green.;  [0:00:02 - 0:00:03]: Three people in white jerseys with red checkered patterns are seen from behind with their arms around each other’s shoulders. The names \"KRAMARIC,\" \"MODRIC,\" and \"MAJER\" are visible on their jerseys with the numbers \"16,\" \"10,\" and \"7\" respectively. The crowd in the background, dressed in yellow, fills the stadium.;  [0:00:03 - 0:00:04]: The three people maintain their group stance with their arms around each other, facing the field. The stadium crowd remains lively in the background.;  [0:00:04 - 0:00:05]: A close-up shot shows the face of the person in the white and red checkered jersey. They appear focused and concentrated, looking straight ahead with a slight frown.;  [0:00:05 - 0:00:09]: The person's face shows determination as their eyes remain fixed forward. The background is blurred, highlighting their concentration.;  [0:00:09 - 0:00:13]: The person turns their head slightly to the right without changing their serious expression. Their facial hair is clearly visible, and the green field remains in the background.;  [0:00:13 - 0:00:14]: The person shifts their gaze back to the front as they remain focused. Their facial expression suggests deep concentration.;  [0:00:14 - 0:00:15]: An overhead view of an ongoing soccer match shows the person in the white and red checkered jersey running towards a ball placed in the penalty area. The goalpost is visible in the distance with a goalkeeper ready to defend.;  [0:00:15 - 0:00:16]: The person prepares to kick the ball as the goalkeeper dives to intercept the shot. The field is well-marked, and the audience in the stands watches attentively.;  [0:00:16 - 0:00:17]: After the kick, the person stands with their back to the camera as the goalkeeper stretches out on the ground. The ball is on its trajectory towards the goal.;  [0:00:17 - 0:00:18]: The person in the white jersey, now considerably closer in the frame, appears with clenched fists and an expression of triumph. The field background remains unchanged.;  [0:00:18 - 0:00:19]: As the video concludes, the person continues to show a composed yet energized demeanor, still on the green field with the black-clad referee slightly blurred in the background.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the number on the back of MAJER's jersey?",
        "time_stamp": "0:00:10",
        "answer": "A",
        "options": [
          "A. 7.",
          "B. 10.",
          "C. 13.",
          "D. 16."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Event Understanding",
        "question": "What event occurred just now?",
        "time_stamp": "0:00:19",
        "answer": "A",
        "options": [
          "A. A penalty kick.",
          "B. A free kick.",
          "C. A goal celebration.",
          "D. A corner kick."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_2_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:40 - 0:02:00] [0:01:40 - 0:01:41]: Two football players with the Croatian national team jerseys, numbers 10 and 27, stand with their arms over each other’s shoulders. They appear to be in a formation, possibly lining up for a penalty shootout or a team huddle. The players’ focus is directed downward. The player's brow furrows in concentration as he looks toward the ground. [0:01:41 - 0:01:43]: An additional player, number 16, and yet another, number 21, on either side of the original pair, also show arms interlocked, forming a close-knit unit. Their faces remain serious and focused. The goalposts, along with the goalkeeper and another player from the opposing team, are visible in the background, indicating the setting is near the penalty area. The crowd behind the goalposts is dressed primarily in yellow, cheering with raised hands and signs. [0:01:44 - 0:01:45]: A close-up shot depicts a fan in the stands wearing a Croatian national team jersey. He raises his hands in anticipation or celebration. His eyes show intensity, and his demeanor is deeply engrossed in the unfolding event. [0:01:45 - 0:01:47]: The same fan now has his hands on his head, encapsulating a moment of suspense. The crowd around him displays various expressions, some capturing the moment with their phones.  [0:01:46 - 0:01:49]: One of the players with slightly long brown hair secured by a headband is shown in a close-up view. His gaze is intensely focused in the direction ahead, suggesting he is preparing for or contemplating an important move. [0:01:50 - 0:01:51]: The player's face is determined. He appears resolute as he approaches the ball. The scene shifts to show him running up towards the ball placed at the penalty spot. [0:01:51 - 0:01:52]: The player is captured mid-stride as he takes the penalty shot. The goalkeeper dives to his left in an attempt to save the ball. The setting includes a clear view of the goalposts and the crowd backdrop. [0:01:53 - 0:01:54]: After the shot, the player’s expression is visible. His head is slightly bowed, indicating he is either reflecting on the outcome or in relief. [0:01:54 - 0:01:55]: In the background, another player in yellow appears to be clearing from the area near the goalpost. The scorer maintains a composed expression, blending into the setting of the active play atmosphere. [0:01:55 - 0:01:56]: The scene transitions to a middle-aged man in a suit who seems to be part of the coaching or managerial staff. His expression is neutral-yet-focused as he oversees the actions on the field. [0:01:56 - 0:01:57]: The man's expression remains unchanged. The determined gaze suggests he is watching the subsequent events of the game closely. [0:01:57 - 0:01:59]: A close-up shot shows a wildly cheering fan, his mouth wide open, fists raised in jubilation, capturing the raw emotions encapsulating the dramatic moment. The intensity in the air is palpable, marked by collective energy and spirited reactions from the crowd surrounding him.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the goalkeeper doing as the player takes the penalty shot?",
        "time_stamp": "0:01:52",
        "answer": "B",
        "options": [
          "A. Standing still.",
          "B. Diving to his left.",
          "C. Running towards the ball.",
          "D. Waving his hands."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_2_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:21]: A soccer player in a white and red checkered uniform is seen running on a green field away from the goalpost. Behind the player, a goalkeeper clad in black is positioned near the goal, which features 'QatarEnergy' advertisements lining the bottom of the net. The stands are filled with spectators dressed predominantly in yellow;  [0:03:21 - 0:03:22]: A closer shot of the same player reveals him in action, identifiable by his number 10 jersey and captain's armband. His long brown hair is slightly disheveled as he moves forward;  [0:03:22 - 0:03:23]: The player looks ahead with focus and determination visible on his face while continuing his stride;  [0:03:23]: Excited spectators, adorned in team colors of white and red, fervently cheer. A young woman with curly hair is visible, her face painted with a bold red and white design, reflecting her enthusiasm;  [0:03:24]: More of the crowd is shown, their expressions filled with anticipation and excitement. The young woman now has her arms raised, and another fan sporting a blue and red checkered hat is seen next to her;  [0:03:25]: A side profile of the cheering crowd highlights their energetic reactions. The camera focuses on the fan adorned in the checkered hat and another individual, whose arm is extended in an animated cheer;  [0:03:26]: The video cuts back to the player. He clasps his hands over his mouth, overwhelmed with emotion, with yellow-clad spectators in the background adding context to the event's significance;  [0:03:27]: The player remains in a state of emotional reverence, eyes closed and hands still covering his mouth as the expressions of joy and disbelief play across his face;  [0:03:28]: The player appears to be speaking or exclaiming something, hands close to his face as he processes the moment amidst the unfolding match;  [0:03:29]: With a resolute expression, he clenches his fists in front of him, celebrating with teammates and fans. His head bends slightly forward, emphasizing his focused determination;  [0:03:30 - 0:03:31]: He turns to his left, his face filled with joy and satisfaction, continuing to celebrate;  [0:03:32]: The player's celebration continues as he looks towards the sidelines or possibly to teammates, sharing his joy and anticipation;  [0:03:33 - 0:03:34]: The camera shifts to a wide-angle view from behind the goal. The goalie's back is framed, and the opposing player is seen preparing for a decisive kick;  [0:03:34 - 0:03:35]: The goalkeeper, clad in a black jersey, extends both arms horizontally to prepare for the incoming shot. The kicker draws closer to the ball, poised for a strike;  [0:03:35 - 0:03:36]: The moment of strike, the player connects with the ball, sending it towards the goal. The goalie reacts swiftly, diving to his right in an attempt to block;  [0:03:36 - 0:03:37]: The ball is seen on the edge of the net as the goalie misses the save. The player's successful shot sends the ball into the goal, and the net bulges in response to the contact;  [0:03:37 - 0:03:38]: Back view of the net capturing the intensity of the moment with the ball securely in the goal. The goalie lies on the ground, extending his arms in an expression of disappointment or dismay;  [0:03:38 - 0:03:39]: Emotion continues as the successful player bends forward, overwhelmed. The spectators dressed in yellow in the background add to the festive atmosphere with their cheers and reactions;  [0:03:39]: Finishing the sequence, the player is seen shouting in elation, throwing his head back in a victorious gesture with arms slightly raised. The intensity of the moment is amplified by the enthusiastic background and the overall celebration of success.",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why did the player celebrate by clenching his fists and bending his head forward?",
        "time_stamp": "00:03:31",
        "answer": "A",
        "options": [
          "A. He just scored a penalty kick.",
          "B. He blocked a shot.",
          "C. He made a successful pass.",
          "D. He received a yellow card."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_2_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:01]: A person wearing a checkered red and white uniform is seen running in motion. The individual appears to be on a grassy field with a large digital advertisement board in the background displaying white geometric patterns. Behind the board, there are several people holding cameras and wearing different shades of blue, green, and brown clothing. There are also flags and banners with varied designs hung in the background. [0:05:01 - 0:05:02]: The same player from the previous frame is in the action of kicking a ball towards the goal. The ball is captured mid-air nearing the goalpost. The background remains largely unchanged with several individuals and equipment visible. [0:05:02 - 0:05:03]: The frame shows a diving goalkeeper wearing black gloves and uniform. The goalkeeper attempts to stop the incoming ball. The ball is seen very close to the goal line, just beyond the goalkeeper's reach. The net and more audience members can be seen in the background. [0:05:03 - 0:05:04]: The goalkeeper is now on the ground, having missed the ball. The ball appears to be inside the goalpost. The audience and equipment still present in the background. [0:05:04 - 0:05:05]: The player from earlier is now shown celebrating passionately with clenched fists. The player's facial expression matches that of jubilation. In the background, audience members draped in green, yellow, and other colors are visible amongst blurred crowds. [0:05:05 - 0:05:06]: The celebrating player continues to display excitement, and his clenched fists and strong posture remain prominent. The audience, many clapping and cheering, is captured behind the player. [0:05:06 - 0:05:07]: The same celebrating player is still emotionally engaged with the scene, his facial expression showing strong emotion. The surrounding audience remains active and engaged as well. [0:05:07 - 0:05:08]: The player continues celebration. The foreground focus remains on the player, with the audience still visible in the background, some holding up phones or cameras. [0:05:08 - 0:05:09]: The next frame shifts to another individual with bleached hair and tattoos on their arms, wearing a yellow shirt. This individual's expression appears intense and focused, as they stand closely with another person wearing matching attire. [0:05:09 - 0:05:10]: The same person from the previous frame is now seen with their eyes closed and mouth open, appearing to shout or express strong emotion. The individual’s furrowed brow adds to the intensity of the moment. [0:05:10 - 0:05:11]: The individual's head is slightly bowed down, and they have a shiny substance, possibly celebratory confetti or something else reflective, on their head. The background still shows people with vivid colors of yellow and bright lights. [0:05:11 - 0:05:12]: A new individual is introduced, wearing the same yellow uniform, and they are holding a football. The person's expression is focused, with a serious demeanor indicating preparation. [0:05:12 - 0:05:13]: The same individual is seen looking down at the ball in their hands, remaining focused. The background shows audience members and security personnel as well. [0:05:13 - 0:05:14]: The individual continues holding and looking at the ball with great focus. The setting remains the same with audiences filling the stands and some visible details such as stairs in the arena. [0:05:14 - 0:05:15]: The frame captures the same individual intently preparing for what appears to be a crucial moment, possibly taking a penalty shot. The scoring board shows \"Croatia 4 - 2 Brazil\" with other visual elements indicating it’s a penalty shootout. [0:05:15 - 0:05:16]: The player brings the ball closer to their face and kisses it with great intentionality. The display board and the scores remain in the view behind them. [0:05:16 - 0:05:17]: The person slightly bends down, still with the ball in hand, preparing for an imminent engagement with the ball. [0:05:17 - 0:05:18]: The same individual is now seen fully bent and concentrating, nearing the moment of possible contact with the ball. The surroundings include diverse arena details and audience expressions. [0:05:18 - 0:05:19]: The frame shows the individual still in concentration, just before they likely engage with the ball. The bright colors of the audience and the arena’s elements add to the anticipation.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the current score shown on the scoreboard?",
        "time_stamp": "00:05:15",
        "answer": "B",
        "options": [
          "A. Croatia 3 - 2 Brazil.",
          "B. Croatia 4 - 2 Brazil.",
          "C. Croatia 4 - 3 Brazil.",
          "D. Croatia 5 - 2 Brazil."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_2_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The frame shows a muscular man standing indoors with a large window in the background that has a grid pattern. He is shirtless, wearing only black shorts, and is facing diagonally to the right while posing and flexing his muscles. On the right side in the foreground, there is gym equipment that is out of focus. The background is primarily a dark-colored wall. Text on the screen says \"HOW I TRAIN.\" [0:00:04 - 0:00:12]: The screen changes to a layout divided into three sections labeled \"CHEST,\" \"SHOULDERS,\" and \"TRICEPS.\" Each section contains smaller images depicting different exercises. The first section (\"CHEST\") shows a person performing a bench press and other chest exercises. The middle section (\"SHOULDERS\") shows various shoulder workouts, including a person lifting weights overhead. The third section (\"TRICEPS\") displays triceps exercises, including a person lying on the ground performing tricep presses. Text in the middle reads \"CHEST, SHOULDERS & TRICEPS.\" [0:00:13 - 0:00:15]: The scene transitions to a gym interior with a person wearing a hat and a dark sweatshirt with an orange and white graphic. He is walking on a treadmill or StairMaster. The background shows more gym equipment like stationary bikes. A text box appears at the bottom of the screen with the heading \"WARM-UP\" and the instruction \"5 MINUTES TREADMILL OR STAIRMASTER.\" [0:00:16 - 0:00:17]: The person is still walking on the treadmill. The camera angle shifts to a side view, highlighting his movements. He is wearing shorts and black shoes. Surrounding him are several other treadmills, and gym users can be seen using similar machines. The background consists of a white wall with black accents. [0:00:18 - 0:00:19]: The setting is inside a large gym. The person, still wearing the same outfit, is engaged in a warm-up stretch, holding onto a bar with one hand and stretching his chest and shoulders. The gym is spacious with high ceilings and lots of workout equipment. The background shows more people working out.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What text is displayed on the screen at the beginning?",
        "time_stamp": "0:00:06",
        "answer": "A",
        "options": [
          "A. \"HOW I TRAIN\".",
          "B. \"CHEST WORKOUT\".",
          "C. \"MUSCLE FLEX\".",
          "D. \"GYM EQUIPMENT\"."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "0:00:16",
        "answer": "A",
        "options": [
          "A. Walking on a treadmill.",
          "B. Performing bench presses.",
          "C. Lifting weights overhead.",
          "D. Doing tricep presses."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_146_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:45]: A man is lying on an incline bench in a gym. He is holding a barbell above his chest with a grip slightly wider than shoulder-width, performing an incline bench press. He is wearing a light gray t-shirt, black shorts, and white compression shorts underneath. The gym has a few other people in the background, various gym equipment, and weight plates on the floor. The man brings the barbell down towards his chest, keeping his elbows pointed outwards. [0:02:46 - 0:02:50]: He continues the bench press, keeping a steady rhythm as he presses the barbell upwards and then lowers it back down to his chest. His facial expression displays focus and effort as he continues his repetitions. [0:02:51 - 0:02:57]: The scene changes, and the man is now standing with his arms extended horizontally to the sides in front of a black wall. He moves his arms in small, controlled circular motions. He is wearing the same gym attire and is standing with his feet shoulder-width apart. The room is well-lit with a plain black background, and the floor is a light-colored carpet. [0:02:58 - 0:02:59]: The scene reverts back to the man on the incline bench. He is in the middle of a bench press repetition, holding the barbell near his chest, preparing to press it upwards again. His form is consistent, keeping his elbows out and his back stable against the bench.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the man holding above his chest right now?",
        "time_stamp": "00:03:00",
        "answer": "C",
        "options": [
          "A. Dumbbells.",
          "B. A kettlebell.",
          "C. A barbell.",
          "D. A medicine ball."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_146_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:38]: The video captures a person lying on a black and white striped gym mat, performing exercises with a barbell. The person is dressed in a grey t-shirt, black shorts, and black socks, positioned flat on their back. They start with their knees bent and the barbell held above their torso, then they methodically lower it back towards their head, ensuring control over the movement. The surroundings indicate a well-equipped gym environment, with various weightlifting machines, benches, and other equipment visible prominently in the background.  [0:05:39]: The scene shifts to show the same individual standing and doing a different exercise using a piece of gym equipment. They are holding onto the handles and appear to be in the middle of a triceps extension or cable fly exercise, surrounded by an extensive array of fitness equipment.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the person doing after the barbell exercise?",
        "time_stamp": "00:05:39",
        "answer": "B",
        "options": [
          "A. Running on a treadmill.",
          "B. Performing cable fly.",
          "C. Stretching.",
          "D. Using a rowing machine."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_146_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:06]: The video splits the screen into two halves, showing an anatomical figure performing two different versions of a front raise exercise with dumbbells. On the left, the exercise is described as \"Front Raise (more externally rotated),\" primarily focusing on the front deltoids. On the right, the exercise is labeled \"Front Raise (more internally rotated),\" which also engages the front and side deltoids. The figure's muscles are highlighted in red, emphasizing the different muscle groups targeted by each variation. The figure on the left gradually raises a dumbbell with an externally rotated arm. The figure on the right begins with the arms down, holding dumbbells. [0:08:07 - 0:08:11]: As the exercise progresses, the figure on the right raises a dumbbell with the arm in a more internally rotated position. Labels appear, noting \"mostly front deltoids\" on the left and \"front and side deltoids\" on the right. Both versions of the exercise are illustrated with the dumbbells raised to shoulder height, highlighting which muscles are engaged in each position. [0:08:12 - 0:08:15]: The video transitions to a gym setting, showing a person lifting a weight plate in front of their body. The individual wears a light gray t-shirt and black shorts. The gym environment includes various weightlifting equipment and machines. The individual starts with the weight plate held close to their body, then lifts it to eye level with a straight arm movement. [0:08:16 - 0:08:19]: The scene shifts again to show the same individual performing lateral raises with dumbbells, lifting both arms out to the sides. The gym setting remains consistent, with weight benches and lifting equipment visible in the background. The individual completes one repetition of the lateral raise, bringing the dumbbells back down to their sides before initiating another lift.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What muscle groups are highlighted in red on the left side of the screen during the front raise exercise?",
        "time_stamp": "00:08:06",
        "answer": "A",
        "options": [
          "A. Front deltoids.",
          "B. Rear deltoids.",
          "C. Side deltoids.",
          "D. Biceps."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_146_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The screen is completely black, showing no visible content. [0:00:01 - 0:00:06]: The video begins with a view of a suburban street, taken from a first-person perspective. The sky is clear and blue. The street is lined with houses on either side, and there is a sidewalk bordered by green grass and shrubs. A tree stands prominently on the right side of the sidewalk. [0:00:07 - 0:00:09]: The viewer appears to be walking along the sidewalk, with a black pickup truck parked on the left side. A white car is visible parked on the street. The sidewalk is partially shaded by a large leafy tree. [0:00:10 - 0:00:12]: The shadow from the tree on the sidewalk becomes more pronounced as the viewer continues to walk. Vegetation, including bushes and flowering plants, is seen on the left side. [0:00:13 - 0:00:16]: The camera shows more parked vehicles on the street, including a white van on the right. The path ahead remains straight, with lush greenery on both sides, including a mix of shrubs and small trees. [0:00:17 - 0:00:20]: Houses are visible through the trees and bushes. The viewer continues to walk straight, with the sidewalk leading further down the residential area. Another tree becomes visible ahead, providing shade across the path. Cars are parkedbacked along the street to the right.",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many red vehicles are shown as parked on the street?",
        "time_stamp": "0:00:15",
        "answer": "B",
        "options": [
          "A. Two.",
          "B. One.",
          "C. Four.",
          "D. Five."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_301_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:20 - 0:01:40] [0:01:20 - 0:01:24]: The video begins with a view of a residential area on a sunny day. A gravel pathway stretches forward, flanked by lush green foliage and trees on the right-hand side. A row of cars is parked on the side of the street. On the left, a white house with a pitched roof and neatly trimmed hedges can be seen. Purple flowers and bushes add vibrant colors to the scene;  [0:01:25 - 0:01:29]: As the video progresses, the camera moves forward slowly along the pathway. The pathway is bordered by a stone wall on the left. The hedge and house from the earlier part of the video remain visible. On the right, large leafy trees provide shade over the parked cars and parts of the pathway. Beneath the trees, patches of sunlight and shade create an interesting pattern on the ground; [0:01:30 - 0:01:32]: Continuing along the path, more of the house’s facade becomes visible, including its windows and surrounding flora. The hedges lining the path remain consistently green and well-maintained. The parked cars remain in view, with their orientations unchanged; [0:01:33 - 0:01:36]: The camera captures more details of the house and its surroundings as it advances along the pathway. More of the stone wall appears, along with a metal fence on top of it. Vibrant purple and pink flowers in the garden add to the lively scenery. The tall trees continue to shade part of the street and pathway; [0:01:37 - 0:01:40]: Toward the end of the video, a part of another house becomes visible on the left side. The garden is filled with various flowering plants and carefully trimmed hedges. The pathway continues ahead under the shade of the large trees. The video concludes with a clear view of the residential area under the bright blue sky.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is on the right-hand side of the pathway right now?",
        "time_stamp": "00:01:24",
        "answer": "C",
        "options": [
          "A. A row of houses.",
          "B. A stone wall.",
          "C. A row of parked cars.",
          "D. A flower garden."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Event Understanding",
        "question": "What new element becomes visible in the video now?",
        "time_stamp": "00:01:40",
        "answer": "B",
        "options": [
          "A. A school.",
          "B. other houses.",
          "C. A playground.",
          "D. A park."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_301_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:42]: A first-person perspective of a sidewalk bordered by a white picket fence on the left and a brown wooden fence in the middle ground, with dense green foliage and small red flowers on the left side. There is a grassy area between the fences and the sidewalk, and the sky is clear blue. [0:02:43 - 0:02:45]: Moving closer, the brown wooden fence is more prominent, and a house with white siding and a dark roof becomes visible behind the fence. The lush green leaves of the bushes and trees dominate the left side of the fence. [0:02:45 - 0:02:47]: The brown wooden fence continues along the side of the sidewalk, with the house now more visible, including its porch and large windows. The sidewalk on the right is bordered by grass and scattered leaves. [0:02:47 - 0:02:49]: The view gets clearer as the house's structure is more distinguishable with a front porch and a staircase leading to the entrance. The sidewalk, bordered by grass, continues to the right. [0:02:50 - 0:02:51]: The house on the left appears fully, with visible stairs, porch, and windows. The wooden fence still runs along the side, with green bushes near the sidewalk. [0:02:52 - 0:02:54]: There is a closer view of the gray house on the left with its fence and lawn. In the distance, there are more houses and a pathway continuing to the right. [0:02:54 - 0:02:56]: Moving further along, the focus shifts to the tall green hedge that lines the sidewalk, partially hiding another house behind them. [0:02:56 - 0:02:58]: The tall green hedge continues to line the sidewalk, and a wooden gate with a white post is visible on the left side. Trees provide shade along the sidewalk. [0:02:59]: Approaching further, the sidewalk continues straight ahead, with trees and hedges on the left and right respectively. The shadows of the leaves create patterns on the pavement.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is located between the white picket fence and the brown wooden fence?",
        "time_stamp": "00:02:42",
        "answer": "B",
        "options": [
          "A. A stone path.",
          "B. A grassy area.",
          "C. A concrete wall.",
          "D. A small pond."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_301_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:03]: The video begins with a view of a sidewalk bordered by a tall, dense hedge on the left and a road with parked cars on the right. The scene is well-lit with sunlight casting shadows from a large tree overhanging the sidewalk. [0:04:03 - 0:04:06]: As the perspective moves forward, the sidewalk continues straight with the hedge remaining on the left side. The shadows from the tree canopy create patterns on the sidewalk. [0:04:06 - 0:04:09]: The view transitions to the end of the hedge revealing a clearer view of the street ahead. A large leafy tree on the right side continues to cast shadows. The sidewalk leads to a crossing point with a street sign visible. [0:04:09 - 0:04:11]: The perspective crosses the intersection, showing more of the street and houses on either side. Vegetation is maintained along the sidewalk, and some cars are parked along the road. [0:04:11 - 0:04:14]: Moving further, the view reveals a residential area with well-kept lawns, houses with distinctive architectural styles, and trees lining the sidewalk. The paved sidewalk is flanked by green lawns and parked vehicles on the street. [0:04:14 - 0:04:18]: The camera continues down the sidewalk. The houses feature small gardens, and some windows have decorations. Trees along the sidewalk provide shade, and the neatly trimmed lawns extend to the curb. [0:04:18 - 0:04:20]: The scene focuses on the end of the sidewalk where it stretches with houses on both sides, maintaining the residential setting. The trees continue to offer a canopy over the path, giving a sense of continuity in the neighborhood.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the nature of the shadows on the sidewalk shown right now?",
        "time_stamp": "00:04:06",
        "answer": "B",
        "options": [
          "A. Shadows from parked cars.",
          "B. Shadows from street signs.",
          "C. Shadows from the tree canopy.",
          "D. Shadows from buildings."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_301_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many trees are on the left side of the road right now?",
        "time_stamp": "00:00:03",
        "answer": "D",
        "options": [
          "A. Four.",
          "B. Six.",
          "C. Seven.",
          "D. Three."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_180_real.mp4"
  },
  {
    "time": "[0:02:16 - 0:02:36]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the hay bales located right now?",
        "time_stamp": "00:02:21",
        "answer": "B",
        "options": [
          "A. On the left side of the road.",
          "B. On the both sides of the road.",
          "C. In the middle of the road.",
          "D. Directly ahead of the cyclist."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_180_real.mp4"
  },
  {
    "time": "[0:04:32 - 0:04:52]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the object located near the lamp post right now?",
        "time_stamp": "00:04:36",
        "answer": "D",
        "options": [
          "A. A car.",
          "B. A bicycle.",
          "C. A mailbox.",
          "D. A tree with white flowers."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_180_real.mp4"
  },
  {
    "time": "[0:06:48 - 0:07:08]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the tree shadow located right now?",
        "time_stamp": "00:07:03",
        "answer": "B",
        "options": [
          "A. On the left side of the road.",
          "B. Across the entire road.",
          "C. On the right side of the road.",
          "D. In the middle of the road."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_180_real.mp4"
  },
  {
    "time": "[0:09:04 - 0:09:24]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the cyclist heading right now?",
        "time_stamp": "00:09:23",
        "answer": "B",
        "options": [
          "A. Towards a commercial area.",
          "B. Towards a residential area.",
          "C. Towards an industrial area.",
          "D. Towards a forest."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_180_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What concept might the speaker explain next?",
        "time_stamp": "00:00:20",
        "answer": "D",
        "options": [
          "A. Definition of a right triangle.",
          "B. What is Pythagorean Theorem.",
          "C. The history of Pythagoras.",
          "D. The formula a² + b² = c²."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_223_real.mp4"
  },
  {
    "time": "[0:02:40 - 0:03:10]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:02:34",
        "answer": "B",
        "options": [
          "A. Applications of the Pythagorean Theorem.",
          "B. How to solve for the hypotenuse.",
          "C. Examples of different right triangles.",
          "D. The history of the Pythagorean Theorem."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_223_real.mp4"
  },
  {
    "time": "[0:05:20 - 0:05:50]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:05:33",
        "answer": "C",
        "options": [
          "A. How to apply the Pythagorean theorem to other shapes.",
          "B. The derivation of the Pythagorean theorem using trigonometry.",
          "C. The concept of area and its proof in the Pythagorean theorem.",
          "D. The historical background of the Pythagorean theorem."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_223_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:08:30]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:08:29",
        "answer": "A",
        "options": [
          "A. Explain how to use the Pythagorean theorem to find the right-angle side length.",
          "B. Discuss the properties of right-angle triangles in detail.",
          "C. Convert all given measurements from meters to centimeters.",
          "D. Illustrate how to plot these points on a Cartesian plane."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_223_real.mp4"
  },
  {
    "time": "[0:10:40 - 0:11:10]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:11:09",
        "answer": "D",
        "options": [
          "A. How to calculate the hypotenuse.",
          "B. Why the triangle is right-angled.",
          "C. The properties of equilateral triangles.",
          "D. The method to verify if it is a right triangle."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_223_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:04]: The video begins with a close-up of a graphic featuring a large white number \"10\" with a fork and knife positioned to look like clock hands on a white plate. The background is a gradient blue, and the number is bordered by a thin white line. Next to it is another rectangular white shape. [0:00:01 - 0:00:02]: The camera zooms out to reveal a person standing against the same blue background. The individual is presumably introducing the show \"Ramsay in 10,\" as the title appears behind them. The person is smiling and wearing a dark blue shirt. Light beams can be seen radiating from above the individual, adding dynamic lighting to the background. The fork and knife from the previous scene remain. [0:00:03 - 0:00:04]: The close-up shot continues, with the person still smiling and standing slightly to the right. The large blue text in the background says \"RAMSAY in 10,\" and the fork and knife clock mechanism remains at the center. [0:00:05 - 0:00:09]: The scene transitions to a kitchen setting. The same individual is now seen standing in the middle of a kitchen with a white brick backsplash and wooden shelves holding various kitchen items, including plates, bottles, and jars. The word \"COOK\" appears in red, large letters on the top shelf. The person starts talking, gesturing with their hands.  [0:00:10 - 0:00:12]: The individual continues talking, making a thumbs-up gesture. Behind them are several knives mounted on the wall and more kitchen utensils. The word \"HOT\" is also visible in small white letters on the countertop. The lighting in the room is bright, giving a clear view of the kitchen setup. [0:00:13 - 0:00:15]: The individual appears to be explaining something, looking directly at the camera. The background remains the same with kitchen utensils and the word \"COOK\" still prominent. The person then clasps their hands together, seeming focused on what they are talking about. [0:00:16 - 0:00:19]: The focus shifts to the individual's left profile as they continue to speak, looking slightly to the side. The camera seems to pan slightly, showing a clearer view of the kitchen appliances, including an oven and more storage jars. The individual maintains an engaged and animated expression while talking.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What graphic is featured in the video at the beginning?",
        "time_stamp": "0:00:20",
        "answer": "D",
        "options": [
          "A. A large white number \"10\" with a spoon and fork positioned like clock hands on a white plate.",
          "B. A large white number \"10\" with a spoon and knife positioned like clock hands on a white plate.",
          "C. A large white number \"10\" with a fork and knife positioned like clock hands on a blue plate.",
          "D. A large white number \"10\" with a fork and knife positioned like clock hands on a white plate."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_47_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:06]: The scene portrays a man in a black shirt in a kitchen setting. He is preparing food on a countertop, facing the camera. The countertop is packed with various items including a wooden cutting board with raw meat, a glass bottle filled with olive oil, and bowls of colorful vegetables. The backdrop is a white brick wall adorned with arranged plates and kitchen utensils. He is seasoning two pieces of raw meat on a white plate placed on the cutting board. [0:02:07 - 0:02:09]: The camera zooms in to provide a close-up of the meat being seasoned with salt and other spices. The meat pieces, glistening with olive oil, are positioned on a wooden cutting board. [0:02:10 - 0:02:12]: The man turns away from the countertop and approaches a stove area with a pan already placed on the burner. On the countertop in front of the stove are bowls of various ingredients, such as spices and herbs. The plate with seasoned meat is still on the wooden cutting board. [0:02:13 - 0:02:15]: He carefully lifts a piece of bread from the pan on the stove with tongs and places it on a grill. He uses his fingers to split the bread and check the texture. [0:02:16 - 0:02:19]: The man adjusts the position of the bread slices on the grill with his hands, ensuring both pieces are evenly aligned. The close-up captures his hands working swiftly as he places the bread flat side down on the grill, pressing them gently to toast.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the man seasoning on the white plate?",
        "time_stamp": "0:02:06",
        "answer": "D",
        "options": [
          "A. Vegetables.",
          "B. Bread.",
          "C. Fish.",
          "D. Raw meat."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_47_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:01]: The scene opens with a grill placed on a stove, where several items are being cooked. Two steak pieces are on the left side of the grill, while two large mushrooms and four slices of onion are on the right. A pair of red tongs is being used to manage the food on the grill;  [0:04:01 - 0:04:02]: The tongs are adjusting one of the mushrooms to ensure even cooking. The two steak pieces and onion slices remain in their original positions on the grill;  [0:04:02]: The tongs are now used to turn over one of the mushrooms. A small cavity in the upturned mushroom cap shows some butter melting inside;  [0:04:03 - 0:04:04]: The person using the tongs is visible from a first-person perspective. They are seen clutching one mushroom cap while the other hand holds the tongs;  [0:04:04 - 0:04:05]: The tongs re-position the mushroom cap on the grill, and its placement is adjusted for even cooking;  [0:04:05 - 0:04:07]: The camera view transitions to a close-up of the grill, focusing on the cooking items. The scene highlights the middle of the grill, where the bulbous parts of the mushroom caps are arranged;  [0:04:08 - 0:04:11]: A hand extends towards the food on the grill, possibly with seasoning or additional ingredients to add. At this point, the mushrooms, steak, and onion slices are in focus;  [0:04:12 - 0:04:13]: The view zooms out to include more of the kitchen environment. The workspace contains a countertop with various cooking items and utensils. At this juncture, a person wearing a black shirt is seen adding seasonings to the food on the grill;  [0:04:14]: The person steps back momentarily, suggesting they are finishing seasoning or adjusting the food on the grill;  [0:04:15 - 0:04:16]: The camera captures a broader view of the kitchen while the person prepares additional ingredients. The setting includes a backsplash of white tiles and shelves lined with plates, jars, and other kitchenware;  [0:04:17 - 0:04:18]: The person moves towards a wooden cutting board placed on the kitchen island. They appear to be preparing a mixture in a bowl;  [0:04:18 - 0:04:19]: A closer view of the hands shows them pouring a liquid seasoning or sauce into the bowl. The wooden cutting board hosts a small red onion segment and nearby greenery in a white bowl, rounding out the scene.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is being used to manage the food on the grill?",
        "time_stamp": "0:04:01",
        "answer": "D",
        "options": [
          "A. A spatula.",
          "B. A wooden spoon.",
          "C. A fork.",
          "D. A pair of red tongs."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_47_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:05]: A man is standing in a kitchen in front of a counter with various ingredients and utensils laid out. He is wearing a black t-shirt and holding a striped cloth in his left hand. In front of him, there are two pieces of bread on a wooden cutting board, a bottle of olive oil, a bowl, and some vegetables. On the left of the counter, a stovetop grill is visible with food cooking on it. A backsplash of white tiles with visible grout lines forms the background, along with shelves containing plates and utensils. The man turns his head to the right, then shifts slightly to place the cloth next to the cutting board; [0:06:06 - 0:06:07]: He picks up a spoon from the counter with his right hand and scoops some sauce from the small bowl in his left hand. All around, the counter is filled with plates and bowls containing various ingredients like lettuce, beetroot, sliced bread, and a large cheese block. The shelves in the background show more kitchen utensils and glassware; [0:06:08 - 0:06:10]: The man carefully spreads the sauce on the bread using the spoon, making sure to cover the surface evenly. His attention remains on his task as he applies the sauce with precision; [0:06:11 - 0:06:12]: He adds more of the sauce to the bread, scooping a generous amount from the bowl and continuing to spread it; [0:06:13 - 0:06:15]: Once the bread is evenly coated with the sauce, he sets the bowl aside and places the bread back on the counter; [0:06:16 - 0:06:17]: The camera shifts to an overhead view, showing the cutting board with several food items such as leaves of lettuce, mushrooms, a block of blue cheese, a few lemons, a bottle of olive oil, containers of salt, pepper, and chili flakes. The man begins to tear the lettuce leaves over a wooden cutting board; [0:06:18 - 0:06:20]: He places the torn lettuce pieces on the bread he's prepared earlier, carefully arranging them to ensure they are evenly distributed. The camera remains focused on the man's hands and the ingredients as he continues to work.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What does the man do with the striped cloth?",
        "time_stamp": "0:06:05",
        "answer": "C",
        "options": [
          "A. He wipes the counter.",
          "B. He holds it in his right hand.",
          "C. He places it next to the cutting board.",
          "D. He uses it to cover the bread."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Event Understanding",
        "question": "What does the man do immediately after picking up a spoon?",
        "time_stamp": "0:06:12",
        "answer": "B",
        "options": [
          "A. He stirs a pot on the stovetop.",
          "B. He scoops some sauce from a bowl.",
          "C. He chops vegetables.",
          "D. He flips food on the grill."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_47_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the type of vehicle directly in front of the camera?",
        "time_stamp": "00:00:04",
        "answer": "C",
        "options": [
          "A. A black sedan.",
          "B. A white bus.",
          "C. A white SUV.",
          "D. A white truck."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_401_real.mp4"
  },
  {
    "time": "[0:01:54 - 0:01:59]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which establishment is next to the \"Happy Family\" restaurant?",
        "time_stamp": "00:01:58",
        "answer": "C",
        "options": [
          "A. A bakery.",
          "B. A hair salon & spa.",
          "C. A nail salon.",
          "D. A gym."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_401_real.mp4"
  },
  {
    "time": "[0:03:48 - 0:03:53]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the traffic light right now?",
        "time_stamp": "00:04:02",
        "answer": "A",
        "options": [
          "A. Green.",
          "B. Yellow.",
          "C. Red.",
          "D. Blue."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_401_real.mp4"
  },
  {
    "time": "[0:05:42 - 0:05:47]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the make of the white truck parked on the right side?",
        "time_stamp": "00:05:45",
        "answer": "B",
        "options": [
          "A. U-Haul.",
          "B. Ryder.",
          "C. Penske.",
          "D. Budget."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_401_real.mp4"
  },
  {
    "time": "[0:07:36 - 0:07:41]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What brand is the white truck directly ahead right now?",
        "time_stamp": "00:07:40",
        "answer": "D",
        "options": [
          "A. U-Haul.",
          "B. Ryder.",
          "C. Penske.",
          "D. Hill-Rom."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_401_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: In the initial second, the first-person view shows someone looking downward while placing boxed items, likely cartons of milk, into a metal storage rack. The person is wearing gloves and dark clothing. Green box-like structures are visible on the left side. [0:00:02 - 0:00:03]: The perspective shifts upwards, revealing more of the storage area with shelves stocked with boxed goods in a systematic order and some shelves containing other products. The individual is seen handling the products, arranging them into the rack. [0:00:04 - 0:00:07]: The primary focus is the shelves in front of the individual. They are seen reaching over to pick up more items from a metal cart on their right and place them into the organized rack. The person continues to sort and place more boxes of the same type. [0:00:08 - 0:00:09]: The camera momentarily shifts, showing a larger workspace with more green box-like structures to the left and other metal trolleys laden with products. The person reaches for another boxed item. [0:00:10 - 0:00:11]: The focus returns to the shelves and trolleys being used by the individual. The person continues to organize and sort more items efficiently into the carts and shelves, maintaining a steady and systematic approach. [0:00:12 - 0:00:15]: A wider view of the surroundings shows a spacious storage area with multiple shelves filled with neatly arranged products. The person appears to be sorting and stacking products from different trolleys, following a consistent pattern. [0:00:16 - 0:00:17]: The individual steps back, momentarily pausing their activity, revealing more of the storage area. The green boxes, additional trolleys, and further contents of the storage room come into focus. [0:00:18 - 0:00:20]: The person continues their task, reaching for another boxed item to place in the rack. The storage area remains organized, with shelves providing clear visibility of neatly arranged products. The person's actions are deliberate and methodical throughout.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the person consistently doing throughout the entire scene?",
        "time_stamp": "0:00:20",
        "answer": "D",
        "options": [
          "A. Taking inventory of products.",
          "B. Rearranging boxes randomly.",
          "C. Cleaning the storage area.",
          "D. Sorting and placing products methodically."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_441_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:24]: The video is shot from a first-person perspective in what appears to be a store or storage area with metal shelves filled with dairy products like yogurt and milk. The person is pushing a large, empty metal cart, maneuvering through narrow aisles filled with various groceries. [0:02:24 - 0:02:30]: The person navigates through the aisle, which contains stacked shelves on both sides and heads towards the back of the room. The left side has rows of dairy products such as milk and cream, and the right side has more groceries including boxes of green-colored items.  [0:02:30 - 0:02:34]: The person picks up a few cartons of milk from the left-side shelves and places them into the cart. The shelves are well-organized, with neatly stacked products that are categorized by type and price labels above. [0:02:34 - 0:02:36]: As the person continues to pick products, they carefully place another item into the cart. The camera captures close-up views of the shelves, showcasing various dairy products like yogurt cups and containers. [0:02:36 - 0:02:40]: Finally, they reach for a product labeled \"lait fraiche\" on the shelf. The person moves some items to get a hold of the desired product, placing it with the other picked items in the cart. The background remains consistent with metal shelves filled with assorted groceries in the narrow aisle.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What did the person pick up from the cart and place into the left-side shelves just now?",
        "time_stamp": "00:02:34",
        "answer": "B",
        "options": [
          "A. Yogurt cups.",
          "B. Green Cartons of milk.",
          "C. Boxes of green-colored items.",
          "D. Containers of cream."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_441_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:42]: Inside what appears to be a storage or inventory room, shelves filled with various items are visible. On the left, a series of metal shelves hold products in boxes and crates. A cart laden with stacked products is pushed along the aisle. Green boxes and stacks of dairy products are on and beside the cart. On the right, shelves are filled with dairy products and other food items. [0:04:43 - 0:04:46]: The perspective shows a closer look at a stacked cart, overloaded with multiple red and white packaged items—possibly milk cartons. One hand appears at the bottom of the frame, suggesting a first-person perspective as the individual maneuvers. [0:04:47 - 0:04:50]: The pathway is flanked by high shelves stocked with goods—mainly dairy products. The individual continues to move forward, guiding the fully loaded cart. [0:04:51 - 0:04:57]: The shelves on the left hold boxed and individual items. The right shelves are more organized with multiples of the same product. Green product boxes are frequently observed, and other items are neatly arranged next to and above these. [0:04:58 - 0:04:59]: The scene shifts towards a row of shelving filled with various boxed items and consumer goods, extending forward. The general atmosphere is that of a densely packed storage area, with a careful arrangement of packaged items on metal shelving units.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the dairy products and other food items primarily located in the storage room?",
        "time_stamp": "0:05:00",
        "answer": "B",
        "options": [
          "A. On the left shelves.",
          "B. On the right shelves.",
          "C. On the central aisle.",
          "D. Near the entrance."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Event Understanding",
        "question": "What activity is the individual primarily engaged in just now?",
        "time_stamp": "0:05:00",
        "answer": "A",
        "options": [
          "A. Pushing a loaded cart.",
          "B. Stocking shelves.",
          "C. Taking inventory.",
          "D. Cleaning the area."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_441_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:10]: The video begins with a first-person perspective in what appears to be a storage or inventory room. There are two main shelving units on each side of the narrow aisle. The left side shelves are filled with various dairy products, including tubs of cream and milk cartons, while the right side has stacks of boxed items, mostly juice cartons labeled \"PRIVA.\" The place is well-organized, and the floor is clear. [0:07:11 - 0:07:13]: The camera pans further down the aisle, revealing more of the shelving units filled with dairy and juice products. On the right side, additional boxed goods are visible on lower shelves. [0:07:14 - 0:07:15]: The view shifts as the person holding the camera walks toward the back of the storage room. Additional metal carts and plastic crates are visible against the wall at the end of the aisle. More assorted grocery items are stored on the shelves to the left. [0:07:16 - 0:07:17]: As the camera continues to move forward, the back wall of the room becomes more visible. Several metal carts are lined up next to a red stack of plastic crates. There is also a door with a small window on it. [0:07:18 - 0:07:20]: At the end of the aisle, the camera shifts slightly, focusing on the carts and crates in front of the door. A few additional items, such as cardboard boxes and plastic packaging, are visible on the carts. The room is still well-lit and clean, with neatly organized storage areas. The video concludes at this point.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "How are the boxed items labeled on the right side shelves in the storage room right now?",
        "time_stamp": "00:07:02",
        "answer": "C",
        "options": [
          "A. \"DAIRY\".",
          "B. \"FRESH\".",
          "C. \"PRVIA\".",
          "D. \"GROCERY\"."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_441_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:16",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_95_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:01:29",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 1."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_95_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:58",
        "answer": "B",
        "options": [
          "A. 3.",
          "B. 4.",
          "C. 6.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_95_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:18",
        "answer": "B",
        "options": [
          "A. 2.",
          "B. 4.",
          "C. 3.",
          "D. 7."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_95_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:40",
        "answer": "B",
        "options": [
          "A. 2.",
          "B. 4.",
          "C. 9.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_95_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins on a racing track set in a miniature landscape. The perspective is from the track, looking towards a corner with a metal mesh fence on the left and grassy hills with trees on the right. The track ahead curves to the right, and a red racing car is seen further ahead.    [0:00:03 - 0:00:07]: The screen transitions to a black background with colorful, distorted text that reads \"NEXT GEN DIECAST RACING.\" The text gradually becomes clearer, with only minor distortions remaining. [0:00:08 - 0:00:17]: The video switches back to the miniature racing track scene. The camera angle provides a broader view from above, showing multiple racing cars lined up on different sections of the track. There are small buildings and trees on the hills surrounding the track, with the text \"Legacy Piston Cup\" appearing on the screen. [0:00:18 - 0:00:20]: The final scene shows a close-up of three diecast race cars positioned on the starting line. The text \"Round 1 - Group 1\" is visible above the cars, indicating the beginning of a racing event. The cars are brightly colored: one is blue, one is purple, and one is yellow.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is seen in the initial scene of the video?",
        "time_stamp": "0:00:20",
        "answer": "D",
        "options": [
          "A. A red racing car is parked on the track.",
          "B. A green racing car in the pits.",
          "C. A blue racing car near a curve.",
          "D. A red racing car sped past on the track."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_487_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:40 - 0:02:00] [0:01:40 - 0:01:44]: The video showcases a vibrant, miniature race track situated amidst a green, hilly landscape with blue skies and clouds. Several toy cars are racing around a sharp, steeply-banked curve of the track. The red, yellow, blue, and orange cars are closely packed as they navigate the turn. The background features lush greenery with scattered tiny trees and a white and blue billboard advertising \"Crazy Car.\" [0:01:44 - 0:01:51]: The scene transitions to a straight section of the track with a backdrop of a scaled-down pit row area featuring toy car garages. Various colored cars can be seen parked behind a wire fence, including green and yellow ones. A red toy helicopter appears above the pit row. [0:01:52 - 0:01:59]: The video ends with a standings board displaying race results. The leaderboard indicates \"Group 1 - Race 1\" with the following standings: 1st place #95 Lightning McQueen with 5 points, 2nd place #56 Brush Curber with 3 points, 3rd place #70 Floyd Mulvihill with 2 points, and 4th place #79 Haul Inngas with 0 points. The backdrop shows the continuation of the race track and surrounding greenery.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What was being advertised on the blue billboard?",
        "time_stamp": "0:02:00",
        "answer": "C",
        "options": [
          "A. Speedy Cars.",
          "B. Toy Race.",
          "C. Crazy Cars.",
          "D. Miniature Motors."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      },
      {
        "task_type": "Event Understanding",
        "question": "Which racing car won first place in this competition?",
        "time_stamp": "0:01:28",
        "answer": "D",
        "options": [
          "A. The purple car.",
          "B. The blue car.",
          "C. The yellow car.",
          "D. The red car."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_487_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:24]: The video shows a group of toy race cars lined up on a black track labeled \"Thunder Hollow Speedway.\" These cars are positioned next to each other, with the yellow car in the front right, followed by a blue car and a purple car. Tall green trees and a light blue sky with fluffy white clouds are seen in the background. [0:03:25]: The cars start moving along the track. The camera angle changes to show the cars from the side as they race forward. [0:03:26 - 0:03:27]: The yellow car leads the race with the blue car right behind it, and the purple car slightly trailing. The background shifts as the camera pans along with the movement of the cars, showing more trees and a billboard. [0:03:28]: The cars approach a sharp turn in the track. The yellow car maintains its lead while the blue car follows closely. [0:03:29]: The camera captures the yellow car navigating the turn, with the blue car slightly behind it. The green hillside is visible in the left-side background. [0:03:30]: The yellow car continues along the track with more surrounding landscape, including a 'DD' sign visible on the right side.  [0:03:31 - 0:03:32]: The yellow car makes another turn, passing some colorful vehicles parked on the inner part of the track. The scene includes grassy hills and a large spectator stand in the top background. [0:03:33 - 0:03:34]: The yellow car speeds down a straight section of the track, passing a fence with several parked cars and a red helicopter above. [0:03:35 - 0:03:38]: The view focuses on the cars behind the fence with the yellow car in the foreground speeding ahead. More toy race cars and background details like parked vehicles and the helicopter are seen. [0:03:39]: Finally, a purple car and a red car race down the same section of the track at high speed, passing quickly through the frame.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the color of the car that leads the race right now?",
        "time_stamp": "0:03:27",
        "answer": "C",
        "options": [
          "A. Red.",
          "B. Blue.",
          "C. Yellow.",
          "D. Purple."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_487_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:05:54]",
    "captions": "[0:05:00 - 0:05:20] [0:05:00 - 0:05:01]: The scene displays a racetrack with a black, glossy surface. A yellow race car is prominently in the foreground, racing from the right to the left side of the screen. Off to the right, there is a fence separating the track from a set of enclosed white structures resembling garages or pit stops. Various colorful race cars are stationed near the garages. The background consists of a green, grassy hill with trees. [0:05:02 - 0:05:03]: The yellow race car moves further left, now partially off-screen. A blue race car enters the screen from the right, following the same path along the black racetrack. The fenced-off area and the garages in the background remain the same, with the various colorful cars still stationary. [0:05:04 - 0:05:09]: The blue race car speeds past and exits the screen to the left. Following it, a purple race car appears from the right, steadily moving on the racetrack. As the purple car advances, a red race car comes into view, following the same route. The fenced pit area and the garages with parked cars remain consistent throughout these frames, with no significant changes in the background. [0:05:10 - 0:05:11]: The purple race car is now near the center of the frame, continuing its progression along the racetrack. The red race car follows closely behind, with part of its body blurred due to speed. The fenced area and garages remain unchanged, with various cars still parked inside. [0:05:12 - 0:05:20]: The scene transitions to standings for the Group 1 final. A red overlay box lists car numbers and names:  1. #56 Brush Curber (13 points) 2. #95 Lightning McQueen (11 points) 3. #79 Haul Inngas (9 points) 4. #70 Floyd Mulvihill (8 points) The background beneath the standings features the racetrack and the grass bank, now with the image static.\n[0:05:40 - 0:05:54] [0:05:40 - 0:05:43]: The video begins with a view of a downhill racing track in the foreground. The track is black and has white lines demarcating lanes. On the left side, there is a grassy area with some sponsors' banners visible. The right side has a fenced area where several toy race cars are parked. Behind the fenced area is a white building with signage and a red toy helicopter on top. The background features a green, grassy hill with some trees. [0:05:43 - 0:05:46]: A red race car appears on the track, moving quickly downhill. Moments later, a yellow race car follows behind. The same background of the fenced area, white building, and green hill can still be seen. [0:05:46]: The perspective shifts to a broader view of the track. The track now curves to the left, and additional cars in various colors—yellow, blue, and red—race downhill. The background shows green hills with trees and clear, blue, cloud-dotted sky. [0:05:47 - 0:05:49]: Several cars, including a yellow, two blue, and a red car, are seen racing down the track. The background remains consistent with green hills. Each car remains in its lane as they navigate the track. [0:05:49 - 0:05:50]: The video frame focuses closely on the cars speeding down the track. The terrain in the background is hilly, with tall trees visible. The blue car leads the race followed by the red car and a yellow car. [0:05:50 - 0:05:51]: The view shifts to another angle, showing the track from a higher vantage point. A billboard with a logo is visible, and a section of the audience stand can be seen in the background. The yellow and blue cars are in focus, continuing down a curved part of the track. [0:05:51 - 0:05:53]: The perspective returns to the initial view of the downhill track with the black surface and fenced area on the right. Another red car enters from the top, zooming downhill, soon followed by other cars. [0:05:53]: The red car continues to speed down the track. The setup of the track, fenced area, white building, and the green hill in the background remain unchanged.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the position of Lightning McQueen in the standings for the Group 1 final?",
        "time_stamp": "0:05:20",
        "answer": "B",
        "options": [
          "A. First.",
          "B. Second.",
          "C. Third.",
          "D. Fourth."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_487_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual taken just now?",
        "time_stamp": "0:00:10",
        "answer": "A",
        "options": [
          "A. The individual gathered ice cubes, placed them into hand-washing sink.",
          "B. The individual poured liquor into a shaker, added ice, and mixed the drink.",
          "C. The individual fetched a glass, filled it with ice, and added a slice of lemon.",
          "D. The individual prepared various garnishes and placed them on the edge of a glass."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_406_real.mp4"
  },
  {
    "time": "[0:01:42 - 0:01:52]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual taken just now?",
        "time_stamp": "0:01:52",
        "answer": "A",
        "options": [
          "A. The individua organized bar utensils, retrieved a measure cup, and poured a clear liquid into it.",
          "B. The individual gathered fresh herbs, chopped them finely, and stored them away.",
          "C. The individual garnished a cocktail with a fruit slice and stirred it gently.",
          "D. The individual placed ice cubes into glasses, poured liquor, and prepared to serve them."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_406_real.mp4"
  },
  {
    "time": "[0:03:24 - 0:03:34]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual taken just now?",
        "time_stamp": "0:03:34",
        "answer": "A",
        "options": [
          "A. The individual mixed and poured cocktails into glasses placed on a napkin,.",
          "B. The individual poured beer into several glasses alongside the shaker.",
          "C. The individual arranged various spirits on the bar counter.",
          "D. The individual cleared empty glasses and sanitized the bar counter meticulously."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_406_real.mp4"
  },
  {
    "time": "[0:05:06 - 0:05:16]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual taken just now?",
        "time_stamp": "0:05:16",
        "answer": "A",
        "options": [
          "A. The individual poured a liquid into a jigger, combined it with other ingredients, and prepared the mixture for a cocktail.",
          "B. The individual participated in a conversation with customers while cleaning glassware.",
          "C. The individual garnished drinks with fruit slices and served them to waiting customers.",
          "D. The individual took orders from a group of patrons and noted them down."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_406_real.mp4"
  },
  {
    "time": "[0:06:48 - 0:06:58]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the individual taken just now?",
        "time_stamp": "0:06:52",
        "answer": "A",
        "options": [
          "A. The individual prepared a cocktail by shaking ingredients, straining into a glass, and adding ice cubes.",
          "B. The individual organized garnishes on the bar counter, then served drinks to customers.",
          "C. The individual retrieved a bottle from a refrigerator, checked its contents, and placed it back.",
          "D. The individual opened a refrigerator, retrieved a bottle, and nearly filled it with liquid."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_406_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video starts with a completely black screen. [0:00:01]: Fuzzy, static interference fills the screen, displaying distorted lines and multi-colored (primarily pink and purple) visual noise. [0:00:02 - 0:00:03]: A yellow oval-shaped logo with a blue and red triangular design appears, containing the text \"GRAVITY THROTTLE RACING\" in white. The background continues to show a bit of static. [0:00:04 - 0:00:05]: The same logo remains on screen with negligible changes, maintaining its centered position on a dark, textured background. [0:00:06]: The scene transitions to a diorama which showcases a miniature landscape: a runway and small aircraft with mountainous terrain in the background. The sky is partly cloudy. [0:00:07]: The view angles slightly to the right, revealing more of the runway and a yellow vehicle parked near a small white airplane. [0:00:08 - 0:00:09]: The camera perspective shifts further right, following a winding road adjacent to the runway. This reveals a larger portion of the mountain and surrounding desert details. [0:00:10 - 0:00:11]: The action focuses on the roadway as several miniature cars (painted in various colors) race along it. Trees and sparse vegetation dot the arid landscape. [0:00:12]: A racing vehicle, primarily white with yellow and red color accents, is visible on the road as it continues to travel. [0:00:13]: The camera view moves dramatically to a higher vantage point, observing a sharp turn in a winding mountain road where the racing vehicle is clearly visible. Pine trees line the mountainside. [0:00:14 - 0:00:15]: The view captures more of the racing track snaking down the mountainside along with a black train on a parallel track, complementing the racing theme. [0:00:16 - 0:00:17]: Focus shifts to the black train, which enters a tunnel at the mountain's base, lined with a red and white arch marking the tunnel entrance. [0:00:18]: The train continues through the tunnel entrance amidst a rocky terrain, with occasional green patches and small trees. [0:00:19]: The final scene shows a wider view of the mountainous landscape, with the train partially visible and a white vehicle in the bottom center, driving along the road.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Which car won first place in this competition?",
        "time_stamp": "0:01:42",
        "answer": "D",
        "options": [
          "A. The blue car.",
          "B. The dark purple car.",
          "C. The red car.",
          "D. The green and black car."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_495_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:40 - 0:04:00] [0:03:40 - 0:03:45]: Three cars, one red, one black, and one green, navigate a snowy, mountainous curve from different positions. Several trees line the snowy cliff, and the winding road is elevated. In the distance, more mountains are visible under a clear blue sky. Another car, blue, appears entering the scene from the bottom left corner, following the red car. [0:03:45 - 0:03:47]: As the cars continue along the curved, elevated road, the red car moves ahead, and the black and green cars follow closely. The scenery transitions to include more rocky terrain, vegetation patches, and the road curving steeply. [0:03:47 - 0:03:51]: The trail moves downhill with the black and green cars turning sharply around a curve. Trees and shrubbery populate the sides of the road. A bridge appears ahead, leading over a lower road section with several other cars. [0:03:51 - 0:03:54]: The cars cross over the bridge, and the scene transitions to a loop around a large rock formation. The road twists and turns through a desert landscape, with sparse vegetation scattered around the sandy terrain. The cars continue to navigate the winding path. [0:03:54 - 0:03:55]: The perspective changes and a black tunnel entrance becomes visible in the background. A white car follows the green and black cars closely. A prominent rock formation sits at the center of the frame with cars encircling it. [0:03:55 - 0:03:57]: Cars continue navigating the windy road past tunnels and rocky terrain. The vehicles move closer together, creating a tight formation. Signs advertise local attractions along the roadside. [0:03:57 - 0:03:59]: The cars negotiate the last few curves. Another curve brings the cars closer together on a straight stretch of road. [0:03:59 - 0:04:00]: The camera focuses on a digital roadside sign displaying messages to drivers. The cars, still closely grouped, prepare to navigate another curve. The landscape continues to feature desert-like terrain with minimal vegetation.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the car that is in the lead in this competition right now?",
        "time_stamp": "00:03:54",
        "answer": "A",
        "options": [
          "A. Black.",
          "B. Red and black.",
          "C. Yellow and red.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_495_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:20 - 0:07:40] [0:07:20 - 0:07:22]: A red toy car is racing on a snowy terrain with multiple wavy tracks. The car is moving from left to right, leaving behind a dusty trail. There are small trees scattered along the sides of the track, and the background comprises tall, snow-covered hills under a blue sky. [0:07:23 - 0:07:25]: The car transitions to a sandy environment. The terrain changes as the car drives on a smoother track, still following a winding path. Wooden frames, like small bridges or support structures, appear on both sides of the track. [0:07:26 - 0:07:29]: The car approaches a black digital timer display that reads \"Race Time: 24.824\" in blue text, with a red banner below asking, \"ARE YOU TRACKIN'?\" Small bushes and rocks flank the sign on both sides. The car passes this display, driving smoothly on the sandy track. [0:07:30 - 0:07:33]: The car continues driving past the wooden structures, maintaining speed on the smooth track. In the background, the terrain alternates between sandy patches and sparse green plants. Another digital timer display is visible in the background, showing \"Race Time: 25.619.\" [0:07:34 - 0:07:35]: The car speeds along the track, surrounded by the typical rocky and bushy landscape. The previous digital timer sign is closer now, confirming the race time as \"25.619\". [0:07:36 - 0:07:39]: The car keeps moving forward on the sandy track, passing under more wooden frames. A digital timer sign repeats the race time \"24.874\" as the car zooms past, with shrubs and rocks closely lining the track.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the digital timer display showing right now?",
        "time_stamp": "0:07:29",
        "answer": "D",
        "options": [
          "A. \"Race Time: 25.619\".",
          "B. \"Race Time: 24.874\".",
          "C. \"Race Time: 23.452\".",
          "D. \"Race Time: 24.824\"."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_495_real.mp4"
  },
  {
    "time": "[0:11:00 - 0:12:00]",
    "captions": "[0:11:00 - 0:11:20] [0:11:00 - 0:11:06]: A table titled \"2023 - 2024 BRAGGIN' WAGON\" displays race results. There are four rows corresponding to different teams: 'NUMBSKULL Racing,' 'HOT NUTS DIECAST,' 'DUBIOUS DIECAST,' and 'LIVE YOUNG DIECAST.' The columns labeled R1 through R7 list their scores across seven races. The total score for each team is at the end of each row: NUMBSKULL has 15, HOT NUTS DIECAST has 16, DUBIOUS DIECAST has 15, and LIVE YOUNG DIECAST has 16 points. [0:11:07 - 0:11:13]: The scene shifts to a snow-covered race track with four scale model cars positioned at the starting line. The track has multiple lanes that bend and curve through the snowy terrain. A blue wall is visible in the background. The four cars are staggered with different colors: green, purple, red, and blue. [0:11:14 - 0:11:16]: The race begins, and the cars speed down the track. The blue car takes the lead, followed closely by the red and purple cars, with the green car in fourth place. [0:11:17 - 0:11:17]: The cars navigate a curve, maintaining their positions with the blue car still in the lead. A truck or additional vehicle can be seen near the bottom left. [0:11:18 - 0:11:18]: The blue car maintains its lead over the others as they approach a bridge over a greenish stream. A billboard is visible on the right side of the track. [0:11:19 - 0:11:19]: The cars continue around the snowy track, maintaining their positions. Spectators or models are seen near the track, adding to the race atmosphere.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which team has the highest R4 score in the \"2023 - 2024 BRAGGIN' WAGON\" race results?",
        "time_stamp": "0:11:06",
        "answer": "D",
        "options": [
          "A. NUMBSKULL Racing.",
          "B. DUBIOUS DIECAST.",
          "C. LIVE YOUNG DIECAST.",
          "D. HOT NUTS DIECAST."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_495_real.mp4"
  },
  {
    "time": "[0:12:00 - 0:13:00]",
    "captions": "[0:11:00 - 0:11:20] [0:11:00 - 0:11:06]: A table titled \"2023 - 2024 BRAGGIN' WAGON\" displays race results. There are four rows corresponding to different teams: 'NUMBSKULL Racing,' 'HOT NUTS DIECAST,' 'DUBIOUS DIECAST,' and 'LIVE YOUNG DIECAST.' The columns labeled R1 through R7 list their scores across seven races. The total score for each team is at the end of each row: NUMBSKULL has 15, HOT NUTS DIECAST has 16, DUBIOUS DIECAST has 15, and LIVE YOUNG DIECAST has 16 points. [0:11:07 - 0:11:13]: The scene shifts to a snow-covered race track with four scale model cars positioned at the starting line. The track has multiple lanes that bend and curve through the snowy terrain. A blue wall is visible in the background. The four cars are staggered with different colors: green, purple, red, and blue. [0:11:14 - 0:11:16]: The race begins, and the cars speed down the track. The blue car takes the lead, followed closely by the red and purple cars, with the green car in fourth place. [0:11:17 - 0:11:17]: The cars navigate a curve, maintaining their positions with the blue car still in the lead. A truck or additional vehicle can be seen near the bottom left. [0:11:18 - 0:11:18]: The blue car maintains its lead over the others as they approach a bridge over a greenish stream. A billboard is visible on the right side of the track. [0:11:19 - 0:11:19]: The cars continue around the snowy track, maintaining their positions. Spectators or models are seen near the track, adding to the race atmosphere.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Which car won first place in this competition?",
        "time_stamp": "0:12:58",
        "answer": "C",
        "options": [
          "A. The blue car.",
          "B. The dark purple car.",
          "C. The red car.",
          "D. The green and black car."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_495_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:08]: The video begins with a close-up of a wooden surface where a blank white cotton canvas, measuring 28 cm by 23 cm (11 inches by 9 inches), is centrally placed. The text on the canvas mentions its size and material. To the right of the canvas is a white palette with some preloaded paint colors: black, cerulean blue, permanent yellow, vermilion, and white. A right hand appears intermittently on the right side, squeezing out the paints onto the palette. The background is a flat wooden surface. [0:00:09 - 0:00:13]: After all the paint colors are dispensed, the canvas remains in the same position on the wooden surface, with the colors visible on the palette. The hand is now holding a 25mm flat brush over the palette. [0:00:14 - 0:00:20]: The hand uses the flat brush to mix the colors on the palette, starting from the black paint and moving among the others. The canvas remains untouched, and the scene maintains the same composition with the canvas in the middle and the palette to the right.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What are the dimensions of the canvas just shown in the video?",
        "time_stamp": "00:00:20",
        "answer": "B",
        "options": [
          "A. 25 cm by 20 cm.",
          "B. 28 cm by 23 cm.",
          "C. 30 cm by 25 cm.",
          "D. 32 cm by 28 cm."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the white palette located relative to the canvas?",
        "time_stamp": "00:00:08",
        "answer": "D",
        "options": [
          "A. To the left of the canvas.",
          "B. Above the canvas.",
          "C. Below the canvas.",
          "D. To the right of the canvas."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_136_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:50]: The scene captures the process of painting a sky with acrylics on a canvas. The background consists of a gradient transition from blue at the top to orange and red toward the bottom, depicting a sunset. Two visible patches of clouds are painted using darker shades of blue and green. These clouds are positioned mainly in the middle third of the canvas. A paintbrush is shown in the lower middle part of the canvas, applying darker colors. The palette is situated to the right of the canvas, holding blobs of paint in various colors: black, blue, yellow, and red. The table under the canvas and palette is light brown. [0:02:50 - 0:02:59]: The hand continues to work on the clouds in the middle area of the canvas, adding details with smooth, horizontal strokes. Simultaneously, the brush moves from the middle towards the top left of the canvas, hinting at new formations of clouds. The color palette remains beside the canvas, and the arrangement of paints is unchanged. [0:02:59 - 0:03:00]: Two paintbrushes are briefly shown in the middle of the canvas, suggesting the potential use of multiple brushes throughout the creation process. The background gradient and existing cloud details remain consistent with previous frames.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the palette located in relation to the canvas?",
        "time_stamp": "00:02:59",
        "answer": "D",
        "options": [
          "A. To the left of the canvas.",
          "B. Above the canvas.",
          "C. Below the canvas.",
          "D. To the right of the canvas."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_136_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:28]: The video depicts a first-person view of someone painting a landscape. The focus is on the canvas being painted, which shows a serene sunset scene with a mix of blue, orange, and yellow hues in the sky. Dark clouds are visible, and the middle ground has a silhouette of a treeline, while the foreground shows a body of water reflecting the colors of the sky. The painting is done on a rectangular canvas positioned on a light-colored wooden surface. The painter is using a small brush to add details to the lower part of the painting, specifically adding dark shapes to the water, suggesting vegetation or shadows. The artist’s hand occasionally enters the frame as they work on the painting. [0:05:29 - 0:05:34]: The frame shifts to a broader view, showing the full canvas and a palette with different colors of paint placed next to it on the wooden surface. The palette has several blobs of paint, including colors such as red, yellow, blue, and black. There is also some indication of mixing on the palette. The painter then begins to mix paint on the palette using a liner brush. [0:05:35]: The painter continues mixing colors on the palette, and their hand is visible, holding the brush delicately. [0:05:36 - 0:05:39]: The focus returns to the canvas, where the painter uses the liner brush to add fine details to the sky, possibly enhancing the cloud formations or adding subtle color gradations. The hand holding the brush moves with precision, adding intricate touches to the artwork.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What does the painter do after shifting to a broader view of the canvas?",
        "time_stamp": "00:05:34",
        "answer": "C",
        "options": [
          "A. Cleans the brush.",
          "B. Adds details to the water.",
          "C. Mixes paint on the palette.",
          "D. Adjusts the position of the canvas."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_136_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:02]: The video shows a canvas with a scenic painting of a sunset over a body of water. The colors of the sky transition from light yellow near the horizon to darker blue higher up. The sun appears near the horizon, casting its reflection in the water below. The scene depicts a landscape with some silhouetted trees and bushes along the edge of the water. A hand holding a thin paintbrush is visible, adding details to the painting. [0:08:03 - 0:08:05]: The view shifts slightly to reveal a palette of mixed paints placed next to the canvas on a wooden table. The palette contains several blobs of paint in various colors including black, blue, white, yellow, and red. A hand holds a brush, possibly to mix colors on the palette. [0:08:06 - 0:08:13]: The hand begins drawing vertical lines on the left side of the canvas, suggesting the start of additional elements in the landscape, such as trees or reeds. Each line is carefully painted with steady movement, from the bottom towards the top of the canvas. The background and the painted details remain consistent, with the sunset's reflection clearly visible in the water. [0:08:14 - 0:08:19]: The artist continues to add more vertical lines of varying heights to the scene, enhancing the landscape with these new elements. The strokes are deliberate and precise, contributing to the realistic portrayal of the scenery. The paint palette remains on the right side of the frame, showing the same array of colors.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the painting process just shown in the video?",
        "time_stamp": "00:08:19",
        "answer": "D",
        "options": [
          "A. The artist paints a bright sunny day with flowers and butterflies.",
          "B. The artist creates a night scene with stars and a moon.",
          "C. The artist illustrates a winter landscape with snow and ice.",
          "D. The artist depicts a sunset over water, adding vertical elements like trees or reeds."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_136_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a first-person perspective on a football field, showing a player with pink shoes in front, running with the football. [0:00:01 - 0:00:03]: The camera remains focused on the ground, catching the movements and shadows of the players around the ball. One player in yellow appears briefly. [0:00:03 - 0:00:05]: Another player in a blue outfit comes closer, appearing to tackle or interact with the person holding the camera and moving forward. [0:00:05 - 0:00:06]: The view shifts slightly, showing more of the football field. The person with the camera runs towards an open space. [0:00:06 - 0:00:07]: The ball is visible again on the field, with players dispersed in the background. The field extends into the distance, with bright overhead lights. [0:00:07 - 0:00:09]: The video captures a wide view of the field under the night sky. Text appears at the bottom, referencing playing freely like a famous football player. The ball is high in the air. [0:00:09 - 0:00:10]: The ball lands and moves along the green turf. A player in pink shoes is visible in the center of the frame. [0:00:10 - 0:00:11]: The camera focuses on the ball again, with players in the distance and the text continuing at the bottom of the screen. [0:00:11 - 0:00:12]: Another player comes into view, interacting closely with the person holding the camera. [0:00:12 - 0:00:13]: The camera captures the field from a new angle, showing a player in pink shoes and others spread out. [0:00:13 - 0:00:14]: The view shifts to a more intense football action, with multiple players close to the ball and moving rapidly. [0:00:14 - 0:00:15]: The camera angle changes, showing players running towards the ball near the center of the field, under bright floodlights. [0:00:15 - 0:00:16]: The camera follows the ball closely, players' shadows and legs are visible, indicating fast movement. [0:00:16 - 0:00:17]: The camera movement is swift, capturing the ball and players in dynamic action. [0:00:17 - 0:00:18]: The person holding the camera is seen running, with some players and the football visible ahead. [0:00:18 - 0:00:20]: The video ends with more intense action on the field, and text at the bottom asks a question about the gameplay quality, showing a player controlling the ball closely.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is about the football field's lighting condition?",
        "time_stamp": "00:00:16",
        "answer": "B",
        "options": [
          "A. It is dimly lit.",
          "B. It has bright overhead lights.",
          "C. It is partially lit.",
          "D. It has no lights."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_259_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:24]: The view shows an artificial turf field illuminated by several overhead lights. Several players appear on the field, with one player in yellow close to the ball near the goal area, possibly setting up for a shot. Another player in blue is crouched down, likely preparing to defend or block the ball. The background consists of trees and a fence surrounding the field. [0:03:25 - 0:03:29]: The player in yellow is seen in the middle of a shot, while another player in blue is positioned closer to the goal, ready to block or save. Another player is running towards the right, maintaining a defensive position.  [0:03:30 - 0:03:31]: The ball reaches closer to the goal area, with the player in blue closer to it and the player in yellow maintaining their position. The background remains consistent with trees, fence, and field lights. [0:03:32 - 0:03:33]: The ball is kicked towards the goal, and the player closest to the goal moves to block it. The defensive players stay alert and ready.  [0:03:34 - 0:03:36]: The focus enlarges, showing more players on the field spread out. One player is positioned further back, wearing blue and marked with the number 25 on their shirt, moving towards the ball.  [0:03:37 - 0:03:40]: Multiple players converge towards the ball in the center, with some close to the goal in defensive stances. One player in blue is seen heavily involved, likely moving to intercept the ball or tackle the player in yellow.  [0:03:41]: The player in yellow is trying to maintain control of the ball, surrounded by defenders. A combination of dynamic play and strategic positioning is evident as players engage closely and spread across the field, illuminated by field lights and surrounded by a fence and trees.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the color of the player's jersey who is dribbling the ball?",
        "time_stamp": "00:03:24",
        "answer": "C",
        "options": [
          "A. Blue.",
          "B. Red.",
          "C. Yellow.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_259_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:42]: The video starts on a fenced soccer field, with artificial turf under artificial lighting. The camera is positioned at a low angle near the ball, indicating it is likely worn by a player involved in the action. Other players can be seen in the distance. The field is surrounded by tall buildings in the background, illuminated by lights. Many players are wearing dark jerseys while one or two are in bright colors. [0:06:42]: The ball is being kicked towards another player, who is running towards it while positioned near the goal. A player in a yellow jersey is guarding the goal. [0:06:42]: As the ball rolls forward, a player in a dark jersey chases it. The video captures the player's silhouette on the ground. The word \"Again!\" appears as a caption, indicating a verbal command or encouragement. [0:06:42]: The ball continues rolling toward another player wearing a dark jersey. The scene occurs under artificial lighting with players scattered across the field. [0:06:42]: The player's foot comes into view, suggesting a kick or attempt to control the ball. The field remains well-lit with visible markings. [0:06:42]: As the action progresses, a shadow indicates the player's movement. The view encompasses other players moving towards the goalposts at the far end. [0:06:42]: The play advances, and the video shows a close-up of a player's foot, likely post-kick, focusing on the movement and immediate football action. [0:06:42]: The player's shadow is visible on the field, with teammates and opponents further upfield. Surrounding fence and goals are also visible. [0:06:42]: The pace of the game quickens, and players move towards the goal area. Updated positions and tactics as the game continues. [0:06:42 - 0:06:43]: As the players move closer, someone is called offside, indicating a common soccer rule infraction. The environment and pace suggest an active, competitive match. [0:06:43]: The game progression captures another player positioning themselves to receive the ball. The caption \"Offside\" emphasizes the previous rule infraction. [0:06:43 - 0:06:44]: Players realign on the field after the offside call. Activities around the field indicate continuing gameplay. [0:06:44]: The video shifts to another angle to show overall field coverage and provides an extended view of the ongoing game under lit conditions. [0:06:44 - 0:06:45]: A player calls out \"Hey!\" as they respond to a pass or signal to teammates. The background includes fencing and visible lighting. [0:06:45]: The game dynamics show quick passes and responsive movements among players. Their shadows cast long on the turf signify the night setting. [0:06:45]: The video shows a close-up of a player in yellow, indicating active defensive positioning while others navigate the ball on the pitch. [0:06:45]: Another player in a dark jersey is running with the ball, moving it across the field as teammates and opponents follow closely. [0:06:45]: The play transitions to the opposing team's area, with players aligning for strategic positioning and directional movements. [0:06:45]: Further gameplay details include a close defense by a player heading towards the ball for interception. The clear field lines highlight tactical areas. [0:06:45]: As the video advances, closer interactions, such as tackles, are captured. The intense play motions underline the game's competitive nature.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What rule infraction happened just now?",
        "time_stamp": "00:06:43",
        "answer": "C",
        "options": [
          "A. Handball.",
          "B. Foul.",
          "C. Offside.",
          "D. Corner kick."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      },
      {
        "task_type": "Object Recognition",
        "question": "What is surrounding the soccer field?",
        "time_stamp": "00:07:00",
        "answer": "C",
        "options": [
          "A. A wall.",
          "B. Tall buildings.",
          "C. A fence.",
          "D. Trees."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_259_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:11:00]",
    "captions": "[0:10:00 - 0:10:20] [0:10:00 - 0:10:02]: The video shows a first-person perspective on a football field with an artificial green turf. The camera is looking down at a football close to the bottom center as the player is dribbling it. Another player in the distance is running towards the ball. The field is surrounded by fencing. [0:10:03 - 0:10:06]: The viewpoint shifts upwards, revealing the broader playing field with multiple players scattered around, actively moving. The background lighting suggests it is nighttime, with the field illuminated by tall floodlights. [0:10:07 - 0:10:10]: The camera operator is running towards a player in a green vest who is dribbling the ball. The field appears expansive, with more players visible in the distance and trees and structures outside the fence. [0:10:11 - 0:10:14]: The player in the green vest comes closer, and the individuals seem to engage in a brief tussle for the ball. The camera retains a focus on the ball and the immediate surroundings. [0:10:15 - 0:10:17]: The camera pans back to the middle of the field. There are several players in navy jerseys and one in yellow, clustered around the ball. The surroundings remain consistent with previous frames, with the field’s lighting providing clear visibility. [0:10:18-0:10:20]: The camera captures the ball being chased by the player wearing the camera, who runs towards it, before the perspective focuses back on an opposing player in a green vest controlling the ball near the far side.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the player in the green vest controlling the ball?",
        "time_stamp": "00:10:20",
        "answer": "C",
        "options": [
          "A. Near the middle of the field.",
          "B. Near the goalpost.",
          "C. Near the sidelines of the field.d.",
          "D. Near the entrance gate."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_259_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What information is being displayed on the screen just now?",
        "time_stamp": "00:00:09",
        "answer": "B",
        "options": [
          "A. Current speed of the cyclist.",
          "B. Details of Susten Pass (East side).",
          "C. A map showing the cycling route.",
          "D. Weather conditions in the area."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_269_real.mp4"
  },
  {
    "time": "[0:03:15 - 0:03:35]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the position of the guardrail right now?",
        "time_stamp": "00:03:24",
        "answer": "A",
        "options": [
          "A. On the right side of the road.",
          "B. Directly behind the cyclist.",
          "C. On the left side of the road.",
          "D. Directly ahead of the cyclist."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_269_real.mp4"
  },
  {
    "time": "[0:06:30 - 0:06:50]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the cyclist descending towards right now?",
        "time_stamp": "00:06:39",
        "answer": "A",
        "options": [
          "A. A dense forest area.",
          "B. A rocky hill.",
          "C. An open grassland.",
          "D. A small village."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_269_real.mp4"
  },
  {
    "time": "[0:09:45 - 0:10:05]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the cyclist approaching right now?",
        "time_stamp": "00:10:01",
        "answer": "C",
        "options": [
          "A. A parking lot.",
          "B. A bridge.",
          "C. A small village.",
          "D. A tunnel."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_269_real.mp4"
  },
  {
    "time": "[0:13:00 - 0:13:20]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is to the right of the cyclist right now?",
        "time_stamp": "00:13:06",
        "answer": "C",
        "options": [
          "A. A canel.",
          "B. A concrete wall.",
          "C. A clear view of mountain and trees.",
          "D. A lake."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_269_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:19]: The video showcases a serene waterfront scene set in what seems to be a small, tranquil harbor or marina. The vantage point is from a slightly elevated and stationary perspective on a dock or pier, overlooking the water and a collection of small boats moored near the shore. The water in the harbor is calm and reflects the sky and surrounding greenery and buildings. Directly across the water are several buildings with a rustic charm, including a couple of boathouses and a small shack-like structure with signs indicating it is a place of business. The boathouses are painted in pastel colors, with red, green, and beige hues. These buildings lie close to the water's edge, with some of the wooden docks extending into the water. The rightmost part of the frame has a building with a \"No Swimming\" sign, and nearby is a boat with a sail, tied to the dock.  Adjacent to the buildings and extending towards the left, there is a larger structure that appears to be a hotel, denoted by the \"Hotel\" sign mounted on the building's roof. There are people visible on the docks and near the water, casually walking or standing around. The scene is bathed in soft daylight, with patches of sunlight filtering through the light cloud cover. Trees with rich green foliage surround the buildings, offering a lush backdrop, and hills are visible in the distance. The boats in the harbor are small to medium-sized, and a few have cabins or coverings while others are more exposed. They're anchored close to the dock, complementing the tranquil environment. The foreground shows some nautical elements like large chains and mooring cleats on the dock.  The setting remains largely serene and unchanged throughout the video, capturing the peacefulness of the harbor and its quaint surroundings. There are few movements, and the primary focus remains on the scenic waterfront and the reflections in the calm water.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What sign is visible on the rightmost building in the video?",
        "time_stamp": "0:00:20",
        "answer": "C",
        "options": [
          "A. \"No Fishing\".",
          "B. \"No Boats\".",
          "C. \"No Swimming\".",
          "D. \"No Parking\"."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Event Understanding",
        "question": "What has the video primarily captured so far?",
        "time_stamp": "0:00:20",
        "answer": "B",
        "options": [
          "A. A bustling harbor with many boats arriving and departing.",
          "B. A serene waterfront scene with calm water and minimal movement.",
          "C. A stormy sea with waves crashing into the docks.",
          "D. A busy marketplace with people shopping and trading."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_315_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:55]: A first-person perspective shows the inside of a tram filled with passengers. The passengers are seated and facing forward, with a few people standing. The tram ceiling is white with visible red wiring. Overhead screens display some video content. The right side of the tram has large windows, offering a view outside. The background outside is lush and green with trees and grass. The tram appears to be moving as it progresses through the frames. [0:02:55 - 0:02:58]: The camera captures more of the outside view, including a grassy hill on the right, with scattered trees and houses in the background. The tram continues moving, and passengers' backs are partially visible in the frames. [0:02:58 - 0:03:00]: The setting transitions entirely to an outdoor scene depicting a residential area. A road sign with a left turn symbol comes into view, indicating a twist or turn in the tram's route. The green scenery, trees, and houses populate the area as the tram continues its path.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the tram ceiling?",
        "time_stamp": "00:02:55",
        "answer": "B",
        "options": [
          "A. Blue.",
          "B. White.",
          "C. Grey.",
          "D. Black."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_315_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:26]: The video begins with a view from a vehicle traveling on a paved road. To the right is a blue building with white-framed windows and a large tree. The building and tree come closer as the vehicle moves forward. [0:05:27 - 0:05:31]: The video continues to show the vehicle moving along the road, passing by several large bushes and a concrete building with visible wooden supports. A couple of yellow traffic signs appear on the left side, one indicating a winding path. [0:05:32 - 0:05:35]: The vehicle passes a small hill on the right with a sign that cautions about construction and directions for pedestrians. The path splits, with the road curving to accommodate the hill. [0:05:36 - 0:05:40]: The vehicle continues moving alongside a grassy hill with lush green trees. A cone and some bushes are visible in the foreground, and the road extends into the distance. The surroundings appear calm and orderly.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What type of windows does the blue building have?",
        "time_stamp": "00:05:26",
        "answer": "C",
        "options": [
          "A. Metal-framed windows.",
          "B. No windows.",
          "C. White-framed windows.",
          "D. Wooden-framed windows."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_315_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:56]",
    "captions": "[0:09:40 - 0:09:56] [0:09:40 - 0:09:43]: The video begins by showing a colorful, old-west-style building with the word \"Tickets\" prominently displayed on its facade. The building has a green roof with a decorative emblem and yellow trim around the windows and doors. A wooden fence surrounds the ticket booth, and there is a clear, blue sky with some streaks of clouds in the background. On the left, there is a partial view of an amusement park ride or vehicle. [0:09:44 - 0:09:46]: As the video progresses, the perspective moves closer to the ticket booth, showing more details of the building, such as ornate trims above the windows and a small wooden step leading to the entrance. On the left side, the \"CLAIM\" sign above another part of the building becomes more visible.  [0:09:47 - 0:09:50]: Moving forward, a large inflatable figure of a character in a green hat and red shirt holding a yellow star becomes visible on the left side of the frame. The character appears to be part of the amusement park's decoration, towering above the colorful facades of the buildings. The pathway in front of the structures is clear, with various vibrant structures lining the side. [0:09:51 - 0:09:53]: The camera shifts further along the row of colorful buildings, passing by more detailed facades decorated with blue, red, and yellow paint. A couple of benches and trash cans are visible along the pathway, indicating a waiting or resting area for visitors. The inflatable figure remains in the frame, towering over the visitors. [0:09:54 - 0:09:56]: The video continues to move along the colorful buildings, showing more attractions and decorations. There is a noticeable increase in detail, including signs pointing to various attractions and more colorful elements near the buildings, enhancing the vibrant amusement park atmosphere. The sky remains clear, providing a bright and cheerful backdrop to the scene.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What word is prominently displayed on the facade of the old-west-style building right now?",
        "time_stamp": "0:09:43",
        "answer": "A",
        "options": [
          "A. \"Tickets\".",
          "B. \"Rides\".",
          "C. \"Games\".",
          "D. \"Food\"."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_315_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:23]: The video captures a first-person view of a player in a block-based game. The player is cutting down a tree with a tool, likely an axe. The surrounding environment is characterized by dense, green foliage and blocky graphics typical of the game. The leaves and tree trunk are visible, with the player’s hand wielding the axe in the center. [0:05:24 - 0:05:26]: The player continues cutting the tree, with the view pointing upwards, indicating they are working on the higher sections of the trunk. The sky, dotted with block-shaped clouds, is partly visible through the leaves. [0:05:27 - 0:05:30]: The player moves to a different tree, evident from the shift in perspective. The new tree has a lighter, stripe-like bark, signifying a different type of tree. The player starts cutting this tree from the base upwards. [0:05:31 - 0:05:34]: The player continues to chop the same tree, transitioning from the base to the upper sections. The leaves and sky remain visible, with the occasional block of wood breaking off as they progress. [0:05:35 - 0:05:37]: The player again moves to another tree. This one has a darker bark and is adjacent to a small pool of water and grassy terrain. [0:05:38 - 0:05:40]: The player chops down this darker tree, from the base upwards, with the sky and treetops visible through the gaps in the canopy. The environment around includes other trees, and the ground has a mix of grass and exposed earth.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What tool is the player using to cut down the trees?",
        "time_stamp": "0:05:23",
        "answer": "A",
        "options": [
          "A. Stone axe.",
          "B. Diamond Axe.",
          "C. Golden axe.",
          "D. SWooden axe."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_199_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:11:00]",
    "captions": "[0:10:40 - 0:11:00] [0:10:40 - 0:10:42]: The video starts with a player standing outdoors in a grassy area of the game, facing a set of tools placed on the ground, including a furnace, crafting table, and fire. The player has raw copper in their hand and beside the tools are other objects like a stone block and some foliage hills in the background. [0:10:43 - 0:10:45]: The player interacts with the furnace interface, placing raw copper in the input slot. Various inventory items are visible at the bottom of the screen, including food, tools, and materials. [0:10:46 - 0:10:48]: The furnace interface remains open, showing raw copper in the input slot again with inventory items like coal, iron pickaxe, and food clearly visible. The cursor is hovering over one piece of raw copper. [0:10:49 - 0:10:51]: The player places coal into the furnace interface, showing raw copper in the input slot and coal in the fuel slot, beginning the smelting process. [0:10:52 - 0:10:54]: The video transitions to the player exploring an open area with a stone structure and the background filled with greenery and water. A tall wooden tower stands prominently in the middle distance. The player is holding a piece of raw copper. [0:10:55 - 0:10:57]: The player moves around the stone structure, which is built next to a grassy patch leading down to a body of water. Additional tools and items are set up nearby, including a red bed, chests, and the previously interacted furnace and crafting table. [0:10:58 - 0:10:59]: The player starts mining a stone block in the ground. The scene is a stony terrain with patches of grass and dirt, near the water. [0:11:00 - 0:11:01]: The player continues to mine the stone, deepening the hole. The background shows the green landscape with some distant water.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What did the player just take out of the furnace?",
        "time_stamp": "00:10:51",
        "answer": "B",
        "options": [
          "A. Iron pickaxe.",
          "B. Stone.",
          "C. Food.",
          "D. Stone block."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_199_real.mp4"
  },
  {
    "time": "[0:11:00 - 0:12:00]",
    "captions": "[0:10:40 - 0:11:00] [0:10:40 - 0:10:42]: The video starts with a player standing outdoors in a grassy area of the game, facing a set of tools placed on the ground, including a furnace, crafting table, and fire. The player has raw copper in their hand and beside the tools are other objects like a stone block and some foliage hills in the background. [0:10:43 - 0:10:45]: The player interacts with the furnace interface, placing raw copper in the input slot. Various inventory items are visible at the bottom of the screen, including food, tools, and materials. [0:10:46 - 0:10:48]: The furnace interface remains open, showing raw copper in the input slot again with inventory items like coal, iron pickaxe, and food clearly visible. The cursor is hovering over one piece of raw copper. [0:10:49 - 0:10:51]: The player places coal into the furnace interface, showing raw copper in the input slot and coal in the fuel slot, beginning the smelting process. [0:10:52 - 0:10:54]: The video transitions to the player exploring an open area with a stone structure and the background filled with greenery and water. A tall wooden tower stands prominently in the middle distance. The player is holding a piece of raw copper. [0:10:55 - 0:10:57]: The player moves around the stone structure, which is built next to a grassy patch leading down to a body of water. Additional tools and items are set up nearby, including a red bed, chests, and the previously interacted furnace and crafting table. [0:10:58 - 0:10:59]: The player starts mining a stone block in the ground. The scene is a stony terrain with patches of grass and dirt, near the water. [0:11:00 - 0:11:01]: The player continues to mine the stone, deepening the hole. The background shows the green landscape with some distant water.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What food did the player just eat?",
        "time_stamp": "00:11:05",
        "answer": "A",
        "options": [
          "A. Fishes.",
          "B. pork.",
          "C. Orange.",
          "D. beef."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_199_real.mp4"
  },
  {
    "time": "[0:16:00 - 0:17:00]",
    "captions": "[0:16:00 - 0:16:20] [0:16:00 - 0:16:05]: The video starts with a view of a landscape from a first-person perspective in Minecraft. In the foreground, the player is holding a map. In the background, there is a tall stone tower with a pointed top. The terrain around the tower is rocky, with some patches of green grass and brown soil. A body of water is visible on the left side of the screen, while the right side shows a grassy hill. The sky is mostly clear with a few scattered clouds. [0:16:06]: The camera angle shifts slightly upward, now showing more of the sky and the tops of trees in the distance. There are a few fluffy clouds in the sky. The greenery appears to be a dense forest. [0:16:07]: The camera angle lowers again, showing a wooden chest on the left side of the screen, a stone path, and grassy areas with birch trees. The ocean is visible in the background. [0:16:08 - 0:16:09]: The perspective now shows the map clearly, held in both hands, as the player stands near the edge of a grassy block with a chest to the left. [0:16:10]: A view of a small setup including a crafting table with a lit torch, a furnace, a single chest, and a fire burning. Green foliage is in the background. [0:16:11 - 0:16:12]: Coordinates \"1320, 1450\" appear on the screen. The player's crafting interface is open, showing various items like sticks, wood planks, and a diamond on the left side. The player appears to be preparing to craft an item. [0:16:13 - 0:16:14]: The player crafts wooden planks, then uses them to craft wooden sticks, visible on the interface. [0:16:15]: The view returns to the tall stone tower, maintaining the same first-person perspective from earlier, with the tower centered and the sky clear with scattered clouds. [0:16:16 - 0:16:20]: Detailed game statistics overlay the screen, providing information on the player’s coordinates, biome, and other relevant data. The clear view of the tower and surrounding terrain remains in the background as the information is displayed.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What object is the player holding right now?",
        "time_stamp": "0:16:05",
        "answer": "A",
        "options": [
          "A. A map.",
          "B. A sword.",
          "C. A pickaxe.",
          "D. A bow."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Event Understanding",
        "question": "What did the player do after the coordinates '1320, 1450' appeared on the screen?",
        "time_stamp": "0:16:12",
        "answer": "C",
        "options": [
          "A. Fight an enemy.",
          "B. Open a chest.",
          "C. Craft an item.",
          "D. Explore the terrain."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_199_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the vehicle immediately in front of the taxi right now?",
        "time_stamp": "00:00:02",
        "answer": "C",
        "options": [
          "A. A bus.",
          "B. A bicycle.",
          "C. A yellow car.",
          "D. A motorcycle."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_381_real.mp4"
  },
  {
    "time": "[0:02:02 - 0:02:07]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What colors are the advertisements on the left buildings right now?",
        "time_stamp": "00:02:05",
        "answer": "A",
        "options": [
          "A. Red and White.",
          "B. Black and Yellow.",
          "C. Green and White.",
          "D. Blue and Red."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_381_real.mp4"
  },
  {
    "time": "[0:04:04 - 0:04:09]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type of vehicle is in front of the yellow taxi right now?",
        "time_stamp": "00:04:07",
        "answer": "A",
        "options": [
          "A. A black car.",
          "B. A white van.",
          "C. A gray SUV.",
          "D. A red truck."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_381_real.mp4"
  },
  {
    "time": "[0:06:06 - 0:06:11]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person in the crosswalk holding in her hand right now?",
        "time_stamp": "00:06:08",
        "answer": "C",
        "options": [
          "A. A handbag.",
          "B. A phone.",
          "C. A shopping bag and a handbag.",
          "D. An umbrella."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_381_real.mp4"
  },
  {
    "time": "[0:08:08 - 0:08:13]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What colors are the cones lining the street?",
        "time_stamp": "00:08:08",
        "answer": "A",
        "options": [
          "A. Orange and white.",
          "B. Yellow and black.",
          "C. Green and white.",
          "D. Blue and white."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_381_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a view of a large white wall in an art gallery. A hanging light fixture is illuminating the space, and part of a blue sign is visible near the ceiling. [0:00:01 - 0:00:03]: The camera angle slightly adjusts to reveal framed paintings on the wall. Two paintings are visible: the larger one on the left depicts a woman with dark, curly hair looking at the viewer, and the smaller one on the right shows the same woman in a different pose. [0:00:03 - 0:00:06]: The camera continues to move along the wall, revealing more details of the two paintings. The backgrounds of the paintings are light and abstract, with touches of green and pink. [0:00:06 - 0:00:08]: The camera moves further to the right, showing another painting of two individuals. The left figure is dressed in a white suit, and the right one wears a hat. Both figures look directly at the viewer. [0:00:08 - 0:00:10]: The camera zooms in on the paintings of the two individuals, revealing a delicate texture and brushwork. Another painting on the left shows a child in a white dress. [0:00:10 - 0:00:12]: The camera shifts slightly, showing the painting of the child in greater detail. The child is looking away, and the background is a mix of light and pastel colors. [0:00:12 - 0:00:13]: As the camera continues to the right, the painting of the child remains prominent. In the background, more paintings and visitors are visible, creating a lively gallery atmosphere. [0:00:13 - 0:00:15]: The camera captures an individual dressed in a leopard-print coat walking past. The scene transitions to another part of the gallery with more paintings on display. [0:00:15 - 0:00:17]: In the new section of the gallery, several framed paintings are visible on white walls. One shows a close-up of a horse, and another depicts a person sitting on the ground. [0:00:17 - 0:00:19]: The camera focuses on the painting of the horse, showcasing intricate details of the animal's face and mane. More paintings in the background include various subjects. [0:00:19 - 0:00:20]: As the video ends, the painting of the horse remains in view, with its expressive eyes and detailed texture being the focal point. The surrounding ambiance continues to reflect a calm and curated art gallery.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What best describes the paintings shown in the video?",
        "time_stamp": "00:00:10",
        "answer": "B",
        "options": [
          "A. Abstract landscapes with vibrant colors.",
          "B. Portraits with light and pastel backgrounds.",
          "C. Still life paintings with intricate details.",
          "D. Surreal scenes with dark and moody tones."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      },
      {
        "task_type": "Counting",
        "question": "How many paintings featuring woman are visible just now?",
        "time_stamp": "00:00:13",
        "answer": "D",
        "options": [
          "A. One.",
          "B. Two.",
          "C. Three.",
          "D. Four."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_479_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:22]: The video begins with a view of two framed artworks on a white wall. The top piece portrays a woman adorned with gold floral designs, with her left arm flexed and her chin resting on her hand. Below this is another artwork of a woman in a golden dress, her eyes closed, and displaying a pensive expression. Both frames are black, and the wall is plain white. [0:05:23 - 0:05:25]: The camera moves down slightly, highlighting more of the lower artwork. The woman continues to exude a tranquil demeanor, surrounded by a dark, slightly blurred backdrop with spots of greenish light effects. [0:05:26 - 0:05:29]: As the camera pans left, it captures an elongated artwork with a predominantly blue background. This piece is framed in black and displays the abstract form of a human figure. The surrounding wall remains white, with minimal background distractions. [0:05:30 - 0:05:32]: The camera shifts slightly right revealing a large, tall artwork of a woman in a dark, shimmery dress. She is gazing upward, holding a glowing sphere overhead. This piece is positioned next to the blue abstract artwork. [0:05:33 - 0:05:35]: The camera continues to move right, showing more of the glowing sphere artwork. The focus on the woman and the glowing sphere intensifies, revealing the details in the dark, mysterious ambiance of the piece. [0:05:36 - 0:05:37]: To the right of the glowing sphere artwork, another painting comes into view. This piece features a person in a white robe, holding a book, with a vibrant red background. An oval of dynamic white and light blue swirls surrounds the figure, creating a striking contrast with the red. [0:05:38 - 0:05:39]: The camera remains steady on the vibrant red artwork, displaying the intricate details of the swirling colors and the figure’s expressions. The figure appears dignified, with the bright colors emphasizing the intense backdrop.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is positioned to the left of the artwork with the glowing sphere?",
        "time_stamp": "00:05:40",
        "answer": "D",
        "options": [
          "A. A painting with a blue background.",
          "B. A painting with a green background.",
          "C. A painting with a white background.",
          "D. A painting with a red background."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "How is the background of the lower artwork described?",
        "time_stamp": "00:05:24",
        "answer": "B",
        "options": [
          "A. Bright and clear with blue skies.",
          "B. Dark and slightly blurred with greenish and blue light effects.",
          "C. White and plain with no distractions.",
          "D. Vivid and colorful with red hues."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_479_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:05]: The video begins with a focus on several framed artworks hanging on a white wall. The first painting on the left side depicts a group of people, some standing and some sitting at a table, seemingly engaged in a game of chess. The painting has dark, contrasting colors, mainly blue and shades of gray. To the right of this painting, a smaller framed artwork shows a vibrant house with bright yellow and red features, set in a street. [0:08:05 - 0:08:10]: As the camera moves slightly to the right, two landscape paintings become visible below the initial two artworks. The first landscape painting, on the left, has a golden sky reflecting on a water body with a cow resting in the foreground. Beside it, another landscape features an expansive field with a distant mountain range under a muted sky. Above these landscapes, there is a colorful abstract painting with geometric patterns and bold colors such as blue, red, green, and yellow.  [0:08:10 - 0:08:15]: The camera then shifts upwards to reveal a large painting of a bull’s head set against a dark, cloudy background. The bull has large horns and a piercing gaze. Next to the bull painting, a blue abstract piece filled with intricate line work and vibrant colors is displayed. [0:08:15 - 0:08:20]: Continuing to the right, the blue abstract piece is further showcased. It has rich textural details with layers of brightly colored lines and shapes. Adjacent to it, on the farthest right, sections of an artwork with white and dark elements can be seen.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What are the features of the painting to the right of painting with a sheep's head?",
        "time_stamp": "00:08:15",
        "answer": "C",
        "options": [
          "A. Geometric patterns and bold colors.",
          "B. A vibrant house on a street.",
          "C. A blue abstract piece with intricate line work.",
          "D. An expansive field under a muted sky."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_479_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video opens on a green football field with artificial turf. The person filming is holding a camera in their right hand, and another individual dressed in a blue shirt and dark shorts stands ahead, slightly to the left. The shadow cast by the person filming and the other player is visible on the turf. [0:00:02 - 0:00:04]: As the scene progresses, it appears to be a casual football game. The person wearing a blue shirt moves closer, and the ball, colorful with multiple patterns, becomes visible near the feet of someone dressed in black athletic clothing. [0:00:04 - 0:00:05]: The ball is now being dribbled past another player in white sneakers and dark athletic wear. A white boundary line on the turf indicates a section of the football field, suggesting the play is near a sideline. [0:00:05 - 0:00:06]: More players, including one in a red outfit and another in white, engage in the game. The player in red seems to be extending his arms, possibly for balance or to prevent another player from advancing. [0:00:06 - 0:00:07]: The play transitions to a wider perspective, showing a player in red approaching the goal area where other players, including one in dark clothing and one in light, prepare themselves. The setting includes a green fence and surrounding trees, indicating an outdoor location with a clear blue sky overhead. [0:00:07 - 0:00:08]: The focus shifts to the other end where two players in different colored outfits (one in black and one in white) actively chase the ball. The scene further includes additional players in casual wear and a lively environment with other park-goers in the background. [0:00:08 - 0:00:09]: The view changes to a closer interaction with a child wearing a yellow shirt and blue shorts dribbling the ball. The person filming aligns themselves behind the child who is directly facing the camera. [0:00:09 - 0:00:10]: Another young player, dressed similarly in yellow, advances to kick the ball towards the camera holder. Other players in the background appear to be part of the same active game. [0:00:10 - 0:00:11]: The scene focuses on another player wearing a grey tracksuit who approaches the ball in possession of the person filming. The position suggests defensive or offensive engagement in the casual game. [0:00:11 - 0:00:12]: The game continues with an emphasis on a player in dark clothes competing against someone in blue. The involvement of multiple individuals shows an intensifying match among peers. [0:00:12 - 0:00:13]: The perspective shows the ball again, with individuals on either side. The players and ball positioning indicate an ongoing effort to maintain possession or to strategize for scoring. [0:00:13 - 0:00:14]: A quick maneuver is captured where the ball gets controlled by another player, continuing the game's dynamic flow. This frame emphasizes the action-driven moment typical in football matches. [0:00:14 - 0:00:15]: The scene depicts a player who appears to have fallen onto the turf while pursuing the ball. The individual wears a light blue top and dark bottoms, showcasing the physical interaction inherent in the sport. [0:00:15 - 0:00:16]: Another encounter is shown where a player in dark clothes confronts an opponent wearing bright orange shoes. This segments how challenges and defensive strategies unfold during play. [0:00:16 - 0:00:17]: The filming continues with a highlight on the ball near the feet of multiple players, indicating ongoing action and control within the match. [0:00:17 - 0:00:18]: The game persists with the same lively energy, focusing on various players' efforts to engage with the ball. The surrounding environment includes spectators and resting players. [0:00:18]: The video transitions to a close-up of a portion of the football field, emphasizing the white boundary line on the green turf. This framing suggests a momentary pause or a focus on the field layout. [0:00:19 - 0:00:20]: The final frame includes text overlays hinting at a welcoming message related to football. The composition combines graphic elements with the continued focus on the field, encapsulating the video's purpose and theme.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What happened in the the video just now?",
        "time_stamp": "0:00:20",
        "answer": "C",
        "options": [
          "A. Players are shown resting on the sidelines.",
          "B. The camera focuses on the crowd.",
          "C. Text overlays hint at a welcoming message related to football.",
          "D. A player scores a goal."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_257_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:40 - 0:04:00] [0:03:40 - 0:03:42]: The video starts with a first-person perspective on a small soccer field. A player in a blue striped shirt is about to kick a soccer ball. The background includes a goal post, trees, and a fence. Multiple players are actively involved in the game. [0:03:42 - 0:03:44]: The camera captures a player scoring a goal and other players reacting. \"Nice goal\" is displayed on the screen. The setting remains on the soccer field with the same background. [0:03:44 - 0:03:45]: The focus shifts to a close-up of people sitting on the grass. One person is holding a phone, showing something to the group. The footwear of the individuals, including some brightly colored sport shoes, is visible. [0:03:45 - 0:03:46]: The perspective changes to the person tying their shoelaces while sitting on the grass. Other players are partially visible around them. [0:03:46 - 0:03:47]: The camera moves back to the game, showing a few players standing on the field. The shadows of the players are cast on the artificial grass surface. [0:03:47]: A blue-shirted player approaches the ball as the camera focuses on the play. The background includes a fence and some green area. [0:03:47 - 0:03:49]: There is a scene where players, including one in a blue-striped shirt, are engaged in active play near the ball. Shadows are prominent on the ground. [0:03:49 - 0:03:50]: The camera captures one player in a blue striped shirt and another in grey pants as they move toward the ball. More players in the background are preparing for action. [0:03:50]: The person filming gets closer to the ball while players are seen maneuvering around. [0:03:50 - 0:03:51]: A player in a black and white striped shirt appears and challenges for ball control, with the camera showing a close view of the play. [0:03:51 - 0:03:52]: The focus is on a player in the black and white striped shirt standing by the corner of the field. Other players continue to move in the background. [0:03:52 - 0:03:53]: The video shows a more comprehensive view of the field, with players positioned around the goal area, actively engaged in play. [0:03:53 - 0:03:54]: The camera captures two players chasing the ball. The wider field, including goalposts and fencing, is visible. [0:03:54 - 0:03:55]: The person filming turns their attention to the ball at their feet. Players in the background are ready for the next move. [0:03:55 - 0:03:56]: The video focuses on repositioning the ball, depicting various players. The field's lines and background fencing are visible. [0:03:56 - 0:03:57]: The perspective shifts to facing a player in white and black, as they prepare to challenge for ball control. [0:03:57 - 0:03:58]: The scene shifts to the side of the soccer field, with a player controlling the ball. The fencing and background elements, such as benches and people, are visible. [0:03:58 - 0:03:59]: The camera points towards the ground, focusing on a soccer ball and items like clothing neatly placed near the field's edge. The green surface and fence are in view. [0:03:59 - 0:04:00]: The final scene captures the ball near the sideline with adjacent objects, showing the boundary line and a neatly maintained field.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the camera showing right now?",
        "time_stamp": "00:04:00",
        "answer": "C",
        "options": [
          "A. Players celebrating a goal.",
          "B. A player tying their shoelaces.",
          "C. The ball near the sideline.",
          "D. The players leaving the field."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_257_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:20 - 0:07:40] [0:07:20 - 0:07:26]: The video takes place on a sports field with individuals playing soccer. The field has artificial grass, and there are white line markings visible. The camera shows players of various ages and in different attires, indicating a casual or friendly game. The camera is at a first-person perspective, showing others playing around and interacting with a soccer ball. [0:07:27 - 0:07:29]: The ball frequently changes possession as players from both teams chase and attempt to control it. A person in a yellow shirt and green shoes frequently appears, indicating active involvement in the game. [0:07:30 - 0:07:32]: The ball is kicked towards one end of the field, and a player in a blue shirt is seen moving towards the goalpost. In this sequence, the video captures the rapid back-and-forth movement typical of a soccer game. [0:07:33 - 0:07:35]: The frame shows a larger section of the field, with players spread out, some standing and others actively engaging with the ball. The field is well-fenced. [0:07:36 - 0:07:39]: Near the edge of the field, two players in blue and black jerseys compete for the ball. Behind them, other players run towards the action, maintaining high energy and involvement throughout the game.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What type of game is being played on the sports field right now?",
        "time_stamp": "00:07:40",
        "answer": "B",
        "options": [
          "A. A competitive league match.",
          "B. A casual or friendly game.",
          "C. A training session.",
          "D. A tournament final."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_257_real.mp4"
  },
  {
    "time": "[0:11:00 - 0:12:00]",
    "captions": "[0:11:00 - 0:11:20] [0:11:00 - 0:11:01]: A group of people is gathered on a small, fenced artificial grass soccer field. The focus is on a child in a blue jersey with dark shorts and red shoes, standing near the viewer. Other individuals are standing or sitting along the fence, parallel to the field;  [0:11:01 - 0:11:02]: The child in the blue jersey walks towards the viewer, while others play or watch the game. Some people are sitting along the fence, with bags and personal items scattered on the ground nearby;  [0:11:02]: The child is now closer, with the viewer raising a hand over his shoulder at a player in blue to the right;  [0:11:03]: The viewer's hands are on the shoulders of the child in the blue jersey, walking together on the field. The background shows other players scattered across the field and people near the fence;  [0:11:04]: They continue to walk towards the center of the field. The background remains the same with the players involved in the game;  [0:11:05]: The child in the blue jersey raises his arm, appearing to cheer, while other players interact on the field;  [0:11:06]: The player in the blue jersey is now farther from the viewer, observing the ongoing game. There's a player in a yellow jersey across the field in action;  [0:11:07]: A wide view of the soccer field shows several players engaged in the game. The viewer stands near the sideline, with people sitting and observing along the fence;  [0:11:08]: The game continues with players focused on the ball. One player in black, close to the viewer, runs towards the action while others follow;  [0:11:09 - 0:11:10]: The ball is shown close to the viewer's feet, suggesting a moment of ball control. Players in the background remain engaged in the game;  [0:11:11]: The viewer's foot is positioned to intercept or kick the ball, which is on the ground, with other players in pursuit;  [0:11:12]: A player in a blue jersey is approaching the ball, ready to make contact;  [0:11:13]: Multiple players interact around the ball, attempting to gain control. The player in the yellow jersey is prominent in this frame;  [0:11:14]: The player in the yellow jersey makes a move with the ball, while another player attempts to challenge;  [0:11:15]: The player in the yellow jersey dribbles the ball, and shadows of surrounding players stretch across the field;  [0:11:16]: The ball is free, with multiple player shadows indicating active pursuit;  [0:11:17]: The child in the blue jersey challenges another player for control of the ball;  [0:11:18]: The child in the blue jersey maneuvers with the ball along the sideline, the viewer’s shadow visible beside him;  [0:11:19]: The ball is passed or about to be passed by the child in the blue jersey, with a player in orange shoes approaching;  [0:11:20]: Finally, a group of players including the child in the blue jersey and others surround the ball actively, indicating a high-energy moment in the game.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What action is the player in the yellow jersey performing right now?",
        "time_stamp": "00:11:15",
        "answer": "C",
        "options": [
          "A. Sitting along the fence.",
          "B. Running towards the viewer.",
          "C. Dribbling the ball.",
          "D. Observing the game."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the people who are not playing the game situated in the video?",
        "time_stamp": "00:11:07",
        "answer": "C",
        "options": [
          "A. Inside the field.",
          "B. Behind the goalposts.",
          "C. Along the fence.",
          "D. Outside the stadium."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_257_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:00:10",
        "answer": "A",
        "options": [
          "A. The individual prepared a cup of matcha tea by scooping matcha powder, and whisking it.",
          "B. The individual prepared a cup of coffee by grinding the beans, brewing the coffee, and serving it in a mug.",
          "C. The individual prepared a cup of herbal tea by selecting herbs, placing them in a teapot, adding boiling water, and steeping it.",
          "D. The individual prepared a smoothie by blending fruits, adding milk, and pouring it into a glass."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_360_real.mp4"
  },
  {
    "time": "[0:02:11 - 0:02:21]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:02:20",
        "answer": "A",
        "options": [
          "A. The individual made a glass of iced water by filling a glass with ice cubes.",
          "B. The individual prepared a cup of cold brew coffee by adding milk, stirring it, and placing it on a tray.",
          "C. The individual filled a glass with ice cubes from the ice machine using a blue scoop.",
          "D. The individual brewed a fresh cup of iced tea by mixing the tea concentrate with water and ice."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_360_real.mp4"
  },
  {
    "time": "[0:04:22 - 0:04:32]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:04:32",
        "answer": "A",
        "options": [
          "A. The individual  cleaned a portafilter, and placed it on the table.",
          "B. The individual retrieved a tool, refilled it using a coffee grinder, and rinsed it under a faucet.",
          "C. The individual placed a filter into a coffee maker, filled it with grounds, and started the brewing process.",
          "D. The individual selected a mug, filled it with milk, and placed it under a milk frother."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_360_real.mp4"
  },
  {
    "time": "[0:06:33 - 0:06:43]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:06:45",
        "answer": "A",
        "options": [
          "A. The individual steamed milk, prepared espresso, grabbed a cloth, and cleaned the workstation.",
          "B. The individual filled a pitcher with water, added coffee grounds, and placed it in the refrigerator to brew.",
          "C. The individual poured milk into a coffee cup, added ice cubes, and stirred using a spoon.",
          "D. The individual brewed coffee, poured it over ice, and then added sweeteners before stirring."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_360_real.mp4"
  },
  {
    "time": "[0:08:44 - 0:08:54]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:08:54",
        "answer": "A",
        "options": [
          "A. The individual cleaned a portafilter, weighed it, and began adding coffee ground to it.",
          "B. The individual brewed coffee and filled a thermos with it before capping it off.",
          "C. The individual prepared a cup of tea by boiling water, adding tea leaves, and waiting for it to steep.",
          "D. The individual cleaned a cup, added milk, and placed it in the microwave to heat."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_360_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The scene is a first-person perspective video taken on a city sidewalk. The background includes tall buildings with large windows and an escalator on the left side, partially enclosed by a glass atrium with a metal frame. Several pedestrians are walking on the sidewalk, including a man in a blue shirt and dark pants with a backpack walking ahead and toward the right, and a man in a light blue shirt and shorts walking slightly to his right, looking at a phone. The sidewalk is made of gray and light brown tiles, and there's an orange cordon at the base of the escalator. [0:00:04 - 0:00:06]: The camera moves forward, capturing more of the sidewalk and background. The man in the blue shirt remains slightly ahead on the left, with the man in the light blue shirt following closely. In the distance, more pedestrians come into view, including some in white and light-colored clothing. The buildings in the background are tall and light-colored, with rectangular windows. A white car is visible on the adjacent street. [0:00:07 - 0:00:09]: The camera moves further along the sidewalk. A few more pedestrians appear, including a person with a white shirt and dark shorts ahead. A green bus is seen driving on the street. Traffic is present with several vehicles, including a white car and a green bus, moving along the road in the same direction as the camera's movement. [0:00:10 - 0:00:12]: As the camera continues forward, the man in the blue shirt and the man in the light blue shirt maintain their positions. More pedestrians appear on the sidewalk, with some people walking in the opposite direction. The background remains consistent with tall buildings and parked cars. The green bus is now further down the road, with more vehicles visible in the same lane. [0:00:13 - 0:00:15]: The camera captures more of the same scene, with the man in the light blue shirt walking to the camera's left and looking backwards. The sidewalk continues with gray and light brown tiles, and the road has various cars, including a red taxi and a black car. Pedestrian and vehicle movement remains consistent. [0:00:16 - 0:00:18]: As the camera progresses, it captures the rear views of a person in a white shirt and black shorts closer to the camera, walking ahead, as well as more people on the sidewalk. A yellow and red double-decker bus is visible on the road to the right. The surrounding environment is urban, with ongoing pedestrian and vehicular activity. [0:00:19 - 0:00:20]: The camera continues to move down the sidewalk. The person in the white shirt and black shorts remains in view ahead. The double-decker bus and other vehicles, including taxis and buses, continue to move along the road. The scene maintains its urban setting with tall buildings and busy streets filled with pedestrians and traffic.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What colour of bus was visible driving on the street just now?",
        "time_stamp": "00:00:12",
        "answer": "A",
        "options": [
          "A. Green bus.",
          "B. Blue bus.",
          "C. Red bus.",
          "D. Yellow bus."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      },
      {
        "task_type": "Counting",
        "question": "How many buses have been shown in the video so far?",
        "time_stamp": "00:00:35",
        "answer": "C",
        "options": [
          "A. One.",
          "B. Two.",
          "C. Three.",
          "D. Four."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_333_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:46]: A busy urban sidewalk features a diverse crowd of people, including families, young adults, and children, some standing and others walking in different directions. The left side displays a gray building wall with a metal railing leading to steps. People in various outfits such as shorts, t-shirts, and dresses are engaged in different activities, including using their phones and conversing. Some individuals are carrying shopping bags or backpacks. [0:02:47 - 0:02:55]: Movement becomes more noticeable as a few people start heading towards the street, while others remain stationary, absorbed in activities like taking photos or using phones. The surrounding environment includes tall buildings, visible through the clear evening sky, and street signs. Traffic is visible on the street in the background, along with streetlights beginning to illuminate. [0:02:56 - 0:02:59]: A group of people continues walking towards the street, where some vehicles can be seen moving. The sidewalk continues to be filled with individuals in various stages of crossing the street or standing by the metal railing. The scene captures the dynamic nature of a busy urban area as daylight transitions to dusk.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are people doing while standing on the sidewalk right now?",
        "time_stamp": "0:02:50",
        "answer": "C",
        "options": [
          "A. Playing musical instruments.",
          "B. Reading newspapers.",
          "C. Using their phones and conversing.",
          "D. Painting murals."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_333_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:21]: The scene shows a pedestrian area with a modern, curved architecture, featuring a combination of glass buildings and other structures. A few people are visible walking or standing, and there is a planter in the background. A large signboard with white text on a black background is situated prominently towards the right side of the frame. [0:05:22 - 0:05:26]: The camera moves closer to the signboard, revealing detailed information, including a map layout labeled “Salisbury Garden.” The map is white on a black background, and shows different pathways and buildings of the garden area. [0:05:27 - 0:05:28]: The perspective changes slightly, with the camera continuing to move closer to the signboard. More people are visibly walking by in the background, including a person with a blue shirt moving away from the camera. [0:05:29]: The camera starts to move away from the signboard, revealing additional area of the pedestrian walkway, which has a tiled surface with intricate patterns. Two individuals in the foreground, one in a green shirt, are walking away. [0:05:30 - 0:05:32]: The camera moves along the path, showing a curved edge with plantings on the right side. More pedestrians are visible along the pathway, with some appearing closer, and others in the distance. [0:05:33 - 0:05:35]: The scene continues to widen, displaying more of the surrounding buildings and the walkway. The left part of the frame shows a glass entry with large, bright signage and a cartoonish figure in the display. [0:05:36 - 0:05:39]: The camera frame shows the glass building more prominently, revealing it as a possible entrance area, decorated with lighting and additional details. Several individuals, including a child and an adult interacting, are seen moving towards the building. Additionally, there are caution cones lined up along the path next to the building.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color scheme of the map labeled \"Salisbury Garden\" shown on the signboard?",
        "time_stamp": "00:05:26",
        "answer": "D",
        "options": [
          "A. Black on white.",
          "B. Blue on white.",
          "C. Green on black.",
          "D. White on black."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_333_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:05]: A modern building labeled \"REGENT\" is visible with a rooftop garden. Surrounding the building are landscaped plants illuminated by yellow lights. The building is next to a body of water with a dock extending out. In the background, a city skyline with numerous illuminated high-rise buildings is visible across the water. The sky is partly cloudy; [0:08:06 - 0:08:19]: The scene transitions to a broader view, showing a pedestrian walkway along the water. The walkway is populated with several people sitting on benches and standing around. The area is well-lit, and a distinctive architectural structure resembling a large, curved canopy is prominent in the middle ground. The skyline continues to dominate the background, featuring a mix of tall buildings and distinctive towers. The serene, twilight atmosphere persists as city lights illuminate the scenery.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What was labeled on the modern building visible in the scene just now?",
        "time_stamp": "0:08:05",
        "answer": "B",
        "options": [
          "A. RESORT.",
          "B. REGENT.",
          "C. REFLECT.",
          "D. RETREAT."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_333_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a completely black screen. [0:00:01 - 0:00:03]: The word \"Walker\" in light gray appears on the black screen. [0:00:03 - 0:00:05]: The text \"Art Gallery\" is added below \"Walker.\" In the left corner, there's a red vertical banner split into sections mentioning \"National Museums Liverpool.\" [0:00:05]: The screen turns black again. [0:00:06]: Transitioning to a view of a white cloth spread out on a table. [0:00:07 - 0:00:10]: A hand appears, holding a butter dish and moving it over a ceramic bowl that features blue drawings of combs, scissors, and other objects. [0:00:11 - 0:00:13]: Now in a larger room with peach walls and a patterned carpet, one person sits comfortably in a wooden chair covered with a white sheet, while another person stands next to them, adjusting the sheet around the seated person's neck. A table with a white cloth and various items, including a wig stand, is visible on the right. [0:00:14 - 0:00:18]: The person standing continues to adjust the seated person's sheet, sometimes also holding other small objects as they work. The seated person remains relaxed, occasionally raising their hand and looking towards the standing person. [0:00:19 - 0:00:20]: The video continues to show the standing person holding something small near the seated person's hand, both individuals maintaining their previous positions.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What text appears first on the black screen?",
        "time_stamp": "0:00:03",
        "answer": "C",
        "options": [
          "A. \"National Museums Liverpool\".",
          "B. \"Art Gallery\".",
          "C. \"Walker\".",
          "D. \"Walker Art Gallery\"."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What are the walls' color in the larger room?",
        "time_stamp": "0:00:13",
        "answer": "B",
        "options": [
          "A. White.",
          "B. Peach.",
          "C. Blue.",
          "D. Gray."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_156_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:20 - 0:01:40] [0:01:20 - 0:01:30]: Two individuals dressed in white clothing with lace details are interacting with a wooden box. The hands of one person, wearing white sleeves, are seen retrieving a gold-colored object from inside the box, while the other person's hands stabilize the box. The background is an interior space with a blurred orange wall and window. [0:01:31 - 0:01:33]: A close-up of a person’s face, dressed in white with light skin. They appear to be looking slightly to the side, with a calm expression. Behind them, a brightly lit window with a beige curtain is visible. [0:01:34 - 0:01:35]: The focus shifts back to the hands. One person's hands are fastening a gold-colored button onto the other’s white sleeve. The gold button has a decorative design. [0:01:36 - 0:01:37]: A close-up of an elderly person’s face, wearing a black vest with white inner clothing. The person is looking down with a focused expression, adjusting the sleeve cuff of another person. [0:01:38]: The camera captures a close-up of the white sleeve with the newly fastened gold button. The button is decorated with an intricate design, and the sleeve has lace trim. [0:01:39 - 0:01:40]: The scene shows the elderly person holding and consulting a yellowish garment. The initial person sits nearby, dressed in white, and looks towards the elderly person. They are in a well-lit room with a large window, peach-colored walls, and ornate decorations.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is notable about the gold button?",
        "time_stamp": "0:01:38",
        "answer": "C",
        "options": [
          "A. It has a plain surface.",
          "B. It is silver with a smooth finish.",
          "C. It has a decorative design.",
          "D. It is wooden with carvings."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_156_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40]: A close-up shows a man in a white, ruffled shirt standing against a brightly lit background. He appears to be looking intently to his left. [0:02:41 - 0:02:43]: The scene shifts to hands fastening buttons on a pair of beige trousers worn by someone seated on a red chair with wooden legs, set against a patterned carpet. [0:02:44 - 0:02:51]: Two men are visible in a richly decorated room with orange walls and curtains. One man is wearing an off-white, ruffled shirt and beige trousers, while the other, an older man in a black vest and white shirt, is helping dress him. They’re positioned near a window, with the older man meticulously assisting with the younger man's clothing, particularly a beige waistcoat. [0:02:52 - 0:02:54]: The younger man adjusts his waistcoat while the older man continues to assist. The room's furnishings and details, including a side table with a white tablecloth and various items on it, are visible in the background. [0:02:55 - 0:03:00]: The older man fastens buttons on the younger man's waistcoat while the younger man stands with his arms slightly raised, appearing poised. The interaction appears deliberate and focused, with continued attention to the details of the young man's attire.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What item of clothing is the older man helping the younger man with?",
        "time_stamp": "00:03:00",
        "answer": "C",
        "options": [
          "A. Shirt.",
          "B. Trousers.",
          "C. Waistcoat.",
          "D. Jacket."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_156_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:07]: A person is seen wearing a white mask and a grey wig, with details of white and grey curls. They are seated, and another person dressed in a white garment is positioned behind them, lifting an object resembling a tool or brush. The room has a refined ambiance, featuring wooden furniture and cream-colored drapes that softly billow with the natural light shining through the window in the background. [0:04:08 - 0:04:18]: A closer view depicts the individual wearing the grey wig from a different angle, exposing more intricate details of the wig's curled structure. The hands of the second person, still dressed in white sleeves, are tying a black ribbon or cloth around the back of the wig, attending to its proper fitting. The meticulous tying of the ribbon continues through this segment, showcasing the careful application. [0:04:19]: The scene expands to a broader perspective, showing a sumptuously decorated room with painted walls and paintings. The broader context unveils a well-decorated table filled with various items including a ceramic pitcher, a fruit bowl, and fabric napkins, hinting at a setting meant for an elegant occupation or period scene. The seated individual with the grey wig and another standing behind them remain as primary focal points.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is one of the items on the well-decorated table?",
        "time_stamp": "00:04:21",
        "answer": "B",
        "options": [
          "A. A book.",
          "B. A ceramic pitcher.",
          "C. A silver tray.",
          "D. A candle."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_156_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:04]: The video starts with a person seated at a white table, in front of a blue background. Shelves with various objects and decorations are positioned on either side of the person. The person is holding a black graphics card, showcasing it to the camera. The person is wearing glasses and a white long-sleeve shirt featuring an illustration.  [0:00:05 - 0:00:08]: The focus shifts to a digital render of two graphics cards labeled \"RTX 4070 Super\" and \"RTX 4080 Super\". They are placed diagonally on a black background with green lines in the foreground giving a dynamic visual effect. [0:00:09]: A close-up of the \"RTX 4070 Super\" showcases its key specifications. Text indicates it has 36 shader TFLOPS, 82 RT TFLOPS, 568 AI TOPS and includes features such as being 20% more cores, faster than RTX 3090, and highly power-efficient. [0:00:10]: Another segment flashes details for the \"RTX 4070 Ti Super\" with specs highlighting 44 shader TFLOPS, 102 RT TFLOPS, and 706 AI TOPS. Additional text indicates memory and display specifications: 16GB G6X 256-bit, supporting 1440p 144Hz, and 2.5X faster than RTX 3070 Ti. [0:00:11]: Attention turns to the \"RTX 4080 Super,\" listed as having 52 shader TFLOPS, 121 RT TFLOPS, and 836 AI TOPS. It emphasizes being the world's fastest G6X card at 23 Gbps, offering 4K full ray tracing and super-fast Gen AI capabilities. [0:00:14 - 0:00:19]: The concluding segment shows a detailed price list and CUDA core count comparison of various RTX models, including RTX 4090, RTX 4090D, RTX 4080 Super, RTX 4070 Ti Super, RTX 4070, and RTX 4060 Ti, with their respective prices outlined in a black and green theme alongside a green-lit, transparent PC case displaying the GPU.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What graphics card is mentioned as having 44 shader TFLOPS?",
        "time_stamp": "00:00:20",
        "answer": "D",
        "options": [
          "A. RTX 4070 Super.",
          "B. RTX 4080 Super.",
          "C. RTX 4090.",
          "D. RTX 4070 Ti Super."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_105_real.mp4"
  },
  {
    "time": "[0:03:20 - 0:03:40]",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:26]: A person wearing glasses and a white shirt is seated at a desk with a small black computer component, likely a graphics card, in front of them. The background wall is blue, with shelves on both sides displaying various items. The person is explaining something about the graphics card, gesturing with their hands. [0:03:27 - 0:03:34]: Close-up shots of three different computer graphics cards arranged on a white surface. The cards are labeled \"RTX 4060 Ti\" and \"RTX 4070\" and appear to be from the same series, featuring similar designs with large cooling fans and metallic finishes. [0:03:35 - 0:03:41]: The person continues to explain, now gesturing more towards the graphics card on the desk. The blue background and shelf setup remain visible, indicating the ongoing discussion about the technical aspects or features of the graphics cards. [0:03:42 - 0:03:44]: The scene shifts to a first-person perspective view of a video game. The player character holds a colorful knife. The game's heads-up display (HUD) shows various stats, including CPU and GPU usage, VRAM usage, and FPS, in the top left corner. The setting appears to be a narrow, outdoor urban area. [0:03:45 - 0:03:48]: The player moves through the game's urban environment, transitioning from using a knife to a rifle. Surrounding buildings feature rustic and worn designs with vibrant colors. The player advances toward an open area while maintaining a view of the game's HUD. [0:03:49 - 0:03:53]: The player continues exploring the urban environment, revealing a more detailed view of the surroundings, including architectural features like arches and stonework. Portions of the environment, such as a small fountain or well, and various decorative elements are visible. [0:03:54 - 0:03:56]: As the player moves further, there is a brief view of another in-game character and additional buildings with faded paint and weathered textures. The player's interactions appear focused on navigating through the space and possibly engaging in gameplay objectives. [0:03:57 - 0:03:59]: The player rounds a corner, encountering potential in-game obstacles or enemies. The scene showcases the detailed textures and varied elements of the game's environment, contributing to the immersive atmosphere of the urban setting.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which series of graphics cards is being displayed right now?",
        "time_stamp": "00:03:34",
        "answer": "B",
        "options": [
          "A. GTX 1080 Ti.",
          "B. RTX 4070 Super.",
          "C. RX 5700 XT.",
          "D. RTX 3060."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_105_real.mp4"
  },
  {
    "time": "[0:06:40 - 0:07:00]",
    "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:49]: The video shows detailed benchmark performance results for three different graphics cards: RTX 4070 Ti, RTX 4070 SUPER, and RTX 4070, with two resolutions (2560x1440 and 3840x2160). The left side of the screen features various benchmarks for these cards, including average frame rates across different settings like “默认” (Default), “光线追踪” (Ray Tracing), and “光线追踪+DLSS平衡” (Ray Tracing + DLSS Balanced). The bar graphs indicate that the RTX 4070 Ti outperforms the other cards in all categories, followed by the RTX 4070 SUPER and then the RTX 4070. On the right side of the screen, detailed performance metrics like GPU clock, usage, power, and temperature for different graphics cards are displayed in smaller boxes. [0:06:50 - 0:07:00]: The perspective switches to a first-person viewpoint, showing a character with \"FBI\" written on the back of their jacket running through a deserted street. The street is wet, with various buildings, signs, and flags visible. A banner overhead reads \"DEERFEST WELCOME,\" signaling some form of local festival. Metrics on the left side of the screen display the performance of a 4070S graphics card, showing GPU clock speed, usage percentage, VRAM usage, power consumption, and GPU temperature. The character runs towards the background, navigating through what seems to be a foggy morning with patches of sunlight breaking through the clouds. The camera follows the character's movement steadily, maintaining a constant frame rate of 41 to 44 FPS throughout this segment.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which graphics card achieves the highest performance in all categories as shown?",
        "time_stamp": "00:07:00",
        "answer": "C",
        "options": [
          "A. RTX 4070.",
          "B. RTX 4070 SUPER.",
          "C. RTX 4070 Ti.",
          "D. RTX 4060 Ti."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_105_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:10:20]",
    "captions": "[0:10:00 - 0:10:20] [0:10:00 - 0:10:07]: The video begins with a first-person perspective in what appears to be a user interface of a software application. A variety of control sliders and dropdown menus for adjustments are positioned on the left side. In the right section of the interface, a character in a brown bear hoodie with large blue eyes and a scarf in a snowy outdoor setting is visible. [0:10:08 - 0:10:09]: The scene continues with minor changes, featuring the character in the bear hoodie. This character performs a thumbs-up gesture while looking directly at the viewer. [0:10:10]: The character now wears a slightly different hat, also shaped like a bear, and the background remains a snowy landscape. The focus slightly shifts, and the character appears more distant and slightly blurred. [0:10:11]: The focus sharpens again, showing the character close-up with clear facial features and a clear winter landscape setting. [0:10:12]: The character in the bear hoodie reappears, continuing the thumbs-up gesture. The background comprises snow-covered trees with a clear blue sky, and the image is sharp. [0:10:13]: The character in the bear hoodie becomes blurred and closer to the viewer, maintaining the same winter background. [0:10:14]: The character is clear once again, holding up a peace sign, and the snowy landscape in the background is visible under a clear blue sky. [0:10:15]: A compilation of four thumbnail images of the character in varied winter outfits appears on the right side of the screen, with the full-body character images and the blue sky in the background. [0:10:16]: The screen shifts focus to a singular clear image of the character in the bear hoodie, pointing upwards with her index finger, smiling, and the blue sky is vivid in the background. [0:10:17]: The view remains the same as the previous frame, clearly displaying the character pointing upwards. [0:10:18]: The same character image continues on screen, with no apparent change. [0:10:19]: The frame shows the same character in the upward-pointing pose with no visible changes. [0:10:20]: The video transitions to an interface view showcasing benchmarking data for \"Stable Diffusion AI,\" comparing different NVIDIA GPUs' performance. Several bar graphs are presented, with different GPU models and their performance in specific AI tasks displayed. [0:10:10 - 0:10:13]: The benchmarking data focuses on GPU comparison, detailing models like RTX 4070, RTX 4070 SUPER, and RTX 4070 Ti. Key performance metrics such as FPS or execution time for specific tasks are highlighted for different test parameters mentioned as \"Euler aSampling\" and \"TensorRT\". The bar graphs reveal detailed performance comparisons between these GPU models.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What type of data is shown regarding \"Stable Diffusion AI\" right now?",
        "time_stamp": "00:10:20",
        "answer": "B",
        "options": [
          "A. User reviews.",
          "B. Benchmarking data.",
          "C. Software tutorials.",
          "D. Game results."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_105_real.mp4"
  },
  {
    "time": "[0:12:20 - 0:12:38]",
    "captions": "[0:12:20 - 0:12:38] [0:12:20 - 0:12:22]: Three high-end graphics cards are placed on a white table. The two on the right are the same model, marked as \"RTX 4070,\" predominantly black with metallic accents and large cooling fans. The one on the left has a silver and black color combination with the label \"LIV,\" and it features a similar design to the other two. [0:12:23 - 0:12:24]: A person is seated at a white table with shelves in the background holding a graphics card vertically. The person, dressed in a light-colored sweatshirt, appears to be explaining something. [0:12:25 - 0:12:28]: The person continues to talk, using hand gestures to emphasize their points. The background remains consistent with books and electronic items on the shelves. [0:12:29]: A large gear icon and an animation of settings appear, obscuring part of the person's face and the object held. [0:12:30]: The scene transitions to three icons above the table in front of the person: a thumbs-up, a restricted access symbol, and a star, suggesting interactive options or menu items. [0:12:31 - 0:12:34]: The person continues explaining, with fewer icons remaining on the screen. The gear system graphics are animated, rotating around the person, who is gesturing towards the objects. [0:12:35 - 0:12:36]: An app interface appears on the left side of the screen, showing products and ratings. The person gestures towards the device, suggesting an action or feature within the app. [0:12:37 - 0:12:38]: The final frames shift to a black screen with a futuristic logo containing a gear icon and stylized text.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What models are the two graphics cards labeled right now?",
        "time_stamp": "00:12:15",
        "answer": "C",
        "options": [
          "A. GTX 3090.",
          "B. RX 6900.",
          "C. RTX 4070 and RTX 4070 SUPER.",
          "D. LIV."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_105_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video begins with a black screen. [0:00:01 - 0:00:05]: The scene transitions to a luxurious bedroom with deep red walls and furniture styled in an antique, ornate manner. The bed in the center of the frame is covered in red velvet, and a person with light hair is sleeping on the right side. The bed has a high headboard with intricate designs, and the person is partially covered by a red blanket. Next to the bed, there is a wooden bedside table with a lamp, a book, and other small items. Large, framed portraits of historical figures decorate the walls. [0:00:06 - 0:00:10]: A closer view shows the person sleeping more clearly. They have closed eyes and are resting their head on a pillow. The sheets and their clothing are white and appear finely detailed. The background remains the same with the red, velvet headboard closely visible. [0:00:11]: There is a slight overlay effect as two frames blend, showing the person's head rising from the pillow. The background of the ornate red bed and wall decor remains consistent. [0:00:12 - 0:00:14]: The person, now awake, starts to sit up in bed. Their actions involve using their hands to push off the bed and the red blanket partially removed. [0:00:15 - 0:00:18]: The person fully sits up on the edge of the bed, adjusting their sleepwear. The details of the bed and the room's background are more profoundly visible. [0:00:19 - 0:00:20]: The person gets out of bed and walks towards a red chair with a golden cloth draped over it. The ambient lighting reveals more details of the comprehensive settings of the room.\n[0:00:40 - 0:01:00] [0:00:40 - 0:00:41]: A person is pulling up a white, knee-high sock embroidered with gold detailing against a marble tile floor background. One hand is holding the top of the sock while the other holds the foot section.  [0:00:42 - 0:00:45]: The camera captures a wider view showing the person, an adult male with light skin, sitting on an ornate green chair, pulling up the sock while partially dressed in a white shirt. The room has marble tile flooring, a marble bathtub, a wooden dresser with various items and framed artwork on the walls. [0:00:46 - 0:00:48]: Close-up of the person adjusting a green garter-like band near the top of the white sock to hold it up securely while kneeling on the floor. [0:00:49 - 0:00:52]: The person starts to don an orange pair of breeches, standing up from the green chair, while the room’s luxurious details such as the marble tub and golden fixtures are visible in the background. [0:00:53 - 0:00:56]: The person is shown pulling up the orange breeches, adjusting them around the waist and ensuring they fit comfortably. The gold details on the breeches catch the light. [0:00:57 - 0:00:59]: Close-up of upper waist section, where the person is fastening the breeches carefully, ensuring everything is properly in place. The white shirt with ruffled sleeves is also prominently visible.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color are the walls and the bed in the bedroom?",
        "time_stamp": "0:00:05",
        "answer": "D",
        "options": [
          "A. Blue.",
          "B. Green.",
          "C. Yellow.",
          "D. Red."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Action Recognition",
        "question": "What does the person do after waking up?",
        "time_stamp": "0:00:19",
        "answer": "C",
        "options": [
          "A. Lies back down.",
          "B. Walks out of the room.",
          "C. Sits up on the edge of the bed.",
          "D. Stands up and stretches."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_154_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:20 - 0:01:40] [0:01:20 - 0:01:23]: A man dressed in historical attire stands in front of a mirror adjusting his clothing. His vest has intricate patterns, and he is in a room decorated with framed pictures and ornate objects. The background features a fireplace with decorative items on the mantel, including a green vase and small figurines. [0:01:24 - 0:01:25]: A close-up view shows him fastening the buttons on his ornate vest. His sleeves are white with lace at the cuffs. The background includes a green upholstered chair and a small table with inlay work. [0:01:26 - 0:01:27]: The man, now sitting, picks up a large white wig from a table. He appears focused as he carefully handles the wig. [0:01:28 - 0:01:29]: He places the wig onto his head, adjusting it with both hands. The mirror in front of him shows his reflection as he ensures the wig is properly positioned. [0:01:30 - 0:01:31]: The man continues to adjust his clothing and wig while looking at himself in the mirror. The tabletop features various ornate items, including a chalice and metal containers. [0:01:32 - 0:01:37]: The man completes the final adjustments of his attire, ensuring every detail is perfect. He looks at his reflection, seemingly satisfied with his appearance. [0:01:38 - 0:01:39]: The scene transitions to a larger room. The man, now in a different location with luxurious red and gold furnishings, places an item of clothing on a red sofa. The room has large windows with natural light and is decorated with elaborate paintings and ornate furniture.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What type are the man's sleeves?",
        "time_stamp": "0:01:25",
        "answer": "D",
        "options": [
          "A. Blue with embroidery.",
          "B. Green with stripes.",
          "C. Black with buttons.",
          "D. White with lace."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Object Recognition",
        "question": "What item does the man pick up from the table while sitting?",
        "time_stamp": "0:01:29",
        "answer": "D",
        "options": [
          "A. A chalice.",
          "B. A green vase.",
          "C. A metal container.",
          "D. A large white wig."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Clips Summarize",
        "question": "How did the scene change just now?",
        "time_stamp": "0:01:39",
        "answer": "A",
        "options": [
          "A. The man picked up a piece of clothing on a red sofa in a different room, preparing to put it on.",
          "B. The man is seen standing in front of a mirror.",
          "C. The man is shown sitting and adjusting his wig.",
          "D. The man walks out of the room."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_154_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins in an outdoor area during daytime. The scene shows an open circular plaza with several palm trees around it and several blue-topped booth structures. On the left side, there is a large beige building. To the right, there are some more booths with blue awnings and greenery in the background. The sky is partially cloudy. [0:00:03 - 0:00:08]: As the video progresses, a person in a light blue uniform walks along the path on the right side of the screen. The camera moves slightly to the right, maintaining the view of the plaza, palm trees, and blue-topped booths, with more details of the background visible. The red rail seen in the right edge of the frame suggests the movement is probably taking place on a vehicle or tram. [0:00:09 - 0:00:13]: The camera continues to move to the right, revealing a mural depicting a famous \"Hollywood\" sign and the surrounding landscape. The mural stands to the right of the palm trees and blue-topped booths. The image includes a green highway sign in the foreground with directions to local attractions. [0:00:14 - 0:00:20]: The camera transitions to a higher vantage point, overlooking a road that descends gently. Tall evergreen trees line the road on the left, and a building partially obstructed by the trees is visible. The landscape view reveals buildings and hills in the background. The road curves downwards to the right, and a multi-car tram is driving down this road.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color are the tops of the booths around the plaza?",
        "time_stamp": "0:00:02",
        "answer": "B",
        "options": [
          "A. Red.",
          "B. Blue.",
          "C. Green.",
          "D. Yellow."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Object Recognition",
        "question": "What does the mural depict?",
        "time_stamp": "0:00:13",
        "answer": "D",
        "options": [
          "A. A large beige building.",
          "B. Palm trees and blue-topped booths.",
          "C. A multi-car tram.",
          "D. A famous Hollywood sign and surrounding landscape."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_317_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:26]: In a brightly lit outdoor area, several structures are visible with vibrant colors. On the left, there is a small blue building with a yellow and red trim surrounded by people, some standing in queues. A sign featuring a cartoon figure and a \"Simpsons\" theme prominently stands in front. To the right of this building is a purple amusement area with red decorations where people are moving around. A distant sign for \"Lower Lot\" can be seen towards the left background. The sky is partially cloudy, providing good lighting. [0:02:22 - 0:02:23]: People are gathered around the blue and yellow structure, probably waiting in line. The focus seems to be on the shop displayed in the previous frames, showing individuals in casual attire. On the right background, a large purple and red structure appears to represent a carnival or amusement park setting, with people actively wandering. [0:02:23 - 0:02:24]: The camera angle shifts, showing additional details of a spacious plaza with people walking or being engaged with their surroundings. The \"Lower Lot\" sign continues to dominate the top portion of the scene, indicating the area’s name. Several people are visible, including one with brown hair and backpack, who appears to be walking toward the camera. [0:02:24 - 0:02:27]: The focus moves further towards the central thoroughfare under the \"Lower Lot\" sign framed by two beige-colored pillars adorned with green and blue signage. More individuals are casually moving around. The sky remains partly cloudy, illuminating the scene softly. [0:02:27 - 0:02:31]: As the camera progresses forward, additional details come into view. The lower lot area has a large, arched, glass-structured building visible in the distance under the large \"Lower Lot\" sign. People are now moving seamlessly under this structure as they walk towards their destinations. The scene captures people from various angles, some walking toward the camera while others move in the opposite direction. [0:02:31 - 0:02:32]: The shot transitions towards a closer view of the \"Lower Lot\" archway with its unique structural details. The two large digital screens mounted on both sides of the arch are more legible now, though their content is not fully discernible. The distant glass archway continues to serve as a visual endpoint in the background. [0:02:32 - 0:02:36]: Continuing forward, more detailed elements of the plaza and shoppers are revealed. Outdoor tables with umbrellas and foliage appear on the edges, suggesting places to rest or dine. The expansive outdoor setting enhances the feeling of open space with several decorative elements including lampposts and planters. [0:02:36 - 0:02:39]: The camera finally approaches the central glass archway, and more intricate details of its design emerge. The pathway is populated with visitors, including two people dressed in burgundy who are walking together. The area is well-maintained and landscaped, featuring various plants arranged in planters along both sides of the walking path. The sky maintains its partly cloudy and bright appearance, casting soft shadows on the ground.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is indicated by the \"Lower Lot\" sign seen in the background?",
        "time_stamp": "0:02:27",
        "answer": "B",
        "options": [
          "A. The name of a ride.",
          "B. The name of the area.",
          "C. The name of a restaurant.",
          "D. The name of a shop."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_317_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:41]: The video begins with a view from a high vantage point, showing two people near a railing. One person is standing near a stroller on the left side, while another person, wearing a black shirt and blue pants, is taking a photo on the right side. The sky is filled with light clouds, and there is a vast landscape in the background featuring buildings, greenery, and mountains. [0:04:42 - 0:04:48]: The camera moves slightly to the right, capturing more of the landscape. There are several large white-roofed buildings below, and the foreground shows a potted plant on the right side of the screen. The landscape in the background includes more greenery, structures, and distant mountains under a partly cloudy sky. [0:04:49 - 0:04:50]: The camera continues to pan to the right, revealing additional buildings in the industrial area below and more landscape in the distance. The potted plant now appears on the lower right corner, while the overall scenery remains consistent with clear views of buildings, greenery, and mountains. [0:04:51 - 0:04:54]: Further panning to the right exposes more of an industrial estate below, with large flat-roofed buildings. The background maintains the same geographical features, showing expansive greenery and mountain ranges beneath a partly cloudy sky. [0:04:55 - 0:04:58]: As the camera moves further, a colorful and green attraction area becomes more apparent in the left central part of the frame. The video shows additional green spaces and more of the larger white-roofed buildings, all contrasting against the distant mountains. [0:04:59 - 0:05:00]: The video ends with a wide view of the entire area, including industrial buildings in the foreground, a colorful attractions area on the left, and extensive green and mountainous scenery in the background under a mostly cloudy sky.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the relative position of the potted plant in the video right now?",
        "time_stamp": "0:04:43",
        "answer": "C",
        "options": [
          "A. In the center of the screen.",
          "B. In the upper left corner.",
          "C. In the lower right side.",
          "D. In the bottom center."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_317_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:11]: The video captures a first-person perspective view descending an escalator. The scene is framed by the structure of the escalator, which is supported by white beams creating a geometric pattern. The escalator is located outdoors with a roof made of glass panels. On the right side, a large building with a beige exterior and a flat roof is visible. The building is adorned with colorful billboards featuring vibrant designs. There is lush greenery, including trees, beside the building, adding a natural element to the scene. In the distance, a suburban landscape, together with mountains, stretches across the horizon under a cloudy sky. On the escalator, there are a few people descending, dressed in dark-colored clothing. The camera's perspective slightly shifts from side to side, maintaining focus on the building and the surroundings. [0:07:12 - 0:07:20]: The video continues to depict the descent down the escalator. The view becomes progressively lower, providing a closer look at the building with its colorful billboards. The lush greenery remains on the right, providing contrast to the man-made structures. The distant mountains under the cloudy sky continue to be visible in the background. The few individuals on the escalator are now seen from a slightly lower angle, emphasizing the movement downward. The scene maintains its vibrant and clear visual quality throughout this segment.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is visible on the right side of the escalator?",
        "time_stamp": "00:07:11",
        "answer": "D",
        "options": [
          "A. A fountain.",
          "B. A park.",
          "C. A parking lot.",
          "D. A large building with colorful billboards."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_317_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:06]: The video begins with a close-up view of a painting on an easel. The painting, which features a portrait of a woman with dark, wavy hair and a light complexion, is prominently displayed in the center of the frame. The woman in the portrait has her eyes closed and is wearing a purple corset-like top. The background of the painting is a mix of red and purple hues, creating a warm and vibrant atmosphere. To the left of the painting, a green potted plant with broad leaves is visible, providing a touch of natural greenery to the scene. In the background, there is a wooden circular frame leaning against the wall, partially obscured by the plant. [0:00:07 - 0:00:10]: The camera zooms in on the woman's face in the painting, providing a closer view of the intricate details. Her closed eyes, slightly parted lips, and relaxed expression capture a sense of tranquility. The brush strokes and color blending in her hair and skin are clearly visible, showcasing the artist's technique. The background colors appear more vibrant and intense in this close-up view. [0:00:11 - 0:00:18]: The camera slowly zooms out from the close-up of the woman's face, reverting to a broader view of the painting. The overall composition and color scheme of the painting remain consistent, with the warm reds and purples creating a striking contrast with the woman's light skin tone and dark hair. The green plant and other background elements, such as the wooden frame and some indistinct objects on a shelf, remain visible and add context to the setting. [0:00:19]: The frame shifts to a different perspective, showing a person—presumably the artist—holding a blank canvas on an easel. The artist is seen from behind, with both arms raised to adjust the canvas. The surroundings include the same green plant, with some additional art supplies and objects visible on the right-hand side of the frame.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the green potted plant positioned relative to the painting in the initial scene?",
        "time_stamp": "0:00:06",
        "answer": "C",
        "options": [
          "A. To the right of the painting.",
          "B. In front of the painting.",
          "C. To the left of the painting.",
          "D. Above the painting."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the scene in the video?",
        "time_stamp": "0:00:19",
        "answer": "C",
        "options": [
          "A. A detailed exploration of a landscape painting.",
          "B. The process of painting a landscape with vibrant colors.",
          "C. A focus on a woman's portrait with a tranquil expression.",
          "D. An artist discussing the inspiration behind an abstract piece."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_128_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:53]: In these frames, a close-up view of a painting in progress is shown. The painting features a woman's partially completed face, focusing on her mouth and chin. The artist's brush, with a fine tip and black handle, is applying paint to the lips and chin area of the portrait. The brush movements are delicate and precise, adding shades and details to the lips. The face in the painting has detailed features with various shades of skin tones, particularly around the mouth area where the artist is working. The background is plain and white, highlighting the artwork. [0:02:54 - 0:02:56]: The artist continues to detail the woman's face, specifically her lips and the area below them. The painting shows progress with additional shading applied. [0:02:57 - 0:02:58]: The camera angle zooms out, revealing more of the canvas and a wider view of the woman's portrait. The artist, holding a brush, continues to add color to the area of the neck and shoulder. The painting uses various tones to create depth in the woman's flowing hair, primarily colored in purple hues. A green plant is visible in the left lower corner of the frame, and the background appears to be a neutral-colored wall. [0:02:59 - 0:03:00]: The artist proceeds to add strokes to the shoulder area of the portrait, using a broader brush. A small decorative statue and a part of the wall-mounted object are visible in the bottom right corner, enhancing the studio's artistic ambiance.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is visible in the left lower corner of the frame after the camera zooms out?",
        "time_stamp": "00:02:58",
        "answer": "B",
        "options": [
          "A. A decorative statue.",
          "B. A green plant.",
          "C. A part of a bookshelf.",
          "D. A palette of colors."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_128_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:21]: A woman with curly, purple hair is standing in front of a white wall, which is decorated with several small artworks related to color and illustration. She is wearing a blue top and appears to be speaking or presenting something. [0:05:22 - 0:05:23]: The image shifts to a Skillshare interface. The viewer is welcomed by the interface, displaying a selection of available workshops. The top banner reads \"Welcome back, Bill, ready for your next lesson?\" [0:05:24 - 0:05:25]: The Skillshare interface remains the same, with the text \"Logo Design\" being typed in the search bar on the screen. [0:05:26 - 0:05:31]: A close-up painting of a woman's face is shown. The painting depicts her with closed eyes and partially painted skin tones. A brush is visible, applying paint to her lower face and shoulder area. [0:05:32]: The painting continues to show intricate details, particularly focusing on the textures and shading of her shoulder. [0:05:33 - 0:05:36]: A screen capture of another Skillshare lesson is visible. The screen shows a lesson on \"Mastering Illustration: Sketching, Inking & Color Essentials.\" A hand is drawing two cartoon characters with a pen on white paper. [0:05:37]: The video continues showing the same illustration lesson, with the artist adding details and outlines to the drawing. [0:05:38]: The artist continues to refine the drawing, shifting focus to another character on the page. [0:05:39]: The artwork is zoomed out showing more of the illustrated scene, with additional characters being outlined. [0:05:40]: The artist proceeds to add darker outlines to a detailed illustration of a character, enhancing the depth and form of the initial sketch.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the painting shown in the video?",
        "time_stamp": "00:05:26",
        "answer": "C",
        "options": [
          "A. A woman with open eyes and vibrant colors.",
          "B. A landscape with intricate textures.",
          "C. A woman's face with closed eyes and partially painted skin tones.",
          "D. An abstract painting with geometric shapes."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_128_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: Two paintings are displayed side by side, depicting similar portraits of a woman with dark hair and closed eyes. The left painting has a lighter background with pink and purple tones, while the right painting has a darker, more intense background with shades of red, purple, and hints of yellow. Both portraits show the woman's head and shoulders from a similar angle. [0:08:02 - 0:08:09]: The focus shifts to a close-up of the right painting, highlighting the details of the woman's face. Her hair is vividly colored with shades of dark purple and red, and her skin tones are meticulously blended, showcasing the artist's technique. The close-up frames her eyes, nose, and mouth, revealing textural details such as brush strokes and layering of paint. [0:08:10 - 0:08:19]: The view returns to the two paintings from the original perspective. The left painting continues to display the soft, pastel colors in the background, while the right painting maintains its rich, deep hues. They are positioned in a well-lit space with elements of interior decor visible, such as a potted plant to the left and fairy lights in the background. A bird sculpture is present near the base of the paintings.",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the content that was just shown in the video?",
        "time_stamp": "00:08:20",
        "answer": "B",
        "options": [
          "A. Two paintings of a landscape are displayed, followed by a close-up of a bird sculpture.",
          "B. Two portraits of a woman are shown, with a close-up revealing details of one painting, and then the original view is restored.",
          "C. A painting of a young man is displayed, followed by a description of the artist's technique.",
          "D. A video of an artist painting is shown, with a focus on the brush strokes and colors used."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "painting",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_128_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video starts with a scene depicting a model race track set in a lush green landscape on a sunny day. The backdrop consists of blue skies with a few clouds, while a race track winds through hills surrounded by miniature trees. Billboards are visible, one on the left and another one in the middle, featuring different designs. There are various miniature cars on the track, seemingly engaged in a race. [0:00:02 - 0:00:07]: The screen shifts to a black background with colorful, dynamic text animations reading \"NEXT GEN\" and \"DIECAST RACING.\" These frames serve as an introduction to the video theme and contents. [0:00:08 - 0:00:10]: The scene transitions back to the race track, but now it is a pit area filled with several brightly colored diecast cars positioned on a black platform with sponsor signs in front of them. The background includes a detailed and organized pit stop area with various equipment, additional model cars and buildings. The pit area features colors like red, yellow, green, and blue, and there's a sense of anticipation for the race. [0:00:11 - 0:00:14]: Text overlays appear on the scene, displaying \"World Grand Prix\" followed by \"Final Round,\" providing context for the race event being depicted. The diecast cars remain lined up, ready to go. [0:00:15 - 0:00:20]: The camera lingers on the pit area, showcasing a close-up of the detailed setup. Cartoony advertisements and colorful set pieces are visible in the background, emphasizing the world of diecast racing. There is no movement as the scene remains static with the cars in their starting positions.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What event is being depicted according to the text overlays?",
        "time_stamp": "00:00:19",
        "answer": "C",
        "options": [
          "A. Local Race.",
          "B. Test Drive.",
          "C. World Grand Prix - Final Round.",
          "D. Exhibition Match."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_490_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:03]: The video shows a racing track from a first-person perspective. There are multiple small racing cars moving quickly on a black, elevated track which seems to be part of a miniature race setup. The surroundings include a grassy area to the left of the track, and behind the track are multiple model trees and a detailed backdrop resembling a hilly terrain. Several colorful buildings, which appear to be part of a pit stop area, are visible to the right of the track. A yellow replay indicator is present at the top right corner, suggesting that the content is a replay of part of the race. [0:03:04]: The video briefly displays the standings for \"Race 2\". The standings list six racers by name and number, along with their respective point totals. The first-place racer has 20 points, while the points for the other racers range from 10 to 3. The composition of this frame suggests a pause from the actual race to display this information to viewers.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Which racing car won first place in this competition?",
        "time_stamp": "0:03:01",
        "answer": "B",
        "options": [
          "A. The pink car.",
          "B. The yellow car.",
          "C. The light green car.",
          "D. The blue car."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      },
      {
        "task_type": "Counting",
        "question": "How many racers are listed in the standings for \"Race 2\"?",
        "time_stamp": "00:03:20",
        "answer": "C",
        "options": [
          "A. Four.",
          "B. Five.",
          "C. Six.",
          "D. Seven."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_490_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:20]: In the frame, a leaderboard lists the standings for \"Race 4.\" The data is presented in a table format with six entries, indicating the positions, car numbers, drivers' names, and scores. The background shows a race track in a hilly, scenic area with green grass, trees, and clouds, indicative of an outdoor setting. The leaderboard has a red title bar that says \"Standings Race 4.\" The first position is held by car #24 with the name \"Jeff Gorvette\" and a score of 24 points, marked with an asterisk and a numeral indicating the number of asterisks. The second position is car #1 \"Francesco Bernoulli\" also with 24 points. The third position is car #9 \"Nigel Gearsley\" with 17 points. In fourth is car #2 \"Lewis Hamilton\" with 17 points as well. Fifth place is occupied by car #95 \"Lightning McQueen\" with 13 points. The sixth position is car #8 \"Carla Veloso\" with 7 points. There are minor changes in the asterisks next to the top positions throughout the video frames.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Which racing car won first place in this competition?",
        "time_stamp": "0:06:59",
        "answer": "C",
        "options": [
          "A. The pink car.",
          "B. The yellow car.",
          "C. The red car.",
          "D. The blue car."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_490_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:00 - 0:09:20] [0:09:00 - 0:09:01]: In the frame, there are two die-cast cars racing on a black track. One car is black, and the other is red. They are moving towards the left side of the screen from the center. To the right of the track, there is a fenced area with a white building in the background, which has some racing logos on it. The grassy area beside the track has multiple advertising boards with various brand logos.  [0:09:02]: A transition screen appears with the text \"NEXT GEN DIECAST RACING\" in bold yellow and white letters against a black and green striped background. [0:09:03 - 0:09:04]: The frame shows the same black track with the fenced area and building in the background. There are no cars currently visible on the track in this frame. [0:09:05 - 0:09:06]: The scene remains the same with an empty racetrack. The white building and advertising boards are unchanged in the background. [0:09:07 - 0:09:08]: Another group of die-cast cars begins to appear on the track moving from right to left. The cars are varied in colors including red and green. Advertising boards and the white building are still prominent in the background. [0:09:09 - 0:09:10]: A larger group of die-cast cars in various colors (yellow, blue, and red) are racing along the black track. The cars are closely packed together as they move towards the left. [0:09:11 - 0:09:12]: The frame displays another set of die-cast cars racing. One is yellow and is leading the way on the black track towards the left. The white building and advertising boards remain consistent in the background. [0:09:13]: Two more die-cast cars, one red and another black, race together down the black track. The scene remains consistent with the same background elements. [0:09:14 - 0:09:15]: A transition screen appears again with the text \"NEXT GEN DIECAST RACING,\" in bold yellow and white letters across a black and green striped background. [0:09:16 - 0:09:20]: The frame shows a leaderboard with standings in a die-cast racing event. The list includes positions, car numbers, racers, and their respective points. \"Lightning McQueen (33 pts)\" is at the top, followed by \"Jeff Gorvette (30 pts),\" \"Francesco Bernoulli (26 pts),\" \"Lewis Hamilton (25 pts),\" \"Nigel Gearsley (23 pts),\" and \"Carla Veloso (17 pts).\" The leaderboard is against a scenic background featuring a race track and a hillside.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Which racing car won first place in this competition?",
        "time_stamp": "0:09:01",
        "answer": "C",
        "options": [
          "A. The pink car.",
          "B. The yellow car.",
          "C. The red car.",
          "D. The blue car."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_490_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins in a storage room filled with shelves and trolleys. Various products are neatly organized on the shelves. On the left side, there are shelves with various items, primarily white and green products. In the middle of the aisle, there are several trolleys filled with green cartons and some red crates. The floor appears to be concrete. [0:00:03 - 0:00:04]: The trolleys filled with green cartons are moved further into the storage room. More shelves and products are visible on both sides. [0:00:05 - 0:00:09]: The perspective shifts as the person pushes the trolley forward. More green cartons and additional products on the shelves come into view. The trolley is maneuvered through the aisle, indicating a task of organizing or resupplying the shelves. [0:00:10 - 0:00:12]: The scene switches to a close-up of shelves on the left side, displaying various dairy products like yogurts. The trolley with green cartons is still visible, and the person continues to move towards these shelves. [0:00:13 - 0:00:14]: The person gradually stops the trolley near the shelf, draws closer to the dairy products, and aligns the trolley, preparing to unload the items. [0:00:15 - 0:00:16]: The trolley stocked with green cartons is positioned right next to the shelf. The person's gloved hands are seen actively ready to unload the items. [0:00:17 - 0:00:20]: The person begins to unload the green cartons from the trolley and place them onto the shelves. Each carton is placed carefully in the allotted slots, ensuring the products are neatly organized and accessible for future use.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing with the green and white cartons?",
        "time_stamp": "0:00:20",
        "answer": "C",
        "options": [
          "A. Moving them to another room.",
          "B. Organizing them in the trolley.",
          "C. Unloading them onto the shelves.",
          "D. Throwing them away."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Event Understanding",
        "question": "What task is the person primarily engaged in right now?",
        "time_stamp": "0:00:54",
        "answer": "C",
        "options": [
          "A. Cleaning the storage room.",
          "B. Counting inventory.",
          "C. Organizing and resupplying shelves.",
          "D. Moving trolleys to the front."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_436_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:01]: The video begins in what appears to be a storage area or backroom of a store. Shelves filled with various items line both sides of a narrow aisle. On the left side, the shelves hold items in rectangular boxes and cylindrical containers, while on the right side, similar items are neatly arranged. The floor is grey, and there are a couple of folded cardboard pieces on it. A shopping cart is visible on the right side, partially captured on the edge of the frame. [0:02:01 - 0:02:03]: The perspective shifts slightly to the left, providing a clearer view of the shelves on both sides. The person holding the camera is pushing a cart with some items. The aisle remains neatly organized, with the shelves well-stocked. The items include various packaged goods and bottles.  [0:02:03 - 0:02:04]: The person continues to push the cart down the aisle. Various items, such as boxes and packaged goods, are stored on metal shelves. To the right, there are larger items and boxes on a trolley. Small items and containers are stacked on the shelves to the left. [0:02:04 - 0:02:05]: The perspective shifts to see the aisle’s end more clearly. Shelves continue to be stocked with a variety of products, both in packages and boxes. There's a sense of orderliness in the arrangement. The right side near the floor has a few scattered pieces of cardboard. [0:02:05 - 0:02:06]: The view down the aisle shows it narrowing slightly as the shelves to the left contain more stock. At the far end, there are carts partially visible.  [0:02:06 - 0:02:08]: The camera continues capturing the well-organized aisle with metal shelves on both sides filled with various items. There is ample lighting giving a clear view of the storage items. The person pushing the cart progresses down the aisle. [0:02:08 - 0:02:09]: As the person pushes the cart further, a detailed view of various items on the shelf can be seen, including dairy products and packaged goods. The scene remains consistent with a focused aisle. [0:02:09 - 0:02:10]: At the far end of the aisle, the view emphasizes stacked cartons and boxes, along with a red cart. The shelves on the left side continue with varied items. [0:02:10 - 0:02:12]: The person pushes the cart closer to the end of the aisle. The right side of the aisle now shows more goods, with red and green items stacked on the trolley. Left shelves hold bottled items. [0:02:12 - 0:02:14]: The person navigating the cart now reaches the end of the aisle. The perspective includes a detailed view of cartons and boxes on both shelving units. More items are meticulously stacked on the trolley, suggesting a consistent organizational system.  [0:02:14 - 0:02:17]: The camera captures a broad view down the entire aisle, showing shelves filled with a variety of items on both sides. The aisle is well-lit, revealing everything clearly. The cart seen earlier is now fully captured, providing a sense of the person’s movement and progress. [0:02:17 - 0:02:20]: The person continues pushing the cart down the aisle, maintaining a clear view of shelving units and organized goods. The overall environment remains consistent with a storage room or backroom, showcasing an orderly setup with well-arranged products. The lighting and layout provide a clear picture of the aisle’s organization and structure.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action is the person holding the camera performing right now?",
        "time_stamp": "00:02:19",
        "answer": "B",
        "options": [
          "A. Taking items off the shelves.",
          "B. Pushing a cart.",
          "C. Rearranging items on the shelves.",
          "D. Cleaning the aisle."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_436_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:01]: The video begins in a storeroom or warehouse section with two stacks of shelves on either side of the aisle. The shelves are stocked with various containers, likely dairy products, such as cream and yogurt in different shapes and sizes, predominantly white in color with some blue accents. The top shelf on the left has containers labeled \"19.95,\" \"19.49,\" \"22.95,\" and so on. The camera operator is holding a small green box in their right hand. [0:04:02 - 0:04:03]: As the camera shifts slightly to the left, more products on the shelves become visible. A cart loaded with many small green items is in the middle of the aisle. The visibility of shelves on the right shows more packaged goods and some cardboard boxes. [0:04:04 - 0:04:05]: The store aisle continues to the back of the room, with tall metal racks full of more boxes and packages on both sides. The floor appears to be concrete, and there are some red crates towards the back of the room. [0:04:06 - 0:04:07]: The camera angle shifts back towards the right, showing more products, some in stacks and some in vertical arrangements. The aisle remains narrow with this perspective, emphasizing the abundance of products. [0:04:08 - 0:04:09]: Continuing to move forward, the camera approaches the end of the aisle. The shelves are filled with different packaged dairy products, such as milk and yogurt containers. Some sections on the right have open cardboard packaging visible. [0:04:10 - 0:04:11]: The cart full of green items takes up the majority of the view as the camera operator moves slowly within the aisle. The shelves on both sides are packed with various products, while the central floor space is congested with carts and crates. [0:04:12]: The camera focuses on a specific section of the shelf filled with dairy products, mainly yogurt containers set on metal racks. [0:04:13 - 0:04:14]: The camera operator moves slightly towards the shelf and the cart filled with green items. The angle gives a close-up view of the products on the metal racks, including boxes and bottles. [0:04:15]: The operator's right hand appears again, reaching out towards one of the products on the metal shelves. The cart filled with green boxes is still in the center frame. [0:04:16 - 0:04:17]: As the operator continues to move along the aisle, the energy remains on the packed shelves to the right, filled with large quantities of packaged products. The floor space continues to have visible small items and packages scattered. [0:04:18 - 0:04:19]: The final frames show an assortment of products on the lower and upper shelves, with the lower racks densely packed with boxes and cartons, and the upper racks containing fewer, more spaced-out packages. The cart in the foreground remains full of green packages, drawing the viewer's attention.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the camera operator holding in his hands right now?",
        "time_stamp": "0:04:01",
        "answer": "B",
        "options": [
          "A. A red crate.",
          "B. A green and white carton.",
          "C. A yogurt container.",
          "D. A bottle of milk."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Event Understanding",
        "question": "What is the primary type of product seen on the shelves?",
        "time_stamp": "0:04:20",
        "answer": "A",
        "options": [
          "A. Dairy products like milk and yogurt.",
          "B. Fresh vegetables.",
          "C. Canned goods.",
          "D. Electronics."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_436_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:01:28",
        "answer": "A",
        "options": [
          "A. How to find the Mean of a data set.",
          "B. An overview of different types of graphs.",
          "C. The definition of a median.",
          "D. Steps to calculate the mode."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_217_real.mp4"
  },
  {
    "time": "[0:02:17 - 0:02:47]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What concept is likely to be explained next?",
        "time_stamp": "00:04:18",
        "answer": "A",
        "options": [
          "A. Median.",
          "B. Variance.",
          "C. Mode.",
          "D. Range."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_217_real.mp4"
  },
  {
    "time": "[0:04:34 - 0:05:04]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What is the next logical step after the operation to the numbers?",
        "time_stamp": "00:05:07",
        "answer": "B",
        "options": [
          "A. Find the mean of the numbers.",
          "B. Find the median of the numbers.",
          "C. Find the mode of the numbers.",
          "D. Find the range of the numbers."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_217_real.mp4"
  },
  {
    "time": "[0:06:51 - 0:07:21]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the instructor explain next?",
        "time_stamp": "00:07:20",
        "answer": "B",
        "options": [
          "A. How to calculate the mean of a set of numbers.",
          "B. How to interpret the result of the median calculation.",
          "C. The difference between mean and median.",
          "D. How to find the mode of a data set."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_217_real.mp4"
  },
  {
    "time": "[0:09:08 - 0:09:38]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:09:14",
        "answer": "C",
        "options": [
          "A. Divide the sum by the first number in the list.",
          "B. Subtract the smallest number from the sum.",
          "C. Divide the sum by the count of numbers.",
          "D. Add all the numbers again."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_217_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions just taken?",
        "time_stamp": "00:00:10",
        "answer": "A",
        "options": [
          "A. The individual opened a refrigerator, took out frozen food, placed it in a fryer basket, and prepared to deep-fry it.",
          "B. The individual put on gloves, searched for cleaning supplies, and began to clean the station.",
          "C. The individual filled a basket with fresh vegetables, washed them, and prepared to cook them.",
          "D. The individual arranged various condiments on the counter and started preparing a sandwich."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_344_real.mp4"
  },
  {
    "time": "[0:01:17 - 0:01:27]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What actions were taken just now by the individual?",
        "time_stamp": "00:01:27",
        "answer": "A",
        "options": [
          "A. The individual opened a refrigerator, retrieved ingredients, and prepared to cook them.",
          "B. The individual opened a pantry, selected a pan, and started cooking the food.",
          "C. The individual arranged utensils on the counter and began preparing a salad.",
          "D. The individual checked the deep fryer, removed fried food, and placed it on a rack."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_344_real.mp4"
  },
  {
    "time": "[0:02:34 - 0:02:44]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions just taken?",
        "time_stamp": "00:02:44",
        "answer": "A",
        "options": [
          "A. The individual opened a storage unit, retrieved packaged food, and began putting it into fryer baskets.",
          "B. The individual organized utensils on the counter, chopped vegetables, and prepared to sauté them.",
          "C. The individual opened a pantry, gathered baking ingredients, and prepared to start baking.",
          "D. The individual cleaned the working station, disposed of garbage, and reorganized supplies."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_344_real.mp4"
  },
  {
    "time": "[0:03:51 - 0:04:01]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What actions were taken just now by the individual?",
        "time_stamp": "00:04:00",
        "answer": "A",
        "options": [
          "A. The individual removed the fried food from the fryer, placed it on a tray, and organized the tray in a storage unit.",
          "B. The individual placed the food in a frying pan, sautéed it, and plated it.",
          "C. The individual chopped vegetables, placed them in a bowl, and started making a salad.",
          "D. The individual cleaned the working station, wiped the counters, and organized the utensils."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_344_real.mp4"
  },
  {
    "time": "[0:05:08 - 0:05:18]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions just taken?",
        "time_stamp": "00:05:18",
        "answer": "A",
        "options": [
          "A. The individual removed fried food from the fryer, transferred it to a tray, and organized the tray in a storage unit.",
          "B. The individual picked up a pan, sautéed vegetables, and plated them.",
          "C. The individual opened a can of vegetables, placed them in a pot, and started boiling them.",
          "D. The individual cleaned the deep fryer, disposed of oil, and prepped it for the next batch."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_344_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is on the left side of the road right now?",
        "time_stamp": "00:00:10",
        "answer": "D",
        "options": [
          "A. A clear field.",
          "B. A group of houses.",
          "C. A river.",
          "D. A line of trees."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_170_real.mp4"
  },
  {
    "time": "[0:02:18 - 0:02:38]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the nearest dense row of trees located right now?",
        "time_stamp": "00:02:30",
        "answer": "D",
        "options": [
          "A. On the right side of the road.",
          "B. On both sides of the road.",
          "C. In the distance ahead.",
          "D. On the left side of the road."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_170_real.mp4"
  },
  {
    "time": "[0:04:36 - 0:04:56]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the wooden bench?",
        "time_stamp": "00:04:48",
        "answer": "D",
        "options": [
          "A. On the left side of the road.",
          "B. Directly ahead.",
          "C. There is no bench.",
          "D. On the right side of the road."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_170_real.mp4"
  },
  {
    "time": "[0:06:54 - 0:07:14]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is present on the left side of the road right now?",
        "time_stamp": "0:07:09",
        "answer": "D",
        "options": [
          "A. A small park with benches.",
          "B. A river with a pedestrian bridge.",
          "C. An open field with trees.",
          "D. A parked cars and several houses."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_170_real.mp4"
  },
  {
    "time": "[0:09:12 - 0:09:32]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the tree casting a shadow located right now?",
        "time_stamp": "00:09:20",
        "answer": "D",
        "options": [
          "A. On the left side, in the field.",
          "B. In the middle of the road.",
          "C. On the right side of the road.",
          "D. Slightly in front of the cyclist, on the left."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_170_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:00:10",
        "answer": "C",
        "options": [
          "A. An individual ground coffee beans, weighed the coffee, and started brewing the coffee.",
          "B. An individual added condiments to a ready-made espresso shot and served it to a customer.",
          "C. An individual prepared a cup of coffee by grinding beans and tamping it.",
          "D. An individual poured milk and water into a cup and served a latte."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_363_real.mp4"
  },
  {
    "time": "[0:01:59 - 0:02:09]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:02:09",
        "answer": "A",
        "options": [
          "A. An individual prepared two cups of iced coffee, sealed the lids, and placed them on the counter.",
          "B. An individual brewed two cups of hot coffee, added sugar, and handed them to customers.",
          "C. An individual cleaned the coffee machine and stored the coffee cups.",
          "D. An individual took out coffee supplies from storage and organized them on the shelf."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_363_real.mp4"
  },
  {
    "time": "[0:03:58 - 0:04:08]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:04:11",
        "answer": "B",
        "options": [
          "A. The individual brewed a fresh pot of coffee, added sugar, and handed it to the customer.",
          "B. The individual added a straw to an iced coffee, stirred it, and handed it to the customer.",
          "C. The individual took a hot coffee, added a lid, and placed it in a tray.",
          "D. The individual steamed milk, poured it into a cup, and added cinnamon topping."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_363_real.mp4"
  },
  {
    "time": "[0:05:57 - 0:06:07]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:06:00",
        "answer": "B",
        "options": [
          "A. The individual cleaned tables and collected used cups from the customers in a cafe.",
          "B. The individual took packaged suger and served them to customers at a table.",
          "C. The individual cleaned the espresso machine and arranged the utensils on the counter.",
          "D. The individual arranged pastries on a display counter for customers."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_363_real.mp4"
  },
  {
    "time": "[0:07:56 - 0:08:06]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:08:06",
        "answer": "B",
        "options": [
          "A. The individual cleaned up spilled milk and put milk containers back in the refrigerator.",
          "B. The individual brewed three cups of coffee, and disposed of used coffee grounds.",
          "C. The individual cleaned the coffee machine and arranged the cups neatly on the counter.",
          "D. The individual brewed a fresh pot of coffee and added sugar and milk before serving it."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_363_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:15]: The video begins with a man standing against a backdrop of a modern kitchen. The kitchen has a white tiled wall with a large metallic refrigerator in the center and a window on the right side, allowing daylight to illuminate the interior. The man is wearing a dark navy blue t-shirt and has light-colored hair. He stands with his hands clasped in front, occasionally gesturing with one hand while speaking. His facial expressions vary as he talks, sometimes looking directly into the camera, emphasizing his points by moving his hands. The refrigerator behind him has a sleek, reflective surface, and to its right, some kitchen appliances are faintly visible along with a plant near the window. [0:00:16]: The scene transitions to an animated graphic featuring a white plate with a spoon and a large number \"10\" beside it against a blue background. [0:00:17 - 0:00:19]: The screen is dominated by the text \"RAMSAY in 10\" in bold red letters. The animation also shows a white plate with a knife and fork positioned to resemble a clock, enhancing the concept of time. A man wearing a white chef's uniform, arms crossed and with a watch on his left wrist, is positioned on the right side of the screen. The blue background remains consistent in these frames.\n[0:00:20 - 0:00:40] \n[0:00:40 - 0:01:00] ",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the man's t-shirt in the initial video?",
        "time_stamp": "0:00:51",
        "answer": "B",
        "options": [
          "A. Light blue.",
          "B. Dark navy blue.",
          "C. White.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " B."
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_20_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:03]: A woman in a white sleeveless top stands in a kitchen. She is holding a can and using a spoon to scoop its contents. On the counter, there are various ingredients and utensils: a small saucepan, measuring cups, and several bags of dry ingredients. The setting is well-lit, showing modern, white cabinets in the background. [0:01:04 - 0:01:05]: The woman reaches for a green container on the counter, which sits between bags of dry ingredients. She grasps the container and lifts it towards a white bowl positioned in front of her. [0:01:06 - 0:01:09]: She holds up the green container, facing the white bowl. She tilts the container and uses a spoon to measure its contents into the bowl, which already has a whisk inside it. The saucepan remains on the stove in the background, and a small cup is nearby on the counter. [0:01:10 - 0:01:13]: The woman's attention is focused as she continues to measure from the green container. Afterward, she sets the container down and appears to be concentrating on her next step, occasionally looking downward with a thoughtful expression. [0:01:14 - 0:01:15]: The woman resumes using the whisk, mixing the ingredients inside the white bowl. Her movements indicate precision and familiarity with the action. Her right hand whisks the mixture, creating a rhythmical motion. [0:01:16 - 0:01:19]: A close-up view reveals the contents of the bowl as the woman continues to whisk. The dry mixture inside begins to blend smoothly, changing its texture under the whisk's motion.\n[0:01:20 - 0:01:40] \n[0:01:40 - 0:02:00] ",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What item is the woman initially holding in her left hand right now?",
        "time_stamp": "0:01:00",
        "answer": "D",
        "options": [
          "A. A green container.",
          "B. A small saucepan.",
          "C. A white bowl.",
          "D. A can."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_20_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:01]: A woman with long dark hair tied back is in a kitchen positioned in front of a metallic, double-door refrigerator. In front of her on the counter are various kitchen items including a carton of eggs, several small canned goods, a whisk, a measuring cup, and a set of measuring spoons. She holds a white mixing bowl in her left hand and is using a whisk with her right hand. Her attire consists of a light-colored top.  [0:02:01 - 0:02:02]: The woman is mid-motion, transitioning to place the white mixing bowl she was holding with her left hand down on the counter. Her focus appears to be on the bowl. [0:02:02 - 0:02:03]: The woman continues to adjust the bowl on the counter while holding the whisk. The eggs, measuring cups, and several other items remain in the same positions on the counter. [0:02:03 - 0:02:04]: A close-up of the woman’s hands shows her placing the whisk down into one of the two large bowls, which is now seen from an overhead view to be empty and beside another bowl containing some yellow mixture. Some kitchen product packaging is partially visible in the foreground. [0:02:04 - 0:02:05]: The camera focuses on the woman's face as she looks down, concentrating on what she is doing. Her lips are slightly pursed and she appears to be working carefully. [0:02:05 - 0:02:06]: The woman stands at the counter again, now with a spoon in her right hand. She scoops a substance into a small white bowl she is holding in her left hand. The items on the counter remain unchanged. [0:02:06 - 0:02:07]: The woman continues to scoop ingredients into the small white bowl, still holding it steady with her left hand. Her eyes and attention are focused on the task. [0:02:07 - 0:02:09]: The camera shifts to show a close-up of her hands as she uses a small metal measuring spoon to scoop a brown substance, which is likely a spice or powder, from the small white bowl into one of the large mixing bowls in front of her. There is a whisk resting inside the large bowl. [0:02:10]: The woman looks up from her task, possibly in response to something off-camera. She is holding the whisk in her right hand. The arrangement of kitchen items on the counter remains the same. [0:02:11 - 0:02:12]: The woman returns her focus to the bowls in front of her, placing the whisk into one of the mixing bowls.  [0:02:12 - 0:02:13]: She starts to whisk the contents of the bowl, her right hand making rotational movements with the whisk as she does so. [0:02:13 - 0:02:14]: She continues to whisk, her concentration evident as she thoroughly mixes the ingredients in the bowl. [0:02:14 - 0:02:15]: Her whisking movements become more vigorous, blending the ingredients more intensely. [0:02:16 - 0:02:18]: The camera zooms in on the bowl as her hands whisk the contents. The mixture is becoming more uniform with each whisk.  [0:02:19]: Finally, the woman looks up again, continuing to whisk. She appears to be speaking or explaining something, judging by her open mouth and expressive hand gestures.\n[0:02:20 - 0:02:40] \n[0:02:40 - 0:03:00] ",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What item is the woman holding in her left hand right now?",
        "time_stamp": "0:02:01",
        "answer": "B",
        "options": [
          "A. A carton of eggs.",
          "B. A white mixing bowl.",
          "C. A whisk.",
          "D. A small white bowl."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_20_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00] A thick yellow batter is being poured from a measuring jug onto a hot griddle. The batter forms a mound as it spreads out on the surface. [0:03:01 - 0:03:02] A person with long brown hair, wearing a white tank top, holds the container of batter and a red spatula above the griddle. They have just started to pour the batter, creating a round shape on the hot surface. [0:03:03 - 0:03:06] The person continues to pour the batter onto the griddle, forming two similarly sized mounds of batter next to each other. The batter is thick and slowly spreads out as it hits the hot surface. [0:03:07 - 0:03:15] The person steadily pours more batter onto the griddle, forming another round shape close to the first two. A pot with a handle is on the left side of the griddle, and various kitchen items like a spray can and bottles are positioned on the right side of the counter.  [0:03:16] A close-up view shows the person using the red spatula to assist in pouring the batter from the measuring jug. Three round mounds of batter are now on the griddle, starting to cook and develop bubbles on their surfaces. [0:03:17] A focused view of the person's hands, emphasizing the careful action of using the red spatula to scrape the remaining batter from the jug. The batter is thick and slightly lumpy as it falls from the jug. [0:03:18] The person continues to manage the batter on the griddle, making sure all the batter is poured out evenly. Two cooked mounds are clearly formed on the griddle. [0:03:19] The person is now finishing the process, using the spatula to scrape the remainder of the batter from the jug, ensuring it is all used. The kitchen background includes a countertop with a shiny marble finish, a spray can, and bottles.\n[0:03:20 - 0:03:40] \n[0:03:40 - 0:04:00] ",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the fluid being poured onto the griddle?",
        "time_stamp": "0:03:01",
        "answer": "B",
        "options": [
          "A. White.",
          "B. Yellow.",
          "C. Brown.",
          "D. Red."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_20_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:00 - 0:05:20] \n[0:05:20 - 0:05:40] \n[0:05:40 - 0:06:00] ",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What has this woman just done?",
        "time_stamp": "0:05:22",
        "answer": "B",
        "options": [
          "A. Pour the cooked seasonings from the pot onto the chicken wings on the grill.",
          "B. Pour the seasonings from the pot onto the panca.",
          "C. Pour the prepared sauce from the pot onto the roasted fish.",
          "D. Pour the mixed spices from the pot onto the steak being seared."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_20_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the condition of the weather right now?",
        "time_stamp": "00:00:15",
        "answer": "C",
        "options": [
          "A. Cloudy.",
          "B. Rainy.",
          "C. Clear.",
          "D. Snowy."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_422_real.mp4"
  },
  {
    "time": "[0:01:51 - 0:02:11]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the pilot adjusting right now?",
        "time_stamp": "00:02:03",
        "answer": "A",
        "options": [
          "A. Control panel.",
          "B. The radio.",
          "C. The headset.",
          "D. The GPS."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_422_real.mp4"
  },
  {
    "time": "[0:03:42 - 0:04:02]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What device is right now located to the pilot's right side and visible in the footage?",
        "time_stamp": "00:03:59",
        "answer": "B",
        "options": [
          "A. A smartphone.",
          "B. A tablet.",
          "C. A radio.",
          "D. A map."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_422_real.mp4"
  },
  {
    "time": "[0:05:33 - 0:05:53]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the predominant color of the pilot's shorts right now?",
        "time_stamp": "00:05:52",
        "answer": "B",
        "options": [
          "A. Black.",
          "B. Blue.",
          "C. Grey.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_422_real.mp4"
  },
  {
    "time": "[0:07:24 - 0:07:44]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the pilot holding in his right hand right now?",
        "time_stamp": "00:07:31",
        "answer": "A",
        "options": [
          "A. A joystick.",
          "B. A map.",
          "C. A throttle lever.",
          "D. A GPS device."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_422_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the banner with the word \"UNO\" on it?",
        "time_stamp": "00:00:16",
        "answer": "A",
        "options": [
          "A. Red.",
          "B. Blue.",
          "C. Green.",
          "D. Yellow."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_389_real.mp4"
  },
  {
    "time": "[0:02:06 - 0:02:11]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type of vehicle is visible in front of the teal food truck right now?",
        "time_stamp": "00:02:10",
        "answer": "D",
        "options": [
          "A. Taxi cab.",
          "B. Truck.",
          "C. Bus.",
          "D. Black car."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_389_real.mp4"
  },
  {
    "time": "[0:04:12 - 0:04:17]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the speed limit indicated on the \"Share the Road\" sign right now?",
        "time_stamp": "00:04:14",
        "answer": "D",
        "options": [
          "A. 10 MPH.",
          "B. 15 MPH.",
          "C. 20 MPH.",
          "D. 5 MPH."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_389_real.mp4"
  },
  {
    "time": "[0:06:18 - 0:06:23]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What type of store has signs visible right now?",
        "time_stamp": "00:06:19",
        "answer": "D",
        "options": [
          "A. Bookstore.",
          "B. Coffee shop.",
          "C. Clothing store.",
          "D. Pet store."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_389_real.mp4"
  },
  {
    "time": "[0:08:24 - 0:08:29]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What type of store is visible with the sign \"METROPOLIS\"?",
        "time_stamp": "00:08:27",
        "answer": "D",
        "options": [
          "A. Bookstore.",
          "B. Pet store.",
          "C. Coffee shop.",
          "D. Clothing store."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_389_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:46]: The video begins with a close-up of a large, colorful artwork on a wall. The artwork consists of a grid of circular, multicolored patterns set against a blue background. The different circles contain a variety of bright colors and are arranged in rows and columns. The camera slowly pans to the right, capturing more of the artwork. [0:02:47 - 0:02:50]: The video then transitions to show a transparent sculpture shaped like a heart, placed on a clear pedestal. The heart sculpture has a gradient of pink at the top and yellow at the bottom. Behind the sculpture, there is a mirror reflecting the surroundings, including a person taking the video and other visitors looking at the art. The camera continues to pan right. [0:02:51 - 0:02:56]: As the camera moves, the reflection in the mirror becomes clearer, showing more visitors and the exhibition space, which has numerous artworks displayed. The next area features a metallic sculpture mounted on the wall. This sculpture has bright colors and a butterfly figure incorporated into its design, with mechanical, robot-like limbs.  [0:02:57 - 0:02:59]: The final part of the video focuses on the wall-mounted metallic sculpture. The mechanical figure appears to be climbing the wall, with red feet, green arms, and blue butterfly wings. The video ends with a close-up view of this sculpture, highlighting its vivid colors and intricate design elements.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What shape is the transparent sculpture placed on the clear pedestal shown just now?",
        "time_stamp": "00:02:53",
        "answer": "C",
        "options": [
          "A. Star.",
          "B. Circle.",
          "C. Heart.",
          "D. Triangle."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_469_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:21]: The video begins by showcasing an art exhibit booth. Several colorful paintings are displayed on a white partition wall. The name \"TREMBLAY\" is visible above a large painting of a face with bold, expressive brushstrokes. Below, there are two paintings, one depicting a face with vivid splashes of yellow, red, and black, and the other featuring an abstract landscape with vertical lines and a variety of colors. On the right side of the frame, smaller artworks and sculptures are visible on shelves and pedestals. [0:05:22 - 0:05:30]: As the camera pans to the right, more paintings and sculptures come into view. A name \"VEILLEUX\" is pictured above several canvases that display abstract, textured designs. The artwork consists of intricate patterns, shapes, and colors, ranging from bright hues to muted tones. The camera continues to pan right, revealing an additional series of paintings on a white wall. [0:05:31 - 0:05:33]: The video continues to pan to reveal another section of the booth. Here the name \"CICIOVAN\" appears above vibrant, abstract artwork. The paintings show a rich blend of colors and textures, adding to the dynamic range of pieces in the exhibit. [0:05:34 - 0:05:39]: The camera pauses shortly to showcase large canvases prominently displayed on the wall, including the names \"TREMBLAY\" and \"CICIOVAN\". Additionally, smaller artworks, sculptures, and art pieces on display shelves and pedestals are in view. The video captures the detailed and vibrant artwork, giving a sense of the variety and creativity on display at the exhibit.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What name is visible above a large painting of a face with bold, expressive brushstrokes?",
        "time_stamp": "00:05:21",
        "answer": "D",
        "options": [
          "A. VEILLEUX.",
          "B. CICIOVAN.",
          "C. MONET.",
          "D. TREMBLAY."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which names are visible now",
        "time_stamp": "00:05:33",
        "answer": "D",
        "options": [
          "A. VEILLEUX and MONET.",
          "B. MONET and TREMBLAY.",
          "C. CICIOVAN and VEILLEUX.",
          "D. TREMBLAY and CICIOVAN."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_469_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:05]: Several pieces of framed art are displayed on a green wall. Each frame is composed of a white mat and dark outer frame, with golden accents inside. There are four frames arranged in a grid, two by two.  [0:08:06 - 0:08:07]: The camera begins moving downward, revealing the lower part of the display which includes two books. The books have vibrant and colorful covers, one with the title \"Klimt.\" The table they rest on is covered with a multi-colored woven cloth featuring a diamond pattern.  [0:08:08 - 0:08:10]: As the camera continues to move, it pans to the left side of the display, showing a table with a box and a shelf above it. On the shelf are two small sculptures. [0:08:11 - 0:08:12]: Further left, a pink ceramic dish with star designs hangs on the wall. It is between two wooden picture frames containing minimalist sketches, one above the other.  [0:08:13 - 0:08:18]: The camera continues to pan left, revealing more of the minimalist sketches and another part of the pink ceramic dish. The movement slows, providing a better view of the framed sketches. [0:08:19 - 0:08:20]: The camera stays focused on the framed minimalist sketches next to the ceramic dish. The frames are golden with a white mat and minimalist figures drawn in light pencil or ink.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the appearance of the pink ceramic dish shown on the wall?",
        "time_stamp": "00:08:12",
        "answer": "D",
        "options": [
          "A. It has floral designs.",
          "B. It is plain with no designs.",
          "C. It has geometric patterns.",
          "D. It has star designs."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Object Recognition",
        "question": "What is on the table beneath the lower part of the display?",
        "time_stamp": "00:08:10",
        "answer": "A",
        "options": [
          "A. Two books and a box.",
          "B. one book and a box.",
          "C. A sculpture.",
          "D. A vase."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_469_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the activities seen in the recent video?",
        "time_stamp": "00:00:10",
        "answer": "C",
        "options": [
          "A. An individual enters a storage area, picks up a cleaning tool, and starts cleaning the floor.",
          "B. An individual navigates through a cluttered area, moves a container, and looks for something on the shelves.",
          "C. An individual walks through a hallway, encounters cleaning equipment, and the washed his hands.",
          "D. An individual checks the inventory stock, organizes boxes, and ensures the supplies are properly arranged."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_379_real.mp4"
  },
  {
    "time": "[0:01:19 - 0:01:29]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which action best summarizes the preparation process seen just now?",
        "time_stamp": "00:02:31",
        "answer": "D",
        "options": [
          "A. The individual is frying chicken, seasoning it, and packaging it for serving.",
          "B. The individual is preparing a salad by slicing vegetables and arranging them on a plate.",
          "C. The individual is cleaning the fryer area, organizing equipment, and storing the fries for later use.",
          "D. The individual is frying French fries, seasoning them."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_379_real.mp4"
  },
  {
    "time": "[0:02:38 - 0:02:48]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now in the video?",
        "time_stamp": "00:02:54",
        "answer": "C",
        "options": [
          "A. The individual took a break, cleaned the surrounding area, and sanitized the working station.",
          "B. The individual selected various food items, placed them in a pan, and started cooking them on a stovetop.",
          "C. The individual poured the fries into the packaging box.",
          "D. The individual prepared a salad by chopping vegetables and setting them onto a plate."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_379_real.mp4"
  },
  {
    "time": "[0:03:57 - 0:04:07]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the scene just now?",
        "time_stamp": "00:04:07",
        "answer": "B",
        "options": [
          "A. The individual inspected the food items in the refrigerator, rearranged them, and closed the door.",
          "B. The individual checked and adjusted multiple fryer baskets of fries, making sure they were cooking properly, before stepping back.",
          "C. The individual cleaned the counter and surrounding areas, ensuring a tidy workspace.",
          "D. The individual organized and labeled various containers in preparation for a cooking task."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_379_real.mp4"
  },
  {
    "time": "[0:05:16 - 0:05:26]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following best summarizes the actions performed just now?",
        "time_stamp": "00:05:26",
        "answer": "B",
        "options": [
          "A. The individual organized frozen ingredients, arranged them on a tray, and placed it in the oven.",
          "B. The individual picked out the burnt fries and throw them into the trash can.",
          "C. The individual prepared a bucket of peeled potatoes, rinsed them in water, and placed them in a storage bin.",
          "D. The individual packaged French fries, added salt, and placed the containers on a service counter."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_379_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the man standing in front of the glider wearing on his head right now?",
        "time_stamp": "00:00:03",
        "answer": "C",
        "options": [
          "A. A helmet.",
          "B. A hat.",
          "C. A cap.",
          "D. No headgear."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_431_real.mp4"
  },
  {
    "time": "[0:01:39 - 0:01:44]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the current texture of the sky right now?",
        "time_stamp": "00:01:42",
        "answer": "B",
        "options": [
          "A. Clear with few clouds.",
          "B. Overcast and cloudy.",
          "C. Sunny with scattered clouds.",
          "D. Stormy with dark clouds."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_431_real.mp4"
  },
  {
    "time": "[0:03:18 - 0:03:23]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "Right now, what is the man holding in his hand?",
        "time_stamp": "00:03:20",
        "answer": "C",
        "options": [
          "A. A map.",
          "B. A burger.",
          "C. A sandwich.",
          "D. A piece of paper."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_431_real.mp4"
  },
  {
    "time": "[0:04:57 - 0:05:02]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is displayed on the central screen of the control panel right now?",
        "time_stamp": "00:04:58",
        "answer": "A",
        "options": [
          "A. A navigation map.",
          "B. A weather forecast.",
          "C. A music player.",
          "D. An engine monitoring system."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_431_real.mp4"
  },
  {
    "time": "[0:06:36 - 0:06:41]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the primary color of the joystick in the pilot's left hand right now?",
        "time_stamp": "00:06:36",
        "answer": "B",
        "options": [
          "A. Red.",
          "B. Blue.",
          "C. Yellow.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_431_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: I'm;  [0:00:01]: of my;  [0:00:02]: boring;  [0:00:03]: setup;  [0:00:04]: So that's why I'm gonna build;  [0:00:05]: beautiful;  [0:00:06]: enchanting setup;  [0:00:08]: a beautiful view;  [0:00:09]: of all of the world;  [0:00:10]: and by the way;  [0:00:11]: a very;  [0:00:12]: dangerous place;  [0:00:13]: to build;  [0:00:14]: so if you do end up enjoying the video;  [0:00:15]: leave a like;  [0:00:16]: leave a comment;  [0:00:17]: leave a comment below;  [0:00:18]: hope you enjoy;  [0:00:19 - 0:00:20]: So for us.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the speaker planning to build?",
        "time_stamp": "0:00:14",
        "answer": "C",
        "options": [
          "A. A boring setup.",
          "B. A new house.",
          "C. An enchanting setup.",
          "D. A dangerous place."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_192_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:20 - 0:04:40] [0:04:20 - 0:04:24]: In a dimly-lit room with pixelated stone walls and a cobblestone floor, an array of items including leather armor drops onto the ground. The player’s perspective shows the edge of a shield held in the left hand and a sword with a purple hue in the right hand;  [0:04:25 - 0:04:28]: As the viewpoint remains steady, the items on the floor start to shift. The room itself is built of uniform gray stone blocks, forming a corner. The viewpoint then moves slightly upward, revealing more of the space;  [0:04:29 - 0:04:30]: More of the square stone room becomes visible, with two torches illuminating the walls. The player’s viewpoint surveys the room and then the text “and I mean” appears;  [0:04:31 - 0:04:32]: The character subsequently turns towards another empty corner of the room. The camera movement is smooth as the scene transitions from the indoor stone walls to the outdoor, grassy terrain;  [0:04:33 - 0:04:34]: The scene changes to an exterior view, showing a close-up of a character holding a water bucket in the right hand next to a stone wall and grassy ground. The character has full health and a high-level (35) experience gauge;  [0:04:35]: The character walks past a stone structure with an opening leading to a different area. The health and experience status remain unchanged;  [0:04:36 - 0:04:37]: The scene transitions again to an enclosed room filled with bookshelves on all sides and the floor made of green material. In the center, there is an enchanting table with a book floating above it;  [0:04:38]: The viewpoint steadies, focusing on the enchanting table and bookshelves lining the walls. The lighting highlights the colorful covers of the books. The shield and water bucket remain in the character’s left and right hands respectively;  [0:04:39]: The interior of the room stays focused as the character inches closer to the enchanting table. The bookshelves fully surround the area, creating a cozy and enclosed space for enchanting;  [0:04:40]: The view narrows in on the enchanting table, showing an interface for enchanting items. The player’s inventory is displayed at the bottom of the screen, revealing various items including a diamond pickaxe ready for enchantment.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the character holding in their right hand right now?",
        "time_stamp": "00:04:24",
        "answer": "B",
        "options": [
          "A. A bow.",
          "B. A purple-hued sword.",
          "C. A water bucket.",
          "D. A diamond pickaxe."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the notable feature of the player's experience level when shown outdoors?",
        "time_stamp": "00:04:38",
        "answer": "C",
        "options": [
          "A. Low-level (5).",
          "B. Mid-level (20).",
          "C. High-level (35).",
          "D. Maximum level (50)."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_192_real.mp4"
  },
  {
    "time": "[0:13:00 - 0:14:00]",
    "captions": "[0:13:00 - 0:13:20] [0:13:00 - 0:13:01]: The video begins with a view of a large, circular red structure made of blocks, resembling a bowl or dish shape. The environment around the structure includes various terrain features such as rivers, small patches of forest, and open land; all viewed from a high altitude. [0:13:01 - 0:13:03]: As the video progresses, the camera moves slightly upward and to the right, revealing more of the structure’s height and showing that a player character, standing on what appears to be a tall central pole, is present within the structure. [0:13:03 - 0:13:05]: The structure continues to be the focal point, and the camera moves to a position where the red blocks comprising the circular formation are more visible. The player character begins to build outward from the central pole, extending a red block bridge towards the edge of the circular structure. [0:13:05 - 0:13:07]: The bridge spans further towards the edge, and more details of the terrain below become apparent. The terrain consists of rivers and patches of land, accentuating the elevated vantage point from which the video is filmed. [0:13:07 - 0:13:09]: The bridge is now nearly complete, reaching almost from the center of the circle to the edge. The camera provides a clear view of the red structure’s intricate design, which appears to be composed of uniformly arranged blocks. [0:13:09 - 0:13:11]: The camera remains steady, depicting the completed bridge within the structure. The red circle with its mid-air bridge stands out against the green and blue tones of the surrounding landscape. [0:13:11 - 0:13:14]: The perspective gradually zooms out, capturing the entirety of the structure from above and its context within the landscape. The player character stands firmly at the bridge's end, located near the edge of the circular formation. [0:13:14 - 0:13:17]: The camera continues to pan around the structure, showcasing its complete appearance from various angles. The intricate pattern of the red blocks inside the perimeter becomes more apparent, and the surrounding patches of land and water give the structure a sense of scale. [0:13:17 - 0:13:19]: The player character appears to be making minor adjustments or movements at the edge of the structure. The camera angle shifts slightly to provide a better view, emphasizing the player’s position relative to the expansive circular design. [0:13:19]: The video ends with the camera focusing on the player character, who is now prominently located at the border of the large red circle, with clear views of the surroundings from the elevated position. The design’s complexity and placement stand in stark contrast to the natural, varied terrain below.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What does the player character do after standing on the central pole?",
        "time_stamp": "00:13:05",
        "answer": "C",
        "options": [
          "A. Jumps off the pole.",
          "B. Dismantles the structure.",
          "C. Continue to construct this red building.",
          "D. Walks around the structure."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the player character located right now?",
        "time_stamp": "00:13:19",
        "answer": "B",
        "options": [
          "A. At the center of the circle.",
          "B. Near the edge of the circular formation.",
          "C. Outside the structure.",
          "D. On the terrain below."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_192_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video starts with a first-person view of a character standing behind a delivery truck. The truck has an open back with dark interior. The character is wearing a high-visibility vest and is holding a package. Another character, dressed in a white shirt and dark pants, is standing next to a utility box near the building on the right side. [0:00:03 - 0:00:06]: The character starts walking towards the person in the white shirt, who is standing against the wall. The ground is paved, and there are various utility boxes attached to the wall on the right. The environment appears to be a back alley or a service area of a building. [0:00:07 - 0:00:10]: The character continues walking closer to the person in the white shirt. They appear to be walking towards each other intentionally, suggesting an interaction is likely to occur. [0:00:11 - 0:00:13]: The character stands directly in front of the person in the white shirt, possibly initiating a conversation or interaction. The background remains the same with the building wall, utility boxes, and paved ground. [0:00:14]: There's a close-up view of the person in the white shirt, who appears to be holding a device and looking at it. Text on the screen identifies the person as \"Harry Miller\" and provides options like \"Tracking Cargo\". The background is slightly blurred, focusing on the character's face. [0:00:15 - 0:00:19]: The screen transitions to an inventory interface showing various items categorized under \"Player,\" \"Backpack,\" and \"Ground.\" The items include documents, parcels, and some personal information of the player. The interaction with the inventory lasts for a few seconds. [0:00:20 - 0:00:22]: The view returns to the first-person perspective, facing the person in the white shirt who continues to look at the device. The player character is no longer holding the package. [0:00:23 - 0:00:24]: The character steps back, giving a wider view of the area. The person in the white shirt remains in the same position next to the utility boxes. [0:00:25 - 0:00:26]: The character begins crouching and moving towards the truck. The environment, including the truck and the building, remains consistent. [0:00:27 - 0:00:28]: The character continues moving in a crouched posture around the truck, perhaps suggesting they are trying to remain unnoticed or cautious. [0:00:29 - 0:00:30]: The character approaches the side of another similar truck parked nearby, looking around possibly to ensure they are not being observed. [0:00:31 - 0:00:32]: The character reaches the side door of the truck and opens it, suggesting they are about to enter or inspect the vehicle. [0:00:33 - 0:00:34]: The character has opened the door and is partially inside the truck, the view slightly shifts to the interior. [0:00:35 - 0:00:37]: The character is now seated inside the truck, preparing to drive. The street ahead is visible through the windshield. [0:00:38 - 0:00:40]: The character starts the vehicle, and the dashboard displays speed and other information. The truck begins moving slowly. [0:00:41 - 0:00:43]: The character drives the truck towards the exit of the service area, the street and surroundings getting more visible. [0:00:44 - 0:00:46]: The truck continues moving down the street, the environment transitioning from the service area to the main road. Trees and buildings are visible ahead. [0:00:47 - 0:00:50]: The truck drives further down the road, merging into traffic as the surroundings become more urban, with noticeable city structures and greenery.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What did the character do just now?",
        "time_stamp": "00:00:11",
        "answer": "D",
        "options": [
          "A. Closed the door of the truck.",
          "B. Inspected the utility boxes.",
          "C. Handed the package to Harry Miller.",
          "D. Opened the side door of the truck and entered the truck."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_290_real.mp4"
  },
  {
    "time": "0:02:20 - 0:02:40",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:24]: A dark-colored van is driving on a slightly inclined highway with palm trees lining the side, and a car is visible ahead in the right lane. The sky is clear and blue, revealing a distant ocean view on the right. A person is visible in a small, inset video at the top left, wearing a headset and appearing to be controlling or commentating on the video game. The right side of the screen is filled with chat messages. [0:02:25 - 0:02:27]: The van continues moving forward, passing more palm trees on the right. The person in the inset video reacts with expressions while continuing to speak. The car ahead is still present, maintaining its lane. [0:02:28 - 0:02:31]: The van approaches a curve to the right, and the surroundings start revealing more infrastructure related to a dock area. The car ahead takes the turn while the van follows. The person in the inset video shows focused expressions. [0:02:32 - 0:02:35]: The van follows the highway curve while descending slightly. A complex of buildings and fenced sections become visible on the left side of the frame. Palm trees continue lining the road on both sides. The person in the inset video wears a focused expression. [0:02:36 - 0:02:39]: The highway is curving to the right, leading towards an entrance to a dock area. Several cars and trucks are visible parked on the left side of the road. The person in the inset video keeps interacting or discussing with someone. [0:02:40 - 0:02:42]: The van continues curving right, approaching a large gate with the label \"Dock Entrance.\" The road becomes flat and the ocean remains visible on the right side. [0:02:43 - 0:02:46]: The van moves closer to the dock entrance, passing between industrial buildings and open storage areas. Traffic is light with only a few vehicles ahead. The inset video shows the person engaged in detaled conversation, likely discussing the game progress. [0:02:47 - 0:02:51]: Nearing the dock entrance, the van approaches a toll-like gate. The surrounding area is industrial with shipping containers, fences, and loading structures visible. [0:02:52 - 0:02:55]: The van nears the dock entrance, preparing to enter. More docks and container areas become visible, indicating it is close to a large loading or unloading zone. [0:02:56 - 0:02:59]: The van drives through the dock gate, continuing into the dock facility. There is a systematic layout with various lanes, barriers, and container stacks to the left. [0:03:00 - 0:03:03]: The van navigates within the dock area, moving towards a designated lane marked for vehicle entry. The inset video shows the person focusing intently on their screen. [0:03:04 - 0:03:07]: Moving deeper into the dock facility, the van aligns with lanes designated for trucks. More industrial structures and storage containers flank the vehicle as it progresses. [0:03:08 - 0:03:11]: The vehicle continues along marked lanes in the dock facility, passing beneath a sign with operational instructions and directly entering a marked lane. [0:03:12 - 0:03:15]: The van continues driving within the dock lanes, approaching an inspection or toll point that ensures proper authorization for entry. [0:03:16 - 0:03:18]: The van proceeds through the designated lane marked \"2,\" heading towards a wider operational area within the dock facility. The inset video shows the person possibly giving directions or commentary. [0:03:19 - 0:03:22]: The vehicle heads towards another sector within the dock, taking a slight turn towards the left lane away from the inspection area. The dock's complex layout becomes more evident as more structures appear in the background.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person in the inset video doing right now?",
        "time_stamp": "00:02:22",
        "answer": "D",
        "options": [
          "A. Doing nothing.",
          "B. Adjusting the headset.",
          "C. Typing on a keyboard.",
          "D. Reacting with expressions while speaking."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_290_real.mp4"
  },
  {
    "time": "0:04:40 - 0:05:00",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:41]: A white cargo truck is parked on the right side of the road, beside a building with orange foliage visible on its right. The view is from the inside of a smaller black van behind it, showing the front of both vehicles facing the road. A person is sitting in the driver’s seat of the van, with a stream chat visible on the right side of the screen. The background includes a clear sky, some utility poles, and distant buildings. [0:04:42 - 0:04:43]: The black van is now moving forward, passing the white cargo truck on the left side. The road ahead is clear, with visible traffic signals, a rail line to the left, and an urban setting in the background with taller buildings and a tower. [0:04:44 - 0:04:46]: The black van continues to drive straight down the road. Palm trees and urban buildings line both sides, with the rail line still visible to the left. The driver maintains a steady speed, and the chat is still active on the right side of the screen. [0:04:47 - 0:04:49]: The black van stays in the right lane, maintaining its path along the road. The environment remains consistent with urban buildings, palm trees, and the rail line to the left. A few cars are seen in the opposite lane. [0:04:50 - 0:04:51]: The van keeps a steady speed as it drives straight on the road. The surroundings include more buildings and palm trees lining the street, with some cars visible ahead in the opposite direction. The chat continues to display comments on the right side. [0:04:52 - 0:04:53]: The van is driving near two tall buildings on the left and right sides of the road. A red car is approaching from behind the van, visible from the rear window. [0:04:54 - 0:04:56]: The black van starts slowing down as it approaches an intersection with a traffic light, the red car following closely. The environment shows urban buildings and fewer palm trees compared to previous frames. [0:04:57 - 0:04:58]: The van is now stopping at the intersection, waiting for the traffic light to change. Several cars are visible in the adjacent lanes, and a few pedestrians can be seen on the sidewalks. [0:04:59 - 0:05:01]: The van continues driving straight after the intersection. The road ahead is clear, with more buildings and city infrastructure visible, including some distant traffic lights. The sunset is starting to be noticeable on the horizon, casting a warm light over the scene. [0:05:02 - 0:05:03]: The van approaches another intersection with a traffic light, briefly stopping before making a turn. The sky continues to show the warm light of the setting sun, and the urban surroundings include more buildings and distant vehicles. [0:05:04 - 0:05:06]: The black van makes a right turn at the intersection, leading onto a wider street with central lanes. The background includes urban buildings and a visible cluster of palm trees under the sunset glow. [0:05:07 - 0:05:08]: Driving straight ahead after the turn, the view reveals a road with fewer cars and a broader perspective of the city under the setting sun. The sun is directly ahead, intensifying the warm highlights on the road. [0:05:09 - 0:05:12]: The van continues down a wide road with lanes partially filled by other vehicles. The glowing sunset remains prominently in view, casting red and orange hues over the scene. The urban environment is consistent with previous views, featuring buildings and some low foliage. [0:05:13 - 0:05:16]: Continuing to drive straight, the background includes a mix of buildings and more open cityscapes. The sunset creates a bright, warm ambiance, and the road remains relatively clear, with a few cars in front. [0:05:17 - 0:05:19]: The van maintains its speed as it drives down the road. The sunset's light intensifies, creating a strong glow over the horizon and reflecting off the urban scenery, which includes sparse buildings and taller structures in the distance. [0:05:20]: The black van keeps moving straight, with the sunset dominating the horizon ahead. The road remains clear with occasional vehicles, and the cityscape blends into the warm light of the sunset.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the black van doing right now?",
        "time_stamp": "00:05:01",
        "answer": "D",
        "options": [
          "A. Stopping at a traffic light.",
          "B. Making a left turn.",
          "C. Reversing back to the previous street.",
          "D. Continuing driving straight after the intersection."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_290_real.mp4"
  },
  {
    "time": "0:07:00 - 0:07:20",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:01]: The video begins with a first-person view from inside a delivery truck. The truck is parked on the right side of a busy city street, near a building with palm trees outside. Several other vehicles, including a blue car and a brown truck, are parked along the street. The driver is visible in the top-left corner of the frame, looking ahead at the street. [0:07:02 - 0:07:06]: The scene transitions to an in-game trucking progression menu, displaying various routes and missions such as \"Los Santos Freeway South\" and \"Sandy Shores Disposal Site.\" The missions show rewards and progress status. The driver can be seen in a smaller window in the top-left corner of the screen, glancing at the menu options and making selections. [0:07:07]: The screen continues to display multiple job options in the trucking progression menu. The driver appears contemplative, possibly deciding which job to select next. [0:07:08]: The view returns to the previous perspective from inside the delivery truck, which is now starting to drive forward. The road ahead is mostly clear, with a few parked cars on the side and buildings in the background. [0:07:09 - 0:07:12]: The truck is moving along the street, passing by other vehicles and approaching an intersection. The road has several lanes with some traffic, and additional buildings and palm trees are visible on the sides. [0:07:13]: The vehicle continues driving toward an intersection with a traffic light. The light is red, and the truck comes to a stop behind another car waiting at the light. [0:07:14 - 0:07:15]: The scene shows a close-up view of the truck's dashboard, displaying a speedometer and other controls. The driver is waiting for the light to turn green. [0:07:16 - 0:07:18]: The camera switches back to a broader view of the street ahead as the truck begins to move again. The traffic light turns green, and the truck drives forward through the intersection. [0:07:19]: The truck proceeds down the street, passing by more parked vehicles and buildings on the sides. A few pedestrians can be seen walking on the sidewalk. [0:07:20]: The video concludes with the truck continuing along the road, heading towards another intersection up ahead. The driver remains focused on navigating through the traffic and surroundings.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the truck doing right now?",
        "time_stamp": "00:07:16",
        "answer": "D",
        "options": [
          "A. It starting to drive back.",
          "B. It passing through an hospital.",
          "C. It parking on the mountain.",
          "D. It caming to a stop at the intersection."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_290_real.mp4"
  },
  {
    "time": "0:09:20 - 0:09:35",
    "captions": "[0:09:20 - 0:09:35] [0:09:20 - 0:09:35]: A person is sitting at a desk, visible to the left side of the video. The individual is wearing a light-colored sweatshirt and appears to be focused on a screen in front of them. The right side of the video shows the screen content, which is from a first-person perspective game. In the game, a character is standing beside a brown wall at nighttime, holding a tool and inspecting or interacting with the wall. The character wears a reflective vest, safety goggles, headphones, and gloves, suggesting they are performing some construction or maintenance task on the wall. Text widgets and viewer comments are constantly streaming on the right side of the screen, indicating that this is a live-streaming scenario. The environment in the game shows a street with a few cars and some buildings in the background. The game interface at the bottom of the screen displays \"Building Vehicle\" and a progress bar, indicating the current task or action being performed in-game.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the character in the game doing right now?",
        "time_stamp": "00:09:35",
        "answer": "D",
        "options": [
          "A. Running away from the wall.",
          "B. Driving a vehicle.",
          "C. Shooting at a target.",
          "D. Using his phone."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_290_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is directly in front of the camera right now?",
        "time_stamp": "00:00:06",
        "answer": "B",
        "options": [
          "A. A white car.",
          "B. A yellow taxi.",
          "C. A red car.",
          "D. A black SUV."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_399_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:02:05]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What text is visible on the right side of the street right now?",
        "time_stamp": "00:02:06",
        "answer": "C",
        "options": [
          "A. Excavator.",
          "B. CRANE.",
          "C. WEST NEW YORK.",
          "D. Loader."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_399_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:04:05]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which type of restaurant is visible on the right side right now?",
        "time_stamp": "00:04:00",
        "answer": "B",
        "options": [
          "A. Italian restaurant.",
          "B. West Side Restaurant.",
          "C. Chinese restaurant.",
          "D. Mexican restaurant."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_399_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:06:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the traffic light right now?",
        "time_stamp": "00:06:02",
        "answer": "C",
        "options": [
          "A. Green.",
          "B. Yellow.",
          "C. Red.",
          "D. Blue."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_399_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:08:05]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What notable landmark is visible on the left side of the street right now?",
        "time_stamp": "00:08:01",
        "answer": "B",
        "options": [
          "A. A statue of liberty.",
          "B. A large globe structure.",
          "C. A Ferris wheel.",
          "D. A library."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_399_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type of vehicles are prominently visible in this clip?",
        "time_stamp": "00:00:04",
        "answer": "B",
        "options": [
          "A. Buses.",
          "B. Yellow taxis.",
          "C. Motorcycles.",
          "D. Police cars."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_384_real.mp4"
  },
  {
    "time": "[0:02:06 - 0:02:11]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color are most of the taxis visible right now?",
        "time_stamp": "00:02:09",
        "answer": "C",
        "options": [
          "A. Green.",
          "B. White.",
          "C. Yellow.",
          "D. Black."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_384_real.mp4"
  },
  {
    "time": "[0:04:12 - 0:04:17]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What are the visible sign colors mounted on the buildings right now?",
        "time_stamp": "00:04:14",
        "answer": "D",
        "options": [
          "A. Green and blue.",
          "B. Red and black.",
          "C. Yellow and white.",
          "D. Green and white."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_384_real.mp4"
  },
  {
    "time": "[0:06:18 - 0:06:23]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is predominantly seen on the poles lining the street right now?",
        "time_stamp": "00:06:20",
        "answer": "B",
        "options": [
          "A. Traffic lights.",
          "B. Banners.",
          "C. Flower pots.",
          "D. Bike racks."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_384_real.mp4"
  },
  {
    "time": "[0:08:24 - 0:08:29]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the mode of transportation that several individuals are using right now?",
        "time_stamp": "00:08:26",
        "answer": "C",
        "options": [
          "A. Roller skates.",
          "B. Skateboards.",
          "C. Bicycles.",
          "D. Scooters."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_384_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: A white digital display shows the number \"10\" with a knife and fork positioned like clock hands inside the zero; [0:00:01 - 0:00:03]: The scene transitions to a person in a navy blue shirt smiling and standing in front of a background with \"RAMSAY in 10\" written in blue and white capital letters; [0:00:04]: The person continues to smile with the same background, but a visual element with changing shapes begins to appear on the left side; [0:00:05]: The scene shifts to a well-organized kitchen setting with white brick walls and wooden shelves filled with cookware, dishes, and glass jars; on the left, large red letters spell out \"COOK\"; the person in the navy blue shirt is standing in the kitchen, clasping their hands together; [0:00:06]: The person leans forward slightly, speaking to the camera; [0:00:07]: The person stands upright again, their hands clasped; [0:00:08]: The individual gestures with their hands, indicating a point; [0:00:09 - 0:00:10]: The camera angle shifts, showing a different part of the kitchen, which includes a double oven on the left; the person is turned to their right, speaking; [0:00:13]: The individual turns back slightly to face forward while speaking; [0:00:10 - 0:00:15]: The person continues to talk, their expression changing subtly, and their body slightly turned; [0:00:16 - 0:00:17]: The camera returns to the original kitchen view with the white brick walls and shelves; the person gestures expressively with their hands; [0:00:18]: The person looks down momentarily while talking, bringing their hands together again; [0:00:19]: Ending the scene, the person stands with their hands clasped in front, as the background with the white brick wall and organized shelves remains in view.\n[0:00:20 - 0:00:40] [0:00:20 - 0:00:22]: In a kitchen setting with white brick walls and wooden shelves, a person is facing the camera. The shelves are stocked with various kitchen items, such as jars, plates, and utensils. The word \"COOK\" is prominently displayed in red letters on the top shelf. The person, wearing a teal shirt, stands behind a counter with folded hands, and there are different items on the counter, including a small wooden board and some jars. [0:00:23]: The person raises their hands with fingers spread wide apart, appearing to emphasize a point. [0:00:24 - 0:00:25]: The person gives a slight smile and clasps their hands together, continuing to speak directly into the camera. Behind them, knives are arranged neatly on a magnetic strip affixed to the wall. [0:00:26 - 0:00:27]: The individual maintains their posture with clasped hands and a focused look, still addressing the camera. He slightly shift their stance to the left. [0:00:28]: Moving to the right side of the frame, the person begins to reach out towards an object on the counter. The background continues to show various kitchen tools and accessories. [0:00:29]: The person focuses on a cauliflower placed on a wooden board. He reach out with both hands to grasp the vegetable. [0:00:30]: The hands are close to the cauliflower, which is placed in a white bowl on the countertop. Various cooking ingredients, including spices, oils, and utensils in glass and ceramic containers, are spread out on the counter around them. [0:00:31 - 0:00:32]: The person adjusts the cauliflower, ensuring it is centered in the bowl. The individual appears to explain something about the vegetable while demonstrating with their hands. You can see a stove burner on the left side of the counter. [0:00:33 - 0:00:34]: Continuing to handle the cauliflower, the person adds further details to their explanation. Their right hand is placing the cauliflower more precisely while various condiments remain on the counter. [0:00:35 - 0:00:36]: The person slightly tilts forward, carefully guiding the cauliflower with their hands. More details about the ingredients and cooking process are demonstrated with precise hand movements. [0:00:37]: As the person looks directly at the camera, their hands rest on the cauliflower. He seem to be sharing a specific point or instruction. [0:00:38 - 0:00:39]: Finally, the person claps their hands together again around the cauliflower, indicating the completion of an explanation or demonstration. The setting continues to show the organized kitchen and varied cooking implements.\n[0:00:40 - 0:01:00] [0:00:40 - 0:00:41]: In a kitchen with white brick walls, a man in a blue t-shirt is seen shaping a large cauliflower head on a counter with various cooking ingredients and utensils in the background. [0:00:41 - 0:00:42]: The camera focuses on an overhead view of the countertop which has multiple small bowls containing green herbs, butter, sliced almonds, a cauliflower head, and some seasoning jars. A hand is positioned over a bowl of sliced almonds. [0:00:42 - 0:00:43]: The hand is now reaching over to a bowl of sliced almonds and sprinkling them. [0:00:43 - 0:00:44]: The hand is seen grabbing lemons from the left side of the table. Various bowls of ingredients, such as fresh herbs, a bottle of oil, and slices of butter are positioned to the right. [0:00:44 - 0:00:45]: The hand places some lemons in a dish on the left side while the right side of the table shows several bowls of ingredients, including a cauliflower head, garlic, and herbs. [0:00:45 - 0:00:46]: The view shifts to the man’s hand reaching for a small bowl of herbs positioned among other cooking ingredients on the countertop. There is a wooden cutting board in the background. [0:00:46 - 0:00:47]: The hand picks up the herb bowl and prepares to add it to other ingredients. Various ingredients such as butter, garlic, a bowl of water, and a large cauliflower head are visible on the table. [0:00:47 - 0:00:48]: The hand is now seen pulling an herb bowl closer while other various small bowls and a bottle of mustard are in view along with a glass mixing bowl. [0:00:48 - 0:00:49]: The hand moves towards placing the herb bowl down as other items like a large glass bottle, other bowls, and a wooden cutting board are arranged on the countertop. [0:00:49 - 0:00:50]: All the bowls, ingredients, and utensils are neatly arranged on the countertop. The wooden cutting board is now clearly focused and ready for use. [0:00:50 - 0:00:51]: The man faces the camera, gesturing with his hands in front of his chest as if explaining something. A brick wall with shelves holding dishes and decor is seen behind him. [0:00:51 - 0:00:52]: The man continues to talk, making expressive hand movements. The background features stacked plates and various kitchen items arranged on the shelves. [0:00:52 - 0:00:53]: The man is seen from a slightly different angle while talking, still in the same setting. His hand movements add emphasis to his speech. [0:00:53 - 0:00:54]: From a head-on perspective, the man is seen gesturing with both hands as he speaks. The kitchen shelves in the background show orderly arranged dishes and decor. [0:00:54 - 0:00:55]: The man clasps his hands together midsentence while standing in front of the kitchen counter. The backdrop continues to display neatly organized kitchenware and utensils. [0:00:55 - 0:00:56]: As he speaks, the man looks down, with a slight bow of his head, holding his hands together. His posture reflects a sense of focus. [0:00:56 - 0:00:57]: The man continues speaking, with his arms slightly apart, maintaining an engaged demeanor. The background stays consistent with well-organized kitchen items. [0:00:57 - 0:00:58]: The man seems to be emphasizing a point while looking towards the left. His hands are in motion as he explains the process. The kitchen setup around him remains the same. [0:00:58 - 0:00:59]: A wider view of the man in front of a kitchen setup shows him making a wide hand gesture. The shelves behind him are decorated with various kitchen utensils and colorful bottles.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is prominently displayed in red letters on the top shelf in the kitchen?",
        "time_stamp": "00:00:22",
        "answer": "B",
        "options": [
          "A. EAT.",
          "B. COOK.",
          "C. CHEF.",
          "D. BAKE."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_11_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:00 - 0:01:20] [0:00:00 - 0:00:06]: A kitchen counter adorned with various cooking implements and ingredients is shown. A person in a blue shirt stands behind the counter. He are in front of a white tile backsplash with shelves holding dishes, spices, and other kitchen items. Large red letters spelling \"COOK\" are mounted on the wall. At the start, He place a lid on a pan on the stovetop.  [0:00:07 - 0:00:12]: The perspective shifts to an overhead view of the counter. Various ingredients, including a bowl of cauliflower and other assorted vegetables, are neatly arranged to the left. The person's right hand hovers above a frying pan placed on the stovetop, feeling for heat. [0:00:13 - 0:00:17]: Returning to the front view, the person engages with the cauliflower, lifting it from the counter. He then look back at the stovetop on their left, likely checking on another element of the dish being prepared. Various kitchen tools and seasonings are spread around the counter. [0:00:18 - 0:00:20]: Focusing back on the cauliflower, the person positions it on a cutting board. An overhead view displays their hands holding a knife poised to cut into the vegetable. Several small bowls and kitchen utensils remain to the side, ready for use.\n[0:01:20 - 0:01:40] [0:01:20 - 0:01:21]: A hand firmly holds a whole cauliflower placed on a wooden cutting board. Another hand grips a large knife, ready to cut;  [0:01:21 - 0:01:22]: The resistance is met as the knife begins to penetrate the cauliflower's surface. A jar and a butter dish are visible nearby;  [0:01:22 - 0:01:23]: The cauliflower begins to split as the knife cuts deeper. A teal shirt and part of the kitchen counter are noticeable; [0:01:23 - 0:01:24]: The knife continues to slice through the cauliflower, which is starting to fall apart. The hand grips the cauliflower firmly to guide the cut; [0:01:24 - 0:01:25]: The cauliflower’s stem becomes visible as the slicing continues; [0:01:25 - 0:01:26]: The cut has almost completed, and bits of the cauliflower flake off; [0:01:26 - 0:01:27]: The intact outer portion remains on the cutting board while small pieces scatter; [0:01:27 - 0:01:28]: The knife is angled downward, nearing the cutting board’s surface, ensuring the cut’s precision; [0:01:28 - 0:01:29]: Gentle pressure is applied while the cut is finalized. The cauliflower is almost fully halved; [0:01:29 - 0:01:30]: The two halves of the cauliflower are clearly defined. One hand ensures stability; [0:01:30 - 0:01:31]: The knife seams to reposition slightly for another cut, focusing on the center; [0:01:31 - 0:01:32]: Another precise cut is aimed at the middle section, dividing further; [0:01:32 - 0:01:33]: The cutting process smoothly continues, with hands directing the knife accurately; [0:01:33 - 0:01:34]: The segments become more defined, and cauliflower pieces are noticeable on the cutting board; [0:01:34 - 0:01:35]: The camera zooms out to reveal the person making the cuts, focusing on the cauliflower and kitchen surroundings; [0:01:35 - 0:01:36]: One of the cut pieces is picked up and examined. The person looks attentive; [0:01:36 - 0:01:37]: The piece is held up to show the results of the cutting. Tools and kitchen decor are visible in the background; [0:01:37 - 0:01:38]: A slight shift in focus toward the camera to perhaps talk about the procedure; [0:01:38 - 0:01:39]: The focus returns to the remaining cauliflower pieces on the board. The person’s attention is on preparing the next action.\n[0:01:40 - 0:02:00] [0:01:40 - 0:01:42]: Hands are seen cutting a cauliflower on a wooden cutting board. On the left side, a white plate with a piece of cauliflower and a small glass container is placed near the stovetop with two visible pans on it. The background shows blue cabinets and a small section of the kitchen countertop. [0:01:43 - 0:01:45]: A man wearing a dark teal shirt is continuing to cut the cauliflower on the wooden board. The kitchen backdrop features white brick walls, a shelf with various kitchen tools, dishes, and glassware. [0:01:46 - 0:01:48]: The man stops cutting and begins to shift his attention towards the stovetop. Various items, such as cooking oil and spices, are arranged near the cutting board. [0:01:49 - 0:01:50]: The man holds a cloth and begins to clean the pan on the stove. Behind him, knives are secured to the wall-mounted magnetic strip. The kitchen shelf holds multiple plates, glasses, and decorative items. [0:01:51 - 0:01:52]: The focus shifts to the man's face as he speaks animatedly. He is still holding the cloth, and the background shows a blue wall-mounted cabinet. [0:01:53 - 0:01:55]: The man reaches for a bottle of oil and pours it into the pan on the stovetop. The kitchen's modern setup includes a countertop with cooking-related items placed nearby. [0:01:56 - 0:01:57]: The man continues to maneuver the pan, spreading the oil evenly. The white brick wall and overhead wooden shelves with kitchen essentials are visible in the background. [0:01:58 - 0:01:59]: The man discusses while holding the pan's handle, positioning it over the burner. The scene remains consistent with the kitchen's decor, featuring modern appliances and various cooking utensils.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action does the person take with the cauliflower?",
        "time_stamp": "0:01:20",
        "answer": "C",
        "options": [
          "A. He put it back on the counter.",
          "B. He wash it under the sink.",
          "C. He position it on a cutting board and cut it.",
          "D. He place it in a bowl."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_11_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:01]: A man, wearing a teal t-shirt, stands in a modern kitchen beside a stove with a countertop. The kitchen has blue cabinets and a white tile backsplash. He holds a stainless steel frying pan in his right hand, slightly angling it. Two frying pans rest on the stove burners, while various cooking utensils and ingredients are arranged around the countertop. [0:02:01 - 0:02:03]: The man bends over, placing a large, white cauliflower segment on the wooden cutting board in front of him. The frying pan he was holding is now resting on the stove. He is concentrating on positioning the cauliflower properly. [0:02:03 - 0:02:06]: The man carefully lifts the cauliflower segment and places it into one of the frying pans on the stove. His focus is on ensuring the cauliflower is placed properly in the pan. [0:02:06 - 0:02:09]: He continues to adjust the cauliflower within the frying pan to make sure it is positioned centrally and begins to season it lightly with his right hand. [0:02:09 - 0:02:11]: He steps back, briefly observing the cauliflower as it cooks in the pan. One hand is placed thoughtfully at his side while the other is still near the stove. [0:02:11 - 0:02:14]: The perspective changes to a top-down view of the stove. Two frying pans sit on the burners, and in the larger pan on the left, the cauliflower segment is visible. The man moves his hand away from the stove and to the side countertop. [0:02:14 - 0:02:16]: The man extends his hand back towards the stove, adjusting the cauliflower in the pan, ensuring it cooks evenly. [0:02:16 - 0:02:18]: He continues to manipulate the cauliflower, with a focus on the frying process. His other hand reaches off-camera, potentially reaching for another ingredient. [0:02:18 - 0:02:19]: The camera angle returns to the initial side view of the kitchen. The man moves away from the stove and prepares to work on the wooden cutting board, possibly to prepare other ingredients or seasonings. Various kitchen tools and containers surround the cutting area.\n[0:02:20 - 0:02:40] [0:02:20 - 0:02:25]: In a modern, well-equipped kitchen with blue cabinets and a white-tiled backsplash adorned with a large red \"Cook\" sign, a person is captured in the process of cooking. Their arms are visible, and He are wearing a teal shirt. On the counter, there are various bowls containing ingredients including cheese, nuts, and fresh herbs. A frying pan with food in it is positioned on the stovetop, adjacent to a wooden cutting board stacked with more ingredients, including garlic cloves. The person is seen using a chef's knife to chop garlic on the cutting board. [0:02:26 - 0:02:27]: The individual continues chopping garlic and then transfers it to the frying pan. The stovetop has multiple burners, with two frying pans placed on them. Various bottles and cooking utensils are visible in the background. [0:02:28 - 0:02:31]: The person seems to be monitoring the food in the frying pan while adjusting the heat on the burner. Their movement suggests He are ensuring the food cooks properly, using a wooden spoon to stir occasionally.  [0:02:32 - 0:02:34]: The person picks up another frying pan and continues to monitor and adjust the heat of the burners. Another portion of cheese is seen on the counter in front of two burners. [0:02:35 - 0:02:36]: He turn towards the counter, concentrating on the cooking process, looking down and moving their hands, possibly seasoning or adjusting something out of the camera's detailed view in the foreground. [0:02:37]: The individual reaches for a spice jar, likely to add to the dish He are preparing in the frying pan.  [0:02:38 - 0:02:39]: A top-down view reveals the stovetop where the frying pan continues to cook, and surrounding are prep bowls and plates including a large white plate, smaller bowls with herbs, spices and other ingredients, amidst a neatly organized cooking space.\n[0:02:40 - 0:03:00] [0:02:40 - 0:02:41]: The perspective is focused on two frying pans on a kitchen stove. The person is sprinkling a small amount of seasoning onto the food in the larger frying pan using a small container held in their left hand. The person is wearing a blue short-sleeved shirt.  [0:02:41 - 0:02:42]: The person continues to season the food, leaning slightly forward. In the background, there are blue cabinets and a double oven. [0:02:42 - 0:02:43]: The person, still leaning forward, appears to be speaking. The food in the frying pan remains visible.  [0:02:43 - 0:02:44]: A top-down view shows various ingredients arranged on the countertop, including fresh herbs in white bowls, blocks of butter, and a bottle of olive oil. The person is reaching out towards a bowl of minced garlic. [0:02:44 - 0:02:45]: The focus returns to the two frying pans on the stove from a bird’s-eye view. The larger pan contains some cauliflower and garlic cloves, while the smaller pan appears to have seeds and nuts being added by the person. [0:02:45 - 0:02:46]: Both frying pans are on the stove, with the person adding more nuts or seeds to the smaller pan. The stove's grill is visible with the two burners in use. [0:02:46 - 0:02:47]: The person, facing the camera, continues speaking while gesturing with one hand. The kitchen background features shelves with various dishes and cooking utensils. [0:02:47 - 0:02:48]: The person continues to speak, gesturing towards the stove. The kitchen background includes a \"COOK\" sign in red letters on the shelf. [0:02:48 - 0:02:49]: The person, still addressing the camera, gesticulates with both hands. A frying pan containing the cauliflower remains in focus on the stove. [0:02:49 - 0:02:50]: Continuing to speak, the person seems to emphasize a point. The stove and counter remain prominent in the foreground. [0:02:50 - 0:02:51]: The person begins to look down at the stove, still speaking. The kitchen environment, with white tiles and various items on shelves, remains unchanged. [0:02:51 - 0:02:52]: The person looks at the pans on the stove while talking, appearing focused on the cooking process. [0:02:52 - 0:02:53]: A top-down view shows the person's hand reaching for a container of seasoning near the back of the countertop. The two frying pans continue cooking the ingredients. [0:02:53 - 0:02:54]: The person's hand, holding the seasoning container, moves back towards the counter. The same top-down perspective highlights the organized cooking area. [0:02:54 - 0:02:55]: The person's hand is seasoning the food in the larger frying pan from above, concentrating on the task. [0:02:55 - 0:02:56]: The view continues to show the person seasoning the food. The two frying pans, burners, and countertop are all clearly visible from above. [0:02:56 - 0:02:57]: The view shifts to a close-up of the frying pans on the stove. The person’s hand is placing the final touches of seasoning onto the food in the larger pan. [0:02:57 - 0:02:58]: The close-up continues to show the frying pans in focus. The person’s hand, no longer in view, has finished seasoning. [0:02:58 - 0:02:59]: The close-up shows the larger frying pan with cauliflower and seasoning. The person’s hand is seen adjusting the position of the pan on the burner. [0:02:59]: The view stays focused on the frying pans. The cauliflower is cooking in the larger one, while the smaller pan continues to roast the nuts or seeds.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing with the cauliflower right now?",
        "time_stamp": "0:02:08",
        "answer": "C",
        "options": [
          "A. Cutting it.",
          "B. Seasoning it.",
          "C. Placing it in the frying pan.",
          "D. Washing it."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_11_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:01]: A spacious, well-lit kitchen is visible with a large island counter in the foreground, featuring a variety of kitchen items including a skillet on the stove. A person in a blue shirt stands behind the counter, focusing on mixing contents in a glass bowl held in their hands; [0:07:02]: The person momentarily looks away from the glass bowl and reaches their left hand towards the right side of the counter, possibly indicating He are grabbing another ingredient or tool; [0:07:03]: Now, the person is pouring a golden liquid, likely olive oil, from a bottle into the contents of the glass bowl while holding the bottle in their right hand at an angle, and the left hand supports the bowl from beneath; [0:07:04 - 0:07:07]: A close-up view of the mixing bowl shows a greenish mixture that contains various finely chopped ingredients. The spoon inside the bowl is partially submerged in the mixture. The person's blue shirt and the wooden counter form the backdrop of these frames. The spoon is slowly being stirred by a pair of hands; [0:07:08 - 0:07:12]: The person's hands continue mixing the greenish ingredients in the bowl. Different camera angles highlight the thorough motion in which the spoon is being rotated and stirred, ensuring all components inside the bowl are well combined; [0:07:13 - 0:07:16]: The camera angle shifts to a wider view, with the person now tilting the bowl slightly and appearing to keenly observe the mixture inside while stirring. The kitchen backdrop showcases shelves with various utensils and kitchen items neatly arranged; [0:07:17 - 0:07:19]: As the person continues stirring the mixture in the glass bowl, He slightly shift their position to have a better view. Their concentration remains on ensuring the mixture is well-combined, as various kitchen elements like appliances and gadgets are visible in the background.\n[0:07:20 - 0:07:40] [0:07:20 - 0:07:21]: A person wearing a teal shirt is working in a kitchen. The countertop is made of light-colored material, and a wooden cutting board is placed on it. Various kitchen utensils and ingredients, including a whisk and some small bowls, are visible close to the person. The person is holding a mixing bowl with some ingredients in it and is stirring the mixture with a spoon; [0:07:22 - 0:07:24]: The camera angle changes to a top-down view, showing the person holding a sauté pan with one hand, while cooking something that appears to be eggs. The person uses a fork to stir the contents of the pan. A dishtowel is held in the other hand. The stovetop has five black burners, and some kitchen items are visible on the side; [0:07:25 - 0:07:27]: The perspective shifts back to a side view. The person places a lid on the sauté pan, which is positioned on one of the stove burners. Steam is seen escaping as the lid is placed, but the person rapidly removes and replaces the lid again. The rest of the kitchen, including the countertop and other utensils, is slightly blurred in the background; [0:07:28 - 0:07:29]: A top-down view of the kitchen counter displays various ingredients spread out on the surface. There are some partially used herbs, sliced butter, a lemon wedge, a jar of pepper, salt, and chopped nuts all arranged around a wooden cutting board. The person is reaching towards the ingredients, moving some of the herbs; [0:07:30 - 0:07:35]: The camera shifts back to a wider view of the kitchen, capturing the person who is once again checking the sauté pan on the stovetop. The background reveals a stylish kitchen setup with blue cabinets, a white-tiled backsplash, and wooden shelves holding plates and kitchenware. The person is busy taking the lid off the pan and adding some liquid from a small container; [0:07:36 - 0:07:39]: The camera angle changes to show a broader perspective where the person is seen moving energetically around the kitchen. He are placing the lid back on the sauté pan after adding the liquid, adjusting the heat, and quickly grabbing some tools, readying themselves for the next step. The background continues showing parts of the kitchen with various utensils and ingredients scattered on the counter.\n[0:07:40 - 0:08:00] [0:07:40 - 0:07:44]: The scene begins in a kitchen with a turquoise and white color scheme and a large island countertop in the center. A person in a blue shirt is standing behind the island, next to a cutting board with various ingredients and kitchen tools. On the countertop, there are several jars, a bowl, a plate with partially eaten food, lemons, garlic, and some bottles. The background includes a brick wall with shelves holding plates, glasses, and spices, with a large red sign spelling \"COOK.\" [0:07:44 - 0:07:47]: The person appears to be in the midst of explaining something, gesturing with their hands while standing next to a stovetop with two pots. Their focus shifts toward the pot on their right-hand side, emphasizing an action He are demonstrating. [0:07:47 - 0:07:50]: He continue to convey their message, using hand gestures for emphasis. The person reaches towards the pot on the right-hand side, holding a lid which He appear to reposition, possibly to demonstrate or check the contents. [0:07:50 - 0:07:53]: As He lift the lid, steam escapes from the pot, indicating that whatever is inside is being cooked. He inspect the contents before placing the lid back down. [0:07:53 - 0:07:56]: The scene captures the same person from an aerial perspective now, focusing on the stovetop where a pan is cooking a dish that includes cauliflower and lemon slices. In the foreground, some bowls with various ingredients and spices are visible. The person uses a spoon to manipulate the ingredients in the pan. [0:07:56 - 0:07:59]: The person squeezes a lemon over the pan, adding juice to the dish. He then continue to stir the ingredients while the camera captures this meticulous culinary process. The scene focuses on the hands and the cooking pan, highlighting the detailed steps being taken.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What action does the person perform after adding the golden liquid to the bowl?",
        "time_stamp": "0:07:11",
        "answer": "A",
        "options": [
          "A. Begins stirring the mixture.",
          "B. Starts cutting vegetables.",
          "C. Places the bowl on the counter.",
          "D. Prepares another ingredient."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_11_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:05]: A person in a teal t-shirt is cooking at a kitchen island. He hold a piece of food over a frying pan with both hands, working with concentration. The kitchen has blue cabinets and a countertop filled with various utensils and ingredients, including bottles, jars, and bowls. Above the stove, there is a white brick wall with shelves holding plates and glasses. [0:08:05 - 0:08:07]: The person continues working with the food in their hands, appearing to prepare ingredients that will be used in the cooking process. Additional kitchen elements like a double oven and yellow cabinets are visible. [0:08:07 - 0:08:09]: As the person continues cooking, He hold a spoon in one hand and a cloth in the other, possibly to handle the heat from the pan. The frying pan contains yellow and green ingredients, possibly vegetables, which are being sautéed.  [0:08:09 - 0:08:11]: The camera angle shifts to an overhead view of the cooking process. The food in the frying pan is more clearly visible, showing detailed ingredients such as butter, oil, chopped pieces, and leaves. The stovetop has three burners, and the person appears to be stirring the food with a spoon. [0:08:11 - 0:08:15]: The person continues to work on the cooking from an overhead perspective, stirring and moving the ingredients around. Their movements suggest careful and precise cooking as He adjust the contents of the frying pan. [0:08:16 - 0:08:19]: The view returns to the first-person perspective, showing the person standing and focusing on the frying pan. The background includes a larger view of the kitchen's blue and white aesthetic, including various cooking tools, spices, and appliances all neatly organized on shelves and countertops.\n[0:08:20 - 0:08:40] [0:08:20 - 0:08:21]: In a kitchen, a person wearing a blue shirt is standing behind a countertop, looking down at a frying pan on a cooktop. The kitchen has a modern design with blue cabinets and shelves stocked with various kitchen items. The person appears to be cooking as He hold a utensil over the pan;  [0:08:21 - 0:08:22]: The person continues to cook using a utensil to stir the contents in the pan. The background includes hanging knives, plates, and jars arranged neatly on shelves; [0:08:22 - 0:08:24]: Standing straight, the individual takes a brief moment to wipe their hands using a towel. The cooktop remains active with the pan on it; [0:08:23 - 0:08:24]: The person reaches towards a small container on a wooden cutting board placed on the countertop, probably to grab an ingredient; [0:08:24 - 0:08:25]: A close-up view shows the individual's head and shoulders with a focused expression. The background is slightly blurred; [0:08:25]: The person's hand is visible sprinkling a small amount of white substance, likely salt, over the frying pan; [0:08:26 - 0:08:27]: An overhead view reveals the frying pan cooking eggs with herbs. The countertop showcases a wooden board, a dish, and small bowls containing various ingredients; [0:08:27 - 0:08:28]: The person continues to sprinkle seasoning into the pan while maintaining focus on the cooking process from an overhead perspective; [0:08:28 - 0:08:29]: Using a utensil, the individual tends to the food cooking in the pan. The eggs appear to be halfway cooked; [0:08:29 - 0:08:30]: The overhead view shows the person adjusting the food in the pan perhaps to ensure even cooking. The stove has three burners, and only one of them is in use; [0:08:30 - 0:08:31]: The person reaches for a small jar containing seasoning, situated on the countertop near the stove as He continue cooking; [0:08:31 - 0:08:32]: The person's face is visible as He focus on the task, their expression suggesting concentration. The background remains slightly out of focus, showing parts of the kitchen; [0:08:32 - 0:08:33]: The individual has a determined look and continues to prepare the dish. The blue cabinets are clearly visible in the background; [0:08:33 - 0:08:34]: Another close-up shows the person bending slightly forward, their attention still fixed on the pan on the cooktop. Kitchen equipment is lightly blurry in the background; [0:08:34 - 0:08:35]: The person continues cooking, stirring the contents of the pan carefully using a kitchen utensil. The stovetop remains active; [0:08:35 - 0:08:36]: From the side, the camera captures the person using a spoon to handle the ingredients in the frying pan. Various kitchen tools and ingredients are visible next to the stove; [0:08:36 - 0:08:37]: The individual picks up a piece of food, possibly to taste or adjust the seasoning. The blue cabinetry remains a prominent part of the background; [0:08:37 - 0:08:38]: The person carefully lifts a spoonful of food from the pan, holding it above the cooktop. Kitchen utensils and containers are seen around the active stovetop; [0:08:38 - 0:08:39]: The individual continues to handle the food with a spoon, making sure to adjust the contents in the frying pan.\n[0:08:40 - 0:09:00] [0:08:40 - 0:08:43]: A person is seen cooking in a modern kitchen. He are holding a spoon and squeezing a lemon over a frying pan placed on a stove. The frying pan contains food that appears to be cooking. Several kitchen utensils and ingredients are placed on the counter beside the stove. The countertop is light-colored, and the stove has multiple burners. [0:08:44]: The person steps back slightly, and the frying pan remains on the stove with the food inside it continuing to cook. More kitchen utensils and various items are visible on the counter. [0:08:45 - 0:08:46]: The camera angle shifts to show more of the kitchen. The person is seen wearing a blue shirt and appears to be stirring something in a clear bowl on the counter. Behind them, blue cabinets, shelves with dishes and kitchen tools, and a red sign that says \"Cook\" on a white-tiled wall are visible. [0:08:47 - 0:08:48]: The person continues to mix the ingredients in the bowl and then turns their head to the left, possibly looking at something in the distance. Their expression seems focused as He hold the mixing bowl in one hand and a whisk in the other. [0:08:49]: The person leans over the stove and uses a kitchen utensil to check or stir the contents of the frying pan. He use the utensil with a precise movement, ensuring the food cooks evenly. [0:08:50 - 0:08:54]: The person focuses on the frying pan, using a utensil to manipulate the food. He methodically stir and adjust, ensuring everything cooks properly. Various tools and jars are visible on the counter behind the stove. [0:08:55 - 0:08:57]: A close-up view shows the person further interacting with the food in the frying pan, making sure it's cooked to their satisfaction. The person’s facial expression is focused as He carefully handle the food. [0:08:58 - 0:08:59]: The scene shifts back to a wider shot of the person in the kitchen. He move away from the stove, placing the frying pan on a different surface. The background shows neatly arranged dishes on shelves and a clean, organized cooking environment.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person holding right now?",
        "time_stamp": "0:08:22",
        "answer": "C",
        "options": [
          "A. A knife and a bowl.",
          "B. A spoon and a knife.",
          "C. A spoon and a towel.",
          "D. A frying pan and a spatula."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_11_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: A person in a green t-shirt stands in a kitchen with white brick walls. There are shelves in the background filled with plates, cups, and various kitchen utensils, including knives and cutting boards. [0:00:01 - 0:00:06]: The individual starts speaking, changing facial expressions and gestures. They clasp their hands and use expressive hand movements while talking. Shelves, filled with more kitchen items in organized arrangements, are visible behind them. [0:00:06 - 0:00:11]: The person's hands come together in front of them as they continue their animated speech. Their facial expressions and hand gestures suggest an enthusiastic or instructional tone. The background remains the same. [0:00:11 - 0:00:13]: The individual continues to speak while changing facial expressions and hand gestures. The setting stays consistent with the kitchen in the background. [0:00:14 - 0:00:14]: A graphic screen appears featuring the number \"10\" and a stylized fork and knife, set against a blue background. [0:00:15 - 0:00:17]: The same person, now wearing a different dark-colored t-shirt, appears in front of a blue background with the words \"RAMSAY in 10\" prominently displayed. The individual continues smiling, suggesting a positive and engaging expression. [0:00:18 - 0:00:18]: A transition effect partially obscures the scene showing white and blue shapes, preparing to reveal the next frame. [0:00:19 - 0:00:20]: The individual is now seen in a different setting, still within a kitchen environment but with a darker tone and different shirt. They continue to use hand gestures and speak as before, with neatly organized kitchen items and utensils visible in the background.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What objects can be seen on the shelves in the background of the kitchen?",
        "time_stamp": "0:00:20",
        "answer": "B",
        "options": [
          "A. plants.",
          "B. Plates, cups, and kitchen utensils.",
          "C. Cans and jars.",
          "D. Pots and pans."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_29_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:01]: The video shows a kitchen scene, shot from a first-person perspective. A person is preparing a steak. On the countertop, there are various kitchen utensils, including a plate with raw steak and a mortar and pestle. The person takes a white plate from a stack on the counter. [0:02:01 - 0:02:02]: The camera angle changes slightly. The steak is marinated and resting on a wooden cutting board. The person places down the white plate and picks up a bottle of oil with their right hand. [0:02:02 - 0:02:03]: The individual continues to handle the oil bottle, moving it above a bowl placed next to the steak. [0:02:03 - 0:02:04]: The person pours oil from the bottle into the bowl. The steak is seen resting on the cutting board with its marinade. [0:02:04 - 0:02:06]: The top-down shot reveals the person's hand pouring a dark liquid over the steak. On the countertop are eggs in a bowl, a bottle of oil, and other ingredients laid out for cooking. [0:02:06]: The dark liquid is continuously being poured over the steak from the bottle. [0:02:07 - 0:02:08]: The scene shifts focus to the person moving around the kitchen. Behind them, the kitchen wall features white subway tiles and several shelves with bowls, plates, and jars. [0:02:08 - 0:02:09]: The individual picks up another container from the counter. Various kitchen components such as frying pans, knives, and more ingredients are present around them. [0:02:09 - 0:02:10]: Continuing from the previous action, the person opens the container, adding another ingredient to the steak preparation. [0:02:10 - 0:02:11]: The person appears to taste a small amount of seasoning with their hand. The kitchen background remains unchanged, displaying shelves and various kitchen items. [0:02:11]: The person makes a slight reaction, perhaps indicating their impression of the seasoning taste. [0:02:12 - 0:02:13]: The individual continues with the seasoning. The kitchen setup includes organized shelves and visible ingredients on the countertop. [0:02:13]: The camera angle returns to a close-up of the person pouring a purple sauce from a bottle into a bowl. The marinade process continues with various ingredients visible around. [0:02:14 - 0:02:15]: The person adds the purple sauce from a bottle into a small bowl next to the steak. The surrounding countertop area is cluttered with additional ingredients like cherry tomatoes, eggs, and oil. [0:02:15 - 0:02:16]: The focus zooms in on the steak while the person proceeds with the preparation steps, placing the sauce bottle on the counter. [0:02:17]: The person positions themselves to the right of the counter, indicating preparation to move forward with the cooking process. [0:02:18 - 0:02:19]: The camera's aerial view shows the workstation with two frying pans on a stove, a cutting board, and various cooking essentials prepped for the meal.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person holding in their right hand?",
        "time_stamp": "0:02:05",
        "answer": "A",
        "options": [
          "A. A bottle of seasoning.",
          "B. A white plate.",
          "C. A mortar and pestle.",
          "D. A cutting board."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_29_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:03]: Several pans are positioned on a stovetop. A person's hands are seen picking up a small bowl with some ingredients in a kitchen surrounded by utensils, spices, and other cooking items. The person scoops some of the ingredients, which appear to be thinly sliced vegetables, from the bowl using black tongs and begins to place them into one of the heated pans. The stovetop has a mix of metal and nonstick pans arranged in a row. [0:04:04 - 0:04:07]: The person turns slightly, momentarily revealing a neatly arranged kitchen background with a white brick wall, shelves containing various kitchen items, and a prominent red \"COOK\" sign. They move back to the counter holding a white cloth. [0:04:08 - 0:04:10]: The person's hands are again seen as they neatly fold a white kitchen towel. The countertop is cluttered with more kitchen tools, bowls with ingredients, a bottle of olive oil, and a gray-colored root vegetable. [0:04:11 - 0:04:13]: The person uses the folded towel to lift more ingredients, potentially pieces of cooked food from the small bowl and transfers them into one of the pans on the stovetop. The other pans contain a piece of meat and what appears to be sautéed vegetables. [0:04:14 - 0:04:16]: Viewed from above, the three frying pans on the stovetop are clearly visible. One contains a piece of meat, another has some vegetable mixture, and the third is currently empty. The person's hand is seen holding the folded towel while they adjust the contents of the pan. [0:04:17 - 0:04:19]: The camera captures a close-up view of the pans on the stovetop with the person using a spoon to stir and flip the contents in one of the pans. The focus is on the cooking process, and steam rises from the pan, indicating heat.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the prominent \"COOK\" sign in the kitchen background?",
        "time_stamp": "00:04:07",
        "answer": "D",
        "options": [
          "A. Blue.",
          "B. Green.",
          "C. Yellow.",
          "D. Red."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:04:07",
        "answer": "B",
        "options": [
          "A. Scooping ingredients from a bowl.",
          "B. Folding a white kitchen towel.",
          "C. Adjusting the heat on the stovetop.",
          "D. Washing their hands."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_29_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:04]: A person is seen standing in a well-organized kitchen, which has a backsplash made up of white tiles and a light gray countertop. The person is wearing a dark gray T-shirt and is positioned behind a counter that is filled with various bowls, plates, and ingredients. In the background, there are shelves containing bottles, kitchen utensils, and decorative items such as a red sign that reads \"COOK.\"  [0:08:04 - 0:08:05]: The view changes to an overhead shot revealing three frying pans on the stove. The pan on the left contains a piece of meat that appears to be cooking, the middle pan is filled with rice mixed with green herbs, and the right pan has two sunny-side-up eggs. [0:08:05 - 0:08:08]: The person’s hands are visible from a top view, interacting with the food. Close attention is given to handling the food in the right pan, perhaps seasoning or adding ingredients to the eggs. [0:08:09 - 0:08:10]: The hands continue to work, refining the dishes in the pans. There is precise movement indicating experienced handling of cooking procedures. [0:08:11 - 0:08:12]: The camera perspective shifts back to the person standing behind the cooking station. The person is engaged, animatedly gesturing with their hands while explaining or talking about something presumably related to the cooking process. [0:08:13 - 0:08:14]: The person then moves towards the right side of the counter, appearing to reach for or place an item on the counter. [0:08:15 - 0:08:16]: The view returns to the aerial shot of the stovetop where the person is now moving the food in the pans, specifically tending to the eggs and adding finishing touches. [0:08:17 - 0:08:18]: The person is seen using a spatula to lift and possibly flip or arrange the food carefully, ensuring that everything is cooked properly and presented well. [0:08:19]: The attention to detail in handling the food is evident as the person continues to work on perfecting the dishes, the eggs being the focal point of this sequence.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What tool does the person use to handle the food in the pans?",
        "time_stamp": "00:08:18",
        "answer": "C",
        "options": [
          "A. A spoon.",
          "B. A fork.",
          "C. A spatula.",
          "D. Tongs."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_29_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:18",
        "answer": "A",
        "options": [
          "A. 0.",
          "B. 2.",
          "C. 3.",
          "D. 4."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_85_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:07",
        "answer": "A",
        "options": [
          "A. 2.",
          "B. 5.",
          "C. 6.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_85_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:02",
        "answer": "A",
        "options": [
          "A. 4.",
          "B. 2.",
          "C. 3.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_85_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:01",
        "answer": "A",
        "options": [
          "A. 7.",
          "B. 2.",
          "C. 5.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_85_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:06:55",
        "answer": "A",
        "options": [
          "A. 8.",
          "B. 5.",
          "C. 4.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_85_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:01]: The video begins with a view of a brightly lit room from a first-person perspective. A person standing in the room, dressed in dark clothing, is smiling and looking towards the camera. In the background, there is a large window that lets in ample daylight, casting light across the room. [0:03:01 - 0:03:02]: The camera pans right to reveal a spacious kitchen area. A large marble countertop occupies the foreground, with various kitchen items scattered on its surface, including bottles, a cutting board, and some utensils. To the left, part of a stovetop is visible. [0:03:02 - 0:03:03]: The camera continues to pan, showing a person standing behind the countertop, wearing a dark shirt. The person seems engaged in preparing something, as utensils and ingredients are placed on the counter nearby. [0:03:03 - 0:03:04]: The view is now centered on the person behind the counter. The person appears to be preparing food as they hold a utensil. The background includes white-painted walls and shelves filled with kitchen items. [0:03:04 - 0:03:05]: The frame now reveals a better view of the kitchen. Ample light illuminates the space, coming from a light fixture above. The person behind the counter is busy with the preparations and occasionally looks towards the camera. [0:03:05 - 0:03:06]: A closer view of the person behind the counter is shown. The person is focused on the task, holding a kitchen towel in one hand while standing near the stove with various kitchen objects around them, such as bottles and containers. [0:03:06 - 0:03:07]: The camera remains focused on the person behind the counter, who appears to be explaining or talking about something, holding the kitchen towel. Surrounding the person, the kitchen shelves are visible with various items neatly arranged. [0:03:07 - 0:03:08]: The person in the frame continues speaking, while the camera angle gives a clear view of the countertop, which has kitchen ingredients, utensils, and a mixer. The cabinets behind have glass doors revealing neatly arranged dishes and glasses. [0:03:08 - 0:03:09]: The person gestures with their hands while continuing to speak. The neat and tidy kitchen showcases more items, including a mixer, a coffee maker, and several containers, all placed on the countertop while cabinets with glass panels stand in the background. [0:03:09 - 0:03:10]: The person in the frame continues to talk with an expressive gesture, and the kitchen setup remains clearly visible in the background. [0:03:10 - 0:03:11]: The person becomes more animated, gesturing with both hands slightly raised. Cabinets with glass doors display a collection of glassware and dishes in the background. [0:03:11 - 0:03:12]: The camera view shifts slightly, but the focus remains on the person behind the counter who seems to be passionate about the topic they are discussing. The backdrop remains the same with organized kitchen shelves. [0:03:12 - 0:03:13]: The person continues to speak with a serious expression, possibly emphasizing a point. The frame continues to show the aesthetically pleasing kitchen layout, with its organized shelves and cabinets. [0:03:13 - 0:03:14]: The person has a more intense expression and brings their hands together as they continue talking. Books and other items stored in the cabinets to the right are now more visible. [0:03:14 - 0:03:15]: The camera zooms in slightly, capturing the person leaning towards the countertop, still talking. The background remains consistent with the white-tiled walls and shelves filled with various kitchen utilities. [0:03:15 - 0:03:16]: The person remains leaning forward, with their hands on the countertop, continuing to speak. The camera maintains a stable view of the kitchen backdrop, with items on the shelf and countertop clearly visible. [0:03:16 - 0:03:17]: Now the camera is slightly out of focus but still captures the person leaning forward. The marble countertop and kitchen appliances like the mixer and containers are partially visible. [0:03:17 - 0:03:18]: The person in the frame carries on the conversation, smiling slightly, with a more relaxed posture. Surrounding them, the kitchen continues to appear organized and clean, with various items neatly arranged. [0:03:18 - 0:03:19]: The person seems to be finishing their point, with a lighter expression and the kitchen once again appearing organized and well-lit in the background",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the main activity occurring in the kitchen during the whole video?",
        "time_stamp": "00:03:10",
        "answer": "A",
        "options": [
          "A. Cooking demonstration.",
          "B. Kitchen cleaning.",
          "C. Family dinner.",
          "D. Food tasting."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_30_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:01]: The video begins with a view of a brightly lit room from a first-person perspective. A person standing in the room, dressed in dark clothing, is smiling and looking towards the camera. In the background, there is a large window that lets in ample daylight, casting light across the room. [0:03:01 - 0:03:02]: The camera pans right to reveal a spacious kitchen area. A large marble countertop occupies the foreground, with various kitchen items scattered on its surface, including bottles, a cutting board, and some utensils. To the left, part of a stovetop is visible. [0:03:02 - 0:03:03]: The camera continues to pan, showing a person standing behind the countertop, wearing a dark shirt. The person seems engaged in preparing something, as utensils and ingredients are placed on the counter nearby. [0:03:03 - 0:03:04]: The view is now centered on the person behind the counter. The person appears to be preparing food as they hold a utensil. The background includes white-painted walls and shelves filled with kitchen items. [0:03:04 - 0:03:05]: The frame now reveals a better view of the kitchen. Ample light illuminates the space, coming from a light fixture above. The person behind the counter is busy with the preparations and occasionally looks towards the camera. [0:03:05 - 0:03:06]: A closer view of the person behind the counter is shown. The person is focused on the task, holding a kitchen towel in one hand while standing near the stove with various kitchen objects around them, such as bottles and containers. [0:03:06 - 0:03:07]: The camera remains focused on the person behind the counter, who appears to be explaining or talking about something, holding the kitchen towel. Surrounding the person, the kitchen shelves are visible with various items neatly arranged. [0:03:07 - 0:03:08]: The person in the frame continues speaking, while the camera angle gives a clear view of the countertop, which has kitchen ingredients, utensils, and a mixer. The cabinets behind have glass doors revealing neatly arranged dishes and glasses. [0:03:08 - 0:03:09]: The person gestures with their hands while continuing to speak. The neat and tidy kitchen showcases more items, including a mixer, a coffee maker, and several containers, all placed on the countertop while cabinets with glass panels stand in the background. [0:03:09 - 0:03:10]: The person in the frame continues to talk with an expressive gesture, and the kitchen setup remains clearly visible in the background. [0:03:10 - 0:03:11]: The person becomes more animated, gesturing with both hands slightly raised. Cabinets with glass doors display a collection of glassware and dishes in the background. [0:03:11 - 0:03:12]: The camera view shifts slightly, but the focus remains on the person behind the counter who seems to be passionate about the topic they are discussing. The backdrop remains the same with organized kitchen shelves. [0:03:12 - 0:03:13]: The person continues to speak with a serious expression, possibly emphasizing a point. The frame continues to show the aesthetically pleasing kitchen layout, with its organized shelves and cabinets. [0:03:13 - 0:03:14]: The person has a more intense expression and brings their hands together as they continue talking. Books and other items stored in the cabinets to the right are now more visible. [0:03:14 - 0:03:15]: The camera zooms in slightly, capturing the person leaning towards the countertop, still talking. The background remains consistent with the white-tiled walls and shelves filled with various kitchen utilities. [0:03:15 - 0:03:16]: The person remains leaning forward, with their hands on the countertop, continuing to speak. The camera maintains a stable view of the kitchen backdrop, with items on the shelf and countertop clearly visible. [0:03:16 - 0:03:17]: Now the camera is slightly out of focus but still captures the person leaning forward. The marble countertop and kitchen appliances like the mixer and containers are partially visible. [0:03:17 - 0:03:18]: The person in the frame carries on the conversation, smiling slightly, with a more relaxed posture. Surrounding them, the kitchen continues to appear organized and clean, with various items neatly arranged. [0:03:18 - 0:03:19]: The person seems to be finishing their point, with a lighter expression and the kitchen once again appearing organized and well-lit in the background",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the person behind the counter doing?",
        "time_stamp": "00:04:36",
        "answer": "B",
        "options": [
          "A. Cleaning the countertop.",
          "B. Preparing food.",
          "C. Washing dishes.",
          "D. Reading a recipe."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_30_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:02]: The scene opens with a view of a kitchen. A person is seen frying sausages in a pan that’s placed on a gas stove. There are two frying pans, one with sausages and the other empty. The kitchen counter in the background has various objects like a mixer, containers, and a paper towel roll. The stove is built into a kitchen island;  [0:06:03]: The person is gesturing with their hands in a conversational manner and seems to be speaking. Their attention is towards the camera; [0:06:04]: The person leans slightly to their right but continues talking, indicating ongoing communication; [0:06:05 - 0:06:16]: The person begins chopping a red onion on a cutting board. The kitchen setup remains consistent, with hints of other ingredients and cooking tools nearby. The person continues to slice the onion with deliberate motions, paying careful attention to the task. They make steady cuts through the onion, resulting in multiple uniform slices. As the chopping continues, the pot on the stovetop next to them is visible, with some steam rising from it, suggesting something is cooking inside. The counter has different items, including a bottle of oil, and a bowl containing yellow chunks, possibly other ingredients for the recipe. The person periodically looks down at the onion, ensuring precise cuts; [0:06:17]: The person momentarily stops chopping, brings their left hand closer to the cuts, and starts adjusting the slices of the red onion, lining them up while having their right hand on the knife; [0:06:18]: The individual reaches to the left, possibly to grab something, as their hand extends away from the cutting area. The knife remains on the chopping board, indicating they're pausing momentarily from the chopping task; [0:06:19]: They move back to the gas stove with the lid-covered pot prominently placed on one of the active burners. The counter next to the stove is cluttered with various cooking items, including a pink spatula, which the person seems to focus on, possibly to use it next.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What does the person do after finishing chopping the onion?",
        "time_stamp": "00:06:32",
        "answer": "A",
        "options": [
          "A. Put the chopped onions into the frying pan.",
          "B. Starts frying sausages.",
          "C. Begins chopping another vegetable.",
          "D. Adds the onion to the pot."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_30_real.mp4"
  },
  {
    "time": "[0:12:00 - 0:13:00]",
    "captions": "[0:12:00 - 0:12:20] [0:12:00 - 0:12:04]: A pair of hands is seen vigorously stirring a pot of yellow mixture on a stovetop. The person is using a pink spatula and a metal spoon. The person is wearing a dark blue shirt and a black watch on the left wrist. The stove has silver burners and is set against a background of dark-colored kitchen drawers with silver handles.  [0:12:04 - 0:12:11]: The person continues to stir the mixture. At one point, they pour a green liquid from a bottle into the pot. The mixture appears to be thickening as it is heated. The blue flame beneath the pot can be seen. The spatula and stirring motion remain consistent, ensuring that the mixture is well combined.  [0:12:11 - 0:12:19]: The person continues to work with the mixture in the pot. The stirring is thorough, and the mixture becomes smoother and more uniform in color. The pot is occasionally tilted, and the spatula is used to scrape the sides of the pot. The nearby countertop remains visible with a spoon resting on it. The person's movements are steady and deliberate.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person using to stir the pot of yellow mixture?",
        "time_stamp": "00:12:12",
        "answer": "C",
        "options": [
          "A. A wooden spoon.",
          "B. A metal fork.",
          "C. A pink spatula.",
          "D. A blue spatula and a plastic spoon."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Event Understanding",
        "question": "How does the mixture change as the person continues to stir it?",
        "time_stamp": "00:12:19",
        "answer": "C",
        "options": [
          "A. It becomes chunkier.",
          "B. It separates into layers.",
          "C. It becomes smoother and more uniform in color.",
          "D. It loses its color."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_30_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions that were just taken?",
        "time_stamp": "00:00:10",
        "answer": "D",
        "options": [
          "A. The individual brewed tea, poured it into a teacup, and added milk.",
          "B. The individual steamed milk, poured it into a jug, and checked its temperature.",
          "C. The individual made coffee, poured it into a cup, and added sugar.",
          "D. The individual steamed milk, transferred it between jugs."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_357_real.mp4"
  },
  {
    "time": "[0:02:06 - 0:02:16]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:02:16",
        "answer": "D",
        "options": [
          "A. The individual prepared a sandwich, added condiments, and served it on a plate.",
          "B. The individual washed plates, dried them, and placed them in a cupboard.",
          "C. The individual cooked pasta, drained it, and added sauce.",
          "D. The individual cleaned a coffee station, wiped the counter, and organized items."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_357_real.mp4"
  },
  {
    "time": "[0:04:12 - 0:04:22]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:04:20",
        "answer": "D",
        "options": [
          "A. The individual made a cup of tea, added honey, and stirred it.",
          "B. The individual brewed a pot of coffee, poured it into a thermos, and placed it on the counter.",
          "C. The individual cleaned a coffee cup, poured milk, and organized the counter.",
          "D. This person cleaned the handle filter and placed it on the coffee machine."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_357_real.mp4"
  },
  {
    "time": "[0:06:18 - 0:06:28]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:06:28",
        "answer": "D",
        "options": [
          "A. The individual made coffee, served it to a customer, and cleaned the table.",
          "B. The individual prepared tea, added sugar, and arranged cups on a tray.",
          "C. The individual cooked food, plated it, and cleaned the kitchen.",
          "D. The individual cleaned a weighing scale, organized the countertop."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_357_real.mp4"
  },
  {
    "time": "[0:08:24 - 0:08:34]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:08:34",
        "answer": "D",
        "options": [
          "A. The individual brewed coffee, added milk, and stirred it.",
          "B. The individual steamed milk, poured it into a jug, and served it.",
          "C. The individual prepared tea, added sugar, and served it in a teapot.",
          "D. The individual cleaned a portafilter, filled another portafilter with coffee grounds and weighted it."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_357_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:18",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_86_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:42",
        "answer": "A",
        "options": [
          "A. 3.",
          "B. 5.",
          "C. 6.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_86_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:02",
        "answer": "C",
        "options": [
          "A. 2.",
          "B. 4.",
          "C. 3.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_86_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:01",
        "answer": "C",
        "options": [
          "A. 2.",
          "B. 5.",
          "C. 4.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_86_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:06:19",
        "answer": "D",
        "options": [
          "A. 4.",
          "B. 8.",
          "C. 6.",
          "D. 5."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_86_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What topics might the speaker explain next?",
        "time_stamp": "00:00:16",
        "answer": "B",
        "options": [
          "A. Simplifying fractions.",
          "B. Adding fractions.",
          "C. Dividing fractions.",
          "D. Subtracting fractions."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_213_real.mp4"
  },
  {
    "time": "[0:00:53 - 0:01:23]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:02:00",
        "answer": "B",
        "options": [
          "A. How to multiply fractions.",
          "B. How to add 1/2 + 1/2.",
          "C. How to divide fractions.",
          "D. How to subtract fractions."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_213_real.mp4"
  },
  {
    "time": "[0:01:46 - 0:02:16]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker suggest next?",
        "time_stamp": "00:02:39",
        "answer": "D",
        "options": [
          "A. How to multiply fractions.",
          "B. How to find a common denominator for addition.",
          "C. How to simplify fractions.",
          "D. How to subtract fractions."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_213_real.mp4"
  },
  {
    "time": "[0:02:39 - 0:03:09]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:03:04",
        "answer": "C",
        "options": [
          "A. How to add two fractions with different denominators.",
          "B. How to convert fractions to decimals.",
          "C. Why the denominators stay the same.",
          "D. How to simplify the resulting fraction."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_213_real.mp4"
  },
  {
    "time": "[0:03:32 - 0:04:02]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:04:01",
        "answer": "D",
        "options": [
          "A. How to convert fractions to decimals.",
          "B. Steps to compare fractions.",
          "C. Methods to simplify fractions.",
          "D. How to find a common denominator."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_213_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video begins with a black frame. [0:00:01 - 0:00:07]: A block crane constructed from various colored and sized blocks is shown. The crane's base is yellow with blue wheels, and it has orange, green, and blue sections leading to a long green arm pointing upward at an angle. A string is connected to the end of the arm, holding an orange block that is suspended in the air. [0:00:08 - 0:00:19]: The background is a solid light grey color. The crane arm's position and the suspended orange block gradually change. Initially, the orange block is close to the crane base. As the video progresses, the crane arm moves, causing the orange block to lift and swing gently in the air.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What happens to the orange block as the video progresses?",
        "time_stamp": "0:00:19",
        "answer": "C",
        "options": [
          "A. It falls to the ground.",
          "B. It remains stationary.",
          "C. It lifts and swings gently.",
          "D. It changes color."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What are the colors of the crane arm?",
        "time_stamp": "0:00:07",
        "answer": "A",
        "options": [
          "A. Yellow, green.",
          "B. Yellow, blue.",
          "C. Red, green, blue.",
          "D. Yellow, blue, green."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_208_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:20 - 0:01:40] [0:01:20 - 0:01:24]: A pair of hands is seen interacting with a colorful structure made of interlocking building blocks. The structure mainly consists of yellow blocks with a green middle section and a blue top layer. The hands are adjusting and placing the blocks on a flat white surface. [0:01:25 - 0:01:26]: The hands momentarily move away, and the structure remains centered on the frame. The background is a plain light color. [0:01:27 - 0:01:28]: The right hand reappears, placing another piece, a yellow block, onto the surface. The block structure is to the left side of the frame while a new block is set down closer to the front right corner. [0:01:29 - 0:01:33]: The previously placed yellow block remains on the surface slightly to the right. The primary structure stays central in the background. The right hand hovers near the block, briefly touching it. [0:01:34 - 0:01:35]: The hand brings in a green block piece from the top right, positioning it next to the yellow block. The main structure is still visible in the background. [0:01:36 - 0:01:39]: The right hand then arranges the yellow and green blocks together, creating a small base that mimics the pattern of the larger structure in the background. The hands continue to adjust the small base, ensuring the pieces fit together securely.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What was the sequence of actions performed with the blocks just now?",
        "time_stamp": "00:01:36",
        "answer": "D",
        "options": [
          "A. The hand first places a green block, then a yellow block, followed by a blue block.",
          "B. The hand builds a structure with red, green, and blue blocks.",
          "C. The hand stacks blocks randomly.",
          "D. The hand places a yellow block, moves it, then places a green block next to it."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_208_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:41]: The video features a set of colorful plastic building blocks arranged on a flat surface. There are predominantly yellow, blue, and green blocks forming a complex shape towards the back, and a smaller structure in the foreground. The smaller structure in the foreground has a blue piece attached to the top of a yellow base. [0:02:42 - 0:02:46]: A hand holding an orange block is seen adding this block to the smaller structure in the foreground. The hand moves the orange block into place, stacking it on top of the yellow and blue pieces. [0:02:47 - 0:02:48]: The smaller structure is moved out of view, leaving only the larger, complex-shaped structure made of yellow, blue, and green blocks in the background. [0:02:49 - 0:02:51]: The hand reappears with the smaller structure, arranging it next to the larger structure. The hand then picks up another orange block. [0:02:52 - 0:02:53]: The hand places the new orange block beside the smaller structure, and then picks up an additional yellow block. [0:02:54 - 0:02:56]: The objects, including both the complex and smaller structures, are shown along with the newly added orange block in the foreground. The hand is seen holding the yellow block and preparing to position it on top of the orange block. [0:02:57 - 0:02:59]: The yellow block is placed onto the orange block in the foreground, and the hand moves away, showing a neatly arranged formation of blocks in various structures.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What happens to the smaller structure after it is first moved out of view?",
        "time_stamp": "00:02:51",
        "answer": "D",
        "options": [
          "A. It is disassembled.",
          "B. It is placed on top of the larger structure.",
          "C. It is left in the foreground.",
          "D. It is placed back beside the larger structure."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_208_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:01]: A hand is holding a green rectangular block in the center of the white surface. In the background, there are various block structures made of colorful interlocking bricks. The structures are primarily yellow, with blue and green accents.  [0:04:01 - 0:04:03]: The hand slightly adjusts the green block, raising it while maintaining a similar grip. The background structures remain unchanged.  [0:04:03 - 0:04:05]: The green block is placed upright on the white surface, and the hand moves away. The focus stays on the green block.  [0:04:05 - 0:04:06]: The hand is seen bringing another green block towards the previously placed upright green block. [0:04:06 - 0:04:07]: The hand places the new green block on top of the first green block, stacking them.  [0:04:07 - 0:04:08]: The stacked green blocks remain stable, and the hand begins to move back. [0:04:08 - 0:04:09]: The hand adjusts the position of the top green block to ensure it is securely stacked. [0:04:09 - 0:04:11]: The hand lifts away from the stack, and the two green blocks remain upright.  [0:04:11 - 0:04:13]: A new green block is brought in by the same hand. The hand positions the new block above the already stacked green blocks. [0:04:13 - 0:04:15]: This new green block is placed on top of the existing stack. The hand ensures it is aligned properly.  [0:04:15 - 0:04:17]: The stacked green blocks are left upright as the hand moves away again.  [0:04:17 - 0:04:18]: The hand returns with another green block and aligns it to be stacked on top of the previous three blocks. [0:04:18 - 0:04:19]: The hand securely places the block on top, creating a stack of four green blocks.",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many green blocks are stacked now?",
        "time_stamp": "00:04:25",
        "answer": "C",
        "options": [
          "A. Ten.",
          "B. Three.",
          "C. Nine.",
          "D. Eight."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_208_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:04]: A close-up view of an artwork in an art gallery, featuring a white chicken with a red comb and a yellow chick standing under a tree branch with white blossoms. The artwork is framed, and a label with details is mounted on the wall next to it. [0:00:05 - 0:00:08]: The camera view shifts to include additional art pieces on either side of the initial artwork. A large, colorful piece featuring plants and another white chicken with a black tail is now visible to the right of the initial piece. [0:00:09 - 0:00:10]: The view continues to shift right, revealing more of the colorful artwork. Another piece with a white chicken and a vase filled with green and colorful flowers starts to come into view. [0:00:11 - 0:00:13]: The camera focuses on the second piece showing a white chicken standing beneath a large vase filled with green and colorful flowers against a black background. The artwork also has a label with details. [0:00:14 - 0:00:16]: The view shifts further right to two more abstract artworks mounted on a white wall. The top artwork has an orange pattern resembling a brain against a blue and white background. The bottom artwork features abstract shapes in green, yellow, and orange colors. [0:00:17 - 0:00:19]: The camera moves to capture a group of people gathered in front of a larger, vibrant piece of artwork in a different section of the gallery. The walls are painted blue, and there is signage for \"GALLERY CHAMAN\" visible. The room has bright lighting and a contemporary atmosphere.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the main subject in the initial artwork shown in the gallery?",
        "time_stamp": "00:00:04",
        "answer": "A",
        "options": [
          "A. A white chicken with a red comb.",
          "B. A black chicken with a white tail.",
          "C. A vase with green flowers.",
          "D. An abstract orange pattern."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What are the predominant colors in the bottom abstract artwork shown right now?",
        "time_stamp": "00:00:16",
        "answer": "B",
        "options": [
          "A. Blue, white, and orange.",
          "B. Green, yellow, blue and orange.",
          "C. Black, white, and green.",
          "D. Red, blue, and yellow."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Text-Rich Understanding",
        "question": "What signage is visible in the gallery right now?",
        "time_stamp": "00:00:19",
        "answer": "A",
        "options": [
          "A. \"GALLERY CHAMAN\".",
          "B. \"ART GALLERY\".",
          "C. \"CONTEMPORARY ART\".",
          "D. \"EXHIBITION HALL\"."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_474_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:46]: The video starts by showing two framed artworks on a white wall. The smaller frame on the left appears to have an abstract green image, and the larger frame on the right contains a collage of various houses with clear, blue skies. The right artwork is observed in more detail, and the frame includes eight different images of houses arranged in a grid format, with the title \"Kathleen Tammel Handel\" beside it. [0:04:46 - 0:04:50]: The camera then shifts focus from the artwork to a blue sign on the right side. The sign reads \"SONG WORD ART HOUSE 1307, LOS ANGELES, CA\" with \"MODERN - CONTEMPORARY\" written beneath it. The ceiling of the room and some fixtures are visible as the camera tilts upward. [0:04:50 - 0:04:56]: The camera tilts down and rotates to the right, revealing a large colorful painting featuring a collage of various images and texts, including a prominent figure wearing sunglasses, and vibrant splashes of red, green, yellow, and blue. The background of the painting has a mix of abstract elements and recognizable objects within it. [0:04:56 - 0:05:00]: Focus then shifts to a different section of the wall, which displays a black and white architectural drawing and several small placards with text and QR codes. There is a series of captions and descriptions for different artworks on display, with detailed write-ups and additional visual content concerning cities and urban settings.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is depicted in the larger framed artwork right now?",
        "time_stamp": "00:04:46",
        "answer": "A",
        "options": [
          "A. A collage of various houses.",
          "B. A single house with a red roof.",
          "C. An abstract green image.",
          "D. A black and white portrait."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is written on the blue sign shown just now?",
        "time_stamp": "00:04:53",
        "answer": "A",
        "options": [
          "A. \"SONG WORD ART HOUSE 1307, LOS ANGELES, CA\".",
          "B. \"ART HOUSE 1307, NEW YORK, NY\".",
          "C. \"MODERN - CLASSICAL\".",
          "D. \"SONG WORLD ART HOUSE 1307, LOS ANGELES, CA\"."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_474_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:04]: The video begins with a character standing in a lush, green forest, surrounded by tall oak and birch trees. The character has light skin, blue eyes, and brown hair, and is wearing a white sweater with black pants and shoes. The background includes dense foliage and a glimpse of a blue sky above. The scene is bathed in natural daylight. [0:00:04 - 0:00:07]: The character remains in the same position, standing still among the trees. The grass beneath them is green and thick, and the trees cast shadows on the ground. The sky is clear and blue, indicating a sunny day. [0:00:07 - 0:00:09]: The character continues to stand in the same spot, and there is a notable pause as they prepare to speak or present information. [0:00:09 - 0:00:12]: Text appears on the screen, emphasizing key phrases in the introduction. The background remains the same with the character standing in the forest, maintaining a steady position. [0:00:12 - 0:00:13]: The character is still at the same location, surrounded by the green foliage and tall trees. The scene is quiet, and there are no visible movements. [0:00:13 - 0:00:15]: Text showing \"Episode of\" is prominently displayed, accentuating the importance of the phrase. The character remains motionless, and the forest setting continues to provide a calm and serene backdrop. [0:00:15 - 0:00:19]: The text \"Minecraft for Beginners\" is displayed in yellow, standing out against the green forest background. The character remains in their position, ready to introduce the content of the video. [0:00:19]: The text on the screen changes, indicating the specific version of the game being discussed. The character stays in the same pose, maintaining attention on the screen. [0:00:20]: The setting remains consistent with the character in the forest. Trees and lush greenery surround the character, providing a peaceful and natural environment. [0:00:21 - 0:00:25]: The text now highlights the type of content being presented, with \"Survival Let's Play\" prominently displayed. The character continues to stand still, with the forest as the backdrop. [0:00:26 - 0:00:28]: The character remains in the identical spot, with consistent surroundings of trees and grass. The text shifts to a new topic, preparing the viewers for upcoming information. [0:00:29 - 0:00:31]: The character is still standing in the forest, and the background stays unchanged. Text continues to display relevant information for the viewers. [0:00:31 - 0:00:33]: The text addresses newcomers to the game, highlighting that the information is aimed at beginners. The character stands still against the forest backdrop, ensuring the viewers' attention is focused on the text. [0:00:34 - 0:00:37]: The scene remains the same, with the character in the forest and text elaborating on the target audience. The trees and foliage continue to provide a serene and natural setting. [0:00:38 - 0:00:39]: The character is stationary, surrounded by the greenery of the forest. Text further specifies the type of audience, reinforcing the video's focus on new players. [0:00:40 - 0:00:43]: The forest scene remains consistent with the character in place. The text emphasizes the purpose of the video, ensuring viewers understand the content's value. [0:00:44 - 0:00:46]: The character continues to stand in the forest, with no changes to the background. The text summarizes the type of information that will be covered in the video. [0:00:46 - 0:00:49]: The character is still and the background unchanging. Text on the screen mentions surviving the first night, hinting at the content to come. [0:00:50 - 0:00:53]: The forest background remains the same, with the character in front. The text outlines more advanced topics like defeating the Ender Dragon, providing a comprehensive view of the video's scope. [0:00:54]: The character stays in place, maintaining the same forest setting. Text invites viewers to get started, setting the stage for the video’s beginning.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the character wearing at the beginning of the video?",
        "time_stamp": "0:00:25",
        "answer": "B",
        "options": [
          "A. A red jacket and black pants.",
          "B. A white sweater and black pants.",
          "C. A blue shirt and white pants.",
          "D. A green hoodie and blue jeans."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_191_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:20 - 0:03:40] [0:00:00 - 0:00:01]: The video begins with a view of a player's inventory screen in a game interface. The interface has multiple slots for items, with one slot displaying a \"Crafting Table.\" The screen offers various crafting options such as planks, sticks, and tables. In the top right corner, the \"Crafting\" grid is visible, and the player’s character is depicted holding brown planks.  [0:00:02 - 0:00:03]: The scene shifts outdoors near a riverbank. The player holds a crafting table in their hand, represented by a wooden square object with a distinct grid pattern on top. The environment includes green grass, a large tree to the right, and more trees and greenery across the river.  [0:00:04 - 0:00:09]: The player continues holding the crafting table while viewing the riverbank. The river appears blue and calm, bordering a grassy area with various tall grasses and a single red flower on the ground. Trees of different varieties, including green and birch, can be seen in the background, providing a lush green frame around the clear blue sky.  [0:00:10]: The focus remains on the riverbank area, but now the viewpoint adjusts slightly closer to the ground. The crafting table is still being held in the player's hand. [0:00:11]: The player places the crafting table on the ground beside the red flower. The crafting table appears integrated into the landscape with its wooden texture contrasting against the green grass. The player's hand is partially visible. [0:00:12 - 0:00:14]: After placing the crafting table, the player's viewpoint lifts slightly, bringing into view more of the surrounding environment, including additional trees and part of the sky. The grass and red flower are still prominent in the foreground. [0:00:15]: The scene switches to the inventory interface again. This time the crafting table is positioned on the ground, leaving an open slot in the inventory. The screen displays various crafting options as the player prepares to use the placed crafting table. [0:00:16 - 0:00:19]: The inventory screen remains visible, showing the crafting grid and available crafting options. The wood plank count in the player's inventory is clearly visible, ensuring there are enough materials for crafting actions.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the player holding right now?",
        "time_stamp": "0:03:25",
        "answer": "C",
        "options": [
          "A. Planks.",
          "B. Sticks.",
          "C. Crafting Table.",
          "D. Red Flower."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Event Understanding",
        "question": "What happens after the player places the crafting table on the ground?",
        "time_stamp": "0:03:48",
        "answer": "B",
        "options": [
          "A. The player throws it into the river.",
          "B. The player crafted 4 sticks from wood at the crafting table.",
          "C. The player holds another item.",
          "D. The player exchanges it for planks."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_191_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:40 - 0:07:00] [0:06:44 - 0:06:45]: The scene begins with a first-person perspective looking over a grassy landscape with varied elevations. The player is holding an item labeled 'White Wool' on the toolbar at the bottom of the screen. The heart icons and hunger bars are also shown, indicating health and hunger levels. A notification in the top right corner states \"New Recipes Unlocked! Check your recipe book!\" against a clear blue sky backdrop.;  [0:06:45 - 0:06:46]: The perspective shifts slightly to the right, and the held item changes to 'Raw Mutton.' The notification in the top right corner changes to \"New Recipes Unlocked! Check your recipe book!\".; [0:06:46 - 0:06:47]: The view continues to shift as the player moves forward toward a grass and dirt-covered hill. The item in the player's hand remains 'Raw Mutton.' The terrain shows a small incline leading towards some mountainous structures and grassy patches in the distance.; [0:06:47 - 0:06:48]: The perspective tilts slightly upward, and more of the terrain comes into view. The player still holds 'Raw Mutton' in hand. The landscape is dominated by grass and dirt blocks, gradually inclining toward a higher elevation. A few sheep are visible in the distance, grazing calmly.; [0:06:48 - 0:06:49]: The player moves forward, approaching the hill and coming closer to some sheep. The landscape shows additional grassy patches, and there are more sheep present in the area. The player is still holding 'Raw Mutton.'. ; [0:06:49 - 0:06:50]: The view includes a white sheep standing on the grassy terrain. The player switches from 'Raw Mutton' to a wooden sword. The surrounding area is depicted with grass blocks and a dirt slope.; [0:06:50 - 0:06:51]: The player uses the wooden sword on the white sheep, and the sheep begins to display damage animation. The immediate area includes grass blocks and a slight incline leading upwards.; [0:06:51 - 0:06:52]: The damage animation continues as the sheep turns red from the player's attack, and wool and mutton drop from the sheep. Grass and dirt blocks surround the area. The player still holds the wooden sword.; [0:06:52 - 0:06:54]: The sheep collapses, and the dropped items are visible on the grass. The player then collects these items, adding them to their inventory. The landscape remains consistent with different inclines and grassy slopes.; [0:06:54 - 0:06:55]: The player moves the view to a new area showing more grass, dirt, and several sheep at various elevations. The player still holds the wooden sword and continues exploring the area.; [0:06:55 - 0:06:56]: As the player looks upward, a mountainous structure with trees on top comes into view, and the sky remains clear with no clouds visible. The player's wooden sword is still equipped.; [0:06:56 - 0:06:57]: The player maintains the upward view, focusing on the tall mountain with lush greenery, including trees dotting the peak and slopes. The wooden sword is still present in the player’s hand.; [0:06:57 - 0:06:58]: The player's perspective stays fixed on the high terrain with the trees slightly swaying. More details of the hill's structure and grassy slopes are visible. The player's wooden sword remains in hand.; [0:06:58 - 0:07:00]: The player continues to look upwards at the mountain and sky before moving their view downward to focus back on the grazing area with sheep. The landscape features a mix of steep and gentle slopes.; [0:07:00 - 0:07:01]: Moving forward, the player comes upon a black sheep entangled in a ditch, stands on the grass. The player holds their wooden sword and approaches the animal. The surrounding area includes grass patches and sloped dirt.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What does the player do after switching to a wooden sword?",
        "time_stamp": "0:07:08",
        "answer": "A",
        "options": [
          "A. Attacks a white sheep.",
          "B. Chops down a tree.",
          "C. Digs into the dirt.",
          "D. Places a block of wool."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_191_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:11:00]",
    "captions": "[0:10:00 - 0:10:20] [0:00:00 - 0:00:03]: The scene starts in a small, enclosed space made up of stone walls and oak wood planks. A crafting table is positioned to the left against the stone wall. Directly ahead is a small, empty doorway leading further into the structure, surrounded by the stone walls and oak wood planks.  [0:00:03 - 0:00:05]: Moving closer to the doorway reveals more details of the stone structure. A stone block obstructs the passage straight ahead, while more oak planks can be seen lining the right side of the corridor. [0:00:05 - 0:00:08]: The stone block is struck repeatedly with a wooden pickaxe, causing it to break apart into smaller pieces that scatter onto the floor. [0:00:08 - 0:00:10]: With the stone block removed, the passageway opens up, revealing an exterior landscape. Oak planks line the doorway on the right, and outside, a nighttime sky and trees can be seen in the background. [0:00:10 - 0:00:13]: The view then shifts back inside, looking down into a wider, more open room where additional wooden structures and stone walls can be seen. A crafting table remains visible on the left side of the screen. [0:00:13 - 0:00:14]: Moving further back into the open space, it becomes apparent that the room is part of a larger wooden and stone structure. The immediate foreground is dominated by the stone crafting table. [0:00:14 - 0:00:16]: A closer inspection of the crafting table begins. The view transitions to a screen displaying the crafting interface, featuring an inventory with various items including tools and raw materials. [0:00:16 - 0:00:18]: The cursor moves through the crafting options, selecting items and placing them on the crafting grid. Wooden planks are selected from the inventory and positioned on the grid. [0:00:18 - 0:00:19]: With the wooden planks in place, a set of three oak doors is crafted and added to the inventory. [0:00:19 - 0:00:20]: The interface is then exited, and the view returns to the crafting table. The player steps back, looking down at the crafting table as the new oak doors are now ready for use in the inventory. [0:00:20]: The view shifts again, looking downwards, towards a wall made of oak planks with a small opening where a pig is visible trying to fit through the gap. An oak door is being held, seemingly ready to be placed at the gap to close it off.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type of pickaxe was used to break the stone block just now?",
        "time_stamp": "00:10:08",
        "answer": "A",
        "options": [
          "A. Wooden.",
          "B. Stone.",
          "C. Iron.",
          "D. Diamond."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_191_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the cyclist located right now?",
        "time_stamp": "00:00:07",
        "answer": "A",
        "options": [
          "A. At an intersection.",
          "B. On a bridge.",
          "C. Passing through a tunnel.",
          "D. In the middle of an open field."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_164_real.mp4"
  },
  {
    "time": "[0:01:52 - 0:02:12]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located on the right side of the road right now?",
        "time_stamp": "00:02:04",
        "answer": "C",
        "options": [
          "A. A river.",
          "B. A fence.",
          "C. Tall bushes and trees.",
          "D. A concrete wall."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_164_real.mp4"
  },
  {
    "time": "[0:03:44 - 0:04:04]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is visible on the left side of the road right now?",
        "time_stamp": "00:03:53",
        "answer": "D",
        "options": [
          "A. A field.",
          "B. A river.",
          "C. A dense forest.",
          "D. A row of trees and bushes."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_164_real.mp4"
  },
  {
    "time": "[0:05:36 - 0:05:56]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is located on the left side of the road right now?",
        "time_stamp": "00:05:50",
        "answer": "B",
        "options": [
          "A. A field with crops.",
          "B. A dense line of trees and bushes.",
          "C. A small river.",
          "D. An open green field."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_164_real.mp4"
  },
  {
    "time": "[0:07:28 - 0:07:48]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the road positioned right now?",
        "time_stamp": "00:07:47",
        "answer": "A",
        "options": [
          "A. Between field and forest.",
          "B. Through a dense forest.",
          "C. Beside a river.",
          "D. Along a mountain ridge."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_164_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the bartending actions just performed?",
        "time_stamp": "00:00:09",
        "answer": "C",
        "options": [
          "A. The bartender filled three glasses with water and handed them to customers.",
          "B. The bartender prepared a cocktail by adding ingredients from various bottles.",
          "C. The bartender filled a glass with ice, placed it on the counter, and then prepared to repeat the process for another glass.",
          "D. The bartender took drink orders from customers and began mixing different spirits."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_407_real.mp4"
  },
  {
    "time": "[0:02:53 - 0:03:03]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the events just now?",
        "time_stamp": "00:03:03",
        "answer": "B",
        "options": [
          "A. A bartender is preparing a cocktail for a customer.",
          "B. A bartender is picking bottles in a refrigerator and assisting a colleague.",
          "C. A bartender is serving a customer and cleaning the counter.",
          "D. A bartender is making an espresso and organizing the bar area."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_407_real.mp4"
  },
  {
    "time": "[0:05:46 - 0:05:56]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions of the bartender just now?",
        "time_stamp": "00:05:56",
        "answer": "B",
        "options": [
          "A. The bartender chatted with the customers while refilling their glasses with water and served snacks.",
          "B. The bartender prepared complex cocktails by mixing various ingredients and engaged in a friendly conversation with seated patrons.",
          "C. The bartender cleaned the bar area, organized bottles, and set up glasses, ensuring everything was in place.",
          "D. The bartender took orders, served wine and simple drinks, and casually interacted with patrons while preparing drinks."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_407_real.mp4"
  },
  {
    "time": "[0:08:39 - 0:08:49]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the content and actions just now?",
        "time_stamp": "00:08:49",
        "answer": "A",
        "options": [
          "A. A bartender talked with partrons while patrons converse and enjoy their drinks at the bar.",
          "B. A bartender prepares a cocktail by mixing various ingredients and garnishing it with a lemon twist.",
          "C. A server delivers a tray of appetizers to a group of friends celebrating a special occasion.",
          "D. A bartender stacks clean glasses behind the bar while patrons request drink menus."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_407_real.mp4"
  },
  {
    "time": "[0:11:32 - 0:11:42]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What did the individual just do step by step?",
        "time_stamp": "00:11:38",
        "answer": "A",
        "options": [
          "A. The individual picked up a champagne bottle, poured champagne into a glass, and handed it to a customer.",
          "B. The individual took a wine bottle, poured wine into a goblet, placed it on the counter, and walked away.",
          "C. The individual grabbed a beer bottle, poured beer into a mug, placed it on the counter, and took a payment.",
          "D. The individual took a cocktail shaker, prepared a cocktail, poured it into a glass, and served it to the customer."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_407_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:08]: The video starts with a close-up of a woman's bare back, revealing her dark curly hair draped over her shoulders, against a light-colored wall. She is wearing a white strapless top that seems to be adjusted by someone else, identifiable by their arm appearing from the right side of the frames. This person, wearing a grayish-blue long-sleeved shirt, uses both hands to secure or adjust the white fabric around the woman's body. [0:00:09 - 0:00:15]: The camera view widens, showing more of the setting. The woman turns slightly, revealing she is facing a mirror, as she adjusts her hair with her hands. The person assisting her, now more visible, seems to be adjusting the back of her clothing, paying attention to specific details to ensure it looks perfect. Behind them, the background shows vertical pink stripes on a light-colored wall. [0:00:16 - 0:00:19]: The scene changes to a close-up of a flat surface draped with various fabrics, including a green one with a luxurious sheen, partially covering a deep red surface. On top of this are a white cloth and another cloth with intricate gold detailing. A pair of hands appears to carefully fold and organize the white and gold-detailed fabrics. The hands work meticulously, suggesting care in handling these materials.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the woman wearing right now?",
        "time_stamp": "0:00:11",
        "answer": "B",
        "options": [
          "A. A black dress.",
          "B. A white strapless top.",
          "C. A red shirt.",
          "D. A grayish-blue long-sleeved shirt."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the wall in front of the woman?",
        "time_stamp": "0:00:14",
        "answer": "C",
        "options": [
          "A. White with blue stripes.",
          "B. Pink with white stripes.",
          "C. Light-colored with pink stripes.",
          "D. Dark-colored with gold detailing."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_159_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:06]: A person wearing a golden robe has assistance from two others who are adjusting a braided belt around their waist. The hands of those helping are primarily focused on tightening and securing the belt. One helper is positioning the belt while the other is making final adjustments. The background is out of focus, ensuring the central focus remains on the belt adjustment process. [0:01:07 - 0:01:08]: The camera view shifts to show the feet of the person wearing the golden robe, which is adorned with delicate, patterned sandals. The floor has a wooden texture. The lower part of the robe partially conceals the feet, while the sandals feature intricate beadwork. [0:01:09 - 0:01:10]: A wider view reveals more of the surrounding scene. The golden-robed person stands near a bed with a green blanket, while a person in a light-colored dress kneels nearby, holding the robe. The floor maintains its wooden texture, and another person stands off to the side, also wearing light-colored clothing. [0:01:11 - 0:01:12]: The camera focuses on a metal tin containing a creamy substance situated on a wooden surface. The lid of the tin is open, revealing the cream inside. [0:01:13 - 0:01:16]: A person's fingers dip into the creamy substance inside the tin, gathering some of it. The view remains close-up, detailing the texture of both the cream and the person's fingertips. [0:01:17 - 0:01:19]: The focus moves to a metal tray holding several shallow shells with various colored powders inside them, along with a small wooden bowl containing a white powder. The powders in the shells are black, white, and green. The setting suggests an activity involving these materials, possibly related to the previous scenes.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are the helpers doing to the person in the golden robe?",
        "time_stamp": "0:01:06",
        "answer": "B",
        "options": [
          "A. Applying makeup.",
          "B. Adjusting a braided belt.",
          "C. Polishing their sandals.",
          "D. Fixing their hair."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_159_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:02]: A wooden box with intricate designs is positioned on a round wooden table. Next to it is a metallic plate with cooked eggs and other food items. A green cup is visible on the table as well. Someone begins to reach for the box. [0:02:03 - 0:02:05]: A person with red hair tied back with a hairband is shown in close-up. The background consists of a cream-colored wall with pink vertical stripes. [0:02:06]: The person in the previous frame is seen holding the box. [0:02:07 - 0:02:08]: Another person’s hand, wearing a yellow garment, reaches into the box held by the person in the blue garment. They are taking out what appears to be a bangle or bracelet. [0:02:09]: The person in the yellow garment is adjusting the bangle on their wrist. [0:02:10 - 0:02:11]: The camera captures the side profile of a person with braided hair and adorned with a hair accessory. They are wearing a yellow garment with earrings. Another person’s arm is seen preparing to place beads or a necklace around their neck. [0:02:12 - 0:02:16]: The person wearing the yellow garment is having a necklace with beads secured around their neck by another individual. [0:02:17]: The person in yellow is fixing their hairstyle. [0:02:18 - 0:02:19]: The individual with the red hair is holding a cream-colored fabric, possibly a garment, and is aiding the person in yellow in putting it on. The background reveals more of the room, showing a wicker chair with colorful cushions and a small table with a green vase and a red bowl.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person in the yellow garment doing right now?",
        "time_stamp": "0:02:09",
        "answer": "B",
        "options": [
          "A. Reaching for the box.",
          "B. Selecting a bracelet from the wooden box and puting it on her hand.",
          "C. Putting on a necklace.",
          "D. Fixing their hairstyle."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_159_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:03:43]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:01]: A young woman with dark hair styled intricately and adorned with gold accessories looks downward while touching her white shawl. She wears a gold dress and a beaded necklace. The background shows large green leaves and a dark vertical structure, possibly a curtain or wall. [0:03:02]: The same young woman remains in focus, but an older woman in a purple and gold robe appears behind her, helping with her shawl. The background is the same dark curtain and green leaves. [0:03:03 - 0:03:05]: The older woman continues to adjust the younger woman's shawl, pulling it over her head. The younger woman looks forward but slightly downward, and a person in blue passes by in the background holding a vessel. [0:03:06 - 0:03:08]: The older woman finishes positioning the shawl on the younger woman, and both adjust it near the shoulders. They stand close to each other. The scene remains framed by the green foliage and dark curtain. [0:03:09 - 0:03:11]: The young woman and the older woman start to walk side by side, maintaining close proximity. The young woman looks down, and the older woman holds her robe close. The background is consistent with green leaves and a dark curtain. [0:03:12 - 0:03:13]: Both women continue walking to the left, the younger with her head still slightly bowed. The older woman adjusts her own robe as they move, showing a warm interaction. [0:03:14 - 0:03:16]: The women continue their walk, engaging in a quiet moment. The younger woman remains with her head slightly down, and the older woman looks at her warmly. [0:03:17 - 0:03:18]: The older woman glances warmly at the younger woman as they continue walking together, both maintaining their positions close to each other. The green leaves and dark backdrop persist. [0:03:19]: The women proceed walking to the left, engaging in what looks like a gentle and comforting exchange. The older woman continues to smile warmly at the younger one, who keeps her head bowed.\n[0:03:40 - 0:03:43] [0:03:40 - 0:03:41]: A logo appears prominently in the center against a black background. The logo consists of a blue circular design resembling a stylized eye or camera shutter with segments radiating outward. Below the logo, there is the text \"CROWSEYE PRODUCTIONS.\" [0:03:41 - 0:03:43]: The text \"CROWSEYE PRODUCTIONS\" disappears, leaving only the blue circular logo in the center, which gradually shrinks in size as the animation progresses towards the center of the screen, maintaining its design and color.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What does the older woman do after appearing behind the young woman?",
        "time_stamp": "00:03:05",
        "answer": "A",
        "options": [
          "A. Adjusts the shawl of the young woman.",
          "B. Hands a vessel to the young woman.",
          "C. Sits down beside the young woman.",
          "D. Leaves the scene immediately."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "getting_dressed",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_159_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the white car parked right now?",
        "time_stamp": "00:00:10",
        "answer": "D",
        "options": [
          "A. Next to the cyclist.",
          "B. On the right side of the road.",
          "C. Directly ahead of the cyclist.",
          "D. On the left side of the road."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_176_real.mp4"
  },
  {
    "time": "[0:02:07 - 0:02:27]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the telegraph pole located right now?",
        "time_stamp": "00:02:07",
        "answer": "D",
        "options": [
          "A. On the left side.",
          "B. In the middle of the road.",
          "C. Directly above the road.",
          "D. On the right side."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_176_real.mp4"
  },
  {
    "time": "[0:04:14 - 0:04:34]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the trees located right now?",
        "time_stamp": "00:04:20",
        "answer": "D",
        "options": [
          "A. On the left side of the road.",
          "B. On the right side of the road.",
          "C. Directly ahead.",
          "D. On both sides of the road."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_176_real.mp4"
  },
  {
    "time": "[0:06:21 - 0:06:41]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the object on the left front of the cyclist right now?",
        "time_stamp": "00:06:39",
        "answer": "D",
        "options": [
          "A. A traffic light.",
          "B. A bus stop.",
          "C. A water fountain.",
          "D. A green bollard."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_176_real.mp4"
  },
  {
    "time": "[0:08:28 - 0:08:48]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the position of the church building right now?",
        "time_stamp": "00:08:45",
        "answer": "D",
        "options": [
          "A. On the left side of the road.",
          "B. On the right side of the road.",
          "C. Behind the cyclist.",
          "D. Directly ahead."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_176_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a black screen. [0:00:01 - 0:00:04]: The scene transitions to a vast indoor space, which appears to be the entrance to an art show. The structural design is modern, featuring a high ceiling with metal trusses and white drapes hanging to create a large entrance area. Signs reading \"LA ART SHOW\" and \"MODERN + CONTEMPORARY\" are prominently displayed above the entrance. Visitors are walking towards and into the exhibition area. [0:00:05 - 0:00:08]: The perspective changes to within the exhibition space, showcasing various art pieces and installations in a spacious, well-lit indoor area. A large, illuminated geometric sculpture draws attention at the center, surrounded by several visitors observing the art and interactive displays. [0:00:09 - 0:00:11]: The camera pans to the left, revealing more of the exhibition area. Additional artworks and installations are visible, along with more visitors engaged with the exhibits. The overall atmosphere appears vibrant and bustling. [0:00:12 - 0:00:18]: The focus shifts to a metallic, triangular structure in the middle of the exhibition hall, with people walking around and interacting with different art pieces. The background continues to showcase more illuminated artworks and displays, highlighting the contemporary and creative theme of the art show.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What prominent signs are displayed above the entrance of the art show?",
        "time_stamp": "00:00:04",
        "answer": "D",
        "options": [
          "A. \"ART EXHIBITION\" and \"MODERN ART\".",
          "B. \"CONTEMPORARY ART\" and \"MODERN EXHIBITION\".",
          "C. \"LA EXHIBITION\" and \"CONTEMPORARY + MODERN\".",
          "D. \"LA ART SHOW\" and \"MODERN + CONTEMPORARY\"."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Clips Summarize",
        "question": "Which description best fits the scene within the exhibition space?",
        "time_stamp": "00:00:11",
        "answer": "A",
        "options": [
          "A. A spacious, well-lit area with various art pieces and installations.",
          "B. A small room with a few art pieces.",
          "C. An outdoor exhibition with sculptures and paintings.",
          "D. A dark room with illuminated paintings."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_461_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:49]: The video begins with a view of a poster for the movie \"Parasite\" affixed to a white wall on the right side. To the left of the poster is a modern art painting. The painting features intricate black and white geometric patterns with a prominent depiction of a face in vibrant colors – blue, pink, black, and red. The face appears abstract with large eyes and bold shapes. As the camera moves, more of the art gallery comes into view, and additional paintings become partially visible on the white walls.  [0:02:50 - 0:02:50]: The camera shifts to the right, revealing a new section of the gallery with several paintings on display. These include black and white abstract portraits and vividly colored modern art. [0:02:51 - 0:02:54]: The camera then focuses on a set of three artworks arranged vertically. The top left piece is a red and black abstract painting, and the top right artwork features a colorful cartoon-like figure. Beneath these is another artwork with complex patterns in multiple colors. [0:02:55 - 0:02:59]: The camera continues to move downward slightly, providing a closer view of the bottom artwork with swirling multicolored patterns forming a face. This piece stands out with its psychedelic, optical illusion style.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the subject of the poster on the white wall?",
        "time_stamp": "00:02:47",
        "answer": "D",
        "options": [
          "A. A modern art exhibition.",
          "B. A famous painting.",
          "C. An art gallery event.",
          "D. The movie \"Parasite\"."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_461_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:02]: The video reveals a gallery wall with a series of artworks. The leftmost artwork is enclosed in a black frame, featuring the word \"GOD\" in white against a gray, waterfall-like background. Adjacent to it, four smaller rectangular pieces are displayed vertically. The first is red, the second is blue, and the third and fourth showcase more abstract designs on gray backgrounds. Above each is the word \"GOD,\" except for the last two, which seem to have stylized illustrations. [0:08:03]: The camera angle slightly shifts to the right, revealing more pieces on the wall. The new artwork on the right is a vertically aligned set of paintings depicting natural scenes, primarily in blue and gray hues, possibly portraying waves and forest landscapes. [0:08:04 - 0:08:06]: The scene further unfolds displaying additional six-frame segments. The top row shows tree-like figures in a mist, while the second row contains seascapes with crashing waves. [0:08:07 - 0:08:09]: The camera shows these framed artworks more clearly, with the lower right displaying floral designs. The precision of each floral artwork stands out against a consistent gray background, with distinct thematic colors for each flower. [0:08:10 - 0:08:15]: As the camera angle expands, more of the white gallery wall is visible, showing a neat display of paired artworks. Notably, the right section highlights four square floral designs, masterfully framed and colored. The left showcases six pieces of natural landscapes, dominated by forest scenes and ocean waves. [0:08:16 - 0:08:19]: The video concludes with a wider shot of the gallery corner. The predominant piece is a floral quadriptych on the right, attributed to \"NANCY R WISE.\" The meticulous arrangement and spacing between each artwork emphasize the gallery's aesthetic and thematic consistency. A subtle shift reveals more viewers and artworks in the background, offering brief glimpses of additional pieces and spectators.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What word is featured in the artwork's black frame?",
        "time_stamp": "00:08:00",
        "answer": "D",
        "options": [
          "A. LOVE.",
          "B. HOPE.",
          "C. PEACE.",
          "D. GOD."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What kind of designs are shown in the top row of the six-paintings segments?",
        "time_stamp": "00:08:06",
        "answer": "A",
        "options": [
          "A. Tree-like figures in a mist.",
          "B. Animal figures.",
          "C. Geometric shapes.",
          "D. Human silhouettes."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_461_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What mathematical topic might the speaker focus on next?",
        "time_stamp": "00:00:10",
        "answer": "D",
        "options": [
          "A. Adding Fractions.",
          "B. Dividing Fractions.",
          "C. Simplifying Fractions.",
          "D. Multiplying Fractions."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_219_real.mp4"
  },
  {
    "time": "[0:01:10 - 0:01:40]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker do next?",
        "time_stamp": "00:00:55",
        "answer": "C",
        "options": [
          "A. Explain how to convert the fractions.",
          "B. Show the steps to add the fractions.",
          "C. Show the steps to multiply the fractions.",
          "D. Discuss the importance of common denominators."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_219_real.mp4"
  },
  {
    "time": "[0:02:20 - 0:02:50]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker do next?",
        "time_stamp": "00:02:29",
        "answer": "C",
        "options": [
          "A. Add the fractions together.",
          "B. Subtract the fractions.",
          "C. Show the steps to multiply the fractions.",
          "D. Divide the fractions."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_219_real.mp4"
  },
  {
    "time": "[0:03:30 - 0:04:00]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What mathematical concept might the speaker move on to explain next?",
        "time_stamp": "00:03:51",
        "answer": "D",
        "options": [
          "A. Addition of fractions.",
          "B. Subtraction of fractions.",
          "C. Simplifying fractions.",
          "D. Another explanation to fractions."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_219_real.mp4"
  },
  {
    "time": "[0:04:40 - 0:05:10]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:05:10",
        "answer": "C",
        "options": [
          "A. How to add fractions.",
          "B. How to divide fractions.",
          "C. How to multiply the denominators.",
          "D. How to subtract fractions."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_219_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:18]: The video begins with a view of a gallery wall adorned with colorful, abstract art pieces. In the initial frames, there is a large centerpiece painting featuring abstract shapes and figures with vibrant colors such as yellow, blue, red, green, and black. This painting is positioned centrally on the wall. To the right hangs another framed artwork characterized by a dynamic splash of colors primarily in green, blue, red, and yellow. As the seconds progress, people are seen walking past the artworks, indicating the setting is possibly a public art gallery. The scene then moves to include more paintings on the wall, introducing a piece with a large, bright yellow star against a multicolored background, followed by another abstract composition. Overall, the gallery has a clean, white interior with multiple paintings displayed, and the artworks are vibrantly colored, making them stand out against the neutral background. To the far left corner, a sign reading \"Montague Gallery\" is visible with additional details about the location (San Francisco, CA) and the type of art (Modern + Contemporary). [0:00:10 - 0:00:13]: The next segment focuses on the blue sign overhead displaying \"Montague Gallery\" more prominently, with both the number of the gallery stand and location (San Francisco, CA) clearly visible. Additionally, there are transitions showing blank spaces on the gallery wall before moving to another set of abstract paintings. [0:00:14 - 0:00:19]: The latter part of the video showcases two large abstract paintings side by side. The left painting predominantly features cool tones of blue and green with a textured, layered presentation, while the right painting is rich in warm tones of red, black, and hints of yellow and orange. These frames again show glimpses of the open gallery space with other artworks and occasional visitors moving about, emphasizing the gallery's expansive ambiance and variety of artworks displayed.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is displayed on the blue sign overhead?",
        "time_stamp": "00:00:15",
        "answer": "C",
        "options": [
          "A. \"Exhibit Closed 625/724\".",
          "B. \"Gallery Entrance 622/625\".",
          "C. \"Montague Gallery 625/724\".",
          "D. \"Montague Gallery 625/725\"."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_478_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:43]: The video begins with a scene in an art gallery, showing a display case containing a detailed sculpture of multiple leopards. The gallery is dimly lit, with spotlights illuminating the artworks. Several people can be seen in the background, with reflections visible on the glass of the display case. The person holding the camera appears in the reflection, holding a phone. [0:02:44 - 0:02:45]: The camera pans slightly to the right, continuing to show the sculpture of leopards but also revealing another section of the display with similar sculptures. Reflections of other gallery visitors can still be seen on the glass, and the gallery environment remains the same with people observing various artworks. [0:02:46 - 0:02:49]: The camera captures another sculpture of leopards, similar in style to the previous one, positioned vertically. The viewer can see different angles of the same space, showing people leisurely walking and observing the artworks on the walls. The lighting is consistent, and details of the sculptures are highlighted. [0:02:50 - 0:02:51]: The video transitions to two framed sculptures side by side. The camera centers on these artworks, ensuring both pieces are in frame. The background remains consistent with the same dark, ambient lighting and spotlit artworks. [0:02:52 - 0:02:55]: The focus shifts to a different wall featuring three framed pieces that resemble complex paper sculptures. These pieces are neatly arranged in a horizontal line, each framed and lit to highlight the details of the paper folds and textures. The background wall is a neutral gray, contrasting with the light-colored sculptures. [0:02:56 - 0:02:57]: Moving further to the right, the video continues to show the series of framed paper sculptures, now including a partial view of the previous set, with the addition of two more framed artworks on a different section of the wall. These new pieces are composed of different textures and colors. [0:02:58 - 0:02:59]: The final part of the video focuses on three vertically framed artworks, each featuring a unique textured pattern in green, red, and yellow, respectively. The camera captures these in detail, again showing a portion of the preceding paper sculptures, maintaining a cohesive transition between different gallery sections.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "How are the three framed pieces arranged on the wall?",
        "time_stamp": "00:02:55",
        "answer": "C",
        "options": [
          "A. In a vertical line.",
          "B. In a diagonal line.",
          "C. In a horizontal line.",
          "D. In a circular pattern."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_478_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:24]: The first frames show a framed art piece featuring a girl in a pale pink dress standing against a light grey background. Her head is obscured by a large, tangled mass of metallic wires or similar material. The art piece is displayed in a gallery setting with other artworks visible in the background. As the frames progress, the camera starts to move laterally, revealing more of the gallery space and additional framed artworks on the right wall.  [0:05:24 - 0:05:29]: The camera shifts focus to another section of the gallery, showing a dark grey wall adorned with four framed abstract paintings. These paintings feature multiple layers of colorful, sweeping brushstrokes in vibrant hues such as green, orange, blue, and red, framed in light wooden frames. The camera continues to move, providing closer views of the brushstrokes and details of the abstract paintings. [0:05:29 - 0:05:31]: There is a shift revealing a portion of the gallery that has various framed artworks. Several paintings in this section display similar abstract styles, with bold brushstroke patterns and bright, contrasting colors. The camera captures both the close-up and medium ranges of this dark-colored wall displaying these vibrant artworks. [0:05:31 - 0:05:35]: The video then starts to widen the frame to showcase more of the gallery’s layout and other exhibited artworks. The viewer gets a clearer view of the surrounding gallery space, including more framed pieces along adjacent walls and additional illuminations from ceiling-mounted lights that provide optimal viewing conditions for the artwork. [0:05:35 - 0:05:39]: Finally, towards the end of the frames, the camera moves further back, capturing the wider gallery space which includes more artworks on grey and white walls. A section of the gallery shows tables and chairs where a few individuals are seated, possibly discussing art or engaging in some activity. The larger area is well-lit, displaying numerous artworks in a dimly neutral setting, indicative of a professional and curated gallery environment.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the lighting condition in the gallery space?",
        "time_stamp": "0:05:39",
        "answer": "C",
        "options": [
          "A. Dim and dark.",
          "B. Bright and natural.",
          "C. Well-lit with ceiling-mounted lights.",
          "D. Dark with spotlights."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Prospective Reasoning",
        "question": "Based on the camera movement and gallery setup, what might the camera show next?",
        "time_stamp": "0:05:39",
        "answer": "B",
        "options": [
          "A. An outdoor scene.",
          "B. Detailed close-ups of other artworks.",
          "C. The entrance of the gallery.",
          "D. A lecture or discussion area."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_478_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:09:48]",
    "captions": "[0:09:40 - 0:09:48] [0:09:40 - 0:09:48]: The video begins with a view of a white wall in a gallery space. Plastic holders with cards, one labeled \"PETER ZULLO,\" are mounted on the wall along with several rectangular metal art pieces. As the camera moves to the right, more of the gallery becomes visible, revealing an abstract sculpture in purple and yellow colors partially obstructing the view, and further wall-mounted artworks. Another colorful abstract structure stands nearby. The camera angle continues to shift, displaying additional angular and colorful sculptures, as well as more visitors and other artworks in the background. The video then looks up to capture a sign that reads \"GALLERY 1261,\" alongside room numbers and a location in Denver, CO, before cutting off.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What additional information does the sign provide besides \"GALLERY 1261\"?",
        "time_stamp": "00:09:48",
        "answer": "B",
        "options": [
          "A. Names of the artists.",
          "B. Room numbers and a location in Denver, CO.",
          "C. A description of the exhibition.",
          "D. Opening hours of the gallery."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_478_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video starts showing an escalator with a silver metallic finish and black handrails. The escalator is situated under a clear, glass-like roof supported by white beams with a few people seen descending. Outside, there is a large red and gold emblem displayed on a beige building in the background. [0:00:04 - 0:00:07]: As the video progresses, the perspective descends down the escalator, and more people come into view, moving both up and down on either side of the escalator. There is additional seating and railing areas below, and a portion of a covered walkway is visible to the right. [0:00:08 - 0:00:11]: Further into the descent, the camera angle starts to reveal more of the area below, showing various levels and people moving around a seating area. A sign for a market and eateries can be seen, and another colorful banner is displayed on the building above. [0:00:12 - 0:00:15]: As the escalator continues downward, the camera captures a busier scene with more people walking in different directions. Additional shop signs are now visible along with more structural details of the building, such as stairs and railings on multiple levels. [0:00:16 - 0:00:20]: Towards the end, the video captures a wide view of the lower level, with people moving through the area. Decorative plants, lamps, and a mix of open and covered spaces are visible, along with people exiting and entering various shops and stairs adjacent to the escalator.",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why are more structural details of the building visible as the camera descends?",
        "time_stamp": "0:00:15",
        "answer": "B",
        "options": [
          "A. Because the camera is zooming in.",
          "B. Because the camera is moving closer to the lower level.",
          "C. Because the lights are turned on.",
          "D. Because the weather clears up."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_318_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:41]: The video begins with a view of a \"Jurassic World\" sign mounted on the facade of a gray, concrete building. The sign features a large dinosaur emblem and is centered on a blue vertical stripe that extends up the building.  [0:02:42]: The image transitions smoothly to show a more full view of the sign and the building, while another scene starts to fade in. [0:02:43 - 0:02:47]: The perspective shifts to a wider view, revealing a water ride with people on a boat coming out of the building, which has the \"Jurassic World\" sign. The sun is shining brightly in the background with rays beaming over the top of the building. Lush greenery, including trees and bushes, surrounds the ride. The water is clear, showing the structure beneath the surface. [0:02:48]: The scene transitions again, overlapping with an entrance structure labeled \"Jurassic World.\" The entrance is an archway, flanked by stone columns with tropical plants around the area. Visitors are seen in the background walking through the entrance. [0:02:49 - 0:02:53]: Focusing on the entrance structure now, the video displays more details of the area. The archway has large block letters spelling \"JURASSIC WORLD.\" There are various structures, likely shops or attractions, and palm trees adding to the tropical theme. People can be seen walking and exploring the premises.  [0:02:54 - 0:02:59]: The camera moves under the archway and continues down the pathway, which is lined with tall, green shrubs and more palm trees. Further into the distance, more visitors are visible, and several canopies and umbrellas hint at areas for relaxation or dining. The pathway appears well maintained, and the overall atmosphere combines modern architecture with lush, tropical landscaping.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the central feature of the \"Jurassic World\" sign on the facade of the building right now?",
        "time_stamp": "00:02:41",
        "answer": "D",
        "options": [
          "A. A large tree.",
          "B. A water fountain.",
          "C. A Ferris wheel.",
          "D. A dinosaur emblem."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_318_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:24]: A first-person view of a boat entering a tunnel with a rocky waterfall on the left. The waterfall cascades down the rocks into a pool of water. The tunnel entrance has a rectangular frame, with metal and concrete sides surrounding it. The sky and some green foliage are visible above the rocks, adding a touch of nature to the scene. [0:05:24 - 0:05:28]: As the boat moves closer to the tunnel, the focus shifts to the rocks and waterfall. The rock formations are brown with patches of green moss. Water flows steadily over the rocks, creating a vibrant, lively atmosphere. The tunnel walls blend into the rocky formations, making the transition between the outside and inside seamless. [0:05:28 - 0:05:34]: The boat moves into the tunnel, and the view shifts from the waterfall to the interior of the cave-like structure. The rocky ceiling and walls are textured and detailed, with various shades of brown, green, and gray. The water becomes darker, reflecting the dim environment inside the tunnel. Blue lighting is visible on the ceiling of the tunnel, adding a mysterious and magical ambiance to the scene. [0:05:34 - 0:05:39]: The boat continues further into the darkened tunnel. The blue lighting from above continues, illuminating parts of the tunnel with a soft glow. Outside light becomes less visible, and the surroundings become primarily rocky walls. The water remains calm and reflective, with a narrow pathway guiding the boat forward. There are occasional glimpses of greenery through openings in the tunnel walls, indicating the proximity to the outside environment.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is visible above the rocks as the boat approaches the tunnel?",
        "time_stamp": "0:05:23",
        "answer": "D",
        "options": [
          "A. A rainbow.",
          "B. A bridge.",
          "C. A bird.",
          "D. Green foliage."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Action Recognition",
        "question": "What happens to the boat as it moved forward just now?",
        "time_stamp": "0:05:34",
        "answer": "D",
        "options": [
          "A. It focuses on the sky.",
          "B. It zooms in on the boat.",
          "C. It looks back at the waterfall.",
          "D. It shifts to the interior of the tunnel."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_318_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:05]: The footage begins in a dimly lit tunnel with a metallic ceiling that features multiple lights. A large screen dominates the center of the frame, displaying a person speaking. The screen is flanked by two smaller screens, emitting a blue glow with triangular warning icons. The ambiance and the structural design suggest an indoor setting, possibly a ride or attraction. [0:08:06 - 0:08:09]: As the video progresses, the focus moves to the smaller screens, which now both display a \"CONTAINMENT ALERT\" message beneath a warning triangle. The room grows darker, indicating a shift in the situation or environment. [0:08:10 - 0:08:12]: Continuing down the tunnel, the camera passes underneath an architectural feature revealing a sign that reads \"TYRANNOSAURUS REX KINGDOM\" with a dinosaur graphic. To the right, an animatronic dinosaur is visible, appearing under a spotlight, adding to the thematic experience. [0:08:13 - 0:08:17]: The environment transitions to near darkness with occasional glimmers of blue light in the distance. This dim lighting creates an enigmatic and suspenseful atmosphere, potentially indicating the start of an immersive segment of the ride. [0:08:18 - 0:08:20]: Gradually, the path ahead becomes visible, illuminated by sparse lighting. The setting appears to be an artificial cave or forest, with vegetation and trees integrated into the design. The ride vehicle moves forward, showing the detailed, immersive scenery on either side.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What message is displayed on the smaller screens beneath the warning triangle?",
        "time_stamp": "00:08:09",
        "answer": "C",
        "options": [
          "A. \"EMERGENCY EXIT\".",
          "B. \"DANGER AHEAD\".",
          "C. \"CONTAINMENT ALERT\".",
          "D. \"MAINTENANCE REQUIRED\"."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_318_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a split screen, showing two blank white surfaces. The left side has a small brown section at the top edge, and the right side has a small yellow section at the top edge. [0:00:01 - 0:00:02]: A hand places several yellow toy blocks on the left side, forming the base of what appears to be a building structure labeled \"RACE CAR.\" On the right side, a mini truck made of colorful blocks, labeled \"MINI TRUCK,\" is visible. [0:00:02 - 0:00:04]: More blocks are added to the structure on the left, while the mini truck on the right remains unchanged. A small blue block is placed on the structure labeled \"RACE CAR\" on the left. [0:00:04 - 0:00:06]: The structure on the left evolves as more yellow and blue blocks are stacked. On the right, a hand places a small green block on the mini truck. [0:00:06 - 0:00:08]: The structure on the left continues to develop with additional layers, while the mini truck on the right is slowly being built with more blocks added. [0:00:08 - 0:00:09]: The structure labeled \"RACE CAR\" now has a more defined shape with colorful blocks. On the right, the mini truck is being built higher with layers of green and yellow blocks. [0:00:09 - 0:00:10]: Additional red block components, including wheels, are being attached to the structure on the left. On the right, the mini truck structure is getting a block on top, forming a more complete mini truck. [0:00:10 - 0:00:11]: Both structures become more refined. The \"RACE CAR\" structure on the left gets wheels and final touches, while the mini truck remains stable on the right side. [0:00:11 - 0:00:12]: The blocks on the left now resemble a more developed race car. The right side continues showcasing the completed mini truck with its multicolored block surface, yellow wheels, and a character next to it. [0:00:13]: The right screen now shows a single toy smoke puff graphic on the white surface. [0:00:14]: A close-up shot focuses on the mini truck on the right, revealing its detailed and colorful block structure with the same smoke puff graphic visible. [0:00:15 - 0:00:17]: The toy mini truck remains in place while a toy character sits alongside it. The background remains a simple yellow wall. [0:00:17 - 0:00:18]: The mini truck and character appear unchanged from the previous frames, maintaining their positions on the table. [0:00:18 - 0:00:19]: The character now sits on the mini truck, completing the observed movement in the frames.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What colors are the wheels of the race car?",
        "time_stamp": "00:00:12",
        "answer": "C",
        "options": [
          "A. Green and blue.",
          "B. Red and blue.",
          "C. Yellow and Red.",
          "D. Brown."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_202_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:12]: A first-person view shows a hand interacting with a colorful plastic block toy on a white surface. The toy consists of small interlocking blocks arranged in a compact structure. The toy's colors include orange, green, and yellow, and it has a happy face on the front. The hand moves the blocks, initially removing the topmost orange block and subsequently rearranging the blocks on top of the toy. [0:01:13 - 0:01:17]: The hand continues to adjust the blocks, adding blue pieces atop the toy. The overall structure remains stable as the blocks are stacked higher. The toy retains its happy face, now surrounded by green, yellow, and blue blocks. The orientation of the blocks indicates a vertical stacking pattern. [0:01:18 - 0:01:19]: The hand turns the toy on its side, showing the underside of the structure. The toy's face is now oriented sideways, with the interlocking block pattern visible on the bottom. The colors remain consistent, and the blocks are neatly aligned with each other.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What feature is present on the front of the toy?",
        "time_stamp": "00:01:12",
        "answer": "B",
        "options": [
          "A. A sad face.",
          "B. A happy face.",
          "C. A star.",
          "D. A heart."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_202_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:08]: A hand is placing a blue building block parallel and directly on top of a series of yellow building blocks arranged in a straight line on a white surface. The blue block is being carefully aligned with the yellow blocks beneath it. [0:02:09 - 0:02:19]: The hand then picks up a yellow building block from the adjacent line and begins to place it on top of the blue block and the yellow blocks below, forming a second layer. The hand carefully places the yellow block, ensuring it is aligned with the blocks below.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "How are the yellow building blocks arranged before the blue block is placed on top?",
        "time_stamp": "0:02:08",
        "answer": "B",
        "options": [
          "A. In a circle.",
          "B. In a straight line.",
          "C. In a random pattern.",
          "D. In a triangular shape."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_202_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:09]: A structure made of interlocking plastic building blocks is placed on a flat surface. The structure consists of various colors - blue blocks at the bottom edges, yellow in the middle, green on top of the yellow, and an arrangement of orange blocks on top. A hand is seen interacting with the blocks, specifically lifting and placing an orange block back in its position on the top layer. Subsequently, another green block is moved from the structure and placed back on a nearby part of the structure. [0:03:10 - 0:03:12]: After removing a green block and an orange block from the structure temporarily, the hand stops interacting and the structure remains intact with blue, yellow, green, and orange layers clearly visible and well-arranged. [0:03:13 - 0:03:19]: A second item consisting of a small structure made of yellow and red interlocking plastic building blocks with wheels is moved into view from the bottom left. The red and yellow wheeled structures are being assembled by the hand, with the red part being connected to the yellow part.\n[0:03:40 - 0:04:00] [0:03:40 - 0:03:44]: A colorful toy vehicle constructed from interlocking building blocks is placed on a white surface against a beige wall. The toy includes various colors such as green, orange, yellow, and blue. The vehicle has red and yellow wheels and is stationary on the table.  [0:03:44 - 0:03:46]: The camera angle shifts slightly, and the toy vehicle appears to be moving to the left side of the screen, creating the impression of motion. [0:03:47 - 0:03:51]: The camera returns to a close-up view of the toy vehicle, focusing on the detailed color blocks and wheels. The toy is situated on the white surface, against the same beige wall. [0:03:52 - 0:03:56]: The scene transitions to an orange background with animated lines and the text \"Thanks for watching!\" in white, stylized handwriting. It then adds, \"DON'T FORGET TO SUBSCRIBE!\" below it. [0:03:57 - 0:04:00]: The video then shows a comparison between two toy vehicles, labeled \"Race Car\" and \"Mini Truck\". Each vehicle is made from colorful interlocking building blocks. The \"Race Car\" resembles the earlier vehicle, while the \"Mini Truck\" has a different design, featuring a rectangular shape with a smiling face on the front.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the new item introduced in the video?",
        "time_stamp": "00:03:22",
        "answer": "B",
        "options": [
          "A. A small structure made of blue and green blocks.",
          "B. A small structure made of yellow and red blocks with wheels.",
          "C. A toy vehicle with blue and yellow wheels.",
          "D. An orange and green interlocking block structure."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Event Understanding",
        "question": "What happened to the toy vehicle just now?",
        "time_stamp": "00:03:47",
        "answer": "C",
        "options": [
          "A. The toy vehicle is disassembled.",
          "B. The toy vehicle moves to the right.",
          "C. The toy vehicle start circling around the table.",
          "D. The toy vehicle is lifted off the surface."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_202_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the current mph of the rider in the video?",
        "time_stamp": "00:00:49",
        "answer": "A",
        "options": [
          "A. 32 mph.",
          "B. 33 mph.",
          "C. 34 mph.",
          "D. 35 mph."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_163_real.mp4"
  },
  {
    "time": "[0:01:52 - 0:02:12]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the current kmh of the rider in the video?",
        "time_stamp": "00:01:01",
        "answer": "B",
        "options": [
          "A. 52 kmh.",
          "B. 51 kmh.",
          "C. 53 kmh.",
          "D. 54 kmh."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_163_real.mp4"
  },
  {
    "time": "[0:03:44 - 0:04:04]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the current grad. of the rider in the video?",
        "time_stamp": "00:03:45",
        "answer": "C",
        "options": [
          "A. -7.",
          "B. -9.",
          "C. -8.",
          "D. -6."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_163_real.mp4"
  },
  {
    "time": "[0:05:36 - 0:05:56]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the current grad. of the rider in the video?",
        "time_stamp": "00:06:11",
        "answer": "D",
        "options": [
          "A. -7.",
          "B. -9.",
          "C. -6.",
          "D. -8."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_163_real.mp4"
  },
  {
    "time": "[0:07:28 - 0:07:48]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the road positioned right now?",
        "time_stamp": "00:07:46",
        "answer": "D",
        "options": [
          "A. Between field and forest.",
          "B. Through a dense forest.",
          "C. Beside a river.",
          "D. Along a mountain ridge."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_163_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What mode of transportation are the two people on the left side of the road using now?",
        "time_stamp": "00:00:04",
        "answer": "D",
        "options": [
          "A. Car.",
          "B. Bus.",
          "C. Airplane.",
          "D. Bike."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_387_real.mp4"
  },
  {
    "time": "[0:02:06 - 0:02:11]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the brand name visible on the building to the left right now?",
        "time_stamp": "00:02:06",
        "answer": "D",
        "options": [
          "A. Greenbox.",
          "B. Green&Go.",
          "C. FreshStop.",
          "D. Fresh&CO."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_387_real.mp4"
  },
  {
    "time": "[0:04:12 - 0:04:17]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many flags are on the buildings to the left right now?",
        "time_stamp": "00:03:50",
        "answer": "B",
        "options": [
          "A. Five.",
          "B. Six.",
          "C. Three.",
          "D. Four."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_387_real.mp4"
  },
  {
    "time": "[0:06:18 - 0:06:23]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the name of the store visible on the awning to the right right now?",
        "time_stamp": "00:06:23",
        "answer": "D",
        "options": [
          "A. Adventure Baggage.",
          "B. Party Supplies.",
          "C. Green & Go.",
          "D. Gifts & Luggage."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_387_real.mp4"
  },
  {
    "time": "[0:08:24 - 0:08:29]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the company name visible on the red billboard to the right right now?",
        "time_stamp": "00:08:26",
        "answer": "D",
        "options": [
          "A. Barclays.",
          "B. Reuters.",
          "C. Tommy Hilfiger.",
          "D. Applebees."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_387_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:06]: The video begins with a view of three framed artworks hung on a white wall at an art gallery. The leftmost painting depicts a mother holding a baby while the central painting shows a figure with boxing gloves and a championship belt, with the text \"STING LIKE QUEEN BEE\" in bright colors. The painting on the right features a woman in a provocative pose with the word \"SUGAR\" written above her. The frames are ornate and different in color, with gold and black finishes. [0:00:06 - 0:00:11]: The camera moves to the right, showing more of the artwork displayed. There is a large painting of an oversized shark with its mouth open wide and a pig trying to balance on its nose. Birds are flying around the shark, and there is some water below, suggesting a seaside scene. The painting catches more of the gallery scene too, with visitors walking and observing other artworks in the background. [0:00:11 - 0:00:15]: The video continues to focus on the shark and pig painting, showing it in greater detail. The painting is surreal and fantastical, with a sharp contrast between the shark’s gaping mouth and the pig’s uneasy balance. Surrounding elements like birds and water are also clearer. [0:00:15 - 0:00:19]: The camera angle shifts to another set of paintings. The first painting shows a dreamlike scene featuring several anthropomorphic figures, including one with a yellow glow. The lower artwork depicts an assortment of fantastical creatures in an indoor setting. The camera briefly focuses on a smaller framed artwork featuring a three-dimensional, phallic-shaped object with a pink and white color scheme. [0:00:19 - 0:00:20]: The video concludes with a broader view of the second set of paintings, including the fantastical creatures and anthropomorphic figures. A bit of the surrounding gallery space is also visible, including a wall with more artwork and an unidentified person walking in the background.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the painting with the text \"STING LIKE QUEEN BEE\" located in relation to the other two paintings?",
        "time_stamp": "00:00:06",
        "answer": "B",
        "options": [
          "A. On the left.",
          "B. In the center.",
          "C. On the right.",
          "D. Above the other two."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Text-Rich Understanding",
        "question": "What text is visible in the central painting with the figure wearing boxing gloves?",
        "time_stamp": "00:00:03",
        "answer": "A",
        "options": [
          "A. \"STING LIKE QUEEN BEE\".",
          "B. \"SUGAR\".",
          "C. \"FIGHT LIKE A CHAMPION\".",
          "D. \"WINNER TAKES ALL\"."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Object Recognition",
        "question": "Which painting includes a seaside scene with birds flying around?",
        "time_stamp": "00:00:25",
        "answer": "C",
        "options": [
          "A. The painting of a mother holding a baby.",
          "B. The painting with the text \"SUGAR\".",
          "C. The painting of the oversized shark and pig.",
          "D. The painting with anthropomorphic figures."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_473_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:24]: Two framed artworks on a white wall are visible. The frames are black and oval-shaped. The left artwork features colorful illustrations with prominent use of blue and purple hues. A smaller piece above it has text labels next to it. The right artwork, similar in style, depicts a character on a web surrounded by colorful flowers. A price tag of $300 is seen below the artist's name, Anthony Ausgang. [0:05:25 - 0:05:26]: The camera pans right to reveal more artwork. There are three smaller framed pieces above the previous artworks, all featuring stylized characters with distinct backgrounds. They are labeled with prices and artist names. The wall socket is visible. [0:05:27 - 0:05:29]: Further right, a larger, three-dimensional piece resembling a Venus flytrap with a pink tongue, surrounded by smaller, similarly styled framed artworks. The camera reveals more wall space containing additional artworks and decorations. [0:05:30 - 0:05:34]:  The camera view shifts to a new, large painting in a different section. This artwork features a fantastical scene with an elephant-like figure playing an instrument and a woman in the lower right. The setting includes a staircase and various mystical elements. The frame is black, and spectators are visible in the background. [0:05:35 - 0:05:39]: The wider scene shows more of the gallery space, with visitors walking around and observing the artworks. The large painting remains centered in the frame. The gallery is spacious, with various pieces hanging on the white walls, creating a vibrant and eclectic atmosphere.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the price tag seen below the artist's name, Anthony Ausgang?",
        "time_stamp": "0:05:28",
        "answer": "B",
        "options": [
          "A. $150.",
          "B. $300.",
          "C. $450.",
          "D. $600."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What does this sculpture, fixed to the wall in the video, demonstrate?",
        "time_stamp": "0:05:29",
        "answer": "B",
        "options": [
          "A. As a colorful parrot with extended wings.",
          "B. As a Venus flytrap with a pink tongue.",
          "C. As a metallic robot with glowing eyes.",
          "D. As a large tree with hanging fruits."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_473_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What is the best summary of the food preparation process that was just shown?",
        "time_stamp": "00:00:10",
        "answer": "C",
        "options": [
          "A. The vendor picked up a hamburger patty, grilled it, and placed it on a bun before serving it to the customer.",
          "B. The vendor took a hot dog from a package, placed it into a bun, and handed it to the customer.",
          "C. The vendor took a hot dog from water, placed it into a bun, and prepared to add condiments.",
          "D. The vendor grilled a hot dog, topped it with relish, and gave it to the customer."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_338_real.mp4"
  },
  {
    "time": "[0:02:53 - 0:03:03]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What is the best summary of the food preparation process that was just shown?",
        "time_stamp": "00:03:03",
        "answer": "C",
        "options": [
          "A. The vendor grilled a hot dog, topped it with mustard and onions, and served it to the customer.",
          "B. The vendor took a hot dog from a grill, placed it into a bun, added mustard and onions, and gave it to the customer.",
          "C. The vendor boiled a hot dog, placed it in a bun, topped it with sauerkraut and onions, and wrapped it.",
          "D. The vendor took a hot dog, placed it in a bun, topped it with relish and mustard, and handed it to the customer."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_338_real.mp4"
  },
  {
    "time": "[0:05:46 - 0:05:56]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions just shown?",
        "time_stamp": "00:05:56",
        "answer": "B",
        "options": [
          "A. The vendor picked up a hot dog, placed it on a grill, and later added sauerkraut before serving.",
          "B. The vendor boiled a hot dog, placed it into a bun, and added mashed potatoes as a topping.",
          "C. The vendor picked up a hot dog, placed it into a bun, added onions, and wrapped it.",
          "D. The vendor grilled a hot dog, placed it into a bun, added mustard and ketchup, and handed it to a customer."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_338_real.mp4"
  },
  {
    "time": "[0:08:39 - 0:08:49]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What is the best summary of the food preparation process that was just shown?",
        "time_stamp": "00:08:49",
        "answer": "C",
        "options": [
          "A. The vendor grilled a hot dog, topped it with relish, cheese, and mushrooms, and handed it to a customer.",
          "B. The vendor took a hot dog from boiling water, placed it into a bun, added cheese and peppers, and served it.",
          "C. The vendor prepared a hot dog, added chili, onions, and shoestring potatoes, and wrapped it.",
          "D. The vendor prepared a hot dog, topped it with mayo, mustard, and jalapenos, and served it with a side of fries."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_338_real.mp4"
  },
  {
    "time": "[0:11:32 - 0:11:42]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following actions best summarizes the actions just shown?",
        "time_stamp": "00:11:42",
        "answer": "B",
        "options": [
          "A. The vendor boiled a hot dog, placed it into a bun, added sauerkraut, and wrapped it before serving it.",
          "B. The vendor took a hot dog from boiling water, placed it into a bun, and prepared to add condiments.",
          "C. The vendor grilled a hot dog, topped it with mustard and onions, and served it to the customer.",
          "D. The vendor prepared a sandwich, added lettuce and tomato, and handed it to the customer."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_338_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:43]: The video begins with a first-person perspective focusing on a man standing in a kitchen, holding an egg up to the camera with his right hand. The kitchen background includes white tiled walls, dark cabinets with silver handles, and glass windows on the upper cabinets revealing some items inside. The man is wearing a dark grey T-shirt and light-colored pants, and he is positioned behind a kitchen island. There are pots and pans on the stove, and a chopping board with various ingredients next to him. [0:02:44 - 0:02:45]: The man is focused on the tasks at hand, and the angle shifts slightly to show more of the preparation area on the right side of the kitchen island. He puts the egg down on the chopping board among other ingredients including colorful bell peppers, chopped onion, and various small bowls. [0:02:46 - 0:02:47]: The man picks up a kitchen towel and uses it to hold onto a frying pan on the stove. The camera captures a close-up shot of the stove’s burners, which are turned on with flames visible. There is a grey plate on the stove next to the pan. [0:02:48 - 0:02:49]: He places the grey plate from the stove back onto the kitchen island and continues to prepare the frying pan, ensuring it is clean, while the camera shows more of the surrounding kitchen space, including appliances like a mixer and a toaster. [0:02:50 - 0:02:52]: The man moves slightly to the right, and he picks up a timer from the kitchen counter. The camera focuses on his actions as he sets the cooking timer. The view also offers a glance at the variety of ingredients laid out on the chopping board, including vibrant vegetables and kitchen utensils. [0:02:53]: The man demonstrates setting the cooking timer, holding it up to the camera closer for a clearer view. The timer shows \"10:00\" in large digits, indicating it is set to count down from ten minutes. [0:02:54 - 0:02:55]: After setting the timer, he places it back down on the counter and prepares to use the frying pan. The man directs his attention back toward the stove area, with a white kitchen towel draped over his shoulder. [0:02:56 - 0:02:57]: The man uses the kitchen towel again to lift the frying pan, and the camera captures a close-up of the stove burner beneath the pan. He adjusts the pan on the burner, continuing the cooking process. [0:02:58 - 0:02:59]: He places the frying pan back on the burner, making sure it is stable, and the camera shows more of the kitchen background including the upper cabinets with glass windows. Various kitchen items and appliances are visible around the spacious and well-lit kitchen as he continues working attentively.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the man holding up to the camera with his right hand right now?",
        "time_stamp": "00:02:34",
        "answer": "A",
        "options": [
          "A. An egg.",
          "B. A spoon.",
          "C. A bell pepper.",
          "D. A chopping board."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Event Understanding",
        "question": "What is the man doing with the egg after holding it up to the camera?",
        "time_stamp": "00:02:58",
        "answer": "B",
        "options": [
          "A. Cracking it into a bowl.",
          "B. Placing it on the table.",
          "C. Cooking it in a pan.",
          "D. Putting it back in the refrigerator."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_37_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:21]: The video begins with a frying pan on a stove. Inside the pan, there are chunks of minced meat, partially cooked, with visible pink and browned parts. Red chili slices are spread over the meat. The stove burner underneath is lit with a blue flame, and the pan is positioned in the center of the frame. The background shows kitchen cabinets with a dark gray finish and metallic handles. [0:05:21 - 0:05:22]: The scene remains mostly the same, with the focus still on the frying pan. A person begins to enter the frame from the right, with part of their arm becoming visible. [0:05:22]: A close-up of the person shows them holding a piece of yellow lemon in their left hand and a metal grater in their right hand over the pan. The person's torso is visible, wearing a dark-colored shirt and beige shorts. They stand in a modern kitchen with white tiles and dark cabinetry. [0:05:23 - 0:05:28]: The person starts grating the lemon zest directly into the frying pan. The focus is on the grating process as the zest falls onto the cooking meat and chilies. The person's actions are steady and deliberate, ensuring the zest spreads evenly over the ingredients. [0:05:29 - 0:05:37]: The grating process continues, with the person adjusting the angle of the grater slightly to scrape off more zest. The grater is held over the pan while zest collects, mixing with the meat and chilies. The background, including the kitchen cabinets and stove knobs, remains consistent, contributing to the setting's domestic atmosphere. [0:05:38 - 0:05:39]: After finishing grating, the person moves away from the stove, placing the grater down. They turn slightly to the left, extending their right arm towards an unseen object on the countertop. The movement indicates the person is reaching for another ingredient or tool needed for the next step in the cooking process.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is spread over the chunks of minced meat in the frying pan?",
        "time_stamp": "0:05:11",
        "answer": "A",
        "options": [
          "A. Red chili slices.",
          "B. Green peppers.",
          "C. Yellow corn.",
          "D. White onion rings."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_37_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:02]: A person is standing at a kitchen counter, holding a knife in their right hand and a piece of red bell pepper in their left. They begin slicing the bell pepper on a wooden cutting board placed on a marble counter. On the counter to the left, there is a gas stove with pots and pans. An olive oil bottle with a metallic cap is situated on the counter to the right of the cutting board.   [0:08:02 - 0:08:04]: The person continues slicing the red bell pepper into thin strips. Next to the cutting board, there are other cooking ingredients, including a small white bowl, a mushroom, and various vegetables like an onion and a leek. A large silver knife is the person’s choice of tool.  [0:08:04 - 0:08:08]: The person steadily progresses with chopping the bell pepper into even smaller pieces, his hands working methodically on the cutting board. The kitchen background includes dark-colored cabinets, a white tile backsplash, and some kitchen appliances.   [0:08:08 - 0:08:12]: The person keeps chopping the bell pepper until it is finely sliced. They then place the knife down on the counter and pick up a chunk of the pepper. Behind them, the gas stove continues to be a focal point, with burners visible and cooking utensils arranged around it. [0:08:13 - 0:08:15]: The person moves to the stove, carrying the chopped bell pepper in their hand. They add the sliced pepper to a pan on the stove, which is turned on with visible flames. Above the stove, glass-front cabinets filled with dishes and kitchen items can be seen. [0:08:15 - 0:08:17]: The person then stirs the contents in the pan with a spatula, ensuring they are cooked evenly. The stove's burners and a countertop with other ingredients and cooking tools are visible in the foreground. [0:08:17 - 0:08:19]: The person continues to stir the contents in the pan, lifting it slightly to toss the ingredients. The kitchen remains brightly lit with a clean, organized setup, showcasing various kitchen equipment on counters and shelves.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is notable about the knife the person is using?",
        "time_stamp": "0:08:23",
        "answer": "A",
        "options": [
          "A. It is a large silver knife.",
          "B. It has a wooden handle.",
          "C. It is a small paring knife.",
          "D. It has a serrated edge."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_37_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:11:00]",
    "captions": "[0:10:40 - 0:11:00] [0:10:40 - 0:10:41]: A colorful assortment of vegetables, including sliced mushrooms, red and green bell peppers, and other greens, are being stir-fried in a black pan. The pan is centered over a flame on a gas stove, and a person's hand is slightly visible at the top-right corner. [0:10:42 - 0:10:42]: The vegetables continue to sizzle and mix in the black pan, maintaining their vibrant colors. The positioning remains constant, with the pan still centered over the stove flame and part of the stovetop controls in view. [0:10:43]: The shot zooms out slightly, revealing more of the kitchen setting. The stovetop is surrounded by a marble countertop, and more of the gas stove's controls and knobs become visible. A person in khaki pants and a dark shirt stands beside the stove, focusing on the pan. [0:10:44 - 0:10:45]: The person adjusts the pan while speaking, using a hand gesture to emphasize their point. A checkered cloth is draped over their shoulder. The kitchen background showcases dark cabinets and countertops, with kitchen utensils and appliances arranged neatly. [0:10:46 - 0:10:47]: The person continues to gesture and speak, occasionally looking into the pan. Their other hand holds a utensil, and they seem to be explaining a cooking technique. The background remains the same, with the neatly arranged kitchen visible. [0:10:48]: The person leans slightly toward the pan, which is still over the flame. The person appears to check the progress of the sautéed vegetables, ensuring they are cooked evenly. [0:10:49]: Raising the pan slightly off the stove, the person prepares to transfer its contents. The hand holding the pan is firm, while the other hand appears ready to assist in the process. [0:10:50 - 0:10:51]: With the pan raised, the person uses a wooden spatula to move the sautéed vegetables from the pan to a white plate on the counter. Next to the white plate is a grey plate filled with a different cooked dish, possibly meat mixed with vegetables. [0:10:52]: The person continues scraping the pan's contents onto the white plate, ensuring all the vegetables are transferred. Steam rises from both plates, indicating the dishes are hot and freshly cooked. [0:10:53]: Standing with the empty pan, the person reaches for a towel to wipe their hands. The kitchen environment, with its clean and organized counters, remains consistent. [0:10:54 - 0:10:55]: The person turns slightly to one side, likely preparing for the next cooking step. On the counter, the two plates with their respective dishes are clearly visible, steam still rising. [0:10:56]: The person faces the counter, ready to begin another task. The stovetop setup, with various cooking tools like a wooden spoon, a cutting board with chopped ingredients, and the empty pan on the stove, is clearly seen. [0:10:57 - 0:10:59]: The person continues with their cooking process, switching between tasks efficiently. The person seems to be multitasking, possibly preparing another dish or cleaning up in between steps.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What vegetables are being stir-fried in the black pan?",
        "time_stamp": "0:10:14",
        "answer": "A",
        "options": [
          "A. Sliced mushrooms, red and green bell peppers, and other greens.",
          "B. Carrots, peas, and broccoli.",
          "C. Zucchini, tomatoes, and onions.",
          "D. Eggplants, spinach, and corn."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_37_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video starts with a cityscape in the background, comprised of multiple high-rise buildings against a light blue sky. In the foreground, there is a road with white dashed lane markings and a few green trees lining the street.  [0:00:01 - 0:00:04]: A toy car made from interlocking plastic bricks enters the scene from the right side, moving leftward across the road. It has a multicolored body primarily composed of blue, green, orange, and yellow bricks, and red wheels. [0:00:04 - 0:00:05]: The toy car continues to move leftward, gradually moving out of the frame. [0:00:06 - 0:00:08]: The scene transitions to a different setting, depicting a park. In the background, there are more high-rise buildings with a light blue sky. The park has green bushes, orange and yellow trees, and an orange bench. The same toy car reappears from the right side, moving leftward along the road. [0:00:09 - 0:00:12]: The scene changes to a close-up, first-person view of a hand manipulating yellow interlocking plastic bricks on a marble-like surface. The hand places one yellow brick onto the surface, adding to the existing bricks. [0:00:13 - 0:00:15]: The hand continues arranging the yellow bricks, with a focus on aligning them into a specific formation. [0:00:16 - 0:00:20]: The hand completes the arrangement of the yellow bricks into a structured pattern on the flat surface.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What colors are the wheels of the toy car?",
        "time_stamp": "00:00:04",
        "answer": "C",
        "options": [
          "A. Blue and Yellow.",
          "B. Green and Red.",
          "C. Red and Yellow.",
          "D. Yellow and Green."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_205_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:00 - 0:01:20] [0:01:00 - 0:01:02]: A hand begins to manipulate a set of large, colorful building blocks on a smooth, light-colored surface. The blocks are arranged in a rectangular pattern with alternating rows of yellow, green, and orange blocks. The hand places a blue block on the left side of the rectangle. [0:01:03 - 0:01:04]: The hand picks up a green block from the left and places it next to the blue block on the left side of the structure, slightly separated from the main rectangle of blocks. [0:01:05 - 0:01:07]: The hand then picks up an orange block and starts to position it beside the green block that was placed earlier. The positioning shows careful alignment with the existing blocks. [0:01:08]: The orange block is placed next to the green block, creating a small rectangular addition on the left side. [0:01:09 - 0:01:11]: The hand picks up another yellow block and positions it over the newly added blocks, maintaining the consistency in the color pattern. Another hand appears, assisting in arranging the blocks. [0:01:12 - 0:01:13]: Both hands are seen adjusting and securing the newly placed blocks to ensure they fit properly, showing a cooperative effort in building the structure. [0:01:14 - 0:01:16]: One hand places another green block on top of the previous blocks, further expanding the pattern on the left side. The hands continue to adjust the blocks, ensuring they are securely connected. [0:01:17 - 0:01:19]: Both hands work together to properly align the new green block with the rest, creating a larger and more stable structure. The hands then step back, showing the expanded creation with additional blocks on the left side.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action did the hand perform after placing the green block next to the blue block?",
        "time_stamp": "0:01:10",
        "answer": "C",
        "options": [
          "A. Placed a yellow block over the blue block.",
          "B. Adjusted the green block.",
          "C. Picked up an orange block.",
          "D. Removed the blue block."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      },
      {
        "task_type": "Counting",
        "question": "How many hands are involved in arranging the blocks right now?",
        "time_stamp": "0:01:19",
        "answer": "B",
        "options": [
          "A. One hand.",
          "B. Two hands.",
          "C. Three hands.",
          "D. Four hands."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_205_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:02]: The video shows a flat surface with several yellow, green, orange, and blue plastic building blocks arranged in a rectangular pattern. A hand begins to place a yellow building block on the left side of the formation. [0:02:03 - 0:02:05]: The hand adjusts the yellow block's position on the left side and makes sure it fits properly with the other blocks. [0:02:06 - 0:02:09]: The hand picks up an orange block and places it below the yellow block on the left side, ensuring it fits securely with the other blocks. [0:02:10 - 0:02:12]: The hand reaches for a green block and attaches it to the existing structure, aligning it with the other green blocks in the center row. [0:02:13 - 0:02:15]: The hand makes a slight adjustment to the green block, pressing it down more firmly to ensure it is securely connected. [0:02:16 - 0:02:19]: The hand continues to rearrange and secure the blocks on the left side, making minor adjustments to ensure all blocks are firmly connected and aligned. The various colors of the blocks—yellow, orange, green, and blue—create a visually appealing and structured pattern on the flat surface.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What does the hand do after picking up the yellow building block?",
        "time_stamp": "0:02:06",
        "answer": "B",
        "options": [
          "A. Places it on the right side.",
          "B. Places it on the left side.",
          "C. Discards it.",
          "D. Hands it to someone else."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_205_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:02]: A hand is holding a green Lego block, attaching it to a structure composed of blue and yellow blocks lying on a light-colored, slightly reflective surface. [0:03:03 - 0:03:06]: The hand adjusts the green block, ensuring it is placed properly beside the others. The structure maintains its layout with the yellow and blue blocks forming a base, while the green block is connected perpendicularly. [0:03:07 - 0:03:09]: The hand positions the green block securely, pressing it into place on the left side of the structure. [0:03:10 - 0:03:12]: The hand then picks up an orange block and begins placing it on the right side of the structure, aligning it beside the existing blocks. [0:03:13 - 0:03:14]: The orange block is pressed into place, forming a continuation of the structure which now includes green, blue, yellow, and orange blocks. [0:03:15 - 0:03:17]: The hand picks up another green block and positions it parallel to the first, on top of the structure. [0:03:18 - 0:03:19]: The hand adjusts both green blocks, ensuring they are securely attached while also maintaining the arrangement of the other colored blocks in the structure.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What does the hand do after positioning the three green blocks securely?",
        "time_stamp": "00:03:11",
        "answer": "B",
        "options": [
          "A. Picks up a red block.",
          "B. Picks up an orange block.",
          "C. Picks up another green block.",
          "D. Adjusts the yellow block."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "block_building",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_205_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:18",
        "answer": "A",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_74_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:31",
        "answer": "B",
        "options": [
          "A. 4.",
          "B. 5.",
          "C. 3.",
          "D. 2."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_74_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:18",
        "answer": "D",
        "options": [
          "A. 1.",
          "B. 3.",
          "C. 2.",
          "D. 5."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_74_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:07:18",
        "answer": "C",
        "options": [
          "A. 1.",
          "B. 3.",
          "C. 5.",
          "D. 7."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_74_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00】",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:08:11",
        "answer": "B",
        "options": [
          "A. 5.",
          "B. 6.",
          "C. 7.",
          "D. 8."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_74_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:07]: In the initial frames, the scene is blurred with text appearing in the center. The text unfolds in stages reading \"In Todays video,\" \"I'am building,\" \"Something special,\" \"bcs,\" \"halloween,\" and \"so I decided to build,\" announcing the theme of the video. The background appears to show greenery and water, hinting at a natural environment;  [0:00:08]: The text \"a haunted mansion\" emerges, with the background still blurred but hints of foliage and terrain becoming visible;  [0:00:09]: \"With a little twist\" is displayed, setting a mysterious tone. The background starts hinting at coherent shapes resembling a game environment possibly with some constructed elements;  [0:00:10 - 0:00:13]: The scene transitions to a clear first-person perspective within a game environment, showing a character in colorful armor standing on a wooden platform surrounded by chests and barrels, adjacent to a body of water and green hills with trees. The text below reads \"as you can tell,\" \"I have also changed,\" announcing a new appearance or modification;  [0:00:14]: The phrase \"my skin\" appears, indicating the change made was to the character’s avatar skin. The character's positioning is static, facing directly in front;  [0:00:15]: The camera view shifts, facing the large body of water, bordered by grassy and hilly terrain on the sides, creating an immersive environment;  [0:00:16]: The text \"But before\" marks a pause before starting the building process. The scene remains unchanged with the water view;  [0:00:17]: The perspective remains consistent with the character, holding an item, looking over the water in a contemplative manner. The text reads \"into building,\" indicating the storyline’s continuation;  [0:00:18]: The water view persists as the text reads \"this beautiful build,\" possibly referring to the upcoming construction activity;  [0:00:19]: Subsequently, an over-the-shoulder view of the character navigating in a boat appears. The on-screen text clarifies using \"this beautiful build, more like creepy,\" suggesting the building is spooky;  [0:00:20]: The viewpoint transitions to the character speaking directly to the camera, positioned against a wooden and earthy background. The text \"But we’re,\" finalizes the introduction, indicating a forward movement to actual building activities.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the theme of the video as revealed in the text at the beginning?",
        "time_stamp": "0:00:20",
        "answer": "C",
        "options": [
          "A. Building a treehouse.",
          "B. Exploring a natural park.",
          "C. Constructing a haunted mansion.",
          "D. Farming in a game environment."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_187_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:41]: The video begins with an expansive view of a dense forest with numerous tall trees and a clear blue sky overhead. Flames and smoke are visible in the trees below, indicating a forest fire. The on-screen text reads, \"Oh, and the forest\". [0:02:41 - 0:02:42]: The camera continues to move forward, providing a wider view of the burning forest below. The text on-screen says, \"down in here.\" [0:02:42 - 0:02:43]: The viewpoint shifts slightly upward, showing more of the blue sky and scattered white clouds above the forest. The display text reads, \"You know what, guys?\" [0:02:43 - 0:02:44]: The camera continues to move forward, maintaining a high vantage point over the forest. The text on the screen states, \"I'm fed up\". [0:02:44 - 0:02:45]: The camera's view remains high over the forest, which appears vast and continues to stretch out to the horizon. The text reads, \"chopping down wood\". [0:02:45 - 0:02:46]: The view from the camera remains steady, showing the expansive forest below and a clear sky above. The text on the screen says, \"because this is going\". [0:02:46 - 0:02:47]: The camera slowly pans forward over the dense forest. The on-screen text reads, \"to take way too long.\" [0:02:47 - 0:02:48]: The view becomes steadier, offering a consistent perspective over the forest moving forward slightly. The text on the screen reads, \"Oh, my goodness.\" [0:02:48 - 0:02:49]: The camera descends quickly, focusing on a tree canopy. The text states, \"Oh, I missed\". [0:02:49 - 0:02:50]: The camera continues to descend closer to the trees, providing a detailed view of the leaves and branches. The text reads, \"But this forest\". [0:02:50 - 0:02:51]: The camera stabilizes just above the tree canopy, focusing on the lush green leaves. The text says, \"is just burning\". [0:02:51 - 0:02:52]: Flames and smoke are visible among the trees in the distance. The text on the screen reads, \"It's burning over\". [0:02:52 - 0:02:53]: The viewpoint shifts to show someone in colorful armor standing on the tree canopy. The person continues standing among the branches.  [0:02:53 - 0:02:54]: The person in the colorful armor remains motionless, still positioned on the tree canopy with the forest in the background. [0:02:54 - 0:02:55]: The camera angle remains the same, keeping the person in the colorful armor centered while standing among the branches. [0:02:55 - 0:02:56]: The person in the colorful armor holds up a block labeled \"Spruce Leaves\". The forest serves as the backdrop. [0:02:56 - 0:02:57]: The person now holds a tool labeled \"Flint and Steel\". The tree branches and forest are still visible in the background. [0:02:57 - 0:02:58]: The person with the colorful armor remains in focus, surrounded by the dense canopy. The text reads, \"I mean.\" [0:02:58 - 0:02:59]: The camera shifts to show the ground level within the forest, where dry underbrush and the bases of tall trees are visible. [0:02:59]: The view now shows the person ablaze, indicating an accidental fire on the character, with the forest's base and surrounding vegetation still visible.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is the video primarily depicting right now?",
        "time_stamp": "0:03:15",
        "answer": "D",
        "options": [
          "A. A peaceful forest.",
          "B. A construction site.",
          "C. A desert landscape.",
          "D. A forest fire."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      },
      {
        "task_type": "Action Recognition",
        "question": "What did this person do in the video just now?",
        "time_stamp": "0:02:59",
        "answer": "B",
        "options": [
          "A. He start flying.",
          "B. He catch on fire.",
          "C. He climb a tree.",
          "D. He swim in a river."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_187_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:03]: The scene starts with a first-person view directed upwards towards a partly cloudy sky. The sky is predominantly bright blue with a few scattered white clouds. Below the horizon, there is a dark, steeply pitched, tiled roof occupying the left side of the frame, and a dense forest of pine trees covers the background. [0:08:03 - 0:08:06]: The camera angle shifts slightly downwards and focuses more on the structure's dark roof, revealing more details of the building made of dark blocks. The forest remains visible in the background as the viewer seems to be hovering or standing on the edge of this roof. [0:08:06 - 0:08:07]: The view adjusts to capture more of the roof's edge made up of dark, rugged tiles. More of the dense pine forest below becomes visible, indicating the person's movement toward the roof's edge. [0:08:07 - 0:08:08]: The camera points down along the dark roofing tiles, offering a close-up view while the green tops of pine trees remain visible in the background. [0:08:08 - 0:08:10]: The focus is now on the wall, showing a textured black block wall sitting above a layer of stone brickwork running horizontally. The viewer's position appears steady just in front of the wall. [0:08:10 - 0:08:12]: The view centers on the dark block wall as the perspective looks upward slightly, revealing the upper portion of the building wall made of black stone blocks. [0:08:12 - 0:08:13]: Focused on placing a TNT block, the camera is positioned close to the wall. The red and white TNT block is being set against the black block wall, aligned neatly. [0:08:13 - 0:08:14]: The viewer's perspective is fixed closely on the dark wall and the first TNT block that is placed. The player's hands, which appear with equipped TNT blocks, are visible. [0:08:14 - 0:08:15]: A second TNT block is placed next to the first one on the dark block wall, showing a close-up of the process. The viewer's hand is holding another TNT block, aligned with the already placed blocks. [0:08:15 - 0:08:16]: The perspective pulls back slightly to reveal both TNT blocks attached to the dark wall. The player's hands are visible in the frame, indicating readiness to proceed with the detonation. [0:08:16]: The view faces downward toward the edge of the dark roof tiles while showing the hand holding a flint and steel, preparing to ignite the TNT. [0:08:17]: The viewpoint shifts to a third-person perspective, showing the character equipped in purple armor holding the flint and steel. The back of the character is toward the camera, standing along the edge of the roof. [0:08:17 - 0:08:18]: The character faces the TNT blocks on the roof from a short distance. The TNT blocks ignite, and white sparkles appear, indicating imminent explosion while the character begins to move slightly. [0:08:18]: An explosion occurs, resulting in the dark block wall being blown apart, creating a square hole. The character is seen recoiling slightly from the blast while maintaining balance on the roof. [0:08:18 - 0:08:19]: The camera shifts back to a view of the aftermath, showing the hole created by the explosion. The character in the purple armor looks at the damage done to the wall. [0:08:19 - 0:08:20]: The same perspective from behind the character in purple armor remains as the character gazes at the hole in the dark wall, ensuring the objective is achieved. [0:08:20]: The character is now standing and appearing satisfied with the result. The camera captures the full profile of the character in front of the damaged wall. [0:08:21]: The view further shifts to show the front of the character's face and armor, set against the background of the blown-up section of the wall with the forest still visible behind. [0:08:22]: The camera remains focused on the character from the front, providing a medium close-up with the entire wall, character, and background forest within the frame. [0:08:23]: The view is slightly zoomed out to capture a larger view of the character, including more of the roof and the scene's surroundings. The character remains still while the blown wall is clearly shown in the frame.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What type of armor is the character wearing?",
        "time_stamp": "0:08:17",
        "answer": "C",
        "options": [
          "A. Golden armor.",
          "B. Iron armor.",
          "C. Purple armor.",
          "D. Leather armor."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Event Understanding",
        "question": "What happens as a result of the TNT explosion?",
        "time_stamp": "0:08:25",
        "answer": "D",
        "options": [
          "A. The roof collapses.",
          "B. The pine trees catch fire.",
          "C. The character is thrown off the roof.",
          "D. The dark block wall is blown apart."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_187_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:06]: The video begins with a view of a shelf stocked with various dairy products, including yogurt containers. There are different types and brands, primarily in blue, white, and green packaging. A gloved hand is seen picking up a container from the shelf, moving it from the lower level to the eye level shelf. The shelving unit is metal with adjustable holdings, and each shelf is illuminated by bright white lights. [0:00:07 - 0:00:10]: The camera then shows a different perspective, likely from an elevated angle, looking down at a cart filled with similar yogurt containers. The cart has a metal framework holding multiple stacks of yogurts, and there are also empty cardboard trays. A person uses both hands to organize the items in the cart. [0:00:11]: The footage shows a wider shot of the storage area, with multiple shelves filled with dairy products. There are more boxes and other grocery items on the left side, stacked on a metal cart. [0:00:12 - 0:00:19]: The view shifts back to the original perspective, focusing on the individual removing more yogurt containers and placing them neatly on the shelf. The gloved hand is continuously in motion, organizing the products. The shelves remain brightly lit, with some additional products in the background behind the glass doors of the refrigerator unit.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person in the video organizing on the shelf?",
        "time_stamp": "0:00:20",
        "answer": "B",
        "options": [
          "A. Boxes of cereal.",
          "B. Containers of yogurt.",
          "C. Bottles of milk.",
          "D. Packages of cheese."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_438_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:21]: The video starts with a view of a metal cart in the center of an aisle, stocked with boxes and containers. The person wearing gloves is holding a cardboard box and setting it on the cart. Shelves filled with various cartons and containers line both sides of the aisle. [0:02:21 - 0:02:24]: The person walks towards a set of shelves, reaches out, and picks up a green boxed item. The hand movement is precise, indicating the person is restocking or organizing items. The shelves contain similar products, mainly milk or juice cartons with different price tags. [0:02:24 - 0:02:27]: The individual returns to the cart, which has metal bars forming a grid structure. Boxes and cans are neatly stacked on the lower shelves. The aisle is narrow, with more products on both sides. The floor is clean, made of gray concrete. [0:02:27 - 0:02:30]: The person continues organizing items, occasionally glancing at the shelves. They move towards a storage shelf on the opposite side, which has several boxes and containers stacked. The shelves appear to store perishable goods, likely in a storage room. [0:02:30 - 0:02:31]: The focus shifts to a higher shelf where the person grabs a large box. This box seems heavier, requiring both hands to maneuver it into place. The background shows additional shelves with uniformly stacked items. [0:02:31 - 0:02:34]: The person places the box on the designated shelf and then moves back, possibly to retrieve another item. The shelves are metallic, and various items, such as food products and boxes, are stored systematically. The spacing between the shelves allows for easy movement. [0:02:34 - 0:02:36]: With both hands, the individual grabs another box, reaches up, and positions it on the upper shelf. Fit neatly among other boxes, these items are similar in size and shape. The lighting in the storage area is bright, ensuring clarity for the task. [0:02:36 - 0:02:39]: The person adjusts the new item on the shelf, ensuring it is secure. The room is climate-controlled, with air conditioners visible on the walls to maintain optimal temperature for stored goods. Once done, they step back to assess the arrangement, ensuring everything is in place.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the person holding right now?",
        "time_stamp": "0:02:21",
        "answer": "B",
        "options": [
          "A. A metal cart.",
          "B. A cardboard box.",
          "C. A green boxed item.",
          "D. A juice carton."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Action Recognition",
        "question": "What did the person do after walking towards a set of shelves just now?",
        "time_stamp": "0:02:24",
        "answer": "A",
        "options": [
          "A. Place green cartons.",
          "B. Organizes the cart.",
          "C. Cleans the floor.",
          "D. Adjusts the lighting."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_438_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:02]: A person wearing black gloves reaches for milk cartons on a supermarket shelf. They adjust the cartons labeled \"Mellanmjölk\" with price tags above reading \"33.50\" and \"37.95.\" The shelves are well-stocked with various brands of milk cartons. [0:07:03 - 0:07:05]: The individual continues to adjust the position of the milk cartons. They appear to be aligning them neatly, moving them slightly. [0:07:06 - 0:07:09]: The person moves another milk carton forward. The shelf above displays the price tag \"34.95\" for Crème Fraîche. The surroundings include stacked cartons and other dairy products. [0:07:10 - 0:07:13]: The person further arranges the cartons on the shelf, pulling forward one of the Mellanmjölk cartons. They seem to be organizing the items, maintaining alignment. [0:07:14 - 0:07:16]: The individual takes a milk carton from the back of the shelf and brings it forward. The surrounding shelves are filled with similar products, and the action is focused on arranging the cartons. [0:07:17 - 0:07:19]: The person continues to adjust the position of the milk carton in the front row. The setting remains consistent, involving a repetitive motion of aligning and pulling cartons forward on the shelves.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What does the person do after adjusting the position of the milk cartons?",
        "time_stamp": "0:07:09",
        "answer": "B",
        "options": [
          "A. Removes a carton from the shelf.",
          "B. Pulls forward another milk carton.",
          "C. Leaves the aisle.",
          "D. Talks to another person."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_438_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:08:46]",
    "captions": "[0:08:40 - 0:08:46] [0:08:40 - 0:08:43]: The video is set in a grocery store, specifically in the dairy section. There are several shelves filled with various dairy products, including multiple stacks of \"Kesella\" brand items in different flavors, such as vanilla and plain. The products are arranged neatly in rows on the shelves. A person's hand is reaching into the shelves, likely to pick up an item. The shelves are well-lit with white LED lights, creating a bright environment. [0:08:43 - 0:08:45]: The view pans left, showing more of the dairy section and revealing additional rows of products such as milk cartons and other packaged dairy goods. Visible to the left are several layers of shelves housing different products. Further down the aisle, a yellow cleaning sign can be seen. [0:08:45 - 0:08:46]: The camera angle shifts to a top-down view of a shopping cart filled with multiple cartons of milk and other items. Additionally, there are stacks of cans and other groceries on the shelves against the wall and inside the cart. The background features a plain wall with product labels adhered. The setting appears to be a storage or stock area in the grocery store.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What brand of dairy products is prominently featured on the shelves right now?",
        "time_stamp": "00:08:42",
        "answer": "A",
        "options": [
          "A. Kesella.",
          "B. Danone.",
          "C. Yoplait.",
          "D. Nestle."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "storage_manage",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_438_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "captions": "[0:00:00 - 0:00:20] [0:00:01 - 0:00:04]: A person is seated at a white desk with a blue background. They are wearing glasses and a beige shirt. They appear to be explaining or reviewing a product. On the desk, there is a black graphics card with two large fans. Behind the person, to the left, there is a digital display with a dark theme showing various technical specifications, likely related to graphics cards. The display includes text in white and green colors with colorful graphs. The person gestures with their hands while speaking, indicating different aspects of their explanation. [0:00:05 - 0:00:08]: The digital display in the background disappears. The person continues to speak and gesture towards the black graphics card on the desk. White text \"RTX 4060\" appears next to the graphics card on the desk. The blue background remains the same and the person is consistently engaging with the camera, explaining the product details. [0:00:09 - 0:00:14]: The focus is primarily on the graphics card as the person continues to discuss it. They occasionally touch the graphics card and point to specific parts. The background has two black shelves on either side filled with various digital devices and decor items. The shelves are against a blue wall, creating a professional and clean setup. [0:00:15 - 0:00:19]: The camera zooms in on the graphics card, showing a detailed close-up of its components. The dual fans are clearly visible, along with the branding indicating it is an ASUS product. The graphics card is positioned at an angle, revealing its ports and connectors. The background is black, and the focus remains solely on the technical details and build quality of the graphics card.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What text appears next to the graphics card on the desk right now?",
        "time_stamp": "00:00:08",
        "answer": "A",
        "options": [
          "A. RTX 4060.",
          "B. GTX 3050.",
          "C. RTX 3090.",
          "D. GTX 1080."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_123_real.mp4"
  },
  {
    "time": "[0:02:20 - 0:02:40]",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:28]: The video begins with a close-up shot of a detailed comparison chart of different NVIDIA graphic cards (RTX 4060, RTX 4060 Laptop, RTX 4070 Laptop, and RTX 4080), placed against a black background. The chart highlights key specifications like the chip model (AD107), number of cores (CUDA), Boost clock speeds (in MHz), memory type and size, and other essential features such as bandwidth and power consumption. RTX 4060 is presented in a larger, green-highlighted box, emphasizing its specific metrics.  [0:02:29 - 0:02:30]: As the timestamp changes, the detailed comparison chart is visible, now including an additional focus on RTX 4060 and RTX 3060. The segment compares the primary statistics like memory size (12GB for RTX 3060, 8GB for RTX 4060) and other critical performance indicators. [0:02:31 - 0:02:35]: The graphic switches to focus on RTX 4090 and RTX 3090, showcasing their substantial specifications such as the number of CUDA cores, memory type, and sizes again (24GB for both models). The detailed numbers highlight the performance improvements in the newer models, with RTX 4090 having a higher count of CUDA cores and better memory specifications. [0:02:35 - 0:02:38]: The visual then emphasizes the CUDA core count, showing a comparison bar between RTX 3090 and RTX 4090, clearly indicating an increase of 56% in CUDA cores for RTX 4090, denoted by a large highlighted green bar extending more than halfway across the screen.  [0:02:39 - 0:02:40]: Following the previous segment, the comparison extends to include RTX 3060 and RTX 4060 in the detailed chart, continuing to stress the differences in CUDA core counts using the same format, with horizontal bars visually indicating updates in performance and improvements.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "According to the comparison chart, what is the memory size of the RTX 3060 and the RTX 4060 right now?",
        "time_stamp": "00:02:30",
        "answer": "A",
        "options": [
          "A. 12GB for RTX 3060, 8GB for RTX 4060.",
          "B. 8GB for RTX 3060, 12GB for RTX 4060.",
          "C. 10GB for RTX 3060, 8GB for RTX 4060.",
          "D. 16GB for RTX 3060, 10GB for RTX 4060."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_123_real.mp4"
  },
  {
    "time": "[0:04:40 - 0:05:00]",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:43]: The video begins with a screen showing a performance benchmark for several NVIDIA graphics cards. The title \"3DMark Time Spy Extreme 图形分\" is displayed at the top, followed by \"DirectX 12 单位:分 越高越好\". Below the title are performance bars for four graphics cards: RTX 4060, RTX 3060, RTX 4060 Ti, and RTX 3060 Ti. Each bar has a numerical score next to it. The RTX 4060 scores 5007, RTX 3060 scores 4918, RTX 4060 Ti scores 6235, and RTX 3060 Ti scores 5626. The background is blurred, and the text is sharp and clear. [0:04:44 - 0:04:49]: The scene changes to a man sitting at a desk. He is looking directly at the camera with a neutral expression. He is wearing glasses and a light-colored shirt, with his hands clasped together in front of him on the desk. In front of him is a graphics card positioned at an angle. Behind him is a blue wall with two bookshelves on either side. The left shelf contains several items, including a lamp and some books, while the right shelf has a few cameras and other electronic devices. The lighting is soft and even, highlighting the subject and the graphics card. [0:04:50 - 0:04:55]: The video transitions back to another screen showing a different performance test titled \"3DMark DXR Feature Test\". The text \"DirectX Raytracing 单位: fps 越高越好\" is shown. Below, there are performance bars for the same four graphics cards: RTX 4060, RTX 3060, RTX 4060 Ti, and RTX 3060 Ti. The RTX 4060 has a score of 27.2 fps, RTX 3060 has 20.1 fps, RTX 4060 Ti has 37.6 fps, and RTX 3060 Ti has 26.9 fps. The image is clear, with the background slightly blurred, maintaining focus on the text and performance bars. [0:04:56 - 0:04:58]: The scene returns to the man at the desk. This time, he is gesturing with one hand, appearing more engaged in his explanation. The graphics card is still positioned in front of him, and the shelves in the background remain unchanged. The lighting continues to be soft and even, ensuring clear visibility of the man and the product. [0:04:59 - 0:05:03]: The video shifts to another benchmark test display, titled \"使命召唤19\". It compares the RTX 4060, RTX 3060, RTX 4060 Ti, and RTX 3060 Ti across various metrics such as clock speed, power, temperature, usage, VRAM, and fps. Each card's details are shown in separate green boxes on the right side, with the corresponding performance graph on the left. The background contains a faint, blurred image, keeping the focus on the performance data.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "According to the performance test titled \"3DMark Time Spy Extreme 图形分\", which graphics card scored the highest?",
        "time_stamp": "00:05:00",
        "answer": "A",
        "options": [
          "A. RTX 4060 Ti.",
          "B. RTX 4060.",
          "C. RTX 3060.",
          "D. RTX 3060 Ti."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_123_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:07:20]",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:01]: The first frame shows a comparison between different graphics cards, specifically the RTX 3060 Ti, RTX 3060, and RTX 4060. The bars on the left graph represent the frames per second (FPS) achieved at different settings for the game 地平线5, with metrics for 2560x1440 resolution. On the right side of the frame, detailed measurements including clock speeds, temperatures, and FPS for each card are displayed while running the game. [0:07:02 - 0:07:02]: The second frame primarily features the RTX 4060 and RTX 3060. It displays similar information to the prior frame, illustrating average FPS, clock speeds, and temperatures using multiple graphs. The background is largely dark, highlighting the graphics card performance statistics. [0:07:03 - 0:07:03]: This frame focuses on a comparison at a 1920x1080 resolution. The left side of the frame has bar graphs showing the performance of RTX 3060 Ti, RTX 3060, and RTX 4060 for 侠盗猎车之旅. The right side still includes detailed statistics for each card under different conditions, displaying temperature, clock speeds, and FPS. [0:07:04 - 0:07:04]: Similar to the previous frame, this one also displays performance metrics at 1920x1080 resolution for the game 侠盗猎车之旅. The bar graphs on the left show FPS values increasing with each new model, while the right side continues to show in-depth data for each card. [0:07:05 - 0:07:05]: This frame continues the display of detailed performance metrics at 1920x1080 resolution for 侠盗猎车之旅. The FPS values and bar lengths are compared among the RTX 3060 Ti, RTX 3060, and RTX 4060, with the right side showing comparable detailed statistics. [0:07:06 - 0:07:06]: In this frame, the FPS values for each card in 侠盗猎车之旅 at 1920x1080 are highlighted. The bar graph shows the RTX 4060 outperforming the RTX 3060 Ti and RTX 3060, and the right side panel still contains intricate statistics for each graphics card's performance. [0:07:07 - 0:07:07]: This frame continues the comparison of FPS values for each card in 侠盗猎车之旅 at 1920x1080 resolution. There is a consistent presentation of bar graphs and side detailed statistics. [0:07:08 - 0:07:08]: The graphics in this frame still focus on the comparison at 1920x1080 resolution for 侠盗猎车之旅. The bar graphs and side panel detailed statistics remain consistent, offering a thorough comparison of the three graphics cards’ performance. [0:07:09 - 0:07:09]: This frame maintains the same structure, showcasing the RTX 3060 Ti, RTX 3060, and RTX 4060 performance comparison at 1920x1080 resolution in 侠盗猎车之旅, with bars on the left and detailed metrics on the right. [0:07:10 - 0:07:10]: The comparison of the RTX 3060 Ti, RTX 3060, and RTX 4060 at 1920x1080 resolution for 侠盗猎车之旅 continues in this frame, with clear bar graphs on the left and side detailed performance metrics on the right, highlighting FPS, clock speeds, and temperatures. [0:07:11 - 0:07:11]: The frame still shows a comparison at 1920x1080 resolution for 侠盗猎车之旅, maintaining the graphical structure with bar graphs on the left and detailed metrics on the right. [0:07:12 - 0:07:12]: This frame continues to reveal performance metrics in 侠盗猎车之旅 at 1920x1080 resolution for the RTX 3060 Ti, RTX 3060, and RTX 4060. The bar graphs and detailed side panel metrics remain consistent. [0:07:13 - 0:07:13]: The focus remains on performance metrics at 1920x1080 resolution for 侠盗猎车之旅. The frame shows bar graphs on the left and a side panel on the right detailing the performance of the RTX 3060 Ti, RTX 3060, and RTX 4060. [0:07:14 - 0:07:14]: The performance comparison for 侠盗猎车之旅 at 1920x1080 resolution continues, with the bar graph indicating",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which game's performance metrics are consistently shown for the RTX 3060 Ti, RTX 3060, and RTX 4060 at a 1920x1080 resolution right now?",
        "time_stamp": "00:07:20",
        "answer": "A",
        "options": [
          "A. 霍格沃兹之旅.",
          "B. 地平线5.",
          "C. Apex Legends.",
          "D. Call of Duty."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_123_real.mp4"
  },
  {
    "time": "[0:09:20 - 0:09:40]",
    "captions": "[0:09:20 - 0:09:40] [0:09:11 - 0:09:12]: On the left side of the video, the MSI Afterburner software is displayed, showing details of an NVIDIA GeForce RTX 4060 graphics card. The core voltage slider is set to 0, and the temperature bar is green. The right side of the screen displays a grid chart with fluctuating lines representing various statistics. Behind this is another graph with colored lines on a white background. [0:09:12 - 0:09:13]: The focus remains on the MSI Afterburner software, with the core clock, memory clock, and fan speed all shown on the left. The memory clock is highlighted in red with a value of +2000. The right side of the screen still displays the grid chart with multiple data lines. [0:09:13 - 0:09:14]: The memory clock remains highlighted at +2000, indicating its significance. Additional details on the left include core clock at +152, and fan speed readings. The grid chart is still visible on the right, depicting performance lines. [0:09:14]: The next scene shows the same elements: the MSI Afterburner with the memory clock still at +2000, and the core clock and fan speed values on the left. The grid chart on the right side continues to display performance metrics. [0:09:15 - 0:09:16]: A new scene replaces the previous one, with the memory clock unchanged at +2000 and the core clock at +1300. A red arrow points to specific metrics, featuring the number 214.09 FPS next to the memory clock value in green color. [0:09:16 - 0:09:17]: The core and memory clock values remain the same, alongside the performance metric highlighted by the red arrow. The focus includes more detailed readings on the grid chart. [0:09:17 - 0:09:18]: The memory clock value remains highlighted at +2000, with minimal changes to the other values. The performance metrics on the right continue to show various readings on the grid chart. [0:09:18 - 0:09:19]: The visual remains constant with the previous scene, indicating the memory clock, core clock, and fan speed values on the left side, and detailed grid chart metrics on the right. [0:09:19 - 0:09:20]: The scene switches to a different software view, displaying a benchmark result on the left with a score of 12,220, which is an improvement of +13%. On the right, the video game character image from \"TIME SPY\" is present, with a graph below showing fluctuating performance metrics. [0:09:20 - 0:09:21]: The next scene maintains the previous benchmark with the same score values. The performance chart below continues to show real-time data, with the \"TIME SPY\" character image remaining on the upper side. [0:09:21 - 0:09:22]: The details remain consistent with the benchmark score of 12,220 and the CPU score below that. The visual includes the \"TIME SPY\" character, and the performance graph below continues to display lines indicating readings. [0:09:22]: No major changes are observed, with the benchmark score remaining visible on the left. The \"TIME SPY\" character image and performance graph below stay consistent, reflecting continued performance metrics. [0:09:23 - 0:09:24]: The benchmark interface continues to display a score of 12,220, highlighting a +13% improvement. Beside it, the \"TIME SPY\" graphic with the performance graph below still present. [0:09:24 - 0:09:25]: The scene transitions slightly to include a new element, an RTX 4060 Ti score integrated within the benchmark interface. The score shown is 13,402, which is an improvement over the previous value. Below it, the performance metrics graph continues to reflect real-time data. [0:09:25 - 0:09:26]: Continuing from the previous scene, the RTX 4060 Ti score of 13,402 is still shown, alongside the other benchmark details. The \"TIME SPY\" character image and the performance graph beneath retain their positions and information. [0:09:26 - 0:09:27]: The benchmark details remain visible, now including the RTX 4060 Ti score. The visual focus stays on the performance score and the metrics graph below, with the \"TIME SPY\" character image on the right side. [0:09:30 - 0:09:31]: The video transitions to a man sitting at a white table, facing the camera. On the table in front of him is a computer graphics",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What benchmark score is being displayed for the time spy?",
        "time_stamp": "00:09:31",
        "answer": "A",
        "options": [
          "A. 13,001.",
          "B. 10,000.",
          "C. 11,380.",
          "D. 13,300."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_123_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video begins with a first-person perspective inside an art gallery. The focus is on a three-dimensional glass sculpture mounted on the left wall. The sculpture consists of several parallel, transparent glass panels with blue trees painted on each panel, creating a layered effect. The panels are attached to a light-colored wooden base.  [0:00:02 - 0:00:09]: As the camera moves slightly to the right, it reveals a large, colorful portrait on the wall. The portrait features a layered effect of a well-known figure, with overlapping and shifting images creating a sense of motion. The background consists of various vibrant colors, including green, purple, and yellow. On the left side of the portrait, shelves with additional glass sculptures are visible.  [0:00:09 - 0:00:12]: The camera continues to pan right, showing another artwork beside the colorful portrait. This new piece is a large, monochromatic red portrait of a different figure. The artwork resembles a woven or threaded texture, with intricate details in the hair and facial features. [0:00:12 - 0:00:15]: Moving further to the right, the camera captures a third piece of art. This piece is a mosaic made up of numerous small, cylindrical objects arranged to form a portrait. The portrait has a pop art style, with vivid colors such as red, yellow, and black creating the face and background. [0:00:15 - 0:00:20]: The camera zooms in closer to the mosaic artwork, highlighting the individual cylindrical elements. The close-up view reveals that each small component is a container with different colored substances inside, arranged meticulously to create the image. The video ends with a detailed shot of these individual elements.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What are the small components of the display shown right now?",
        "time_stamp": "00:00:15",
        "answer": "B",
        "options": [
          "A. Small paintings.",
          "B. Cylindrical objects.",
          "C. Rectangular tiles.",
          "D. Triangular pieces."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_477_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:23]: The video frames show a white wall on which a framed display of colorful donuts is mounted. The display is a grid of four rows, each with four donuts, making a total of 16 donuts. Each donut is uniquely decorated with various colors like pink, green, purple, blue, and yellow, and different patterns like sprinkles, stripes, and dots. [0:02:24 - 0:02:27]: The camera shifts to the right, revealing a wall covered with two adjacent pieces of artwork. The first piece features a blonde woman with blue eyes, lying down on a blue surface. Above her, the text reads, \"BUT I’M A FIRST CLASS GIRL… I CAN’T FLY COACH.\" The background is a collage of various images and text snippets. [0:02:28 - 0:02:31]: The camera continues to pan right, showing more of the second artwork, which depicts a couple embracing. The text in this artwork says, \"LET’S TRAVEL THE WORLD TOGETHER… & NEVER LOOK BACK.\" Similar to the first piece, the background is a collage of various images and text. [0:02:32 - 0:02:34]: The couple embracing in the second artwork becomes more prominent in the view, with the background collage becoming clearer and more detailed. [0:02:35 - 0:02:37]: The camera moves further to the right, where a black and white artwork of a heart is visible. The heart has a gradient effect from dark at the top to light at the bottom, formed by numerous small, intricate elements. [0:02:38 - 0:02:39]: Adjacent to the black and white heart artwork, there are other pieces of art featuring heart shapes. One heart is turquoise, and another artwork has a circular pattern with red, white, and blue gradients, composed of the same small, intricate elements.",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many donuts are displayed in the framed grid on the white wall?",
        "time_stamp": "0:02:23",
        "answer": "C",
        "options": [
          "A. 12.",
          "B. 14.",
          "C. 16.",
          "D. 18."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Text-Rich Understanding",
        "question": "What text is displayed above the blonde woman in the artwork?",
        "time_stamp": "0:02:27",
        "answer": "A",
        "options": [
          "A. \"BUT I'M A FIRST CLASS GIRL… I CAN'T FLY COACH.\".",
          "B. \"LET'S TRAVEL THE WORLD TOGETHER… & NEVER LOOK BACK.\".",
          "C. \"LIVE LOVE LAUGH… ALWAYS AND FOREVER.\".",
          "D. \"DREAM BIG… AND NEVER GIVE UP.\"."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_477_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:02]: At the start of the video, the camera shows an exhibit wall with various artworks. On the left, there is a large, intricate tapestry with various abstract shapes and creatures in predominantly blue, white, and black hues. To the right of it are colorful abstract paintings of various sizes, arranged in a grid pattern; two rows with two paintings in each row. The paintings feature vibrant, abstract designs. [0:07:03 - 0:07:08]: As the camera moves closer to the wall, it focuses more on the details of the abstract paintings. Each painting is rich in color, with layers of intricate shapes and figures. There are visible themes of nature and surreal figures. A yellow sticky note is seen positioned below the bottom left painting. [0:07:09 - 0:07:14]: The camera then pans slightly to the right, revealing a large, textured 3D artwork of a lion's head in gold on a dark wooden background. It is prominently mounted on the wall next to the colorful abstract paintings. The texture and detail of the lion's head are emphasized as the camera gets closer. [0:07:15 - 0:07:16]: The camera angle shifts upward slightly to capture a framed palette with blobs of paint in yellow, blue, and red. This framed piece has an inscription at the bottom reading \"3.6.61\". [0:07:17 - 0:07:18]: The focus then moves downward, showing the entirety of the framed palette. Below this frame, there is another frame which appears to contain a black and white sketch or print. [0:07:19 - 0:07:20]: The camera captures the lower part of the exhibit wall, giving a full view of the two frames. The upper frame displays the paint palette, and the lower frame contains a finely detailed sketch in black and white.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is prominently mounted on the wal right now?",
        "time_stamp": "0:07:14",
        "answer": "C",
        "options": [
          "A. A large, intricate tapestry.",
          "B. A framed palette with blobs of paint.",
          "C. A large, textured 3D artwork of a lion's head.",
          "D. A black and white sketch."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      },
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the inscription at the bottom of the framed palette?",
        "time_stamp": "0:07:16",
        "answer": "C",
        "options": [
          "A. 1.2.34.",
          "B. 2.5.89.",
          "C. 3.6.61.",
          "D. 4.7.73."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_477_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video begins with a first-person view stop in front of a painting hanging on a white wall. The painting is a minimalistic depiction of a beige car parked in front of a modern house with brown and black colors. The background of the painting is a bright blue sky with two green palm trees. On the right edge of the frame, part of another artwork featuring a brown animated cartoon character in a pink shirt holding a book is partially visible. [0:00:02 - 0:00:04]: The view shifts upward, revealing more of the surrounding area on the white wall. Another painting is now visible above the previous one. This upper painting shows two cars driving on a road, a red car in the foreground and a brown car slightly behind, both heading away. The sky in this painting is a gradient of light blue. [0:00:05 - 0:00:07]: As the view continues to pan upward and to the right, the letters \"PADO\" mounted on the wall become visible. To the right of the letters, a large, textured fabric artwork depicting a tall tree with a green and blue canopy appears. Additionally, the bottom half of a framed image of an animated cat, sitting and reading a newspaper, is in view. [0:00:08 - 0:00:10]: The perspective centers on the fabric artwork of the tree more prominently, which extends from about waist height to nearly the ceiling. Beneath parts of the tree is a pastoral scene with a figure sitting next to what appears to be a tent or dome shape, with green, brown, and light pink coloration. [0:00:11 - 0:00:14]: The perspective focuses more on the left section of the wall again, showing the fabric tree artwork fully and its surrounding images. The animated cat artwork is entirely visible in the frame, showing the cat dressed in clothes and holding a newspaper while sitting. [0:00:15 - 0:00:16]: The camera shifts right, capturing another textured artwork of a tree on the wall. In the foreground is another art piece, this time depicting a mighty rhinoceros with a colorful, floral design throughout its body. Multiple black birds are depicted on and around the rhino. Above the rhino image, abstract rectangular gray and multi-colored paintings are displayed. [0:00:17 - 0:00:19]: The view holds on the green rhinoceros artwork, with the gray abstract painting above. The background shows more parts of the gallery space, with a glimpse of more artworks framed and illuminated on the white walls. The perspective captures more details of the gallery's lighting and ceiling features. [0:00:20]: The video concludes with a focus on the rhino painting and its surrounding area, showcasing the detailed, colorful floral patterns and birds on the rhino's body.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What letters are mounted on the wall?",
        "time_stamp": "00:00:07",
        "answer": "A",
        "options": [
          "A. PADO.",
          "B. GATO.",
          "C. LADO.",
          "D. FADO."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the fabric artwork depicting a tall tree located in relation to the letters \"PADO\"?",
        "time_stamp": "00:00:10",
        "answer": "B",
        "options": [
          "A. To the right of the letters.",
          "B. Below the letters.",
          "C. To the left of the letters.",
          "D. Above the letters."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_480_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:40 - 0:04:00] [0:03:40 - 0:03:41]: In the first-person perspective video, a viewer is observing two vertically aligned paintings on the left wall and one painting on the right wall of an art gallery. The upper left painting features a large tree in bloom with pink flowers in a bright, clear landscape. The lower left painting shows another tree with green foliage against a more subdued background. To the right, there is an abstract, colorful portrait with a distinctive face, featuring a mix of vibrant colors and abstract shapes. [0:03:42]: As the camera continues to pan right, another piece of artwork blends into view, featuring a whimsical figure with flowers around its head and a background with text and varied colors. [0:03:43 - 0:03:44]: The camera fully unveils the artwork on the right - a cheerful, colorful piece depicting a smiling figure with a floral crown, holding a red heart shape. This artwork is next to another, more abstract piece on the left, characterized by soft, light blue and grey colors. [0:03:45 - 0:03:46]: The camera moves further to the right, showing a new pair of vertically aligned paintings. The left artwork has a dominant green, abstract design with swirling shapes and splatters, while the one on the right features a series of rectangular, colorful patches organized vertically from red to turquoise. [0:03:47 - 0:03:49]: As the viewer continues to move to the right, both abstract paintings come fully into view. The lighting is bright, and the paintings stand out against the white gallery walls. The setup of the gallery suggests it is spacious and well-lit. [0:03:50 - 0:03:54]: The video reveals another abstract painting directly ahead, featuring a chaotic mix of dark colors with a stark white streak down the center. To its left, there are two framed, smaller artworks. One depicts a figure in a red outfit against a flowery background, and below it, another figure appears in a similar flower-filled background. [0:03:55 - 0:03:59]: Finally, the camera captures a broader view of the gallery. It shows an open, spacious room with various artworks displayed on the walls and some sculptures positioned around. A shiny, abstract metal sculpture is prominently displayed near the center of the room, reflecting the surroundings. The gallery lighting adds a warm ambiance, enhancing the vivid colors of the artworks. People are seen meandering through the space, observing the art closely.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What does the upper left painting feature?",
        "time_stamp": "00:03:42",
        "answer": "A",
        "options": [
          "A. A large tree in bloom with pink flowers.",
          "B. A large tree with green foliage.",
          "C. A whimsical figure with flowers around its head.",
          "D. An abstract design with swirling shapes."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_480_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:20 - 0:07:40] [0:07:20 - 0:07:24]: The video begins with a close-up of three pieces of artwork displayed on a white wall. The artworks depict ornate crowns and are differently framed. The names \"Shin, Ye Jin\" are displayed above these pieces. The first crown artwork is dark with a sparkling silver crown in a transparent frame. The second is a pink background with a gold crown in a gold frame. The third is a circular piece with a green background and a silver crown. Beneath the artworks are information plaques. [0:07:24 - 0:07:26]: The camera pans downward and right, showing a fourth framed artwork on the lower part of the wall. It depicts a crown sitting on a reflective water surface, with a dark blue, starry background. The frame is ornate and golden. On the right side, there is partial visibility of another artwork. [0:07:26 - 0:07:28]: The camera continues to move right, revealing the neighboring artworks. There is an artwork with vibrant blue colors and abstract representations of fish, and below it, a painting of unusual geometric buildings against a green and blue landscape with a purple sky. [0:07:28 - 0:07:30]: The camera steadily shifts rightward, fully showcasing the geometric building painting. Next to it are two grayscale drawings featuring floating cities and islands, balanced by geological formations. [0:07:30 - 0:07:34]: The focus shifts to the detailed black-and-white drawings of floating land masses and architectural structures, displayed in simple wooden frames. Higher up, there are more grayscale architectural sketches, including intricate buildings with reflective surfaces. Small plaques provide information about the artworks. [0:07:34 - 0:07:36]: The video continues showcasing more black-and-white pencil constructions by the same artist. The artworks appear surreal, with elements floating in mid-air. The artist's name, \"SungTae Kim,\" is visible alongside the words \"Surreal pencil constructions,\" indicating the artist's style. [0:07:36 - 0:07:38]: The last frames focus on the top portion of the wall with SungTae Kim's name and a large vertical artwork depicting a floating castle. Beside it are smaller works featuring detailed architectural renderings, showcasing various perspectives and artistic techniques. [0:07:38 - 0:07:39]: The camera captures in detail the large floating castle artwork with accompanying smaller architectural pieces, emphasizing the intricate designs and floating elements that characterize Kim's style.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is a common feature in the artworks by SungTae Kim?",
        "time_stamp": "00:07:37",
        "answer": "B",
        "options": [
          "A. Bright colors and abstract shapes.",
          "B. Surreal pencil constructions.",
          "C. Ornate crowns.",
          "D. Reflective surfaces with water."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_480_real.mp4"
  },
  {
    "time": "[0:11:00 - 0:12:00]",
    "captions": "[0:11:00 - 0:11:20] [0:11:00 - 0:11:05]: The video appears to be taken in an art gallery showcasing various paintings. At the start, the camera focuses on two sets of paintings. The first set, on the left, consists of three smaller paintings in frames. The top painting depicts a silhouette of two animals under a tree during sunset. Below it is a painting of a green tree with pink flowers growing on a giraffe and an evening sky in the background. The third painting further below shows a tree with green and yellow foliage. [0:11:00 - 0:11:04]: The center-right painting, slightly larger than those on the left, features two zebras grazing under a tree with green and yellow leaves against a light blue sky. The bottom part of the painting has a detailed backdrop of a cityscape in grayscale tones. [0:11:05 - 0:11:06]: The camera slowly pans to the right, revealing more artwork. The next painting, identical in size to the previous one, showcases a tree with striking purple and pink flowers. Compared to the previous paintings, this one focuses on a giraffe family, with the cityscape retaining a grayscale tone. [0:11:07 - 0:11:10]: The view further zooms into the painting with the giraffe family, highlighting their intricate patterns and the tree's blossoms while blurring the cityscape behind. The camera then continues to move downwards, focusing in on the giraffes' legs and the ground beneath. [0:11:11 - 0:11:14]: The video zooms in closer to the bottom part of the painting where scattered pieces of what appear to be shattered fragments or confetti could be observed, placed on a little shelf protruding from the wall. [0:11:15]: The camera shifts back to a zoomed-in view of the giraffe family, focusing on the details of the giraffe's pattern and the surrounding tree foliage. [0:11:16 - 0:11:18]: Transitioning to the next painting to the right, it showcases another urban skyline under a dark blue night sky with stars. This painting prominently features two zebras grazing at the edge of a water body reflecting city lights.  [0:11:19 - 0:11:20]: The video culminates with a clear view of this urban nightscape painting, highlighting the lit skyscrapers and serene water reflecting the night lights. The painting has a distinctively calm and reflective ambiance.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the main theme depicted in the painting just now?",
        "time_stamp": "0:11:15",
        "answer": "B",
        "options": [
          "A. A cityscape with zebras.",
          "B. A giraffe family under a tree.",
          "C. An urban skyline at night.",
          "D. Animals in a forest."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "art_show",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_480_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video begins with a view of a miniature race track set against a background painted with blue skies and fluffy clouds. The track has a winding path through green hills dotted with trees. Several small racecars are visible, with one at the forefront in a bright blue color accompanied by others in various colors. Signboards with logos are positioned next to the track. [0:00:04 - 0:00:07]: Transitioning from the race track scene, a title screen appears with a glitch effect. The text reads \"Next Gen Diecast Racing\" and \"The Next Generation of Diecast Racing,\" written in bold, colorful letters on a black background. The glitch effect distorts the text before it stabilizes. [0:00:08 - 0:00:16]: The scene shifts back to the miniature race track. A lineup of colorful diecast racecars is positioned at the starting line. The cars are meticulously detailed, varying in designs and colors, with a silver car, an orange car, gray cars with purple accents, a yellow car, and a blue car. In the background, there is a depiction of a pit lane with various equipment and vehicles. The text \"Next-Gen Piston Cup\" and \"Race 2 - Round 1\" appears over the scene. [0:00:17 - 0:00:20]: Further information about the race is displayed, indicating \"Group 7 and 8\", while the camera focuses on the lineup of cars positioned at the starting line. The background continues to showcase the pit lane area with detailed miniatures, including service vehicles and advertisements.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What group numbers are displayed for the race participants?",
        "time_stamp": "0:00:20",
        "answer": "D",
        "options": [
          "A. Group 1 and 2.",
          "B. Group 3 and 4.",
          "C. Group 5 and 6.",
          "D. Group 7 and 8."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_488_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:41]: The video begins with a view of a racetrack with a car speeding by on the left side. The track surface is black with white lines, and there are fences on both sides. In the background, various cars and small structures are visible. A flying red vehicle can be seen in the air above the buildings.  [0:02:41 - 0:02:50]: The scene shifts to a curved section of the racetrack. The track curves to the left, surrounded by green grass and trees. Multiple cars are zooming on the track, with a spectator area and various vehicles stationed in the center of the curve. Some of the cars on the track appear to be the same ones from the previous scene. Advertisements and logos are visible around the track. The vehicles are moving quickly, making the pictures appear blurry. [0:02:49 - 0:02:53]: The view changes to a close-up of a red fire truck and a blue tow truck on a raised section of the track. On a higher section of the track in the background, more cars can be seen driving. There is a blue roofed structure with spectators, as well as a mountainous landscape with grassy surfaces in the backdrop. The same curved section is visible from a different angle, emphasizing more signage and billboards around the racetrack. Various colorful vehicles are parked on the pathway. [0:02:54 - 0:02:59]: A transition shows a scoreboard listing standings for Group 7 - Heat 2. The standings show four racers, with \"#21 Ryan Laney *(1)\" in the first position with 7 points, \"#123 Jonas Carvers\" in second with 6 points, \"#57 Junyi *(1)\" in third with 5 points, and \"#28 Tim Treadless\" in fourth with 3 points. The background of the scoreboard shows a portion of the racetrack and parked cars in a fenced area. Trees and a cloudy sky can be seen in the distance.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Which racing car won first place in this competition?",
        "time_stamp": "0:01:28",
        "answer": "D",
        "options": [
          "A. The orange car.",
          "B. The blue car.",
          "C. The red car.",
          "D. The grey car."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_488_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:25]: The video begins by showcasing a yellow and black toy race car with the number 52 and \"Leak Less\" written on its side. The car is positioned on a racing track with a mesh fence as the background. Various statistics are displayed in the frame, including the car's position (13th), heat wins (1), and fastest time (9.791 seconds). The car belongs to George New-Win from team G4 Diecast Racing. [0:05:26]: The yellow and black car, featuring \"Leak Less\" number 52, is still shown but without the textual overlays. [0:05:27 - 0:05:32]: The scene changes to feature a new toy race car, this time it is predominantly blue with yellow and magenta accents. This car is number 70 and belongs to Richie Gunzit from team Elst Racing. Basic details about the car's position (12th), heat wins (1), and fastest time (9.495 seconds) are displayed. [0:05:33]: The blue, yellow, and magenta car is shown without any textual overlays. [0:05:34 - 0:05:39]: The video shifts to another toy race car, this one white with blue accents, numbered 094. This car belongs to Jay from team VCD132 Racing. The displayed statistics indicate Jay's car is in 6th position with 3 heat wins and a fastest time of 8.267 seconds.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the fastest time achieved by Jay's car from team VCD132 Racing?",
        "time_stamp": "00:05:39",
        "answer": "C",
        "options": [
          "A. 9.495 seconds.",
          "B. 9.791 seconds.",
          "C. 8.267 seconds.",
          "D. 8.999 seconds."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_488_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: The scene opens with a close-up view of a winding race track in a miniature diorama setting. Various toy cars are seen positioned on the track and surrounding areas. To the right, there are several toy emergency vehicles and other cars set up in a grassy area with a backdrop of advertising signs. [0:08:01 - 0:08:02]: The perspective shifts slightly to show more of the background and additional toy cars positioned on and around the track. The sky is painted with puffy white clouds against a blue backdrop, and there is a fenced section in the middle part of the track. [0:08:02 - 0:08:03]: The camera angle changes to focus on three racing cars: one yellow, one blue, and one silver. The cars are making their way around a bend in the track, with the yellow car in the lead. The surroundings continue to display a detailed miniature landscape with rolling hills and trees. [0:08:03 - 0:08:04]: The yellow race car speeds ahead down the track as the blue and silver cars follow closely behind. The background consists of grassy hills, a painted sky, and a track that runs parallel above the racing cars. [0:08:04 - 0:08:05]: The yellow car is seen taking a corner with a large \"DD\" signage in the background. The landscape is consistent, showing a winding track that runs through hilly terrain with some trees. [0:08:05 - 0:08:06]: The viewpoint shows a section of the race track with additional parked toy cars including a fire truck and some classic cars near the starting point. The yellow car is visibly taking a turn around the track. [0:08:06 - 0:08:07]: The yellow and blue cars are seen from above as they make another turn, moving along a steeply banked section of the track. The track is bordered by red and white barriers, and there are various toy cars and structures off to the side. [0:08:07 - 0:08:08]: The three racing cars continue to maneuver through the banked turn with the yellow car leading. The miniature landscape features various toy cars and colorful advertisements positioned around the track. [0:08:08 - 0:08:09]: The cars are tightly packed as they continue the race. The blue car is trying to overtake the silver and yellow cars while negotiating the curved track. The background scene includes trees, grassy areas, and various miniature elements. [0:08:09 - 0:08:10]: The blue car successfully overtakes the silver car and is now closely following the yellow car. The track remains in a curved section surrounded by detailed model scenery featuring green hills and miniature trees. [0:08:10 - 0:08:11]: The blue car takes the lead position, surpassing the yellow car. The track is banked and the landscape continues to display a lush miniature environment with painted sky, hills, and trees. [0:08:11 - 0:08:12]: The blue car pulls further ahead on the track. The racing action remains intense as the cars navigate through the meticulously designed miniature scene. [0:08:12 - 0:08:13]: The frame transitions to a straight section of the track. The miniature landscape now includes some toy buildings, vehicles, and a helicopter on a grassy hill. The cars are seen speeding down the track alongside a fence. [0:08:13 - 0:08:14]: The yellow car is ahead again on the straight track. The track runs parallel to a fenced area with toy buildings and parked toy cars. A red helicopter is also visible in the background. [0:08:14 - 0:08:16]: The yellow car speeds further down the straight section of the track. In the background, there are toy buildings with more parked toy cars and a helicopter. [0:08:16 - 0:08:17]: The yellow car is followed closely by the silver and blue cars as they race straight down the track. The surroundings still feature toy structures, vehicles, and a fenced area. [0:08:17 - 0:08:18]: The blue car starts to gain on the yellow car, coming closer to overtaking it again. The toy buildings and other vehicles remain part of the consistent backdrop. [0:08:18 - 0:08:19]: The blue car now closely follows the yellow car, racing down the track. The scene includes a fence, toy buildings, and a red helicopter on a grassy section in the backdrop.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "Which racing car won first place in this competition?",
        "time_stamp": "0:08:01",
        "answer": "B",
        "options": [
          "A. The orange car.",
          "B. The yellow car.",
          "C. The red car.",
          "D. The grey car."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_488_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:10:48]",
    "captions": "[0:10:40 - 0:10:48] [0:10:40 - 0:10:42]: The video begins with a view of a racetrack from a first-person perspective, situated at a curve. The track is surrounded by green synthetic grass and red and white barriers. There are model cars on the track, with one orange car prominently in the foreground on the right curve of the track. The surrounding area features a diorama with miniature emergency vehicles, race cars, and trees. [0:10:43 - 0:10:48]: The scene shifts to another part of the track which is straightened and bordered by tall fencing. Multiple race cars, mostly blue and yellow, speed down the track. There are model pit stop structures with small construction vehicles and a red helicopter on top. This segment features cars moving rapidly past, implying the vehicles are in a competitive race.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Which team won first place in Race 2 competition?",
        "time_stamp": "0:01:28",
        "answer": "A",
        "options": [
          "A. Dom's Tuner Shop.",
          "B. VCD132 Racing.",
          "C. Krime Sindakit Racing.",
          "D. Elst Racing."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "rc_model_competition",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_488_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the red car located right now?",
        "time_stamp": "00:00:02",
        "answer": "C",
        "options": [
          "A. In front of the cyclist, on the left.",
          "B. Just behind the cyclist, on the right.",
          "C. Parked on the side of the road, on the right.",
          "D. Ahead in the distance, slightly to the left."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_165_real.mp4"
  },
  {
    "time": "[0:02:02 - 0:02:22]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the trees located right now?",
        "time_stamp": "0:02:20",
        "answer": "A",
        "options": [
          "A. On both sides of the road.",
          "B. Only on the left side of the road.",
          "C. Only on the right side of the road.",
          "D. Directly in front of the cyclist."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_165_real.mp4"
  },
  {
    "time": "[0:04:04 - 0:04:24]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the trees located right now?",
        "time_stamp": "00:04:15",
        "answer": "A",
        "options": [
          "A. On both sides of the road.",
          "B. On the right side of the road.",
          "C. In the center of the road.",
          "D. Evenly distributed on both sides."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_165_real.mp4"
  },
  {
    "time": "[0:06:06 - 0:06:26]",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where are the trees located right now?",
        "time_stamp": "0:06:25",
        "answer": "C",
        "options": [
          "A. On both sides of the road.",
          "B. On the left side of the road.",
          "C. On the right side of the road.",
          "D. There are no trees visible right now."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_165_real.mp4"
  },
  {
    "time": "[0:08:08 - 0:08:28]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the current value of 'distanz' displayed in the video?",
        "time_stamp": "00:08:13",
        "answer": "A",
        "options": [
          "A. 76.2KM.",
          "B. 76.3KM.",
          "C. 76.1KM.",
          "D. 76.0KM."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "cycling",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_165_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:04]: The video starts with an overhead view of a football stadium packed with spectators. The camera is positioned high above the pitch, capturing the entire field and a significant section of the crowd. The field is green with well-maintained grass, and the goals are white. The stand opposite the camera is filled with thousands of fans, and the arena's roof structure is visible, giving a circular frame to the scene. The video is captured during a significant football match, as indicated by the large crowd and the FIFA logo in the top right corner. [0:00:05 - 0:00:11]: The perspective shifts to a close-up of a football player on the field. The player's uniform is dark blue. The player is standing still, looking focused and solemn. There is some movement and change in the player's facial expression as they turn their head slightly. The background is blurry, filled with indistinct colors suggesting other players, spectators, and the stadium environment. [0:00:12 - 0:00:16]: The view changes to a different perspective focused on the penalty area of the football field. One player is positioned near the penalty mark, preparing to take what appears to be a penalty kick. The goalkeeper, wearing a bright green jersey, stands on the goal line. A referee, dressed in an orange shirt, is standing next to the penalty taker. The background is filled with an audience, creating an intense atmosphere. [0:00:17 - 0:00:19]: The focus shifts to the goalkeeper in the bright green jersey. The goalkeeper is preparing for the penalty, appearing concentrated, and moving along the goal line. The backdrop continues to show blurred figures, indicating movement and anticipation among the crowd and other players in the vicinity.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is happening in the penalty area of the football field?",
        "time_stamp": "0:00:16",
        "answer": "A",
        "options": [
          "A. A player is preparing to take a penalty kick.",
          "B. A player is scoring a goal.",
          "C. A fight is breaking out.",
          "D. The referee is ending the game."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_7_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:03]: A goalkeeper in a neon green jersey stands in front of a goal, wearing red gloves with white accents. He appears to be focused, looking intently ahead. The background is a stadium filled with spectators, which is slightly blurred. [0:02:04 - 0:02:05]: A player in a navy blue jersey is shown up close, standing with his hand covering his face, possibly wiping his nose or mouth. The player's jersey features a white patch on the sleeve and the team's logo on the chest. [0:02:06 - 0:02:11]: The point of view changes to behind the goal as a penalty kick is being taken. The goalkeeper in green dives to the left, attempting to block the shot. The ball moves towards the bottom left of the net, but the goalkeeper misses. The scene takes place in a large, illuminated stadium with a packed crowd. [0:02:12 - 0:02:14]: The goalkeeper, having missed the save, is seen lying on the ground near the goalpost, recovering from the dive. Close-up shows the goalkeeper clutching the net in frustration as the ball crosses the line. [0:02:15]: A player in a white and light blue jersey celebrates with arms raised and an open mouth, shouting in triumph. His teammates and a crowd of spectators can be seen in the background. [0:02:16 - 0:02:19]: The view shifts to another player in a white and light blue jersey walking away from the penalty spot towards the center of the field. In the background, additional team members and spectators are visible, with a large portion of the stands filled with fans, some holding banners and wearing team colors.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What happens after the penalty kick is taken?",
        "time_stamp": "0:02:15",
        "answer": "A",
        "options": [
          "A. The penalty kick was saved by the goalkeeper.",
          "B. The goalkeeper catches the ball.",
          "C. The ball goes into the bottom left of the net.",
          "D. The ball is deflected by a defender."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_7_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:04]: A group of players wearing dark blue jerseys are gathered, standing on the green grass of a football field. The players are huddled closely, with one player's arm around another, appearing to engage in a moment of focus and support. The background reveals a packed stadium with numerous spectators wearing white shirts, showcasing the large audience. [0:04:05]: A close-up view shows something in orange, but it is unclear due to its proximity to the camera. [0:04:06 - 0:04:08]: A different player, this time from a different team, is shown in close-up. He is wearing a white jersey with black stripes, focused and intense, with a blurred background of the stadium. [0:04:09 - 0:04:14]: The scene shifts to a wide-angle view of the football field. A player in a white and light blue striped jersey stands poised, ready to take a penalty kick. He is positioned just outside the penalty area, with the goalkeeper in yellow ready in front of the net. The stadium, packed with spectators, forms the backdrop to this tense moment. [0:04:15 - 0:04:16]: The player in the white and light blue striped jersey runs forward and kicks the ball towards the goal. The goalkeeper dives to his right in an attempt to save the shot. [0:04:17]: The ball goes past the diving goalkeeper towards the goal. The player follows through on his kick, continuing his movement as the crowd appears enthusiastic. [0:04:18 - 0:04:19]: The player, now turning, runs to celebrate his action. The back of his jersey reveals his name and number \"5\". He displays triumphant body language, running on the green field.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What does the player in the white and light blue striped jersey do after running forward?",
        "time_stamp": "00:04:20",
        "answer": "D",
        "options": [
          "A. Passes the ball.",
          "B. Stops running.",
          "C. Celebrates.",
          "D. Kicks the ball towards the goal."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Event Understanding",
        "question": "What happens after the goalkeeper dives to his right?",
        "time_stamp": "00:04:18",
        "answer": "A",
        "options": [
          "A. The ball goes past the goalkeeper.",
          "B. The goalkeeper catches the ball.",
          "C. The player misses the goal.",
          "D. The ball hits the post."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_7_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:02]: A shirtless individual is seen in the foreground, holding a blue and white striped shirt and embracing another person who is wearing a white jersey. The background shows a blurry crowd in a stadium; [0:06:03 - 0:06:06]: The embrace continues, and additional people join in the celebration. The shirtless individual appears emotional, partially covering his face with the shirt. The background crowd is mostly out of focus; [0:06:07 - 0:06:09]: The camera shifts to a high-angle, wide view of the stadium, showing a soccer field and large, crowded stands. Players and other individuals are scattered across the field, some near the goals and others toward the center; [0:06:10 - 0:06:12]: The camera returns to the first-person perspective. Two individuals, both in yellow vests, celebrate and embrace on the field surrounded by a cheering crowd; [0:06:13 - 0:06:14]: The individuals in yellow vests begin to run across the field. The stands are filled with spectators, many holding banners and flags; [0:06:15 - 0:06:19]: The camera angle changes again to a high, wide view of the stadium. Different groups of players and individuals are seen scattered across the field, with a central group celebrating around the center circle. Other individuals spread out towards the goals and the sidelines.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What event is taking place right now?",
        "time_stamp": "0:06:36",
        "answer": "A",
        "options": [
          "A. A soccer match celebration.",
          "B. A concert.",
          "C. A protest.",
          "D. A graduation ceremony."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "football",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_7_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The screen is black at the beginning of the video.  [0:00:01 - 0:00:08]: The video shows two \"Happy Mother's Day\" cake toppers with intricate designs. Each topper is made of a combination of brown and pink paper, featuring multiple layers to create a 3D effect. Surrounding the text are numerous small pink flowers with yellow centers, and the letters themselves have both an outer layer of brown and an inner layer of pink or white. There is an intricate lattice design as the background within the topper's frame. Both toppers are mounted on wooden sticks which are held by two hands at the bottom of the frame. [0:00:09 - 0:00:11]: The screen becomes black again, and the text \"3D Cake Topper\" appears in the center of the frame. [0:00:13]: The screen remains black. [0:00:14 - 0:00:18]: The background changes to a peach color, and the text \"Templates and Materials used are listed in the description\" is displayed in the center of the frame. [0:00:19]: The video shows a close-up of some pink and brown paper lying on a gray surface, along with a hand entering the frame from the top left corner. The words \"DIY Craft Tutorials\" are visible at the bottom right of the frame.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the hand doing right now?",
        "time_stamp": "00:00:23",
        "answer": "B",
        "options": [
          "A. Cutting the paper.",
          "B. Displaying some heavy weight cardstock.",
          "C. Holding a pen.",
          "D. Arranging flowers."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_59_real.mp4"
  },
  {
    "time": "0:01:20 - 0:01:40",
    "captions": "[0:01:20 - 0:01:40] [0:00:01 - 0:00:03]: The video begins with a screen displaying a dialog box with the title \"How will you load all of your materials for this project?\" Below this, there are three choices labeled \"Without Mat,\" \"On Mat,\" and \"Multiple Ways,\" each with respective illustrations. The \"On Mat\" option is highlighted with a dark border and a circular selection mark. Below the main selection, there are additional specifications about the mat size (12 in x 12 in or 12 in x 24 in) and a checkmark box for \"Remember this selection.\" The screen has a button labeled \"Confirm\" in green, which is selected. [0:00:04 - 0:00:07]: The confirmation button is clicked, and the screen transitions to a different view within the same Cricut Design software. The screen now shows a visual layout of a \"Happy Mother's Day\" text design on a cutting mat grid. The grid is marked with measurements, and the text design is positioned in the upper left corner of the mat. There are some options displayed on the left side of the screen such as \"Basic Cut\" and \"Project Copies.\" [0:00:08]: The video frame shows a different design layout with the same cutting mat grid interface, but this time, the mat is filled with different cutout shapes instead of text. The brown grid mat appears, displaying intricate shapes spread across it. [0:00:09 - 0:00:10]: The layout changes once more, now showing another design on a tan-colored mat grid within the Cricut Design Space. The grid is populated with various shapes spread across the mat. [0:00:11 - 0:00:12]: The view remains consistent, continuing to display the tan-colored mat with the shapes in the Cricut Design Space. The focus seems to be on finalizing the layout of the shapes on the grid. [0:00:13 - 0:00:14]: The screen transitions back to the \"Happy Mother's Day\" text design on the gray grid, positioned in the same upper left corner as before. The system seems to cycle back to the original design. [0:00:15 - 0:00:16]: The video then shifts to a new interface, under the \"Set Base Material\" title. It presents various material options to choose from, such as \"Popular,\" \"Favorites,\" and \"Art Board\" categories. The list includes \"Cardstock,\" \"Poster Board,\" \"Vinyl,\" and \"Everyday Iron-On.\" The \"Medium Cardstock, 80 lb (216 gsm)\" option is highlighted. [0:00:17]: The material selection is elaborated upon, displaying a detailed view of the chosen material. There's an indication that the user selected \"Medium Cardstock\" with a setting modification to apply \"more pressure.\" [0:00:18]: The frame indicates the base material set to \"Medium Cardstock - 80 lb (216 gsm)\" with \"Default\" settings. This is complemented by an additional instruction in the displayed text, instructing to choose the \"Medium Cardstock\" setting with additional pressure. [0:00:19]: The text instruction overlays the screen, reiterating the choice of \"Medium Cardstock\" with more pressure.  [0:00:20]: The screen continues to display the material selection and modifications in the Cricut Design software, ready for the next steps in the crafting process.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action is being performed just now?",
        "time_stamp": "00:01:23",
        "answer": "C",
        "options": [
          "A. Displaying different material options.",
          "B. Highlighting the \"On Mat\" option.",
          "C. Clicking the continue button.",
          "D. Changing the project type to \"Multiple Ways.\"."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_59_real.mp4"
  },
  {
    "time": "0:02:40 - 0:03:00",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:41]: A person's hands are seen peeling a strip of double-sided tape from its backing using a pair of scissors. The location of the action is over a white sheet of paper placed upon a grey surface. The person’s hands have a ring on the left ring finger and the person is wearing a red sweater. A roll of tape is positioned on the left side of the frame. [0:02:41 - 0:02:43]: The person places the cut strip of double-sided tape vertically onto the sheet of white paper. They then smooth down the tape with their right hand while holding the remaining strip with their left. [0:02:44]: The person finishes smoothing down the tape and adjusts the paper. [0:02:45 - 0:02:46]: The person lifts the white sheet of paper and places it onto a grey cutting mat. The edges are clearly visible, with the person’s hand ensuring the paper is centered well. [0:02:47]: A green Cricut cutting mat is shown, held by the person’s left hand, over a grey background. [0:02:48 - 0:02:51]: The white paper is positioned onto the green Cricut cutting mat as the person aligns it carefully. The person's hands adjusting the paper are clearly shown. [0:02:52 - 0:02:55]: The person presses the white paper onto the green mat to ensure it sticks well. Their hands smooth the paper down firmly on each corner. [0:02:56 - 0:02:58]: The person carefully applies a small piece of tape to the top edge of the white paper on the green mat, securing it in place. [0:02:59]: The video ends with the cutting mat, now holding the white sheet of paper, being slid into a Cricut cutting machine.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "0:03:03",
        "answer": "C",
        "options": [
          "A. Cutting strips of double-sided tape.",
          "B. Positioning the paper onto the mat.",
          "C. Sliding the cutting mat into a Cricut cutting machine.",
          "D. Peeling a strip of tape using scissors."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_59_real.mp4"
  },
  {
    "time": "0:04:00 - 0:04:20",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:03]: On a flat work surface, several crafting components are visible. Two yellow-brown heart-shaped cutouts with intricate internal designs are placed on the left. A large brown text cutout reading \"Happy Mother's Day\" is positioned at the top center. To the right of this, there are small white, cream, and pink paper cutouts along with various sizes of round metallic decorations scattered across the surface. A pair of white-handled craft scissors lies at the bottom right corner. In the foreground, a person is holding a piece of white adhesive foam tape and starting to stick it to one of the yellow-brown heart cutouts, while their other hand holds the cutout steady.  [0:04:04 - 0:04:05]: The person's hands, still holding the white adhesive tape, are now pressing it firmly onto the bottom of the heart cutout to secure it. Another hand, with a ring, is still holding the cutout steady.  [0:04:06 - 0:04:07]: The individual then peels off the protective layer of the foam tape and uses their left hand to hold down the cutout. With their right hand, they begin placing additional foam tape along the curved edges of the heart-shaped cutout. The background components remain in their previous positions.  [0:04:08]: The person uses the white-handled craft scissors to further trim and secure the edges of the yellow-brown heart-shaped cutout.  [0:04:09 - 0:04:12]: The individual continues to secure the foam tape onto the bottom of the heart cutout, using both hands alternatively to press and adhere the tape. The right hand, occasionally positioned near the white-handled scissors, ensures a neat finish.  [0:04:13 - 0:04:14]: A large white roll of adhesive tape enters the frame from the left. The person’s hand is now seen pulling a new strip of tape from the roll. The heart cutout with tape markings is securely placed on the work surface. [0:04:15]: The individual uses the white roll of tape to pull out another piece with their right hand, while their left hand steadies the tape roll. [0:04:16 - 0:04:17]: They expertly affix the new piece of tape to another area on the yellow-brown cutout. Both hands work collaboratively to ensure the tape is firmly placed.  [0:04:18 - 0:04:19]: The person then begins to peel off the protective layer from the new piece of foam tape, ensuring it sticks properly to the desired sections of the yellow-brown heart-shaped cutout.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:04:19",
        "answer": "A",
        "options": [
          "A. Peeling off the protective layer from the foam tape.",
          "B. Using the white-handled craft scissors to cut the foam tape.",
          "C. Writing \"Happy Mother's Day\" on a cutout.",
          "D. Arranging small paper cutouts on the surface."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_59_real.mp4"
  },
  {
    "time": "0:05:20 - 0:05:40",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:21]: Two hands carefully place a golden paper cutout with the text \"Happy Mother's Day\" in white letters on a flat, gray surface. On the right side of the work area, there are small decorative elements including pink paper flowers and circular sequins. [0:05:21 - 0:05:22]: The hands adjust the \"Happy Mother's Day\" cutout, moving the paper slightly to ensure it is centered. The decorative elements on the right remain unchanged. [0:05:22 - 0:05:23]: The hands lift the \"Happy Mother's Day\" cutout and hold it in front of the camera, showing the details of the intricate pattern within the cutout. The small decorations remain on the flat gray surface. [0:05:23 - 0:05:24]: The hands continue to hold the cutout, rotating it slightly to demonstrate the design from different angles. The background is still the plain gray surface with the decorative elements positioned on the right. [0:05:24 - 0:05:25]: The hands show a side view of the \"Happy Mother's Day\" cutout, giving a sense of its thickness and structure. The small decorative items on the right side of the surface stay in place. [0:05:25 - 0:05:26]: The hands return the cutout to the flat gray surface, partially overlapping with the decorative elements. The focus returns to the \"Happy Mother's Day\" message. [0:05:26 - 0:05:27]: One hand remains holding the cutout in place, while the other hand starts picking up the small pink flower decorations from the right side of the work area. [0:05:27 - 0:05:28]: One of the hands begins to place a pink flower decoration onto the cutout, positioning it carefully to enhance the design. [0:05:28 - 0:05:29]: The hands continue to arrange pink flowers on the golden cutout, softly pressing them into place to ensure they stick. [0:05:29 - 0:05:30]: More pink flower decorations are added to the cutout, creating a decorative border around the \"Happy Mother's Day\" message. The hand's movements are deliberate and gentle. [0:05:30 - 0:05:31]: Both hands are focused on perfectly placing the small pink flowers evenly around the cutout. The decorative elements on the right still include loose flowers and circular sequins. [0:05:31 - 0:05:32]: More flowers are picked up and positioned on the cutout, enhancing its visual appeal. The placement is becoming more elaborate and detailed. [0:05:32 - 0:05:33]: The hands press on the flowers that have been added, ensuring they adhere properly to the cutout. The work surface still has a few remaining decorative elements. [0:05:33 - 0:05:34]: With more pink flowers in hand, they intricately place them around the text. The design on the golden cutout starts to become more vibrant and decorated. [0:05:34 - 0:05:35]: Carefully picking up more flowers, the hands add them to the upper area of the cutout. The effort is to create a harmonious design pattern around the text. [0:05:35 - 0:05:36]: The hands work together to evenly distribute the flowers across the cutout, ensuring balance and visual appeal. Small circular sequins on the right remain untouched. [0:05:36 - 0:05:37]: The hands adjust and refine the position of the pink flowers on the cutout, paying attention to detail in the crafting process. [0:05:37 - 0:05:38]: Adding the final few flowers, the hands complete the decorative border. The \"Happy Mother's Day\" message is now framed with an intricate floral design. [0:05:38 - 0:05:39]: The hands make final adjustments to the placed flowers, ensuring they stick perfectly. More small decorative elements are still visible on the flat gray surface.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What are the hands doing right now?",
        "time_stamp": "0:05:41",
        "answer": "C",
        "options": [
          "A. Placing a new cutout on the surface.",
          "B. Adding more circular sequins.",
          "C. Making final adjustments to the placed flowers.",
          "D. Writing \"Happy Mother's Day\" on the cutout."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "handcraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_59_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the recent actions taken just now?",
        "time_stamp": "00:00:10",
        "answer": "D",
        "options": [
          "A. The individual cleaned a machine, selected a sandwich, and prepared it for a customer.",
          "B. The individual set a table with utensils, organized a menu, and greeted a guest.",
          "C. The individual washed dishes, stacked them, and prepared a meal.",
          "D. The individual positioned the portafilter, and ground coffee into it, and cleaned the portafilter,."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_367_real.mp4"
  },
  {
    "time": "[0:01:58 - 0:02:08]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:02:08",
        "answer": "D",
        "options": [
          "A. The individual brewed a pot of tea, added honey, and served it to a customer.",
          "B. The individual prepared a bowl of soup, garnished it with herbs, and handed it over.",
          "C. The individual mixed a smoothie, poured it into a glass, and placed it on the counter.",
          "D. The individual poured frothed milk into a cup of coffee, creating latte art."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_367_real.mp4"
  },
  {
    "time": "[0:03:56 - 0:04:06]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:04:06",
        "answer": "D",
        "options": [
          "A. The individual removed a foil cover from a container, stirred a hot soup, and served it to a customer.",
          "B. The individual picked up a plastic container, opened the lid, and prepared the contents for blending.",
          "C. The individual placed a small container back on the shelf after stirring its contents and wiped the counter.",
          "D. This individual put a plastic lid on a prepared iced latte."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_367_real.mp4"
  },
  {
    "time": "[0:05:54 - 0:06:04]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken just now?",
        "time_stamp": "00:06:04",
        "answer": "D",
        "options": [
          "A. The individual selected a teapot, steeped tea leaves, and poured the tea into a cup.",
          "B. The individual prepared a smoothie by blending ingredients, pouring it into a glass, and garnishing it.",
          "C. The individual washed a tea strainer, added herbs into it, and boiled water.",
          "D. The individual tamped coffee grounds, and prepared to extract an espresso shot."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_367_real.mp4"
  },
  {
    "time": "[0:07:52 - 0:08:02]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the recent actions taken just now?",
        "time_stamp": "00:08:02",
        "answer": "D",
        "options": [
          "A. The individual washed a coffee cup, set it down, and filled it with milk.",
          "B. The individual cleaned a teapot, boiled water, and steeped tea leaves.",
          "C. The individual prepared a paper cup, filled it with brewed tea, and added a slice of lemon.",
          "D. The individual wiped and cleaned the portafilter, weighted the portafilter, and adjusted the grinder settings."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_367_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:04]: The video starts with a view of a parking lot on the left-hand side, featuring a few vehicles including a black truck at the beginning. Yellow bollards line the side of the road, and a building with \"BASECAMP\" written on it is visible at the right side of the frame. Trees and greenery are present in the background under a partly cloudy sky. [0:00:05 - 0:00:08]: The camera continues to move forward on the road, passing more yellow bollards. The building labeled \"BASECAMP\" becomes more prominent, showing additional details such as outdoor seating areas with red umbrellas.  [0:00:09 - 0:00:11]: As the camera proceeds, the \"BASECAMP\" building is seen with people sitting under the red umbrellas. The road approaches a crosswalk leading to a residential area. Trees and a large green tree are present near the crosswalk. [0:00:12 - 0:00:15]: The camera captures a section of the parking area containing golf carts and parked cars. The road intersects with another street where a few houses and trees are visible. One noticeable feature is a large teddy bear statue beside a fire hydrant.  [0:00:16 - 0:00:18]: Moving further, the video showcases a residential driveway and more houses. Manicured gardens with flowers and shrubs line the sidewalks. The teddy bear statue comes into clearer view, with more details of the surrounding residential buildings. [0:00:19 - 0:00:20]: The final frame of the video shows the continuation of the residential street lined with houses, trees, and well-maintained lawns. The scene is calm with few cars visible, and greenery dominates the landscape as the camera moves forward.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is prominently visible on the right side of the frame at the beginning of the video?",
        "time_stamp": "0:00:04",
        "answer": "C",
        "options": [
          "A. A black truck.",
          "B. A red umbrella.",
          "C. A building with \"BASECAMP\" written on it.",
          "D. A crosswalk."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_313_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:42]: The video begins with a view of a large beige building with visible structural beams on its facade. The sky is bright and blue, with the building occupying most of the screen. In the foreground, there are yellow poles and orange traffic lines on the ground, along with a parked white utility truck and a silver car entering the frame. [0:02:43 - 0:02:45]: The silver car gradually moves to the right while the scene remains dominated by the same large beige building and clear sky. A white SUV with organizational decals appears from the right side and drives towards the center of the frame. [0:02:46 - 0:02:47]: The white utility truck is stationary, with traffic cones placed around it. The beige building remains prominent in the background, and a glimpse of more structures further back emerges to the right. [0:02:48]: This frame shows the same white utility truck, with the surrounding traffic cones intact. In the distance, more buildings become slightly visible, with clearer details. [0:02:49]: A small black utility vehicle enters the frame from the right side, traveling towards the left. The backdrop and the stationary utility truck remain the same. [0:02:50 - 0:02:51]: Another car, this time a dark-colored sedan, enters the scene from the right, moving towards the left. The larger beige building and the parked utility truck stay unchanged. [0:02:52 - 0:02:53]: The dark-colored sedan continues to move across the frame. The white utility truck and the structures in the background remain static. [0:02:54]: The scene shows the white utility truck more closely, with traffic cones around it and the edges of some additional buildings coming into view from the right side. [0:02:55]: The shot captures the same utility truck and cones, with more of the surrounding pavement and part of a nearby building visible. The far background reveals more urban structure elements. [0:02:56 - 0:02:57]: The utility truck stays parked as the video reveals more of the adjacent street and surroundings. A tour bus with \"Universal Studios\" signage enters the scene from the right. [0:02:58 - 0:02:59]: The tour bus continues to make its way into the frame. The large beige building remains in the background, and the sky is bright and clear. More buildings and an urban landscape backdrop become increasingly visible on the right side.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type of vehicle appeared from the right side and drove away just now?",
        "time_stamp": "0:02:46",
        "answer": "B",
        "options": [
          "A. A white utility truck.",
          "B. A white SUV with organizational decals.",
          "C. A dark-colored sedan.",
          "D. A small black utility vehicle."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      },
      {
        "task_type": "Event Understanding",
        "question": "What is shown entering the scene from the right now?",
        "time_stamp": "0:02:59",
        "answer": "C",
        "options": [
          "A. A dark-colored sedan.",
          "B. A small black utility vehicle.",
          "C. A tour bus with \"Universal Studios\" signage.",
          "D. A silver car."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_313_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:23]: The video begins with a first-person view passing near a rocky structure adorned with greenery. Tall wooden poles are visible, some of which have skeletal decorations. In the background, a large brick wall and some trees are visible under a blue sky with wispy clouds. [0:05:24 - 0:05:27]: As the camera moves forward, the rocky structure with greenery continues to come into view, while the tall wooden poles remain visible on the right. The surroundings begin to darken as the camera enters a more enclosed, darker area.  [0:05:28 - 0:05:30]: The view now transitions into a darker passageway lined with stacked stones on the right. The area is dimly lit, and several people are visible ahead, seated in what appears to be a vehicle. [0:05:31 - 0:05:34]: The camera continues through the dark passageway with the stone wall on the right. The seats and people ahead become more visible, and the passageway is lit by overhead lights. [0:05:35 - 0:05:39]: The view inside the vehicle shows people facing forward, looking at screens ahead that show some bright content. The right wall of stacked stones remains consistent, and the surroundings become even darker as the video progresses.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What adorns the rocky structure seen now?",
        "time_stamp": "00:05:23",
        "answer": "B",
        "options": [
          "A. Colorful lights.",
          "B. Greenery.",
          "C. Painted murals.",
          "D. Metal sculptures."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_313_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:04]: The video features a simulated ride or a 3D experience where the perspective is from whoever or whatever is moving through a jungle environment. The background shows dense jungle foliage, rocks covered in moss, and entangling vines. There is a constant movement through narrow paths between large rock formations. [0:08:05 - 0:08:09]: As the video progresses, a large ape-like creature appears in close proximity to the camera. It's captured moving towards and interacting with objects and the environment, possibly reaching and grabbing. This creature comes in and out of frame dynamically, reflecting a blend between animation and realism. [0:08:10 - 0:08:15]: The movement intensifies with rapid changes in perspectives, depicting a fall or a descent. The camera's viewpoint shifts downward, tumbling through more dense vegetation and entangling vines. The sense of motion includes leaves and branches moving quickly past the field of view. [0:08:16 - 0:08:20]: Towards the end of the sequence, the large ape-like creature reappears, looking directly at the camera. There's an interaction implying a menacing or protective stance. The background remains consistent with thick jungle scenery, filled with detailed textures of leaves and vines, signifying a climax of the encounter.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What event is depicted in the video now?",
        "time_stamp": "00:08:21",
        "answer": "A",
        "options": [
          "A. The large ape-like creature reappears and interacts with the camera in a menacing stance.",
          "B. The camera zooms out to show an aerial view of the jungle.",
          "C. A group of animals gathers around a waterhole.",
          "D. The jungle catches fire and the scene becomes chaotic."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_313_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the name of the street sign right now?",
        "time_stamp": "00:00:05",
        "answer": "B",
        "options": [
          "A. Houston St.",
          "B. Bleecker St.",
          "C. Prince St.",
          "D. Spring St."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_390_real.mp4"
  },
  {
    "time": "[0:02:04 - 0:02:09]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What type of taxi is visible right now?",
        "time_stamp": "00:02:07",
        "answer": "C",
        "options": [
          "A. Ford Crown Victoria.",
          "B. Toyota Prius.",
          "C. Toyota Sienna.",
          "D. Nissan NV200."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_390_real.mp4"
  },
  {
    "time": "[0:04:08 - 0:04:13]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What brand is currently seen on the black banner outside the left side building?",
        "time_stamp": "00:04:11",
        "answer": "C",
        "options": [
          "A. GUESS.",
          "B. GUCCI.",
          "C. ARMANI EXCHANGE.",
          "D. VERSACE."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_390_real.mp4"
  },
  {
    "time": "[0:06:12 - 0:06:17]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the primary color of the traffic signs on the lamp posts right now?",
        "time_stamp": "00:06:13",
        "answer": "A",
        "options": [
          "A. Blue.",
          "B. Green.",
          "C. White.",
          "D. Red."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_390_real.mp4"
  },
  {
    "time": "[0:08:16 - 0:08:21]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What brand of truck is seen delivering goods on the right side of the road right now?",
        "time_stamp": "00:08:20",
        "answer": "B",
        "options": [
          "A. U-Haul.",
          "B. Ryder.",
          "C. Penske.",
          "D. Enterprise."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_390_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What attributes describe the traffic light on the right side?",
        "time_stamp": "00:00:07",
        "answer": "B",
        "options": [
          "A. It is red.",
          "B. It is green.",
          "C. It is yellow.",
          "D. It is blinking."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_395_real.mp4"
  },
  {
    "time": "[0:01:59 - 0:02:04]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What building logo is visible in the background?",
        "time_stamp": "00:02:03",
        "answer": "A",
        "options": [
          "A. MetLife.",
          "B. Citibank.",
          "C. Chase.",
          "D. Bank of America."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_395_real.mp4"
  },
  {
    "time": "[0:03:58 - 0:04:03]",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "Right now, which traffic sign is visible near the traffic light?",
        "time_stamp": "00:03:58",
        "answer": "B",
        "options": [
          "A. Stop sign.",
          "B. One way sign.",
          "C. Yield sign.",
          "D. Do not enter sign."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_395_real.mp4"
  },
  {
    "time": "[0:05:57 - 0:06:02]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "Right now, what is the color of the car directly in front of the camera?",
        "time_stamp": "00:05:58",
        "answer": "D",
        "options": [
          "A. Blue.",
          "B. Red.",
          "C. Black.",
          "D. Grey."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_395_real.mp4"
  },
  {
    "time": "[0:07:56 - 0:08:01]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "Right now, what is the color of the taxi driving ahead?",
        "time_stamp": "00:08:00",
        "answer": "B",
        "options": [
          "A. Red.",
          "B. Yellow.",
          "C. Blue.",
          "D. White."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "driving",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_395_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:02]: The video shows a first-person perspective of a character standing in front of an apartment complex with a sign that reads \"Grapeseed Apartments\" and litter scattered around the entrance. The character is wearing a striped short-sleeved shirt and beige pants. The scene unfolds on a paved walkway with some greenery on the sides providing suburban ambiance.  [0:00:03 - 0:00:07]: As the character walks away from the entrance, a yellow car appears in the background, driving past on the street. Several palm trees, well-maintained bushes, and buildings populate the scene, providing a suburban feel. [0:00:08 - 0:00:10]: The character approaches the road and then transitions towards a different direction, walking around the building. His movement is steady, showing the intricate paved patterns on the walkway. The apartments are characterized by modern architecture, with visible balconies at each level. [0:00:11 - 0:00:13]: The camera captures a close-up view of the character’s profile as he makes his way around the complex. The scene shifts to a broader view looking up towards the multi-story apartment building, highlighting its height and the surrounding tall palm trees.  [0:00:14 - 0:00:17]: The character continues to walk, and the perspective changes, often angling upwards to include the sky and the taller elements of the architecture. The video exhibits various angles, emphasizing the verticality of the building and the surrounding lush greenery. [0:00:18 - 0:00:20]: As the character reaches a broader road, the camera pans around, showing more buildings, including a blend of residential houses and modern apartments. The distance reveals more red-leafed trees and vehicles moving along the street, maintaining the suburban atmosphere.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the game character in the video standing right now?",
        "time_stamp": "00:00:10",
        "answer": "B",
        "options": [
          "A. Standing in the middle of the highway.",
          "B. Standing at the entrance of Unit A's building.",
          "C. Standing on the mountain.",
          "D. Standing inside the house."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_270_real.mp4"
  },
  {
    "time": "0:02:40 - 0:03:00",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:42]: The video shows a first-person perspective of a man in a striped shirt and shorts walking down a city sidewalk. The sidewalk is flanked by a row of square bushes and a palm tree on the right. A woman in a pink top walks ahead, holding a cup in her right hand. Cars and buildings are visible in the background, and there are several chat comments on the right side of the screen.  [0:02:42 - 0:02:44]: The man continues to walk forward, approaching a wooden bench on the sidewalk. [0:02:44 - 0:02:46]: The man stands in front of the bench, and a green icon appears on the bench, possibly indicating an interactable object. [0:02:46 - 0:02:48]: The man begins to lower himself, preparing to sit on the bench. [0:02:48 - 0:02:50]: The man sits on the bench, leaning forward slightly with his hands resting on his knees. [0:02:50 - 0:02:52]: The man sits on the bench, looking straight ahead. The view changes to show more of the surroundings, including a trash can and more buildings across the street. [0:02:52 - 0:02:54]: The man sits and slightly turns his head to the right, observing his surroundings. A yellow car is visible in front. [0:02:54 - 0:02:56]: The man continues to sit, still looking around. The yellow car is now more to the left in the frame. [0:02:56 - 0:02:58]: The man sits on the bench, with the yellow car slightly obscured by a traffic sign. [0:02:58 - 0:03:00]: The man stands up from the bench and shifts his gaze left, preparing to resume walking.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the man doing right now?",
        "time_stamp": "00:02:51",
        "answer": "D",
        "options": [
          "A. Sitting on the bench, observing the surroundings.",
          "B. Lowering himself to sit on the bench.",
          "C. Walking towards a wooden bench.",
          "D. Standing up from the bench and preparing to walk."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_270_real.mp4"
  },
  {
    "time": "0:05:20 - 0:05:40",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:21]: The video shows a graphical settings menu from a video game. Some of the settings displayed include \"DirectX Version\" and \"Output Resolution.\" The screen has a greenish tint. A webcam feed is visible at the top left, showing a person wearing headphones. [0:05:22 - 0:05:25]: There are adjustments made to the settings, and the highlighted option changes to \"MSAA.\" The content on the right side of the screen keeps changing as different options are selected. [0:05:26 - 0:05:30]: The person continues to navigate through the settings. The highlighted option moves to \"Pause Game On Focus Loss.\" The options visible include various tuning sliders for \"Population Density.\" [0:05:31 - 0:05:34]: The highlighted option moves to \"Extended Texture Budget.\" More graphical settings like \"Distance Scaling\" and \"Texture Quality\" are visible on the screen. The webcam feed remains constant. [0:05:35 - 0:05:36]: The highlighted options continue to move through the graphical settings list, with settings like \"Shadow Quality\" and \"Reflection Quality\" appearing. The person in the webcam appears focused on the task. [0:05:37 - 0:05:38]: The navigation through settings persists, highlighting \"Reflection MSAA.\" Additional settings such as \"Water Quality\" and \"Particles Quality\" start to appear. [0:05:39]: The person selects \"Post FX,\" among other graphical settings like \"Soft Shadows\" and \"Motion Blur Strength,\" displayed on the menu.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person doing right now?",
        "time_stamp": "00:05:25",
        "answer": "D",
        "options": [
          "A. Selected \"DirectX Version\".",
          "B. Highlighted \"Pause Game On Focus Loss\".",
          "C. Changed \"Output Resolution\".",
          "D. Adjusted \"SETTINGS\"."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_270_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:05]: The scene takes place on a sidewalk next to a road dotted with palm trees and streetlights. The viewpoint is that of a character running down the sidewalk. The character is dressed in a blue and white striped outfit with white shoes. In the distance, there are buildings and vehicles driving on the road, which appear to be part of a modern urban environment. [0:08:06 - 0:08:10]: The character continues running along the sidewalk towards an intersection with traffic lights. There are vehicles seen passing by on the road. The structures of the buildings become more distinct, displaying multiple-floor designs with some under construction. Traffic signs and poles are also visible on the sidewalk. [0:08:11 - 0:08:14]: The character approaches the intersection and slightly turns to the left, where the roads diverge to elevated highways. More vehicles are visible, including one driving under an overpass. A large open area is observed with construction equipment and barriers, indicating development work in progress. [0:08:15 - 0:08:20]: The perspective shifts to the game's inventory screen, showing a character's inventory items categorized under Player, Ground, and Backpack sections. The character's health and status information are visible on the left side of the screen. Different items like a GC card, mobile phone, food, and drinks are listed within the inventory slots. The character navigates the inventory, opening various item categories and accessing different belongings.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the character doing right now?",
        "time_stamp": "00:08:00",
        "answer": "D",
        "options": [
          "A. Walking along the sidewalk.",
          "B. Standing at an intersection.",
          "C. Climbing a building.",
          "D. Running along the sidewalk."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_270_real.mp4"
  },
  {
    "time": "0:10:00 - 0:10:03",
    "captions": "[0:10:00 - 0:10:03] [0:10:00 - 0:10:03]: A first-person perspective video shows two individuals standing on a concrete sidewalk near a busy intersection with elevated highways in the background. The person closer to the foreground is wearing a white and blue striped short-sleeved shirt and white shorts, looking at a smartphone. The other individual, slightly further away and facing mostly away from the camera, is dressed in a white shirt with a pattern of red roses paired with dark pants. Various cars are seen on the road in the background under the highways, and several palm trees are scattered throughout the scene. To the right of the scene, a graphical overlay displays live chat comments and a list of player names, seemingly from a gaming interface. The overlay includes colorful text and icons, adding an interactive element to the video.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the position of the person in the white and blue striped short-sleeved shirt right now?",
        "time_stamp": "00:10:03",
        "answer": "A",
        "options": [
          "A. Standing on a concrete sidewalk near a busy crossroads.",
          "B. Standing in the middle of the road under elevated highways.",
          "C. Standing next to a palm tree on the side of the road.",
          "D. Standing on a grassy area near the intersection."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_270_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:05]: A muscular man without a shirt, wearing black workout shorts, stands in an indoor gym. He poses by flexing and turning his body, showcasing his back and biceps. The gym equipment and a black wall are in the background. The text \"HOW TO TRAIN\" and \"BACK & BICEPS\" appear on the left side of the frames. [0:00:06 - 0:00:10]: The scene shifts to a collage of six workout images. These frames show a man engaging in different exercises using gym equipment, each image labeled with a number from 1 to 6. The background consists of gym equipment and weights, with bright, even lighting. [0:00:11 - 0:00:13]: The video transitions to show the same man on a treadmill or stairmaster, wearing a black sweatshirt, blue shorts, black socks, and a gray cap. He exercises on the machine in a well-lit indoor gym space. The text \"WARM-UP\" and \"5 MINUTES TREADMILL OR STAIRMASTER\" appear at the bottom of the screen. [0:00:14 - 0:00:16]: The man steps off the machine and walks towards the back, passing other equipment. He moves with a steady pace, wearing the same black sweatshirt, blue shorts, and cap. [0:00:17 - 0:00:19]: The man begins stretching his arms and warming up in the gym area. He wears a black sweatshirt and blue shorts, surrounded by various gym equipment and machines. The background includes a large space with exercise machines and a few gym bags on the floor.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color are the shorts the man is wearing while stretching his arms and warming up?",
        "time_stamp": "0:00:19",
        "answer": "D",
        "options": [
          "A. Red.",
          "B. Black.",
          "C. Gray.",
          "D. Blue."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Text-Rich Understanding",
        "question": "What text appears on the screen when the man is shown exercising on the treadmill?",
        "time_stamp": "0:00:13",
        "answer": "A",
        "options": [
          "A. \"WARM-UP\".",
          "B. \"HOW TO TRAIN\".",
          "C. \"BACK & BICEPS\".",
          "D. \"WORKOUT PLAN\"."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_147_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:00 - 0:02:20] [0:02:00 - 0:02:08]: A man is performing pull-ups on a black pull-up bar inside a gym. He is wearing a black t-shirt, blue shorts, and black shoes. The viewpoint is from behind him, showing him lifting himself up and down in a continuous motion. The gym background includes various weightlifting equipment, reflective surfaces, and a dimly lit environment with a monochromatic color scheme. [0:02:09 - 0:02:19]: The camera angle shifts to a close-up view of the man from underneath as he continues with the pull-ups. His face shows signs of exertion, with his muscles visibly contracting. The perspective highlights the upward movement of his chin above the bar and the downward extension of his arms. The man maintains his grip on the bar and continues the exercise with focused determination.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the man using for his exercise?",
        "time_stamp": "00:02:20",
        "answer": "C",
        "options": [
          "A. A treadmill.",
          "B. A dumbbell.",
          "C. A pull-up bar.",
          "D. A bench press."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_147_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:09]: In a gym setting, a man is engaged in a strength training exercise. He is dressed in a black shirt, blue shorts, and black shoes. The room features a wall-mounted mirror reflecting numerous dumbbells organized neatly in racks. He is holding a large dumbbell in his right hand, bending over with his left knee resting on a weight bench positioned at an angle. The man's left arm is extended to maintain balance, gripping the bench. He lifts and lowers the dumbbell in a controlled motion, focusing on his form. The lighting is bright, evenly illuminating the workout space. [0:04:10 - 0:04:19]: A sidebar graphic appears on the left side of the screen, showing a sliding scale labeled \"HARD\" at the top and \"EASY\" at the bottom. The indicator on the scale shifts towards the \"HARD\" end, likely representing the difficulty of the exercise. The man continues his workout routine, maintaining consistent form and movements while lifting the dumbbell. The background, featuring gym equipment and mirrored walls, remains the same, providing a consistent environment for the workout demonstration.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is the man holding in his left hand?",
        "time_stamp": "0:04:09",
        "answer": "D",
        "options": [
          "A. A barbell.",
          "B. A kettlebell.",
          "C. A resistance band.",
          "D. A dumbbell."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_147_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:00 - 0:06:06]: A study on the effect of hand position on EMG activity of posterior shoulder musculature during horizontal abduction exercise is shown in a scientific paper format. The title and authors' names are visible. [0:06:01 - 0:06:012]: Two black-and-white images compare the pronated hand position (PRO) and neutral hand position (NEU) during an exercise. The images are labeled \"Figure 1: Pronated hand position (PRO)\" and \"Figure 2: Neutral hand position (NEU).\" In the pronated position, the subject's hand is gripping a bar with the palm facing down, while in the neutral position, the hand grips the bar with the palm facing sideways.   [0:06:08 - 0:06:12]: Annotations indicate more EMG (Electromyography) activity in the rear delts for the neutral position than the pronated position. The text \"less rear delt EMG activation\" appears below the pronated hand image, and \"more rear delt EMG activation\" appears below the neutral hand image. [0:06:13 - 0:06:20]: Text explains notable interindividual variability in EMG activity with different grip positions during internal and neutral rotation exercises. The highlighted portion of the text emphasizes greater activity during these movements and differences in EMG readings among subjects.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the main focus of the study shown in the scientific paper format?",
        "time_stamp": "0:06:20",
        "answer": "A",
        "options": [
          "A. The effect of hand position on EMG activity of posterior shoulder musculature during horizontal abduction exercise.",
          "B. The effect of hand position on shoulder pain.",
          "C. The effect of different exercises on overall shoulder strength.",
          "D. The comparison of EMG activity between different exercises."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "gym_workout",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_147_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:05]: The video starts with a city street view where people are walking on the sidewalk. Shops line both sides of the street. On the left side, there is a sign for a restaurant named \"Bhavan,\" with what appears to be promotional text. People walk under the shop awnings, and a woman in a yellow jacket is talking on the phone while carrying a brown bag. A double-decker bus is visible on the right side of the street, and a French flag is displayed in the center of the frame with the text \"CREIL TO PARIS NORD, FRANCE\" and \"CITY WALK.\" The video has a subscribe button graphic in the bottom right corner. [0:00:06]: The scene transitions to a train station platform, the image initially blurred as if moving quickly. [0:00:07 - 0:00:09]: The video frame stabilizes, featuring a clear platform view with train tracks in the middle underside of a roofed area. There are several trains along the opposite platform, and overhead power lines and posts are visible. The sky appears cloudy. [0:00:10 - 0:00:14]: The text \"CREIL Metro Station\" overlays the screen, with different camera angles showing various parts of the station, including benches, signage, and a waiting area. People can be seen on the platform, including two individuals with bicycles. [0:00:15 - 0:00:17]: Without the overlay text, the scene shows more of the platform and the tracks. Specific details such as the concrete, metal structures of the station, and some graffiti are visible. [0:00:18 - 0:00:20]: The camera focuses on one end of the platform where a bicycle and a person are closer to the foreground. The lines and poles of the station are prominent, and the distant trains and infrastructure form the backdrop, presumably awaiting the next train's arrival or departure.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What does the text overlay \"CREIL Metro Station\" indicate in the video?",
        "time_stamp": "0:00:14",
        "answer": "D",
        "options": [
          "A. The start of a parade.",
          "B. An advertisement for a new restaurant.",
          "C. The title of a music video.",
          "D. The location of the current scene."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_303_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:00 - 0:04:20] [0:04:00 - 0:04:03]: The video begins on a wet street in an urban area. The camera is facing down a street lined with tall, beige buildings on both sides. A blue and white bus is visible farther down the road, slightly to the left. On the right side, there is a shop with large glass windows and black framing. The sidewalk is wet, and a series of traffic lights are positioned on the sidewalk. [0:04:03 - 0:04:07]: The camera slowly moves forward, maintaining focus on the street. The shop on the right side, with large glass windows, displays several signs and posters. The traffic light on the sidewalk becomes more prominent as the camera approaches. A bus is seen closer, and the sidewalk leads to more storefronts. [0:04:07 - 0:04:10]: As the camera continues moving forward, more details of the shop to the right become visible, including its entrance door. Further ahead, people can be seen walking on the sidewalk. There are additional shops, and the blue and white bus remains stationary in the distance. [0:04:10 - 0:04:12]: The camera moves closer to the storefronts. More pedestrians become visible; some are carrying umbrellas, indicating that it might be raining. The camera captures more details of the advertisements in the windows. The shop next to the first one has a larger display area with bright signs. [0:04:12 - 0:04:15]: The camera continues to move forward, capturing more pedestrians. A woman holding an umbrella and wearing a coat walks past the shop. Various items and mannequins are displayed through the shop windows. The pavement and shopfronts remain slick with rain. [0:04:15 - 0:04:17]: The video shows more of the scene as the camera nears the storefront. The woman with the umbrella walks closer to the curb. A man in a red jacket and another pedestrian are further down the sidewalk. The entrance of the shop displays promotional signs, with one sign reading \"PROMOTION.\" [0:04:17 - 0:04:20]: The camera continues forward, passing the storefront, and focusing on the pedestrians ahead. The man in the red jacket walks past the camera on the left side. More details of the cityscape, including parked cars on the street, become visible. The urban setting remains consistent, with high-rise buildings and shops lining the wet street.",
    "questions": [
      {
        "task_type": "Event Understanding",
        "question": "What is displayed through the shop windows?",
        "time_stamp": "0:04:17",
        "answer": "D",
        "options": [
          "A. Books and paintings.",
          "B. Electronics and gadgets.",
          "C. Food and beverages.",
          "D. Various items and mannequins."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_303_real.mp4"
  },
  {
    "time": "[0:08:00 - 0:09:00]",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:05]: The video shows a commercial street with wet pavement from recent rain. Various shops line the left side of the street, including a clothing store with mannequins wearing colorful dresses in the window display. A man, viewed from behind, walks along the sidewalk past these shops. The right side features a bus stop shelter with an advertisement and a tree partially obstructing the background. Pedestrians and vehicles are present in the scene. [0:08:06 - 0:08:07]: As the video continues, the man in the foreground approaches the bus stop shelter. Across the street, various establishments such as “Good Day” and others are visible, with cars parked along the roadside and others moving. [0:08:08 - 0:08:09]: The focus shifts alongside the bus stop shelter, revealing more of the street's storefronts on the left and continuing to show people walking. A sign for \"SP Traders\" becomes visible, suggesting a shop of some sort. [0:08:10 - 0:08:15]: The camera moves past the shelter, showing more shops, including \"SP Traders”, which appear to sell groceries and other items. There are boxes of produce and other goods displayed outside under an awning. The man in the foreground, still walking forward, gets closer to the shops as more pedestrians are seen walking in the same direction. [0:08:16 - 0:08:20]: The video continues on the wet sidewalk alongside \"SP Traders,\" showing more of its storefront and its produce display. The man in the dark jacket, now seen browsing the items, is near the shop entrance. The camera captures more depth of the street with additional shops and the bus stop now further in the background.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the condition of the pavement on the commercial street?",
        "time_stamp": "0:08:05",
        "answer": "D",
        "options": [
          "A. Dry.",
          "B. Covered in leaves.",
          "C. Snowy.",
          "D. Wet from recent rain."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Action Recognition",
        "question": "What is the man in the dark jacket doing near the shop entrance?",
        "time_stamp": "0:08:18",
        "answer": "D",
        "options": [
          "A. Talking on the phone.",
          "B. Waiting for a bus.",
          "C. Drinking coffee.",
          "D. Browsing the items."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_303_real.mp4"
  },
  {
    "time": "[0:12:00 - 0:13:00]",
    "captions": "[0:12:00 - 0:12:20] [0:12:00 - 0:12:01]: The video begins with a first-person perspective on a rainy street. The camera faces a storefront displaying various fruit juices. The ground is wet, suggesting recent rain, and a pedestrian, wearing blue jeans and a dark jacket, walks away from the camera.   [0:12:02 - 0:12:05]: As the camera moves forward, the focus remains on the pedestrian in blue jeans walking along the wet sidewalk. A tree and several storefronts run parallel to the street on the right. At the same time, public buses and a motorcycle pass by on the street. [0:12:06 - 0:12:08]: The camera continues moving forward, capturing the pedestrian walking further ahead. A bus stop and traffic are visible on the road. A green trash can and a tree are on the right side of the sidewalk. [0:12:09 - 0:12:11]: The scene showcases more storefronts, including a telecommunications shop on the left. More pedestrians are visible, some standing in front of stores, while the primary pedestrian in blue continues down the street. [0:12:12 - 0:12:13]: The camera shows a colorful storefront with a bright decor on the left. The street remains busy with a mix of pedestrians, vehicles, and storefronts. The sidewalk and street still appear wet from the rain. [0:12:14 - 0:12:15]: The camera moves past a large blue door on the left and a pedestrian in a patterned jacket walks towards the camera. The primary pedestrian in blue is further ahead. [0:12:16 - 0:12:17]: The scene shows another storefront, with a pink and white sign and posters. A woman, holding a green bag, walks past the display window. The main pedestrian in blue continues further ahead on the wet sidewalk. [0:12:18 - 0:12:20]: The camera moves alongside a shop with a vibrant pink facade, showcasing various products in its display window. The sidewalk is busy with pedestrians, and the main pedestrian in blue continues walking along, heading further down the street.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What item is visible in the hands of a woman passing by a storefront with a pink and white sign?",
        "time_stamp": "0:12:17",
        "answer": "D",
        "options": [
          "A. An umbrella.",
          "B. A fruit juice.",
          "C. A newspaper.",
          "D. A green bag."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_303_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What is the best summary of the actions taken by the individual just now?",
        "time_stamp": "00:00:10",
        "answer": "B",
        "options": [
          "A. The individual sliced fresh vegetables, seasoned them, and placed them on a grill.",
          "B. The individual carefully unwrapped a pair of gloves, stretched them open, and began putting them on.",
          "C. The individual assembled the necessary ingredients for a sandwich and started layering them.",
          "D. The individual cleaned a work surface, arranged cooking utensils, and prepared a workstation."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_340_real.mp4"
  },
  {
    "time": "[0:02:03 - 0:02:13]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What best summarizes the actions taken by the individual just now?",
        "time_stamp": "00:02:13",
        "answer": "C",
        "options": [
          "A. The individual gathered various vegetables, chopped them, and prepared a salad.",
          "B. The individual picked up a hamburger patty, grilled it, and assembled a cheeseburger.",
          "C. The individual took a hot dog from the water, placed it in a bun, and added sauerkraut.",
          "D. The individual prepared a pasta dish by boiling noodles, adding sauce, and serving it in a bowl."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_340_real.mp4"
  },
  {
    "time": "[0:04:06 - 0:04:16]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What best summarizes the actions taken by the individual just now?",
        "time_stamp": "00:04:16",
        "answer": "A",
        "options": [
          "A. The individual placed a hot dog in a bun, added onions and peppers, wrapped it in paper, and placed it aside.",
          "B. The individual assembled a hamburger with cheese, lettuce, and tomatoes, wrapped it, and handed it to a customer.",
          "C. The individual prepared a burrito with beans and rice, wrapped it, and placed it in a serving tray.",
          "D. The individual made a salad by chopping vegetables, adding dressing, and setting it on a counter."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_340_real.mp4"
  },
  {
    "time": "[0:06:09 - 0:06:19]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What is the best summary of the actions taken by the individual just now?",
        "time_stamp": "00:06:19",
        "answer": "C",
        "options": [
          "A. The individual sliced onions and tomatoes, placed them carefully in a hot dog bun, and wrapped it.",
          "B. The individual grabbed a hamburger bun, cooked a patty, and assembled a cheeseburger.",
          "C. The individual picked up a hot dog bun, retrieved a hot dog from water, placed it in the bun, added ketchup.",
          "D. The individual prepared a sandwich by layering several ingredients between two slices of bread and served it."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_340_real.mp4"
  },
  {
    "time": "[0:08:12 - 0:08:22]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions taken by the individual just now?",
        "time_stamp": "00:08:21",
        "answer": "C",
        "options": [
          "A. The individual cooked a pasta dish by boiling noodles, adding sauce, and plating it.",
          "B. The individual prepared a burrito, added beans and rice, and wrapped it in foil.",
          "C. The individual assembled a hot dog by placing it into a bun, adding toppings, and wrapping it.",
          "D. The individual chopped vegetables, added them to a salad bowl, and tossed them with dressing."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_340_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:06]: The video begins with the view of a computer screen displaying a \"Trucking Progression\" menu. The left side of the screen contains detailed information about various missions related to trucking, such as Distance, Time, Deliveries, and Payout. The right side features an image of a modern semi-truck, accompanied by a teal background. There is a person visible in a small window at the top-left corner of the screen, appearing to be livestreaming with chat messages scrolling on the right side. [0:00:07 - 0:00:12]: The person at the top-left corner expresses enthusiasm, making animated gestures. He continues to interact with the interface of the trucking progression menu on the computer display. Chat messages persist on the right side of the screen. [0:00:13 - 0:00:15]: The screen's visible content remains the same, but the person's gestures change to suggest he is speaking.  [0:00:16 - 0:00:17]: As the video progresses, the perspective shifts to an outdoor setting. Two people are seen standing; one is wearing a reflective vest and the other is in a white uniform with \"GOPostal\" branding. They appear to be having a conversation. The person in the reflective vest is holding a device that looks like a tablet or smartphone. [0:00:18 - 0:00:19]: The person in the reflective vest faces the other individual, who continues to look at the device in his hand. Both individuals are standing near a brick wall with some grass growing at its base.  [0:00:20]: The person in the white uniform reads from his handheld device. The video ends with the two individuals continuing their interaction outdoors.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person in the white uniform doing right now?",
        "time_stamp": "00:00:12",
        "answer": "D",
        "options": [
          "A. Typing on a keyboard.",
          "B. Writing on a notepad.",
          "C. Speaking to a crowd.",
          "D. Reading from a handheld device."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_284_real.mp4"
  },
  {
    "time": "0:02:40 - 0:03:00",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:41]: The scene starts with a low perspective view of the rear part of a vehicle, positioned on a road. The camera is tilted slightly upward, revealing part of the license plate and the texture of the road surface.  [0:02:42 - 0:02:47]: As the perspective changes, the scene shifts to a dense forest area. Tall trees with thick foliage dominate the backdrop, and the viewer seems to be moving through the forest, with the trees swaying slightly. [0:02:48 - 0:02:55]: The forest scene continues, with more detail of the undergrowth becoming visible. Fern-like plants and shrubs populate the forest floor, adding to the dense greenery. The view shifts slightly to the right, showing more of the background. [0:02:56 - 0:02:58]: The camera moves past a large tree trunk, which is in the foreground on the right side of the screen. The thick foliage continues to dominate the background as the viewpoint emerges slightly from the dense forest. [0:02:59 - 0:03:01]: The scene transitions to a clearer view of a forested area with a house or building visible in the background. Smoke or mist can be seen in the distance, giving a somewhat eerie feeling. The green tint of the video suggests night vision or a similar effect.  [0:03:02 - 0:03:09]: The camera orientation shifts upward, bringing the entire building and surrounding area into focus, showing rooftops and additional structures. Trees surround the area, and smoke rises from behind the buildings.  [0:03:10 - 0:03:14]: As the camera continues to pan, more buildings come into view, and the dense tree line becomes a consistent backdrop. The smoke continues to billow in the background.  [0:03:15 - 0:03:20]: The perspective moves indoors to what appears to be a museum or gallery with dark lighting. Large paintings or digital screens adorn the walls, and some debris is scattered on the floor, suggesting disarray.  [0:03:21 - 0:03:30]: The camera continues to pan through the indoor setting, revealing more details of the interior. The area remains dimly lit, with greenish hues, and it seems abandoned or in disrepair. The images on the walls become clearer, showing artwork or diagrams.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the camera focusing on right now?",
        "time_stamp": "00:02:55",
        "answer": "D",
        "options": [
          "A. The texture of the road surface.",
          "B. The forest floor with ferns and shrubs.",
          "C. The rear part of a vehicle.",
          "D. The rooftops, additional structure and forest."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_284_real.mp4"
  },
  {
    "time": "0:05:20 - 0:05:40",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:23]: The scene starts in an open industrial area. Three people, a blonde man in a high-visibility vest, a person with a dark jacket and striped scarf, and a man in a white shirt holding a clipboard, are conversing near a brick wall;  [0:05:23 - 0:05:30]: The camera angle shifts slightly to show more of the industrial background, including a bright yellow \"Cluckin' Bell\" truck parked near a loading dock with a few parked cars and scattered debris;  [0:05:30 - 0:05:39]: The characters continue their conversation with minor changes in their positions and gestures. The man in the white shirt occasionally looks down at his clipboard, while the others listen attentively; [0:05:39]: The video briefly shifts to a close-up view of an inventory screen displaying items in a virtual format; [0:05:39 - 0:05:41]: The display changes to a \"Trucking Progression\" screen, featuring a list of locations and respective rewards, focusing on a delivery task from Sandy Shores Clothing Shop to Los Santos Clothing Shop; [0:05:41 - 0:05:43]: The screen details the specifications of the job, including distance, time, and payout; [0:05:43 - 0:05:44]: The screen returns to the list of available deliveries, highlighting different routes and associated rewards; [0:05:44 - 0:05:46]: The video transitions back to the three individuals standing in the industrial yard, maintaining their conversation; [0:05:46 - 0:05:48]: The man in the dark jacket starts to turn away and walk towards the parked cars while the others continue to stand and talk; [0:05:48 - 0:05:49]: The man in the high-visibility vest begins to follow the person in the dark jacket toward the parked cars, indicating the continuation of their task or activity.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person in the dark jacket doing right now?",
        "time_stamp": "00:05:48",
        "answer": "D",
        "options": [
          "A. Talking to the man in the white shirt.",
          "B. Checking the clipboard.",
          "C. Standing still and listening.",
          "D. Turning away and walking towards the parked cars."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_284_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:03]: A blue car is driving on a road near a coast. Another car, a black sports car, is visible on the left side, partially blocking the path. The area has white lane markings and some barricades in the distance. The sky is clear, and the sea is visible. [0:08:04 - 0:08:07]: The blue car continues to drive forward, approaching a roundabout with a \"No Entry\" sign. The black car is further on the right now, and another car is seen in the distance near the coast. [0:08:08 - 0:08:11]: The blue car navigates through the roundabout, closer to a security gate, with red and white barricades on both sides. The sea remains visible in the background. [0:08:12 - 0:08:19]: The blue car positions itself in front of the security gate. The red and white barrier rises, allowing the blue car to proceed. There's a straightforward road ahead leading to an overpass. The vehicle climbs onto the bridge. [0:08:20 - 0:08:23]: The blue car drives uphill on a wide bridge, bordered by safety railings on the left and right sides. There’s an industrial area and a railway line on the right. [0:08:24 - 0:08:27]: The blue car continues steadily uphill. The bridge goes further, and a few palm trees and industrial buildings are noticeable on the right side. One more vehicle appears in the distance. [0:08:28 - 0:08:35]: As the blue car proceeds further uphill, the road bends slightly to the right. There are no other cars immediately in front, and the elevated landscape ahead features more industrial structures and signals the approach towards the cityscape. [0:08:36 - 0:08:39]: The blue car drives past a few palm trees and a building on the right side while continuing straight uphill. Some red barricades can be seen to the left, marking the path. [0:08:40 - 0:08:43]: The car maintains its speed as it drives uphill towards the skyline. The bridge is relatively empty, and the coastal area is visible on the left. [0:08:44 - 0:08:47]: Progressing forward, the blue car stays in the right lane. More structures, including a large building and several industrial facilities, are observable in the distance. [0:08:48 - 0:08:51]: The car approaches the top of the bridge. There are a few visible palm trees on both sides, and the background shows an industrial zone extending towards the city. [0:08:52 - 0:08:55]: As the car continues, the bridge slopes slightly downward. A city skyline can be discerned at the far end of the bridge, with the industrial area extending on both sides. [0:08:56 - 0:08:59]: The blue car maintains a steady pace while descending slightly. The road extends forward with the city's buildings becoming clearer in the backdrop. [0:09:00 - 0:09:03]: Driving further, the blue car transitions slightly to the left lane while approaching another vehicle. The industrial area continues on both sides, with more buildings visible ahead. [0:09:04 - 0:09:07]: The blue car starts to overtake a slower-moving black sedan on the left lane. The city skyline ahead is becoming more prominent as the bridge continues to slope downwards. [0:09:08 - 0:09:11]: The blue car successfully overtakes the black sedan, staying in the left lane. The urban landscape and industrial buildings stretch further into the view. [0:09:12 - 0:09:15]: The blue car drives through the mid-section of the bridge. More vehicles can be seen ahead in the lanes, and both sides of the road are bordered by barriers and guardrails. [0:09:16 - 0:09:19]: The vehicle continues its journey, descending gradually with a view of the cityscape comprising skyscrapers and infrastructure. Industrial elements dominate the roadside on the right. [0:09:20]: The blue car continues forward, approaching the descent of the bridge with a clearer view of the city ahead, maintained in the left lane.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the blue car doing right now?",
        "time_stamp": "00:08:22",
        "answer": "D",
        "options": [
          "A. Driving through a roundabout.",
          "B. Climbing onto an overpass.",
          "C. Overtaking a black sedan.",
          "D. Approaching the descent of the bridge."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_284_real.mp4"
  },
  {
    "time": "0:10:20 - 0:10:22",
    "captions": "[0:10:20 - 0:10:22] [0:10:20 - 0:10:22]: The video captures a first-person perspective of a gaming or simulation interface. In the middle of the screen, there's a detailed, digital character with a serious expression, wearing a dark green hoodie and a gray cap. In the top left corner, there is a small window showing the person playing the game, sitting in front of various screens, with headphones on, and focusing on the game. In the bottom half of the main screen, details about various car care products are displayed with items such as 'Rusty Wrench', 'Car Polish', and other maintenance tools. On the right side of the screen, there are numerous user comments and emojis streaming in a vertical chat box, indicating a live interaction, possibly from a streaming platform.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the gamer doing right now?",
        "time_stamp": "00:10:22",
        "answer": "D",
        "options": [
          "A. Adjusting the headset.",
          "B. Taking a break from the game.",
          "C. Chatting with the audience.",
          "D. Streaming live while focusing on the game."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_284_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:05]: In a parking lot, a person wearing a green reflective vest is seen standing next to a black car. The person appears to have short blond hair and is wearing headphones. In the background, there is another person standing against a graffiti-covered concrete wall. Various cars are parked in the lot, and a few trees and buildings are visible in the background. [0:00:06 - 0:00:10]: The person continues to stand in the same spot. On the right side of the frames, a blue car is parked. The person is near several yellow bollards, intended to restrict vehicle access, and is positioned near a black car with the driver's door open. The person in the car is partially visible, communicating with the person in the vest. [0:00:11 - 0:00:14]: The camera angle remains focused on the person in the green vest. In this sequence, the person slightly adjusts their posture but remains in conversation with the driver of the black car. The backdrop shows a clear blue sky with palm trees and buildings visible in the distance across the street.  [0:00:15 - 0:00:17]: The person in the vest starts to walk away from the black car. The camera follows the movement, first turning left to reveal a broader view of the lot. Additional vehicles, including another black car and a blue car, become visible. The individual in the forefront begins to walk towards a nearby building. [0:00:18 - 0:00:20]: The person continues to walk towards a building with peeling paint and a worn-out facade. A second individual wearing light-colored clothing emerges from the building. There are various signs on the building's walls, and the camera angle captures more of the surroundings, including more parked cars and some scattered debris on the ground. The background features residential buildings and clear skies.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person in the green vest doing right now?",
        "time_stamp": "00:00:17",
        "answer": "C",
        "options": [
          "A. Standing next to the black car.",
          "B. Adjusting their headphones.",
          "C. Walking towards a man with white hair and wearing white T-shirt.",
          "D. Talking to the driver in the black car."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_286_real.mp4"
  },
  {
    "time": "0:02:40 - 0:03:00",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:41]: The video starts with a first-person view of a game inventory screen. Different items are visible, including a green container and a map. The character’s avatar is shown on the left side of the screen, indicating the player’s current status. [0:02:42 - 0:02:45]: The perspective changes to a view of a person with blonde hair and wearing glasses and a reflective vest standing by a car in a parking lot next to a building with a “NO TRESPASSING” sign. [0:02:46 - 0:02:49]: The person in the reflective vest shifts positions, raising their hand towards their face, possibly to adjust the headset they are wearing. Another person is visible in the background, leaning against the building. [0:02:50 - 0:02:51]: The video captures the individual in the vest standing in front of the car while looking around. The background includes boxes and tires beside the building. [0:02:52 - 0:02:52]: Returning to the view where the person in the vest raises their hand again near their face, potentially signaling or using an in-game communication device. [0:02:53 - 0:02:55]: The scene shifts to a different angle showing the person in the vest facing another man in a gray suit while standing near a car in an alleyway. A green garbage container is present nearby. [0:02:56 - 0:02:57]: The view captures three people by the car, with two individuals in white shirts and one in a gray suit. The person in the vest is in the foreground with their back turned to the camera. [0:02:58 - 0:02:59]: Slight variations in the individuals' positions are noticed as they seem to be engaged in a conversation. [0:03:00 - 0:03:01]: The camera angle changes to show the person in the reflective vest standing next to the car in a parking lot with numerous cars and palm trees in the background. [0:03:02 - 0:03:04]: The individual in the vest appears to be discussing something, maintaining a similar posture near the car. [0:03:05 - 0:03:06]: The scene continues with another angle of the person in the vest as they look towards another person in a white shirt standing near the car. [0:03:07 - 0:03:08]: A more focused view reveals the person in the vest remains by the car, with the person in a white shirt and another individual talking in the background. [0:03:09 - 0:03:10]: A discussion seems to be ongoing as the background details, like the surrounding cars and palm trees, remain consistent. [0:03:11 - 0:03:12]: The individual in the vest continues standing near the car, showing minimal movement in the ongoing interaction. [0:03:13 - 0:03:14]: The people in the background adjust their positions while engaging in conversation.  [0:03:15 - 0:03:16]: The individual with glasses and a white shirt steps forward slightly while talking to the person in the reflective vest. [0:03:17 - 0:03:18]: The video briefly captures an overview of the scene, showing the backdrop and cars positioned in the distance. [0:03:19 - 0:03:20]: A shift in the camera angle gives a slightly different view of the individual in the reflective vest, with cars, trees, and buildings visible in the background.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person in the reflective vest doing right now?",
        "time_stamp": "00:02:43",
        "answer": "B",
        "options": [
          "A. Adjusting his headset.",
          "B. Pointing to the person behind him with his finger.",
          "C. Picking up a map.",
          "D. Sitting down on the car's hood."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_286_real.mp4"
  },
  {
    "time": "0:05:20 - 0:05:40",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:24]: The video starts with a scene at an outdoor location in front of a building with a \"No Trespassing\" sign. Two people are engaged in a conversation near a parked car. The person on the left is wearing goggles, a reflective vest, and headphones. The person on the right is dressed in a white shirt and appears to be listening intently. There are various items like cardboard boxes and tires in the background, along with another person casually standing against the wall. [0:05:25 - 0:05:29]: The two people continue their conversation, with slight movements indicating they are in a discussion. The person in the vest gestures slightly with their hand, and the individual in the white shirt maintains eye contact. The background elements remain unchanged, with the same person leaning against the wall. [0:05:30 - 0:05:34]: The perspective shifts slightly, showing more of the side of the car and the surroundings. The person in the reflective vest looks to the side briefly, possibly acknowledging something or someone off-screen. The conversation persists, maintaining the same tone and involvement. [0:05:35 - 0:05:39]: The camera angle changes to give a top-down view of the interaction between the two individuals. The person in the white shirt is looking up towards the reflective vest person, who remains focused on the discussion. The car and background remain consistent, unchanged from the previous frames. [0:05:40]: The perspective returns to a close-up of the two individuals engaged in conversation. The focus is primarily on their faces and upper bodies, with the man in the reflective vest looking slightly towards his left again. The background, including the car and items against the wall, stays consistent, with the third individual still present.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person in the reflective vest doing right now?",
        "time_stamp": "0:05:34",
        "answer": "B",
        "options": [
          "A. Gesturing with their hand.",
          "B. Talking with the person in front of him.",
          "C. Leaning against the wall.",
          "D. Listening intently."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_286_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:10]: In an outdoor setting near a brick wall, a person wearing a green safety vest and dark clothing stands facing a man in a white uniform and blue cap, who is holding a tablet. Behind them, there is a parked black car and some scattered items on the ground, including cans and debris. There are multiple containers behind the parked cars; [0:08:11]: The perspective shifts to show a detailed view of a 'Trucking Progression' screen with various job listings and rewards displayed;  [0:08:12 - 0:08:15]: The camera zooms in slightly on the job listings on the screen, with a total of six different jobs available for selection; [0:08:16]: The view returns to the original two people, with the person in the green safety vest still engaging with the man holding the tablet; [0:08:17 - 0:08:19]: The person in the green safety vest starts to move away from the man with the tablet, heading towards the background with a series of shipping containers and an industrial environment visible;  [0:08:20]: The person in the green safety vest is now seen running towards a large black van parked on the side of the road, near what appears to be a shipping or industrial area with large cranes and containers in the background.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the person in the green safety vest doing right now?",
        "time_stamp": "00:08:14",
        "answer": "C",
        "options": [
          "A. Standing near a man with a tablet.",
          "B. Holding a tablet near a parked black car.",
          "C. Running towards a large black van.",
          "D. Selecting a job listing from the screen."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_286_real.mp4"
  },
  {
    "time": "0:10:00 - 0:10:01",
    "captions": "[0:10:00 - 0:10:01] [0:10:00 - 0:10:01]: The video frame depicts a first-person perspective from inside a virtual environment, most likely a video game. There is a large grey van making a turn at an intersection within a city location. The van is positioned diagonally, indicating a turn to the left. The street has markings and a signal light visible on the right, showing a red light. In the background, there are several buildings, including one prominently with rectangular windows and beige walls. There are a few trees visible along the sidewalk. In the top left corner of the frame, a small live video feed shows a person with headphones, likely the game's streamer, seated indoors with another screen visible in front of them. The person's room has some furniture and typical indoor decor. The right side of the video frame includes a running chat box with various messages from viewers.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the large grey van doing right now?",
        "time_stamp": "00:10:01",
        "answer": "B",
        "options": [
          "A. Stopping at a red light.",
          "B. Making a left turn at an intersection.",
          "C. Driving straight through an intersection.",
          "D. Parked on the side of the street."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_286_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:05]: The video begins with a first-person perspective on an escalator moving upward. The escalator is flanked by a curved, glass and steel canopy structure. The canopy frames are painted in beige and turquoise colors. On the left side, a massive concrete building with a prominent black and white mural of a T-Rex dominates the scene. The right side shows part of the railing and some green plants; [0:00:06 - 0:00:10]: As the camera continues to move upward, additional parts of the escalator and canopy structure become visible. The terminal point of the escalator appears ahead, where the ground levels out. To the left, people walk along the pathway beside the escalator, including a group heading towards the looming glass-roofed structure that leads to another escalator. The large building with the T-Rex mural remains in the frame, and more details of the surroundings, including greenery and posters, can also be seen; [0:00:11 - 0:00:15]: The upper end of the escalator becomes fully visible. The video gradually reveals more of the spacious walkway area at the top. People are walking around the walkway, including a person with a stroller. A significant portion of the T-Rex mural and the building on the right continues to be a noticeable part of the background. Flora and decor along the walkway accent the environment; [0:00:16 - 0:00:20]: As the video progresses towards the end, the upper part of the T-Rex mural and the tall green trees engulf more of the frame. The escalator's endpoint clearings display groups of people moving about and give a definitive view of the pathways leading to areas beyond the observed scene. The environment retains its bright, clear skyline, and the movement takes the viewer closer to the large structure's architecture, foliage, and design elements.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is prominently featured on the massive concrete building?",
        "time_stamp": "00:00:05",
        "answer": "B",
        "options": [
          "A. A giant robot.",
          "B. A T-Rex mural.",
          "C. A cityscape painting.",
          "D. A famous portrait."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_322_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:22]: The video starts with a panoramic view of a suburban area taken from an elevated vantage point. In the foreground, there is a dark gray railing and a postbox on the right side. Below, a road curves through a green landscape, flanked by tall, verdant trees. The background includes numerous buildings with light-colored rooftops, a lush green golf course on the left, and distant mountain ranges under a clear, blue sky. [0:03:23 - 0:03:31]: The camera appears to pan slowly to the right, maintaining the elevated viewpoint. The lush greenery and road continue to dominate the foreground. More buildings become visible, revealing a sprawling suburban area. The well-manicured golf course remains a prominent feature on the left, while the mountains remain in the background, framing the scene. [0:03:32 - 0:03:36]: The view continues to shift, showing an expansive green area with more clear detail of buildings nestled among trees. The suburban layout becomes clearer, and a few more roads are noticeable. A small vehicle appears on the road, moving through the greenery. [0:03:37 - 0:03:40]: The camera focuses down the road, which heads towards the horizon. The vehicle on the road becomes more distinguishable as a small, red vehicle, possibly a golf cart. The surrounding greenery includes various species of trees casting shadows, highlighting the sunny and clear weather. The expansive mountainous landscape continues to dominate the horizon.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What was located on the right side of the scene just now?",
        "time_stamp": "00:03:22",
        "answer": "B",
        "options": [
          "A. A dark gray railing.",
          "B. A postbox.",
          "C. A small vehicle.",
          "D. A tree."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_322_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:52]: In an area with a green, metal-framed roof and green trim, there are several picnic-style tables with green benches neatly arranged. The tables and benches are made of a metal mesh material. Brick pillars are positioned between the tables, and some walls are adorned with colorful posters. One such poster features a muscular cartoon character promoting \"Duff\" beer. Mounted televisions are also placed on the pillars, displaying animated content. The area is well-lit with a combination of natural light and hanging lamps. Greenery outside the fenced area adds a vibrant backdrop. [0:06:53 - 0:06:56]: The focus is primarily on a specific poster with a character promoting \"Duff\" beer. The poster continues to draw closer, making the details on it more visible. The surroundings still feature the same picnic tables, benches, lights, and fenced greenery. [0:06:57 - 0:07:00]: The scene transitions to reveal an outdoor area adjacent to the covered section. The outdoor area features more green metal tables with umbrellas providing shade. The open space is bustling with activity. Additional large-scale decorations and buildings are visible in the distance, indicating a lively and vibrant environment. The scene suggests a blend of outdoor dining and entertainment.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What material are the picnic tables and benches made of?",
        "time_stamp": "00:07:00",
        "answer": "C",
        "options": [
          "A. Wood.",
          "B. Plastic.",
          "C. Metal mesh.",
          "D. Concrete."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_322_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:11:00]",
    "captions": "[0:10:00 - 0:10:20] [0:10:00 - 0:10:05]: The video begins with a first-person perspective view of a street. On the left side, there is a tall building with a detailed facade and a box office at its base. The building is adorned with sculptural elements and has a marquee with \"Box Office\" written on it. To the right of the street is a row of smaller buildings with storefronts, including a green awning over a café or shop with people sitting outside. The sky is clear and bright. [0:10:05 - 0:10:10]: As the perspective moves forward, more of the buildings on the right become visible. There are people walking along the street and some are gathered in small groups near the shops. More of the tall building to the left is also revealed, showing its height and intricate design. [0:10:10 - 0:10:15]: The view continues towards the middle of the street, showing a busy scene with groups of people dispersed along both sides of the street. There is a kiosk or small stand with an umbrella in front of a building directly ahead. The building has a mural painted on it and a green awning over one of its windows. [0:10:15 - 0:10:20]: As the perspective progresses further down the street, there is a clearer view of the group of people in front of the building with the mural. Some of the people are interacting, while others are standing and observing. The building to the right now shows a sign indicating it is associated with animation, having a grand arched entrance. The overall scene is bustling with activity and conversation amidst the architectural surroundings.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is prominently displayed on the marquee of the tall building to the left?",
        "time_stamp": "00:10:05",
        "answer": "B",
        "options": [
          "A. Café Entrance.",
          "B. Box Office.",
          "C. Storefronts.",
          "D. Green Awning."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "What is located in front of the building directly ahead in the middle of the street?",
        "time_stamp": "00:10:00",
        "answer": "C",
        "options": [
          "A. A café.",
          "B. A box office.",
          "C. A small stand with an umbrella.",
          "D. A mural."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_322_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:18",
        "answer": "A",
        "options": [
          "A. 0.",
          "B. 2.",
          "C. 3.",
          "D. 4."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_76_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:23",
        "answer": "A",
        "options": [
          "A. 3.",
          "B. 2.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_76_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:31",
        "answer": "A",
        "options": [
          "A. 4.",
          "B. 5.",
          "C. 3.",
          "D. 2."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_76_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:18",
        "answer": "A",
        "options": [
          "A. 4.",
          "B. 1.",
          "C. 3.",
          "D. 2."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_76_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:07:07",
        "answer": "A",
        "options": [
          "A. 5.",
          "B. 1.",
          "C. 3.",
          "D. 7."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_76_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:10]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions performed just now?",
        "time_stamp": "00:00:10",
        "answer": "D",
        "options": [
          "A. The individual grabbed a bottle of condiments, applied them to a hamburger, and closed the sandwich.",
          "B. The individual washed hands at the sink, selected a large bowl, and began mixing ingredients.",
          "C. The individual arranged vegetables and meats on a tray and placed it into the oven, setting the timer.",
          "D. The individual fetched a pack of hot dogs from the refrigerator, closed the refrigerator door, and proceeded with the preparation."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_374_real.mp4"
  },
  {
    "time": "[0:02:39 - 0:02:49]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "What activities were performed just now to prepare the hot dog?",
        "time_stamp": "00:02:49",
        "answer": "D",
        "options": [
          "A. The individual added ketchup to the hot dog, chopped onions, and added them as a topping.",
          "B. The individual toasted the bun, grilled the hot dog, and served it with French fries.",
          "C. The individual microwaved the hot dog, added relish, and served it with a cold drink.",
          "D. The individual added mustard to the hot dog, followed by meat sauce, and topped it with a scoop of mashed potatoes."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_374_real.mp4"
  },
  {
    "time": "[0:05:18 - 0:05:28]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the actions performed just now?",
        "time_stamp": "00:05:31",
        "answer": "D",
        "options": [
          "A. The individual seasoned food in a bowl, then proceeded to bake it in the oven.",
          "B. The individual prepared a beverage, poured it into a cup, and handed it to a customer.",
          "C. The individual prepared a sandwich, wrapped it in paper, and served it at the counter.",
          "D. The individual packaged food into a bag, placed it on the counter, and then gave the package to the customer."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_374_real.mp4"
  },
  {
    "time": "[0:07:57 - 0:08:07]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the activities performed just now?",
        "time_stamp": "00:08:07",
        "answer": "D",
        "options": [
          "A. The individual selected a hot dog bun, retrieved a cooked hot dog from the water, and added ketchup and mustard as condiments.",
          "B. The individual toasted a hot dog bun, placed a raw hot dog on the grill, and then garnished it with relish and onions.",
          "C. The individual boiled multiple hot dogs, toasted multiple buns, and served the hot dogs directly to customers without any condiments.",
          "D. The individual selected a hot dog bun, retrieved a cooked hot dog from the water with tongs, and prepared to finalize the hot dog."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_374_real.mp4"
  },
  {
    "time": "[0:10:36 - 0:10:46]",
    "questions": [
      {
        "task_type": "Clips Summarize",
        "question": "Which of the following options best summarizes the activities performed just now?",
        "time_stamp": "00:10:46",
        "answer": "D",
        "options": [
          "A. The individual toasted a bun, grilled a hot dog, added ketchup and mustard, and served it to the customer.",
          "B. The individual selected a hamburger bun, grilled a patty, added cheese, lettuce, and tomato, and served it to the customer.",
          "C. The individual microwaved a hot dog, placed it in a bun, and added pickles and onions as toppings.",
          "D. The individual selected a hot dog bun, retrieved a cooked hot dog from the water, placed it in the bun, and added meat sauce as a topping."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "preparation_of_meals",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_374_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00]: The video opens with a graphic featuring the number \"10\" in large, bold white text on the left. To the right of the \"10\" is an image of a white plate with cutlery (a knife and fork) arranged to resemble clock hands;  [0:00:01 - 0:00:04]: The view changes, revealing a person with light-colored hair and a blue shirt standing in front of a backdrop with the text \"RAMSAY in 10.\" The person appears to be smiling and talking; [0:00:05]: The video then shifts to a kitchen scene. The same person is now in a kitchen with white brick walls and shelves. Various kitchen utensils and ingredients are displayed on the shelves. There is a large red \"COOK\" sign on the top shelf; [0:00:06 - 0:00:08]: The person interacts with the camera by gesturing and talking. There is a mix of modern and rustic kitchen elements in the background, including jars, bottles, and a mix of utensils; [0:00:09 - 0:00:14]: The person glances down and then looks up, continuing to speak. The camera angle occasionally changes, showing different perspectives of the kitchen and the person; [0:00:15 - 0:00:17]: The person raises both hands while speaking and then brings them together, emphasizing a point. The kitchen background remains consistent, with the shelves and utensils creating a homey atmosphere; [0:00:18 - 0:00:19]: The video ends with the person making a hand gesture, possibly wrapping up their point. The shelves behind them feature an array of cooking items, neatly organized and visually appealing.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is located on the top shelf in the kitchen?",
        "time_stamp": "00:00:08",
        "answer": "B",
        "options": [
          "A. Jars and bottles.",
          "B. A large red \"COOK\" sign.",
          "C. A clock.",
          "D. A set of knives."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_45_real.mp4"
  },
  {
    "time": "[0:01:00 - 0:02:00]",
    "captions": "[0:01:40 - 0:02:00] [0:01:40 - 0:01:44]: A man wearing a black t-shirt is seen from a close-up perspective in what appears to be a kitchen setting. He is standing in front of a white brick wall with a wooden shelf, filled with various kitchen items like jars, bowls, and utensils. On the wall is a large red sign with the letters \"C-O-O-K.\" The man is handling a piece of food, likely kneading or mixing something with his hands. His facial expression is focused and concentrated. [0:01:45 - 0:01:49]: The scene transitions to an overhead view of the man’s hands shaping a ball of mixture, possibly meat, over a chopping board. There are several kitchen items around, including a grater, a spoon, and ingredients in small bowls. A lemon and knife are visible on the left side. The surroundings feature a countertop with a stove and sink nearby. The man appears to be forming a round shape with the mixture. [0:01:50 - 0:01:54]: The man continues to roll and shape the mixture with his hands, ensuring it maintains a round and compact form. His movements are methodical as he focuses on achieving a consistent shape. The bowl of mixture sits in front of him, partially filled. [0:01:55 - 0:01:57]: The man places the newly formed mixture ball onto a wooden chopping board. He then reaches into the bowl to grab more of the mixture, preparing to form another ball. His hands work swiftly and skillfully. [0:01:58 - 0:01:59]: The scene provides a closer view of his hands as he continues to roll the mixture into a round shape. His focus remains on ensuring that the consistency and shape are uniform throughout. The kitchen background, with the stovetop and counter, is slightly blurred, emphasizing the action in his hands.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the man doing with his hands right now?",
        "time_stamp": "0:01:44",
        "answer": "B",
        "options": [
          "A. Washing dishes.",
          "B. Kneading and mixing food.",
          "C. Cutting vegetables.",
          "D. Stirring a pot."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Action Recognition",
        "question": "What action is the man performing with the mixture on the bowl?",
        "time_stamp": "0:01:58",
        "answer": "A",
        "options": [
          "A. Rolling it into a ball.",
          "B. Flattening it.",
          "C. Cutting it into pieces.",
          "D. Mixing it in a bowl."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_45_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:21]: In a modern kitchen with white brick walls and wooden shelves in the background, a person reaches for a red pepper on a countertop which features several items, including various cooking ingredients and utensils. The individual is wearing a black T-shirt; [0:03:21 - 0:03:31]: The person starts to slice the red pepper on a wooden cutting board positioned on a textured countertop. The cutting board is surrounded by several jars and small bowls containing spices and other small cooking items. A frying pan with food is sizzling on the stove adjacent to the cutting board. The camera angle shifts to show a top-down view of the cutting board as the person continues to slice the pepper [0:03:31 - 0:03:34]: The individual gathers the sliced red pepper and starts to move it towards the pans on the stove. The brick wall and shelves filled with dishes remain in the background; [0:03:34 - 0:03:36]: They add the sliced red pepper to a stainless steel pan filled with white rice which is being heated on the stove. Another pan next to it contains meatballs being fried. The stove is modern with a gas burner; [0:03:36 - 0:03:37]: Close-up of the hot pan, focusing on the white rice mixed with the freshly added red pepper slices; [0:03:38 - 0:03:39]: The camera shifts to an overhead view of a kitchen countertop where the person’s hands adjust a bunch of fresh vegetables including lettuce, cucumbers, radishes, and chives on a white plate.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What does the person do with the sliced red pepper after cutting it?",
        "time_stamp": "0:03:36",
        "answer": "B",
        "options": [
          "A. Stores it in a container.",
          "B. Adds it to a pan with white rice.",
          "C. Throws it away.",
          "D. Eats it."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_45_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:41]: A person wearing a black shirt is in a kitchen with a white brick backsplash, wooden shelves stocked with dishes, and various kitchen utensils. Green onions are being picked up with one hand while standing behind a counter. [0:06:41 - 0:06:42]: The person places the green onions on a wooden cutting board, with lettuce and a small black bowl on a white platter to the left. The counter has a cutting board, various bottles, and ingredients. [0:06:42 - 0:06:45]: The person begins chopping the green onions with a large knife. Several green onion stalks are being precisely sliced, with a bottle of oil and a blue measuring spoon nearby. [0:06:45 - 0:06:47]: The chopping continues, showing the sharp knife cutting through the green onions rapidly and precisely. The sliced green onions pile up on the wooden board next to the remaining whole green onions. [0:06:47 - 0:06:50]: The camera provides a top-down view of the cutting board and platter. The person continues to chop green onions, with sliced cucumbers and lettuce leaves arranged neatly on the platter. [0:06:50 - 0:06:52]: The person arranges the sliced green onions on the platter with lettuce and cucumbers. The kitchen counter displays various ingredients, tools, and bowls utilized in the cooking process. [0:06:52 - 0:06:53]: The person shifts focus, now holding a small red pepper above the plate, appearing to add it to the dish. The kitchen appears well-organized, with colorful bottles, spice jars, and a pepper mill within easy reach. [0:06:53 - 0:06:56]: Switching to fine chopping, the camera captures the person’s methodical knife skills. The preparation showcases the organized arrangement of fresh ingredients ready to be included in the dish. [0:06:56 - 0:06:57]: The person picks up a halved lime, positioning it to squeeze over the dish. The kitchen environment remains consistent, with white tiled walls and well-arranged kitchen tools and ingredients. [0:06:57 - 0:06:59]: The person continues carefully adding finishing touches to the plate, demonstrating attention to detail. The surrounding area includes various herbs and seasonings on the countertop, ready for use.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What wa being picked up by the person just now?",
        "time_stamp": "0:06:46",
        "answer": "B",
        "options": [
          "A. Red pepper.",
          "B. Green onions.",
          "C. Sliced cucumbers.",
          "D. Lettuce."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "cooking",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_45_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:01]: The video starts with a black screen showing the word \"SO\" in large white letters centered on the screen. [0:00:01 - 0:00:02]: The black screen continues with the text changing to \"so in today's video\" in white letters. [0:00:02 - 0:00:03]: On the black screen, the text changes to a single word, \"a,\" in white letters. [0:00:03 - 0:00:04]: The scene changes to a vibrant outdoor landscape, depicting a lush green area with a variety of vegetation, including some large, overhanging vines and leaves. Water with lilypads and flowers can be seen in the lower part of the frame, with a distant savanna biome landscape in the background. [0:00:04 - 0:00:05]: The scene remains the same, with minor changes in the vegetation due to wind or other minor movements. [0:00:05 - 0:00:06]: The view becomes blurred, making it difficult to distinguish the details, with text appearing at the bottom of the frame: \"and yeah,\". [0:00:06 - 0:00:07]: The blurry view continues, and the text changes to \"a bit of fun as well.\". [0:00:07 - 0:00:08]: The image becomes pixelated, showing a disordered pattern of black, white, and grey squares with some blue and other colors intermixed. The text at the bottom reads, \"So before we do\". [0:00:08 - 0:00:09]: The scene transitions to a clear view of a Minecraft character in purple armor standing on a wooden dock by the water and a small structure. The text at the bottom reads, \"get started with\". [0:00:09 - 0:00:10]: The view remains the same, focusing on the character on the dock. The text changes to \"this build as usual,\". [0:00:10 - 0:00:11]: The scene remains focused on the character, now gesturing with one hand. The text reads, \"we're going\". [0:00:11 - 0:00:12]: The scene remains unchanged, and the text reads, \"a lot of sand,\". [0:00:12 - 0:00:13]: The scene is still focused on the character on the dock with the same text: \"a lot of sand,\". [0:00:13 - 0:00:14]: The scene transitions to a dark and narrow tunnel made of nether bricks, with the player's inventory visible at the bottom of the frame. The tools in the inventory include an axe, a pickaxe, and other items. [0:00:14 - 0:00:15]: The view of the tunnel remains the same, indicating continuity in the player's exploration. [0:00:15 - 0:00:16]: The scene remains focused on the tunnel with no apparent changes in the environment. [0:00:16 - 0:00:17]: The scene remains consistent, with the player continuing through the tunnel. [0:00:17 - 0:00:18]: The scene remains the same with no significant changes. [0:00:18 - 0:00:19]: The tunnel view persists with the same environment and inventory layout. [0:00:19 - 0:00:20]: The player's perspective changes slightly as they select a firework rocket from the inventory, indicating preparation for an action.",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What does the player prepare to do?",
        "time_stamp": "0:00:19",
        "answer": "C",
        "options": [
          "A. Dig for resources.",
          "B. Fight a monster.",
          "C. Use a firework rocket.",
          "D. Build a house."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_196_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "captions": "[0:03:00 - 0:03:20] [0:03:00 - 0:03:03]: Standing on a grassy hillside, I can see a vast landscape. The grassy terrain slopes downwards and meets a body of water. To the right is a cliff face, with patches of grass and small trees scattered around. Above the water hovers a large metallic structure, casting a shadow over the land. In the foreground, part of my right arm holding tools is visible. [0:03:03 - 0:03:05]: Disorienting from the grassy hilltop, I descend down closer to the water's edge. The hills remain on my right, while two birch trees stand directly ahead near the water. My hand holds tools with a purple sheen. [0:03:06]: I plunge into the water. The scenery shifts to an underwater view, revealing a rocky underwater landscape. Sunlight filters through the water, highlighting floating particles and the rocky bed beneath me. [0:03:07]: Submerged underwater now, the visibility is limited. The rocky sides rise steeply from the sandy bottom. Despite the reduced light levels, I can clearly see my descent through the water column. [0:03:08]: Continuing underwater, heading deeper. Large shadowed shapes loom i the distance. The rocky walls encompassing me almost form a tunnel-like structure, guiding my path forward. [0:03:09 - 0:03:10]: As I move deeper into the underwater pathway, the shadowy outlines of the surrounding rock walls get more defined. The dimming light indicates a deepening depth. [0:03:11]: I emerge in an underground cave, transitioning from deep underwater to a waterlogged cavern. The torches illuminate the stalactites hanging from the cavern's ceiling. Small waterfalls trickle down into the pool. [0:03:12]: Just before a large portal inside the cavern. The portal made of dark purple square blocks, crackling with energy, stands imposingly in front. The rocky surroundings glisten with dampness from the water. [0:03:13]: Stepping closer to the portal, rippling arcs of purple energy can be seen. My view is focused entirely on this portal. My left hand and the enchanted tools it holds are visible against the dark surroundings. [0:03:14 - 0:03:15]: The words \"#PrayForRake\" displayed prominently. The background is fuzzy, focusing solely on this message, emphasizing its importance. [0:03:16]: Nearing the portal, the words \"#PrayForRake\" remain dominant on the screen. [0:03:17]: Suddenly, I find myself standing on a wooden dock, facing a large body of water. The immersive landscape stretches out with a small cliff on the left and scattered trees on an island on the right. In my hand, a golden apple shines. Two wooden crates sit nearby. [0:03:18]: Surveying the waterscape, the sun is low on the horizon, casting an orange hue over the scene. The calm water reflects the sky, and some underwater plants are faintly visible. The golden apple remains in my raised hand. [0:03:19]: Shifting perspective slightly to focus on two chests on the dock. The larger chest sits to the right and smaller to the left. My golden apple glints in the sunlight, and verdant hillside forms the backdrop. [0:03:20]: I stride inside a wooden house, leaving behind the scenic dock. Inside, a wooden door, braced with iron hinges, opens inward. Next to the door is a furnace embedded in the wall.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What object is held in the hand while standing on the wooden dock?",
        "time_stamp": "0:03:20",
        "answer": "D",
        "options": [
          "A. An enchanted tool.",
          "B. A wooden crate.",
          "C. A torch.",
          "D. A golden apple."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_196_real.mp4"
  },
  {
    "time": "[0:06:00 - 0:07:00]",
    "captions": "[0:06:00 - 0:06:20] [0:06:01 - 0:06:04]: Two orange frogs are positioned on a patch of grass next to a small water-filled pit. The frogs are oriented towards the water and seem to be interacting with each other. One frog is partially in the water, while the other frog is on the grass, looking into the pit. [0:06:05]: The first-person view now shows a close-up of one of the orange frogs on the grass, with some text displayed at the bottom of the frame. [0:06:06 - 0:06:07]: On a grassy patch next to a pond, two frogs are visible. The larger, orange frog is on the right side of the water. Another, smaller, white frog is situated at the top left of the water's surface, surrounded by clusters of eggs. The water has a deep blue hue with reflections of the sky. [0:06:08]: The perspective shifts to show the scene from above, with the pond in the center surrounded by patches of green grass and brown dirt, framed by trees in the background. [0:06:09]: The view remains centered on the pond from an overhead perspective, with the clusters of frog eggs clearly defined in the water. [0:06:10 - 0:06:11]: The aerial perspective of the pond continues, now showing the surrounding ground becoming gradually darker, indicating the transition to dusk. The clusters of frog eggs are still prominent in the water. [0:06:12]: The pond and the eggs are now shown under noticeably dimmer light, while the surrounding grass and dirt maintain a consistent texture. [0:06:13]: The focus remains on the pond with frog eggs clustered in the water, now under even dimmer lighting conditions. [0:06:14]: An overhead shot shows the pond with six egg clusters clearly visible, the clarity of the water revealing the details beneath the surface. [0:06:15]: The perspective tightens further on the pond, zooming in on the clusters of frog eggs. [0:06:16]: The view further narrows to the center of the pond, highlighting the detail of the water texture and egg clusters. [0:06:17]: One frog is visible in the top-left corner as the focus remains on the pond with several egg clusters floating. [0:06:18 - 0:06:19]: A pair of buckets and a baked potato are now in the first-person view, with a crosshair indicating a targeting action. The terrain appears similar with patches of long grass and a partly filled water pit. [0:06:20]: The perspective shows the player holding a water bucket and standing beside the pond that contains clusters of frog eggs, ready for some interaction.",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many egg clusters are visible in the pond?",
        "time_stamp": "0:06:14",
        "answer": "C",
        "options": [
          "A. Four.",
          "B. Five.",
          "C. Nine.",
          "D. Seven."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_196_real.mp4"
  },
  {
    "time": "[0:09:00 - 0:10:00]",
    "captions": "[0:09:00 - 0:09:20] [0:09:04 - 0:09:05]: The camera view shows an enclosed area with mesh walls, a grassy floor, and a small body of water running through the middle. A rock formation is visible near the back right corner. A hot air balloon is floating in the sky outside the mesh enclosure. [0:09:05 - 0:09:06]: A player character dressed in purple gear appears near the middle of the scene. The water continues to flow along the same path. [0:09:06 - 0:09:07]: The player character moves to the right, toward the water stream, still in the middle of the scene. The rock formation remains in the same position. [0:09:07 - 0:09:08]: The player character reaches a small landmass in the water. The rock formation is now slightly off to the left. [0:09:08 - 0:09:09]: The player character is on the landmass, near what looks like a small mound of dirt. The water continues to flow around both sides of the landmass. [0:09:09 - 0:09:10]: The player character begins digging into the dirt on the landmass; a hole starts to form. The rock formation is visible in the background to the left. [0:09:10 - 0:09:11]: The player character continues digging, and more of the landmass becomes a grassy terrain. The rock formation becomes more distant in the background. [0:09:11 - 0:09:12]: The landmass is now mostly grass-covered with some flowers. The player character stands on the green landmass in the middle. [0:09:12 - 0:09:13]: Trees with vine-like structures emerge on the landmass, adding greenery. The player character is just visible behind one of the trees. [0:09:13 - 0:09:14]: The trees' foliage increases, obscuring the view of some parts of the landmass. The rocks are still on the left side. [0:09:14 - 0:09:15]: The view shifts around the trees, and the rock formation in the background becomes partially hidden by tree foliage.  [0:09:15 - 0:09:16]: A broader view shows the entire enclosed area with the grassy landmass and water flowing around it. The rock formation is further in the distance, and a castle structure becomes visible in the background outside the enclosure. [0:09:16 - 0:09:17]: The player character is now on top of the rock formation on the left. The water continues to run around the greenery. [0:09:17 - 0:09:18]: The player character stands on the rock formation looking around. The water body and the enclosed mesh walls are clearly in view. [0:09:18 - 0:09:19]: The camera angle changes to show a large green tree next to the water stream that runs through the grassy landmass. The viewer gets a wider perspective of the enclosed area. [0:09:19]: The focus shifts out of the enclosed area, showing a large open field with sparse greenery leading towards a distant castle on top of a hill. The sky is clear with some scattered clouds, and the mesh enclosure is partially seen to the left, indicating the end of the recording session.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What object is floating in the sky outside the mesh enclosure?",
        "time_stamp": "0:09:20",
        "answer": "D",
        "options": [
          "A. A drone.",
          "B. A kite.",
          "C. A glider.",
          "D. A hot air balloon."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      },
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the gear worn by the player character?",
        "time_stamp": "0:09:40",
        "answer": "D",
        "options": [
          "A. Red.",
          "B. Blue.",
          "C. Green.",
          "D. Purple."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "minecraft",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_196_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:05]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the pilot's headset right now?",
        "time_stamp": "00:00:03",
        "answer": "A",
        "options": [
          "A. Black.",
          "B. Blue.",
          "C. Red.",
          "D. White."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_434_real.mp4"
  },
  {
    "time": "[0:02:04 - 0:02:09]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What device is currently mounted to the top of the helicopter's control panel?",
        "time_stamp": "00:02:07",
        "answer": "A",
        "options": [
          "A. A camera.",
          "B. A GPS unit.",
          "C. An altimeter.",
          "D. A radar detector."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_434_real.mp4"
  },
  {
    "time": "[0:04:08 - 0:04:13]",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "What is currently seen on the belt of the police officer?",
        "time_stamp": "00:04:08",
        "answer": "C",
        "options": [
          "A. Flashlight.",
          "B. Handcuffs.",
          "C. Taser.",
          "D. Radio."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_434_real.mp4"
  },
  {
    "time": "[0:06:12 - 0:06:17]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the control stick's top?",
        "time_stamp": "00:06:16",
        "answer": "A",
        "options": [
          "A. Yellow.",
          "B. Red.",
          "C. Blue.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_434_real.mp4"
  },
  {
    "time": "[0:08:16 - 0:08:21]",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What color is the streamlining structure of the helicopter that is visible right now?",
        "time_stamp": "00:08:18",
        "answer": "A",
        "options": [
          "A. Black.",
          "B. Red.",
          "C. Blue.",
          "D. Green."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "flying_pov",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_434_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What topic might the teacher explain next?",
        "time_stamp": "00:00:11",
        "answer": "A",
        "options": [
          "A. mixed numbers and improper fractions.",
          "B. Add mixed numbers.",
          "C. Subtract improper fractions.",
          "D. Multiply fractions."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_218_real.mp4"
  },
  {
    "time": "[0:02:38 - 0:03:08]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the teacher explain next?",
        "time_stamp": "00:03:10",
        "answer": "B",
        "options": [
          "A. Compare the fractions using decimal values.",
          "B. Explain how improper fractions is connected to proper fractions and whole fractions.",
          "C. Demonstrate how to multiply two fractions.",
          "D. Discuss the concept of mixed numbers."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_218_real.mp4"
  },
  {
    "time": "[0:05:16 - 0:05:46]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "0:05:31",
        "answer": "A",
        "options": [
          "A. How to convert mixed numbers to improper fractions.",
          "B. The method for adding mixed numbers and improper fractions.",
          "C. Simplifying improper fractions.",
          "D. Dividing fractions by whole numbers."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_218_real.mp4"
  },
  {
    "time": "[0:07:54 - 0:08:24]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What concept will the speaker likely explain next?",
        "time_stamp": "00:08:20",
        "answer": "B",
        "options": [
          "A. Multiplying mixed fractions by a whole number.",
          "B. Converting improper fractions to mixed numbers.",
          "C. Adding improper fractions together.",
          "D. Simplifying mixed fractions."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_218_real.mp4"
  },
  {
    "time": "[0:10:32 - 0:11:02]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:10:46",
        "answer": "D",
        "options": [
          "A. Add the remainder to the quotient.",
          "B. Subtract 6 from 1.",
          "C. Divide 7 by 3.",
          "D. How to use dividing to convert improper fractions to a mixed number form."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_218_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:00:18",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " D"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_80_real.mp4"
  },
  {
    "time": "[0:03:00 - 0:04:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:03:05",
        "answer": "A",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 4.",
          "D. 0."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_80_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:04:02",
        "answer": "A",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 6.",
          "D. 4."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_80_real.mp4"
  },
  {
    "time": "[0:05:00 - 0:06:00]",
    "questions": [
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:01",
        "answer": "B",
        "options": [
          "A. 2.",
          "B. 5.",
          "C. 3.",
          "D. 4."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      },
      {
        "task_type": "Counting",
        "question": "How many times in total have movie clips been inserted during this person's explanation so far?",
        "time_stamp": "00:05:33",
        "answer": "D",
        "options": [
          "A. 2.",
          "B. 3.",
          "C. 5.",
          "D. 6."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "introduction_to_film",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_80_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:00:13",
        "answer": "C",
        "options": [
          "A. How to plot 25/50 on a graph.",
          "B. How to further simplify 1/2.",
          "C. How to simplify fractions.",
          "D. How to multiply fractions."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_222_real.mp4"
  },
  {
    "time": "[0:01:45 - 0:02:15]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:01:24",
        "answer": "C",
        "options": [
          "A. Converting 1/2 to a decimal.",
          "B. Adding fractions with different denominators.",
          "C. Equivalent fractions of 3/6.",
          "D. Subtracting fractions with common denominators."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_222_real.mp4"
  },
  {
    "time": "[0:03:30 - 0:04:00]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:02:33",
        "answer": "C",
        "options": [
          "A. Converting improper fractions to mixed numbers.",
          "B. Adding fractions with different denominators.",
          "C. Simplifying fractions by finding common factors.",
          "D. Multiplying fractions by whole numbers."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_222_real.mp4"
  },
  {
    "time": "[0:05:15 - 0:05:45]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker explain next?",
        "time_stamp": "00:05:45",
        "answer": "C",
        "options": [
          "A. How to find the greatest common divisor.",
          "B. How to convert the fraction to a decimal.",
          "C. How to factorize each number to prime factors.",
          "D. How to add fractions with different denominators."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_222_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:07:30]",
    "questions": [
      {
        "task_type": "Prospective Reasoning",
        "question": "What might the speaker discuss next?",
        "time_stamp": "00:07:17",
        "answer": "C",
        "options": [
          "A. How to find common denominators.",
          "B. The process of cross-multiplication.",
          "C. Simplifying fractions using greatest common factor.",
          "D. The difference between factors and multiples."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "math tutorials",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_222_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:30]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the older woman appear annoyed or angry?",
        "time_stamp": "00:00:26",
        "answer": "B",
        "options": [
          "A. Because the character shouted loudly.",
          "B. Because the character disturbed her.",
          "C. Because the character knocked on the door.",
          "D. Because the character broke something."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_250_real.mp4"
  },
  {
    "time": "[0:02:09 - 0:02:39]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why is the teddy bear covered in flour?",
        "time_stamp": "00:02:22",
        "answer": "B",
        "options": [
          "A. Because the teddy bear fell into a bag of flour.",
          "B. Because Mr. Bean just blew on the flour.",
          "C. Because Mr. Bean was baking and accidentally spilled flour on it.",
          "D. Because the teddy bear was used to dust off a surface covered in flour."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_250_real.mp4"
  },
  {
    "time": "[0:04:18 - 0:04:48]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why does the character have a complete list of ingredients for the grocery list?",
        "time_stamp": "00:04:42",
        "answer": "C",
        "options": [
          "A. Because the character finds a recipe book.",
          "B. Because the character asks someone else for the list.",
          "C. Because the character is talking on the phone and writing down ingredients for a grocery list.",
          "D. Because the character visits each grocery store to check items."
        ],
        "required_ability": "episodic memory",
        "rekv": " C"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_250_real.mp4"
  },
  {
    "time": "[0:06:27 - 0:06:57]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "How did Mr. Bean earn all the money on his desk?",
        "time_stamp": "0:07:23",
        "answer": "A",
        "options": [
          "A. Earnings from selling pizza.",
          "B. Earnings from doing odd jobs around the neighborhood.",
          "C. Earnings from a garage sale he held over the weekend.",
          "D. Earnings from winning a small bet with a friend."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_250_real.mp4"
  },
  {
    "time": "[0:08:36 - 0:09:06]",
    "questions": [
      {
        "task_type": "Causal Reasoning",
        "question": "Why do the two guards fight each other?",
        "time_stamp": "00:09:05",
        "answer": "A",
        "options": [
          "A. Because Mr. Bean dodged their attack downward, it caused them to attack each other.",
          "B. Because they misunderstood each other's orders.",
          "C. Because they were competing to capture Mr. Bean first.",
          "D. Because they were tricked by Mr. Bean into thinking the other was an enemy."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "kids cartoon",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_250_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:00:20]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:19]: The video starts with a black screen at the timestamp 0:00:00. At 0:00:01, the scene transitions to feature a person seated behind a white table in a studio setup. The background wall is blue, and the person is wearing glasses and a black shirt. They are facing the camera and gesturing with their hands. On the table in front of them are two large computer graphics cards standing upright side by side. Both graphics cards have multiple cooling fans visible. The person seems to be explaining or discussing the graphics cards, occasionally using hand movements to emphasize points. At 0:00:08, the camera zooms in to show a closer view of one of the graphics cards, specifically focusing on the text \"RTX 4080 SUPER\" etched on it. The view remains steady to give a clear look at the branding on the graphics card. From 0:00:10 onwards, the scene changes to a black background with information graphics overlayed on the left and a green-lit computer setup on the right. The text lists different models of RTX graphics cards along with their specifications and prices. The RTX 4080 SUPER is highlighted in the list, showing its CUDA cores, memory size, and price in the local currency. The green lighting from the computer setup creates a striking contrast with the black background, highlighting the technology displayed. The view stays consistent, focusing on the specifications displayed on the screen.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What specific text is etched on the graphics card shown right now?",
        "time_stamp": "00:00:07",
        "answer": "C",
        "options": [
          "A. RTX 3060 Ti.",
          "B. RTX 4070 Ti.",
          "C. RTX 4080 SUPER.",
          "D. RTX 5000 ULTRA."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_117_real.mp4"
  },
  {
    "time": "[0:03:20 - 0:03:40]",
    "captions": "[0:03:20 - 0:03:40] [0:03:20 - 0:03:24]: Two graphics cards are visible on a white background. They are oriented diagonally, with their fan sides facing up. One card is labeled \"RTX 4080 SUPER\" and the other \"RTX 3080 TI.\" The cards have a black and metallic design, with visible components such as fans, metal edges, and PCIe connectors. [0:03:25 - 0:03:28]: The graphics cards remain in a similar orientation as the previous frames. The card on the right labeled \"RTX 4080 SUPER\" clearly displays two large fans, while the card on the left labeled \"RTX 3080 TI\" shows a single fan. Both cards have complex, sleek designs, with alternating black and metallic segments. [0:03:29]: The frame shows the internal designs of two different graphics cards. The one on the left is labeled \"Founders Edition RTX 3080 Ti,\" showing a detailed view of its circuits and components. The card on the right is labeled \"Founders Edition RTX 4080 SUPER,\" showcasing a different internal layout compared to the 3080 Ti, with a clear label for easier identification. [0:03:30 - 0:03:40]: A close-up of a graphics chip is displayed on a gray, semi-transparent circuit board background. The chip is labeled \"NVIDIA\" at the top, with the code \"TS MC224A1\" in the center and \"AD103-400A1\" highlighted in green at the bottom, indicating it as a \"PRESS SAMPLE.\" The image stays focused on the chip with slight changes in the clarity and sharpness of the text, emphasizing the chip's details.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the highlighted label on the graphics chip right now?",
        "time_stamp": "00:03:32",
        "answer": "A",
        "options": [
          "A. AD103-400A1.",
          "B. AD104-300B1.",
          "C. TS MC223A2.",
          "D. AD102-500A3."
        ],
        "required_ability": "working memory",
        "rekv": " D"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_117_real.mp4"
  },
  {
    "time": "[0:06:40 - 0:07:00]",
    "captions": "[0:06:40 - 0:07:00] [0:06:40 - 0:06:49]: The video displays performance benchmarks for various NVIDIA graphics cards, focusing on \"CS2\" gameplay. The background consists of a blurred scene, likely in-game footage, with the benchmark details overlaid. Graphs and metrics appear in white, with standout figures highlighted in red or green. In the first half, four boxes on the right showcase real-time performance data for the RTX 4080 SUPER, RTX 4090, and RTX 4080 Ti SUPER in \"2560x1440\" resolution. Values for GPU clock, usage, memory, power, and temperature are indicated for each card. The FPS for the RTX 4090 is 356, and for the RTX 4080 SUPER, it is 291.  [0:06:50]: The screen dims briefly before transitioning to a similar benchmarking scene but showcasing the same graphics cards' performance in a different resolution setting. The resolution \"3840x2160\" is indicated, and the FPS values are adjusted accordingly. This segment emphasizes the RTX 4080 SUPER performance with a different set of real-time data metrics in another set of four boxes on the right. The FPS changes are also highlighted, whether increases or decreases, with the RTX 4090 maintaining the highest position and RTX 4080 SUPER showing significant variation. Metrics continue displaying real-time GPU clock, usage, memory, power, and temperature values.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What resolution setting is highlighted right now?",
        "time_stamp": "00:06:52",
        "answer": "C",
        "options": [
          "A. 2560x1440.",
          "B. 1920x1080.",
          "C. 3840x2160.",
          "D. 3200x1800."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_117_real.mp4"
  },
  {
    "time": "[0:10:00 - 0:10:20]",
    "captions": "[0:10:00 - 0:10:20] [0:10:00 - 0:10:10]: The video shows a comparison chart of NVIDIA RTX 4080 SUPER and RTX 4090 SUPER graphics cards, comparing their performance at 1440P and 2160P resolutions. The upper half of the screen shows the performance metrics for the RTX 4080 SUPER. It includes bars representing performance gains in traditional rasterized games and ray-traced games. The 4080 SUPER has a performance gain of +2.93% in 1440P and +3.21% in 2160P for traditional rasterized games, and a gain of +2.05% in 1440P and +3.11% in 2160P for ray-traced games. The lower half displays the RTX 4090 SUPER with a red text indicating -3% for 4080 and performance losses of -19.46% in 1440P and -28.83% in 2160P for traditional rasterized games, and larger losses of -27.88% in 1440P and -39.34% in 2160P for ray-traced games. The chart is presented on a black background with green and white text. [0:10:10]: The video transitions to a person sitting at a white table in a well-lit room with a blue background. They're wearing glasses and a black shirt with a white design. On the left side of the table, there are two large graphics cards displayed vertically. There are shelves in the background with various objects, including a smaller camera and decor items. The person appears to be in the middle of a discussion, holding a small figurine while resting their other arm on the table. [0:10:11 - 0:10:12]: The person continues speaking and makes a gesture with their right hand, as if emphasizing a point. [0:10:12 - 0:10:13]: The person extends their right hand further out, appearing to elaborate on their explanation. [0:10:13 - 0:10:14]: The person crosses their arms on the table and looks forward. [0:10:14 - 0:10:15]: The person leans forward and touches their face with their right hand, maintaining eye contact with the camera. [0:10:15 - 0:10:16]: The person gestures with their left hand across their body, pointing towards the graphics cards on their left. [0:10:16 - 0:10:17]: The person places both hands on the table while continuing to speak. [0:10:17 - 0:10:18]: The person moves their hands slightly and maintains eye contact with the camera, emphasizing their speech. [0:10:19]: The video cuts back to the same performance comparison chart seen at the beginning of the segment, summarizing the graphics card performance data.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What performance gain does the NVIDIA RTX 4080 SUPER achieve in ray-traced games at 2160P resolution right now?",
        "time_stamp": "0:10:20",
        "answer": "C",
        "options": [
          "A. +2.93%.",
          "B. +3.21%.",
          "C. +3.11%.",
          "D. -3%."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_117_real.mp4"
  },
  {
    "time": "[0:12:40 - 0:12:59]",
    "captions": "[0:12:40 - 0:12:59] [0:12:40 - 0:12:44]: Three graphics cards are positioned horizontally on a white surface. The leftmost card has a black and silver design, the middle card is labeled \"GeForce RTX 4090 SUPER\" and has a black finish with a slight reflection, and the rightmost card, labeled \"RTX 4090,\" is similar but with a different arrangement of design lines and also featuring a black and silver finish. All cards have a cooling fan visible and a visible metallic part at the bottom. [0:12:45 - 0:12:49]: The same three graphics cards continue to be shown on the white surface. Their orientations and relative positions remain unchanged. The lighting and reflections on their surfaces slightly change due to the ongoing video. [0:12:50 - 0:12:54]: The scene transitions to a man seated behind a white desk. He is wearing glasses and a black shirt with a graphic on it. Two graphics cards are placed to his right, positioned upright, revealing their cooling fans. The background is blue, and there are shelves with various items on the left and right sides. Overlay icons such as a “like” symbol appear in front of the graphics cards. [0:12:55 - 0:12:58]: The man continues to sit behind the desk, sometimes gesturing with his hands. Overlay icons change positions around the graphics cards and the man. The background remains consistent, with the blue hue and shelving units in place. At one point, an animated overlay shows a smartphone screen nearing the left edge of the frame, displaying an online store interface. [0:12:59]: The scene shifts to an animated logo with a mechanical gear and stylized text next to it. The background is black with small star-like points of light sparkling. The text and gear emit a faint blue glow with light streaks descending from the top.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What is the label on the rightmost graphics card right now?",
        "time_stamp": "0:12:43",
        "answer": "C",
        "options": [
          "A. GeForce RTX 4090.",
          "B. GeForce GTX 4090 SUPER.",
          "C. GeForce RTX 4080.",
          "D. GTX 4090."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "evaluation_of_electronic_products",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_117_real.mp4"
  },
  {
    "time": "[0:00:00 - 0:01:00]",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:03]: The video starts with a first-person perspective along a busy city street at dusk. The sky displays a gradient of colors transitioning from pale yellow to deep red and purple. Tall buildings on either side line the street, which is bustling with activity. Vehicles are present, including a cyclist in the foreground and pedestrians crossing the street. [0:00:04 - 0:00:10]: As the video progresses, the viewer moves forward through the intersection. People continue to cross the road from different directions, and cars are visible, both stationary and moving. The cityscape remains consistent with tall buildings, streetlights, and storefronts illuminated. [0:00:11 - 0:00:13]: The perspective shifts to the right, capturing a broader view of the people and surroundings. A dog on a leash is visible, and several pedestrians are walking along the sidewalk. The buildings remain lit with overhead lights glowing in the evening dusk. [0:00:14 - 0:00:16]: Continuing the walk, the camera moves past a green space with planters and benches while capturing more pedestrians and the lively city atmosphere. The sky's color deepens, adding to the urban evening vibe. [0:00:17 - 0:00:19]: The viewer approaches an area where pedestrians are gathered near street signs and entranceways. Several people are visible interacting with their surroundings, with city lights adding to the scene's vibrancy. The buildings on either side continue to form a bustling urban environment.",
    "questions": [
      {
        "task_type": "Attribute Recognition",
        "question": "What is the color of the sky now?",
        "time_stamp": "0:00:03",
        "answer": "B",
        "options": [
          "A. Blue and white.",
          "B. Pale yellow to deep red and purple.",
          "C. Grey and yellow.",
          "D. Orange and blue."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      },
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the dog seen when the perspective shifts to the right?",
        "time_stamp": "0:00:14",
        "answer": "B",
        "options": [
          "A. In a park.",
          "B. On a leash.",
          "C. In a car.",
          "D. At a storefront."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_305_real.mp4"
  },
  {
    "time": "[0:02:00 - 0:03:00]",
    "captions": "[0:02:20 - 0:02:40] [0:02:20 - 0:02:24]: The video shows a first-person perspective of a busy urban street in the evening. The camera is moving forward, capturing several people walking on the sidewalk. The left side features a building with large glass windows and an overhang with lights, under which several pedestrians are walking. The building has the number \"10\" displayed prominently. The right side shows a street with parked bicycles and another building under construction or renovation. Multiple pedestrians are seen walking in different directions. [0:02:24 - 0:02:27]: As the camera continues to move forward, more people become visible. A man in a green shirt is walking ahead of the camera on the right side of the sidewalk, while a child holding the hand of an adult is moving just slightly ahead. The street remains busy with a few more pedestrians walking both in the same direction as the camera and in the opposite direction. [0:02:28 - 0:02:30]: The video further reveals more of the same scenery. Pedestrians continue to walk on the sidewalk, with the building on the left side still prominently featuring its glass windows and hanging lights. A decorative hanging basket with flowers hangs from the overhang. Evening lights are visible on the buildings and streets. [0:02:31 - 0:02:35]: As the video progresses, the scene becomes slightly more dynamic, with a few more people entering the frame from the opposite direction. The child previously seen is now raising their arms toward some overhead element. The urban setting is consistent, with tall buildings, street lights, and busy pedestrian traffic adding to the vibrant city atmosphere. [0:02:36 - 0:02:39]: In the final seconds, more people approach from the opposite direction, adding to the sense of a bustling city street. The camera continues moving forward, maintaining the view of the buildings on the left and the street with parked bicycles on the right. The scene is lively, with numerous people engaging in their routines as they walk on the sidewalk.",
    "questions": [
      {
        "task_type": "Text-Rich Understanding",
        "question": "What number is displayed prominently on the building with large glass windows just now",
        "time_stamp": "0:02:24",
        "answer": "C",
        "options": [
          "A. 5.",
          "B. 7.",
          "C. 10.",
          "D. 12."
        ],
        "required_ability": "episodic memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_305_real.mp4"
  },
  {
    "time": "[0:04:00 - 0:05:00]",
    "captions": "[0:04:40 - 0:05:00] [0:04:40 - 0:04:47]: The scene shows a busy city street in the early evening or dusk, with shops, restaurants, and tall buildings lining both sides. The street is filled with people walking in various directions. Most of the buildings have illuminated signs, some reading \"SPA\", \"NOW OPEN,\" and \"FELLOW BARBER.\" The streetscape includes a prominent sign for the \"BROADWAY PLAZA HOTEL\" on the left side of the frame. There are also cars and bicycles visible on the street. [0:04:47 - 0:04:50]: People continue to walk along the sidewalk, appearing to be engrossed in their own activities, such as using their phones or chatting with companions. The sidewalk has a garbage bag lying openly on the ground. The lighting is a mix of streetlights and lit-up shop windows, giving the street a vibrant and lively ambience. The skyline features more modern buildings, contributing to the bustling urban environment. [0:04:50 - 0:04:55]: The scene becomes busier with people of diverse appearances and outfits. All shops and cafes are brightly lit, creating an inviting atmosphere. Vehicles drive by, and some pedestrians are seen window shopping or heading towards various destinations. The characters in the scene vary their actions: one man appears to fix his hair, while another group of women walk side by side, chatting. [0:04:55 - 0:04:59]: Two women in stylish black dresses walk confidently on the left side of the sidewalk. More pedestrians roam about, some in casual clothing, displaying the diverse fashion of the city dwellers. The frontage of the buildings alternates between glass windows showing interiors of shops or eateries and solid walls with prominent signs. The sky above shows a gradient from darkening blue to twilight. [0:04:59 - 0:05:00]: The activity on the street maintains its lively pace, with pedestrians, illuminated signs, and vehicles contributing to the quintessentially urban scene. The backdrop continues to feature a mix of commercial establishments and hospitality venues, while the sidewalk remains animated with foot traffic, capturing the essence of city life.",
    "questions": [
      {
        "task_type": "Object Recognition",
        "question": "Which sign is prominently displayed on the left side of the street now?",
        "time_stamp": "00:04:56",
        "answer": "A",
        "options": [
          "A. BROADWAY PLAZA HOTEL.",
          "B. CITY LIBRARY.",
          "C. CENTRAL PARK CAFE.",
          "D. GRAND THEATER."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_305_real.mp4"
  },
  {
    "time": "[0:07:00 - 0:08:00]",
    "captions": "[0:07:00 - 0:07:20] [0:07:00 - 0:07:11]: At the beginning of the video, the first-person perspective shows a lively street scene with various pedestrians. A man in a green shirt and tan pants walks alongside two women, one in all black and the other in light-colored clothing. The group is walking toward the sidewalk from the center of an intersection. Behind them, more pedestrians are visible, some seated at a few small tables around the pedestrian area, and others walking near storefronts with bright, inviting lights. The street is lined with buildings, the tallest of which towers in the background, adorned with modern architecture and bright lights. Some storefronts have signs, such as \"ONE WAY\" and \"Tin Pan Alley.\" [0:07:12 - 0:07:17]: Moving further down the street, the camera perspective shifts slightly, focusing more toward the pedestrian sidewalk. Two individuals, both dressed in white, walk closely together, while a man in a black shirt walks ahead on their left side. Bright lights continue to illuminate the storefronts and street, highlighting parked cars, more pedestrians, and several yellow umbrellas covering seating areas. The evening blue sky is partially visible, contrasting with the buildings' artificial lights. [0:07:18 - 0:07:20]: The video captures additional lively street activities. One person appears to be engaged in taking a photo or video with their phone, while another casually walks in a white shirt near the sidewalk. A woman in light-colored clothing can be seen next to a storefront, adding to the bustling, vibrant atmosphere of the city street. The line of sight emphasizes the street's depth, stretching towards the brightly lit buildings in the distance and the pedestrians going about their activities.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What action is the person near the sidewalk doing now?",
        "time_stamp": "00:07:20",
        "answer": "C",
        "options": [
          "A. Walking a dog.",
          "B. Talking on the phone.",
          "C. Taking a photo or video.",
          "D. Eating at a table."
        ],
        "required_ability": "working memory",
        "rekv": " C"
      }
    ],
    "video_categories": "city_walk",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_305_real.mp4"
  },
  {
    "time": "0:00:00 - 0:00:20",
    "captions": "[0:00:00 - 0:00:20] [0:00:00 - 0:00:06]: In a first-person perspective, a person wearing a striped shirt is standing in a well-lit indoor area. They face another individual in uniform, holding a phone while interacting. A red fire hose box is mounted on the wall beside them. On the right side of the screen, a job-searching interface shows various chat messages. [0:00:07 - 0:00:11]: The person wearing the striped shirt continues to interact with the individual in uniform while observing their phone. No significant movement occurs, but the job-searching interface is still visible with chat messages updating continuously. [0:00:12 - 0:00:18]: The individual in the striped shirt begins to turn away from the person in uniform and starts walking down a hallway lined with industrial equipment. The environment remains consistent, with concrete walls, floors, and overhead lighting, creating a utilitarian atmosphere. [0:00:19 - 0:00:20]: The individual moves slightly further down the hallway and then pauses. They look toward the end of the corridor, where more industrial equipment and possible workstations are visible. The job-searching interface and chat messages remain active on the right side of the screen.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the red fire hose box located right now?",
        "time_stamp": "00:00:06",
        "answer": "A",
        "options": [
          "A. Hanging on the wall.",
          "B. Set on the table.",
          "C. Placed on the shelf.",
          "D. Attached to the ceiling."
        ],
        "required_ability": "working memory",
        "rekv": " A"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_274_real.mp4"
  },
  {
    "time": "0:02:40 - 0:03:00",
    "captions": "[0:02:40 - 0:03:00] [0:02:40 - 0:02:41]: The video begins with a first-person view through tall grass in a field. Farther in the background, trees are visible, and power lines run horizontally across the scene. The field is densely covered with tall green grass and some flowers. [0:02:42 - 0:02:45]: The camera moves slightly, and more details of the grass and flowers become visible. It appears to be daylight, though the scene has a greenish tint. The power lines and trees remain visible in the background. [0:02:46]: A black and white cow appears in the field of view. The cow is initially lying down, partially obscured by the tall grass. [0:02:47 - 0:02:50]: The cow stands up fully. It is a typical Holstein cow with black and white patches.  [0:02:51 - 0:02:55]: The cow begins to turn its head to the left, showing more of its profile. The surrounding grass and flowers sway slightly, likely due to a breeze. [0:02:56 - 0:02:58]: The cow starts to lower its head towards the grass, perhaps to graze, while the view remains steady. [0:02:59 - 0:03:02]: The cow eats some grass, and then the camera begins to pan left, leaving the cow partially out of view until it disappears completely from the frame. [0:03:03 - 0:03:05]: The scene transitions to a new location indoors, resembling a museum or gallery. The camera is positioned low, almost at floor level, and points toward a variety of statues in the background. [0:03:06 - 0:03:09]: The room is dimly lit with a greenish ambient light. Several large statues of human figures stand on plinths, with some artworks mounted on the walls. [0:03:10 - 0:03:15]: The camera slowly moves forward, getting closer to a statue that depicts a sitting figure. Other statues, including some standing ones, are seen in the periphery. The room has a quiet and still atmosphere. [0:03:16 - 0:03:20]: The camera slightly pans to the right, giving a broader view of the other statues and the room itself. The floor appears to be wooden, and there's a soft, ambient light that maintains the dim environment.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "What is the relationship between the power lines and the trees right now?",
        "time_stamp": "0:02:41",
        "answer": "B",
        "options": [
          "A. The power lines run vertically next to the trees.",
          "B. The power lines run horizontally across the scene with trees in the background.",
          "C. The power lines are not visible.",
          "D. The power lines are beneath the trees."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_274_real.mp4"
  },
  {
    "time": "0:05:20 - 0:05:40",
    "captions": "[0:05:20 - 0:05:40] [0:05:20 - 0:05:24]: The video starts in a large, covered parking garage. A blonde man in a striped shirt stands next to the camera. Another man wearing a black jacket moves towards a van;  [0:05:25 - 0:05:27]: The camera pans slightly left, showing the striped shirt man and a third individual in a pink shirt. The man in the black jacket crouches near the van; [0:05:28 - 0:05:30]: As the camera remains focused, the man in the striped shirt starts using a smartphone. The man in the black jacket has stood up; [0:05:31 - 0:05:37]: The camera captures the smartphone screen showing an interface filled with various icons. The man in the black jacket begins walking away; [0:05:38 - 0:05:39]: The phone screen shows a \"Job Searching\" interface. The man in the black jacket talks to a fourth man in a white shirt; [0:05:40 - 0:05:43]: This man in the black jacket continues interacting with others. A person in red shoes and shorts appears further from the camera, walking away; [0:05:44 - 0:05:45]: The camera angles to the left, showing more people gathering, with the man in the black jacket interacting with a gentleman in a black leather jacket; [0:05:46 - 0:05:47]: The man in the leather jacket engages in conversation while the striped-shirt man faces forward, and the person in shorts walks away;  [0:05:48 - 0:05:55]: The camera continues panning left to reveal more individuals, including a man with a white beard. The scene is set in the enclosed parking facility with dim lighting; [0:05:56 - 0:05:58]: The camera focuses on the small group of people, capturing their interactions. The smartphone screen still shows the \"Job Searching\" interface; [0:05:59 - 0:06:00]: The video zooms slightly back to show the collective group standing near the vans and conversing in the garage.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What was the man in the black jacket and white hoodie doing just now?",
        "time_stamp": "0:05:30",
        "answer": "A",
        "options": [
          "A. Standing up near the man wearing a striped shirt.",
          "B. Using a smartphone.",
          "C. Walking towards the camera.",
          "D. Talking to a man in a white shirt."
        ],
        "required_ability": "episodic memory",
        "rekv": " B"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_274_real.mp4"
  },
  {
    "time": "0:08:00 - 0:08:20",
    "captions": "[0:08:00 - 0:08:20] [0:08:00 - 0:08:01]: The video begins showing a first-person perspective in a virtual environment. The character is standing near a parked car on the street, facing a sidewalk with trees and buildings in the background. The lighting and coloration suggest it might be nighttime or have a night-vision filter. [0:08:02 - 0:08:03]: The perspective shifts to display a map of a city. Streets, parks, and various landmarks can be seen in detail. The map is colorful with various icons and points of interest marked. [0:08:04 - 0:08:20]: The display continues to show the detailed city map. The perspective does not change. On the left side of the screen, there's a list of objectives and tasks, such as \"Gruppe6 Contractor\" and \"Gruppe6 gold delivery vehicle.\" The character’s miniature avatar appears on the map, positioned towards the left, near the coast. The background shows a variety of urban and natural features, indicating a complex city layout. Alongside the map, there are side streams of what might be chat comments and notifications continuously appearing on the screen.",
    "questions": [
      {
        "task_type": "Spatial Understanding",
        "question": "Where is the character's camera footage positioned on the city map right now?",
        "time_stamp": "00:08:20",
        "answer": "B",
        "options": [
          "A. In the center of the map.",
          "B. Near the coast towards the left.",
          "C. At the top of the map.",
          "D. In a park towards the right."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_274_real.mp4"
  },
  {
    "time": "0:10:20 - 0:10:28",
    "captions": "[0:10:20 - 0:10:28] [0:10:20 - 0:10:22]: The video starts with a character in third-person view running towards a closed garage door. The character has blonde hair and is wearing a white, blue, and green striped shirt with white shorts and white shoes. On the left, there is a small window inset showing a person in a blue shirt playing a video game. This character runs on a concrete sidewalk next to a building with a \"GREEN\" sign. [0:10:22 - 0:10:24]: The character reaches the garage door and stops in front of it. The door is metallic with horizontal slats. The left side of the screen continues to show the person's face as they focus on the game. [0:10:24]: The character turns to face the camera after stopping in front of the closed garage door. [0:10:25]: The garage door starts to open, and the character remains stationary, looking inside. [0:10:26]: As the door continues to lift, more of the building's interior becomes visible. The character begins to move forward slightly. [0:10:27]: The character stands in front of a partially open garage door, revealing the entryway of what seems to be an office or workshop with checker-pattern flooring and glass windows. [0:10:28]: The character walks into the building. The interior is a typical office space with framed pictures on the walls, plants, and waiting chairs. There are large glass windows to the left, and various objects are scattered around inside.",
    "questions": [
      {
        "task_type": "Action Recognition",
        "question": "What is the character doing right now?",
        "time_stamp": "0:10:28",
        "answer": "C",
        "options": [
          "A. Running towards a closed garage door.",
          "B. Stopping in front of a closed garage door.",
          "C. Walking into the building.",
          "D. Turning to face the camera."
        ],
        "required_ability": "working memory",
        "rekv": " B"
      }
    ],
    "video_categories": "live_streams",
    "video_path": "/mnt/data0/public/back/huggingface/hub/StreamingBench/videos/sample_274_real.mp4"
  }
]